Commit Message
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/main/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
TODO: enable type aliases
napoleon_preprocess_types = True  # needed for type aliases to work
napoleon_type_aliases = {
"""array_like"": "":term:`array_like`"","
"""ndarray"": ""~numpy.ndarray"","
"""RandomState"": "":class:`~numpy.random.RandomState`"","
"""DataFrame"": "":class:`~pandas.DataFrame`"","
"""Series"": "":class:`~pandas.Series`"","
}
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of strings:
""
"source_suffix = ['.rst', '.md']"
The root toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
def mlasso_model(): return MultiTaskLassoCV(
"cv=3, alphas=alpha_regs, max_iter=200)"
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
"alpha_regs = [5e-3, 1e-2, 5e-2]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
subset of features that are exogenous and create heterogeneity
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
subset of features wrt we estimate heterogeneity
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
introspect the constructor arguments to find the model parameters
to represent
"if the argument is deprecated, ignore it"
Extract and sort argument names excluding 'self'
column names
transfer input to numpy arrays
transfer input to 2d arrays
create dataframe
currently dowhy only support single outcome and single treatment
call dowhy
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
cate estimator but not the effect.
don't proxy special methods
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
For when checking input values is disabled
Type to column extraction function
if not all column names are strings
coerce feature names to be strings
Prefer sklearn 1.0's get_feature_names_out method to deprecated get_feature_names method
"Some featurizers will throw, such as a pipeline with a transformer that doesn't itself support names"
"Get number of arguments, some sklearn featurizer don't accept feature_names"
Handles cases where the passed feature names create issues
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
Get feature names using featurizer
All attempts at retrieving transformed feature names have failed
Delegate handling to downstream logic
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
Normalize weights
This class is mainly derived from statsmodels.iolib.summary.Summary
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
"want to enforce that each bad_arg was either in kwargs,"
or else it was in neither and is just taking its default value
Any access should throw
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
return plain dictionary so that erroneous accesses don't half work (see e.g. #708)
for every dimension of the treatment add some epsilon and observe change in featurized treatment
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
return plain dictionary so that erroneous accesses don't half work (see #708)
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
return plain dictionary so that erroneous accesses don't half work (see #708)
input feature name is already updated by cate_feature_names.
define the index of d_x to filter for each given T
filter X after broadcast with T for each given T
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
return plain dictionary so that erroneous accesses don't half work (see #708)
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
return plain dictionary so that erroneous accesses don't half work (see #708)
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
""
This code contains some snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_export.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
make any access to matplotlib or plt throw an exception
make any access to graphviz or plt throw an exception
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
Initialize saturation & value; calculate chroma & value shift
Calculate some intermediate values
Initialize RGB with same hue & chroma as our color
Shift the initial RGB values to match value and store
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use mean of targets
Write node mean CATE
Write node std of CATE
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
Fetch appropriate color for node
Write node mean CATE
Write node mean CATE
Write recommended treatment and value - cost
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
This flag is true when names are set in a child class instead
"If names are set in a child class, add an attribute reflecting that"
This works only if X is passed as a kwarg
We plan to enforce X as kwarg only in future releases
This checks if names have been set in a child class
"If names were set in a child class, don't do it again"
"Wraps-up fit by setting attributes, cleaning up, etc."
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
apply defaults before calling inference method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"Treatment names is None, default to BaseCateEstimator"
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
Get input names
Summary
add statsmodels to parent's options
add debiasedlasso to parent's options
add blb to parent's options
TODO Share some logic with non-discrete version
Get input names
Note: we do not transform feature names since that is done within summary_frame
Summary
add statsmodels to parent's options
add statsmodels to parent's options
add blb to parent's options
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
fully materialize folds so that they can be reused across models
and precompute fitted indices so that we fail fast if there's an issue with them
NOTE: if any model is missing scores we will just return None even if another model
has scores. this is because we don't know how many scores are missing
"for the models that are missing them, so we don't know how to pad the array"
for convenience we allos a single model to be passed in lieu of a singleton list
"in that case, we will also unwrap the model output"
"when there is more than one model, nuisances from previous models"
come first as positional arguments
"scores entries should be lists of scores, so make each entry a singleton list"
Adding the kwargs to ray object store to be used by remote functions
for each fold to avoid IO overhead
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
generate an instance of the final model
generate an instance of the nuisance model
Define Ray remote function (Ray remote wrapper of the _fit_nuisances function)
Create Ray remote jobs for parallel processing
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
alteration even when we only want to fit_final.
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
upgrade to a GroupKFold or StratiGroupKFold if groups is not None
"we won't have generated a KFold or StratifiedKFold ourselves when groups are passed,"
"but the user might have supplied one, which won't work"
self._models_nuisance will be a list of lists or a list of list of lists
so we use self._ortho_learner_model_nuisance to determine the nesting level
for each mc iteration
for each model under cross fit setting
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"TODO: This could be extended to also work with our sparse and 2SLS estimators,"
if we add an aggregate method to them
Remember to update the docs if this changes
mix in the appropriate inference class
assign all of the attributes from the dummy estimator that would normally be assigned during fitting
TODO: This seems hacky; is there a better abstraction to maintain these?
"This should also include bias_part_of_coef, model_final_, and fitted_models_final above"
Assign treatment expansion attributes
Methods needed to implement the LinearCateEstimator interface
Methods needed to implement the LinearFinalModelCateEstimatorMixin
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Policy Forest
=============================================================================
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base Policy tree
=============================================================================
The values below are required and utilitized by methods in the _SingleTreeExporterMixin
HACK: sklearn 1.3 enforces that the input to plot_tree is a DecisionTreeClassifier or DecisionTreeRegressor
This is a hack to get around that restriction by declaring that PolicyTree inherits from DecisionTreeClassifier
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"Unique treatments (ordered, includes control)"
Number of treatments (excluding control)
Indicator for whether
Get DR outcomes in training sample
Get DR outcomes in validation sample
Get DR outcomes in validation sample
Calculate ATE in the validation sample
Fit propensity in treatment
Predict propensity scores
Possible treatments (need to allow more than 2)
Predict outcomes
T-learner logic
"if CATE is given explicitly or has not been fitted at all previously, fit it now"
Assign units in validation set to groups
Proportion of validations set in group
Group average treatment effect (GATE) -- average of DR outcomes in group
Average of CATE predictions in group
Calculate group calibration score
Calculate overall calibration score
Calculate R-square calibration score
"if CATE is given explicitly or has not been fitted at all previously, fit it now"
treat each treatment as a separate regression
"here, prop_preds should be a matrix"
with rows corresponding to units and columns corresponding to treatment statuses
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"since we only allow single dimensional y, we could flatten the prediction"
override only so that we can exclude treatment featurization verbiage in docstring
override only so that we can exclude treatment featurization verbiage in docstring
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Handles the corner case when X=None but featurizer might be not None
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
TODO: support freq_weight and sample_var in debiased lasso
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
Replacing to remove docstring
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
data is already validated at initial fit time
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE: important to use the rlearner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the rlearner's copy to retain the information from fitting
Handles the corner case when X=None but featurizer might be not None
override only so that we can update the docstring to indicate support for `LinearModelFinalInference`
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support freq_weight and sample_var in debiased lasso
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add blb to parent's options
override only so that we can update the docstring to indicate
support for `GenericSingleTreatmentModelFinalInference`
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
note that groups are not passed to score because they are only used for fitting
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
NOTE: important to get parent's wrapped copy so that
"after training wrapped featurizer is also trained, etc."
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
Fit a doubly robust average effect
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
"If custom param grid, check that only estimator parameters are being altered"
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
override only so that we can update the docstring to indicate support for `blb`
Get input names
Summary
Determine output settings
"Important: This must be the first invocation of the random state at fit time, so that"
train/test splits are re-generatable from an external object simply by knowing the
random_state parameter of the tree. Can be useful in the future if one wants to create local
linear predictions. Currently is also useful for testing.
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
Set min_weight_leaf from min_weight_fraction_leaf
Build tree
We calculate the maximum number of samples from each half-split that any node in the tree can
hold. Used by criterion for memory space savings.
Initialize the criterion object and the criterion_val object if honest.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
""
This code is a fork from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_base.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
Set parameters
Don't instantiate estimators now! Parameters of base_estimator might
"still change. Eg., when grid-searching with the nested object syntax."
self.estimators_ needs to be filled by the derived classes in fit.
Compute the number of jobs
Partition estimators between jobs
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
covariance matrix
get eigen value and eigen vectors
simulate eigen vectors
keep the top 4 eigen value and corresponding eigen vector
replace the negative eigen values
generate a new covariance matrix
get linear approximation of eigen values
coefs
get the indices of each group of features
print(ind_same_proxy)
demo
same proxy
residuals
gmm
log normal on outliers
positive outliers
negative outliers
demean the new residual again
generate data
sample residuals
get prediction for current investment
get prediction for current proxy
get first period prediction
iterate the step ahead contruction
prepare new x
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
get new covariance matrix
get coefs
get residuals
proxy 1 is the outcome
make fixed residuals
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=None here since we measure the effect across all Ts
"y is a vector, rather than a 2D array"
once the estimator has been fit
"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
an intercept even when bias is part of coef.
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
squeeze the first axis
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
squeeze the first axis
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
d_t=None here since we measure the effect across all Ts
d_t=None here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
scale preds
scale std errs
"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
offset preds
"offset the distribution, too"
scale preds
"scale the distribution, too"
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
"if stderr is zero, ppf will return nans and the loop below would never terminate"
so bail out early; note that it might be possible to correct the algorithm for
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
be clean
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
don't proxy special methods
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
second level bootstrap which would be prohibitive computationally?
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
can't import from econml.inference at top level without creating cyclical dependencies
Note that inference results are always methods even if the inference is for a property
(e.g. coef__inference() is a method but coef_ is a property)
Therefore we must insert a lambda if getting inference for a non-callable
"If inference is for a property, create a fresh lambda to avoid passing args through"
"try to get interval/std first if appropriate,"
since we don't prefer a wrapped method with this name
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base GRF tree
=============================================================================
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
=============================================================================
A MultOutputWrapper for GRF classes
=============================================================================
=============================================================================
Instantiations of Generalized Random Forest
=============================================================================
"Append a constant treatment if `fit_intercept=True`, the coefficient"
in front of the constant treatment is the intercept in the moment equation.
"Append a constant treatment and constant instrument if `fit_intercept=True`,"
the coefficient in front of the constant treatment is the intercept in the moment equation.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Base Generalized Random Forest
=============================================================================
TODO: support freq_weight and sample_var
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
We calculate the min eigenvalue proxy that each criterion is considering
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Generating indices a priori before parallelism ended up being orders of magnitude
faster than how sklearn does it. The reason is that random samplers do not release the
gil it seems.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
####################
Variance correction
####################
Subtract the average within bag variance. This ends up being equal to the
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
The negative part is just sq_between.
Objective bayes debiasing for the diagonals where we know a-prior they are positive
"The off diagonals we have no objective prior, so no correction is applied."
Finally correcting the pred_cov or pred_var
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
we have to filter the folds because they contain the indices in the original data not
the indices in the period-filtered data
translate the indices in a fold to the indices in the period-filtered data
"if groups was [3,3,4,4,5,5,6,6,1,1,2,2,0,0] (the group ids can be in any order, but the"
"time periods for each group should be contguous), and we had [10,11,0,1] as the indices in a fold"
(so the fold is taking the entries corresponding to groups 2 and 3)
"then group_period_filter(0) is [0,2,4,6,8,10,12] and gpf(1) is [1,3,5,7,9,11,13]"
"so for period 1, the fold should be [10,0] => [5,0] (the indices that return 10 and 0 in the t=0 data)"
"and for period 2, the fold should be [11,1] => [5,0] again (the indices that return 11,1 in the t=1 data)"
filter to the indices for the time period
"now find their index in the period-filtered data, which is always sorted"
sanity check that the folds are the same no matter the time period
TODO: update docs
"NOTE: sample weight, sample var are not passed in"
Compose final model
Calculate auxiliary quantities
X ⨂ T_res
"sum(model_final.predict(X, T_res))"
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix J"
override only so that we can exclude treatment featurization verbiage in docstring
override only so that we can exclude treatment featurization verbiage in docstring
"we need to set the number of periods before calling super()._prefit, since that will generate the"
"final and nuisance models, which need to have self._n_periods set"
Set _d_t to effective number of treatments
Required for bootstrap inference
for each mc iteration
for each model under cross fit setting
Handles the corner case when X=None but featurizer might be not None
Expand treatments for each time period
NOTE: important to use the _ortho_learner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the _ortho_learner's copy to retain the information from fitting
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
test that the subsampling scheme past to the trees is correct
The sample size is chosen in particular to test rounding based error when subsampling
test that the estimator calcualtes var correctly
test api
test accuracy
test the projection functionality of forests
test that the estimator calcualtes var correctly
test api
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
test that the subsampling scheme past to the trees is correct
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
filter directories by regex if the NOTEBOOK_DIR_PATTERN environment variable is set
omit the lalonde notebook
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
create directory if necessary
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
can interpret without uncertainty
can't treat before interpreting
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
for is_discrete in [False]:
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
No heterogeneity
Define indices to test
Heterogeneous effects
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
pass sample weight to final step of pipeline
create data with missing values
model that can handle missing values
"test X, W only"
test W only
dowhy does not support missing values in X
assert that fitting with missing values fails when allow_missing is False
and that setting allow_missing after init still works
assert that we fail with a value error when we pass missing X to a model that doesn't support it
assert that fitting with missing values fails when allow_missing is False
and that setting allow_missing after init still works
metalearners don't support W
metalearners do support missing values in X
dowhy never supports missing values in X
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
identity featurization effect functions
polynomial featurization effect functions
1d polynomial featurization functions
2d-to-1d featurization functions
2d-to-1d vector featurization functions
use LassoCV rather than also selecting over RandomForests to save time
test that treatment names are assigned for the featurized treatment
expected shapes
check effects
ate
loose inference checks
temporarily skip LinearDRIV and SparseLinearDRIV for weird effect shape reasons
effect inference
marginal effect inference
const marginal effect inference
fit a dummy estimator first so the featurizer can be fit to the treatment
edge case with transformer that only takes a vector treatment
so far will always return None for cate_treatment_names
assert proper handling of improper feature names passed to certain transformers
"depending on sklearn version, bad feature names either throws error or only uses first relevant name"
ensure alpha is passed
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
initialize parameters
initialize config wtih base config and overwite some values
predict tree using config parameters and assert
shape of trained tree is the same as y_test
initialize config wtih base honest config and overwite some values
predict tree using config parameters and assert
shape of trained tree is the same as y_test
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
compare with weight
compare with weight
compare with weight
compare with weight
Multi-dimensional output y
1-d y
compare when both sample_var and sample_weight exist
multi-d y
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
dgp
StatsModels2SLS
IV2SLS
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
fixed functions as first stage models
they can be anything as long as fitting doesn't modify the predictions
"that way, it doesn't matter if they are trained on different subsets of the data"
all estimators must have opted in to federation
all estimators must have the same covariance type
test coefficients
test effects
fixed functions as first stage models
they can be anything as long as fitting doesn't modify the predictions
"that way, it doesn't matter if they are trained on different subsets of the data"
test coefficients
test effects
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
test that the subsampling scheme past to the trees is correct
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test inference results when `cate_feature_names` doesn not exist
Test inference results when `cate_feature_names` doesn not exist
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
pvalue for second column should be greater than zero since some points are on either side
of the tested value
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
ensure alpha is passed
only is not None when T1 is a constant or a list of constant
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Generate synthetic data
Run _crossfit with Ray enabled
Run _crossfit without Ray
Compare the results
"Nuisance model has no score method, so nuisance_scores_ should be none"
Test non keyword based calls to fit
test non-array inputs
Test custom splitter
Test incomplete set of test folds
"y scores should be positive, since W predicts Y somewhat"
"t scores might not be, since W and T are uncorrelated"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
creating an instance should warn
using the instance should not warn
using the deprecated method should warn
don't warn if b and c are passed by keyword
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
make any access to matplotlib or plt throw an exception
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
Invert indices to match latest API
Invert indices to match latest API
The feature for heterogeneity stays constant
Auxiliary function for adding xticks and vertical lines when plotting results
for dynamic dml vs ground truth parameters.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
tests that we can recover the right degree of polynomial features
implicitly also tests ability to handle pipelines
since 'poly' uses pipelines containing PolynomialFeatures
"generate larger coefficients in a set of high degree features,"
weighted towards higher degree features
"just test a polynomial T model, since for Y the correct degree also depends on"
the interation of T and X
test corner case with just one model in a list
test corner case with empty list
test selecting between two fixed models
"DGP is a linear model, so linear regression should fit better"
"DGP is now non-linear, so random forest should fit better"
these models only work on multi-output data
SeparatedModel doesn't support scoring; that should be fine when not compared to other models
"on the other hand, when we need to compare the score to other models, it should raise an error"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
test at least one estimator from each category
test causal graph
need to set matplotlib backend before viewing model
test refutation estimate
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Generate random Xs
Random covariance matrix of Xs
Effect of Xs on outcome
Effect of treatment on outcomes
Effect of treatment on outcome conditional on X1
Generate treatments based on X and random noise
"Generate Y (based on X, D, and random noise)"
"Simple classifier and regressor for propensity, outcome, and cate"
test the DR outcome difference
"Simple classifier and regressor for propensity, outcome, and cate"
test the DR outcome difference
"Simple classifier and regressor for propensity, outcome, and cate"
test the DR outcome difference
use evaluate_blp to fit on validation only
"Simple classifier and regressor for propensity, outcome, and cate"
test the DR outcome difference
fit nothing
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
accuracy test
"accuracy test, DML"
uncomment when issue #837 is resolved
"NonParamDMLIV(discrete_outcome=discrete_outcome, discrete_treatment=discrete_treatment,"
"discrete_instrument=discrete_instrument, model_final=LinearRegression())"
make sure the auto outcome model is a classifier
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"since we're running so many combinations, just use LassoCV/LogisticRegressionCV"
for the models instead of also selecting over random forest models
ensure we can serialize unfit estimator
ensure we can serialize fit estimator
expected effect size
test effect
test inference
only OrthoIV support inference other than bootstrap
test summary
test can run score
test cate_feature_names
test can run shap values
dgp
no heterogeneity
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"if we aren't fitting on the whole dataset, ensure that the limits are respected"
ensure that the grouping has worked correctly and we get exactly the number of copies
of the items in whichever groups we see
DML nested CV works via a 'cv' attribute
"want to validate the nested grouping, not the outer grouping in the nesting tests"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
parameter combinations to test
"we're running a lot of tests, so use fixed models instead of model selection"
"IntentToTreatDRIV only supports binary treatments and instruments, and doesn't support fit_cov_directly"
TODO: serializing/deserializing for every combination -- is this necessary?
ensure we can serialize unfit estimator
ensure we can serialize fit estimator
expected effect size
assert calculated constant marginal effect shape is expected
const_marginal effect is defined in LinearCateEstimator class
assert calculated marginal effect shape is expected
test inference
test can run score
test cate_feature_names
test can run shap values
"dgp (binary T, binary Z)"
no heterogeneity
with heterogeneity
fitting the covariance directly should be at least as good as computing the covariance from separate models
set the models so that model selection over random forests doesn't take too much time in the repeated trials
directly fitting the covariance should be better than indirectly fitting it
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Create a wrapper around Lasso that doesn't support weights
since Lasso does natively support them starting in sklearn 0.23
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least 6 of every element
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
NOTE: this number may need to change if the default number of folds in
WeightedStratifiedKFold changes
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
make sure we can get out post-fit stuff
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
predetermined splits ensure that all features are seen in each split
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
"with this DGP, since T depends linearly on X, Y depends on X quadratically"
so we should use a quadratic featurizer
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
"we have quadratic terms in y, so we need to pipeline with a quadratic featurizer"
Compare results with and without Ray
create a simple artificial setup where effect of moving from treatment
"a -> b is 2,"
"a -> c is 1, and"
"b -> c is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
should rule out some basic issues.
Note that explicitly specifying the dtype as object is necessary until
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
estimated effects should be identical when treatment is explicitly given
but const_marginal_effect should be reordered based on the explicit cagetories
1-> 2 in original ordering; combination of 3->1 and 3->2
test outer grouping
"with 2 folds, we should get exactly 3 groups per split, each with 10 copies of the y or t value"
test nested grouping
"with 2-fold outer and 2-fold inner grouping, and six total groups,"
should get 1 or 2 groups per split
"Try default, integer, and new user-passed treatment name"
FunctionTransformers are agnostic to passed treatment names
Expected treatment names are the sums of user-passed prefixes and transformer-specific postfixes
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure that we can serialize unfit estimator
ensure that we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef_ and intercept_ inference
verify we can generate the summary
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
test outer grouping
NOTE: StratifiedGroupKFold has a bug when shuffle is True where it doesn't always stratify properly
so we explicitly pass a StratifiedGroupKFold with shuffle=False (the default) rather than letting
cross-fit generate one
"with 2-fold grouping, we should get exactly 3 groups per split"
test nested grouping
"with 2-fold outer and 2-fold inner grouping, we should get 1-2 groups per split"
helper class
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
#########################
WeightedLassoCVWrapper #
#########################
perform 1D fit
perform 2D fit
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
_penalized_inds is only set during fitting
cv exists on penalized model
now we can access _penalized_inds
check that we can read the cv attribute back out from the underlying model
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
continuous treatments have typical treatment values equal to
the mean of the absolute value of non-zero entries
discrete treatments have typical treatment value 1
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
continuous treatments have typical treatment values equal to
the mean of the absolute value of non-zero entries
discrete treatments have typical treatment value 1
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
make sure we don't run into problems dropping every index
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
dgp
model
model
"columns 'd', 'e', 'h' have too many values"
"columns 'd', 'e' have too many values"
lowering bound shouldn't affect already fit columns when warm starting
"column d is now okay, too"
verify that we can use a scalar treatment cost
verify that we can specify per-treatment costs for each sample
verify that using the same state returns the same results each time
set the categories for column 'd' explicitly so that b is default
"first column: 10 ones, this is fine"
"second column: 6 categories, plenty of random instances of each"
this is fine only if we increase the cateogry limit
"third column: nine ones, lots of twos, not enough unless we disable check"
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
forest heterogeneity won't work
"sixth column: just 1 one, not enough even without check"
increase bound on cat expansion
skip checks (reducing folds accordingly)
"Add tests that guarantee that the reliance on DML feature order is not broken, such as"
"Creare a transformer that zeros out all variables after the first n_x variables, so it zeros out W"
Pass an example where W is irrelevant and X is confounder
"As long as DML doesnt change the order of the inputs, then things should be good. Otherwise X would be"
zeroed out and the test will fail
"shouldn't matter if X is scaled much larger or much smaller than W, we should still get good estimates"
rescaling X shouldn't affect the first stage models because they normalize the inputs
"to recover individual coefficients with linear models, we need to be more careful in how we set up X to avoid"
cross terms
scale by 1000 to match the input to this model:
"the scale of X does matter for the final model, which keeps results in user-denominated units"
rescaling X still shouldn't affect the first stage models
TODO: we don't recover the correct values with enough accuracy to enable this assertion
is there a different way to verify that we are learning the correct coefficients?
"np.testing.assert_allclose(loc1.point.values, theta.flatten(), rtol=1e-1)"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
DGP constants
Define data features
Added `_df`to names to be different from the default cate_estimator names
Generate data
################################
Single treatment and outcome #
################################
Test LinearDML
|--> Test featurizers
"ColumnTransformer behaves differently depending on version of sklearn, so we no longer check the names"
|--> Test re-fit
Test SparseLinearDML
Test ForestDML
###################################
Mutiple treatments and outcomes #
###################################
Test LinearDML
Test SparseLinearDML
"Single outcome only, ORF does not support multiple outcomes"
Test DMLOrthoForest
Test DROrthoForest
Test XLearner
Skipping population summary names test because bootstrap inference is too slow
Test SLearner
Test TLearner
Test LinearDRLearner
Test SparseLinearDRLearner
Test ForestDRLearner
Test LinearIntentToTreatDRIV
Test DeepIV
Test categorical treatments
Check refit
Check refit after setting categories
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
import here since otherwise test collection would fail if matplotlib is not installed
Treatment effect function
Outcome support
Treatment support
"Generate controls, covariates, treatments and outcomes"
Heterogeneous treatment effects
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
through shap package.
test shap could generate the plot from the shap_values
"waterfall is broken in this version, fixed by https://github.com/slundberg/shap/pull/2444"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
"Note: unlike other Metalearners, we need the controls' encoded column for training"
"Thus, we append the controls column before the one-hot-encoded T"
"We might want to revisit, though, since it's linearly determined by the others"
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
TODO: prel_model_effect could allow sample_var and freq_weight?
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous"
reshape the predictions
concat W and Z
check nuisances outcome shape
Y_res could be a vector or 1-dimensional 2d-array
"We're projecting, so we're treating E[T|X,Z] as the instrument (ignoring W for simplicity)"
"Then beta(X) = E[T̃ (E[T|X,Z]-E[E[T|X,Z]|X)|X] and we can apply the tower rule several times to get"
"= E[(E[T|X,Z]-E[T|X])^2|X]"
"and also     = E[(E[T|X,Z]-T)^2|X]"
so we can compute it either from (T_proj-T_pred)^2 or from (T_proj-T)^2
The first of these is just Z_res^2
"fit on T*T_proj, covariance will be computed by E[T_res * T_proj] = E[T*T_proj] - E[T]^2"
"return shape (n,)"
we will fit on the covariance (T_res*Z_res) directly
"fit on TZ, covariance will be computed by E[T_res * Z_res] = TZ_pred - T_pred * Z_pred"
"target will be discrete and will be inversed from FirstStageWrapper, shape (n,1)"
"shape (n,)"
"shape (n,)"
"shape(n,)"
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous"
reshape the predictions
"in the projection case, this is a variance and should always be non-negative"
check nuisances outcome shape
"all could be reshaped to vector since Y, T, Z are all single dimensional."
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
A helper class that access all the internal fitted objects of a DRIV Cate Estimator.
Used by both DRIV and IntentToTreatDRIV.
Maggie: I think that would be the case?
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
NOTE: important to use the ortho_learner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
Handles the corner case when X=None but featurizer might be not None
NOTE This is used by the inference methods and is more for internal use to the library
"this is a regression model since the instrument E[T|X,W,Z] is always continuous"
"we're using E[T|X,W,Z] as the instrument"
Define the data generation functions
Define the data generation functions
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
Define the data generation functions
TODO: support freq_weight and sample_var in debiased lasso
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
Define the data generation functions
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
concat W and Z
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
concat W and Z
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
reshape the predictions
"T_res, Z_res, beta expect shape to be (n,1)"
Define the data generation functions
maybe shouldn't expose fit_cate_intercept in this class?
Define the data generation functions
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: do correct adjustment for sample_var
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
concat W and Z
concat W and Z
concat W and Z
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
Define the data generation functions
"train E[T|X,W,Z]"
"train E[Z|X,W]"
note: discrete_instrument rather than discrete_treatment in call to _make_first_stage_selector
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
NOTE: important to use the ortho_learner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
Handles the corner case when X=None but featurizer might be not None
NOTE This is used by the inference methods and is more for internal use to the library
concat W and Z
note that groups are not passed to score because they are only used for fitting
concat W and Z
note that sample_weight and groups are not passed to predict because they are only used for fitting
concat W and Z
A helper class that access all the internal fitted objects of a DMLIV Cate Estimator.
Used by both Parametric and Non Parametric DMLIV.
override only so that we can enforce Z to be required
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Handles the corner case when X=None but featurizer might be not None
Define the data generation functions
Get input names
Summary
coefficient
intercept
Define the data generation functions
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
make T 2D if if was a vector
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO: this utility is documented but internal; reimplement?
TODO: this utility is even less public...
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
use same Cs as would be used by default by LogisticRegressionCV
NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification
which could affect how many times each distinct Y value needs to be present in the data
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
but also supports get_feature_names with expected signature
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
NOTE: we rely on the passthrough columns coming first in the concatenated X;W
"when we pipeline scaling with our first stage models later, so the order here is important"
TODO: remove once older sklearn support is no longer needed
Wrapper to make sure that we get a deep copy of the contents instead of clone returning an untrained copy
Convert python objects to (possibly nested) types that can easily be represented as literals
Convert SingleTreeInterpreter to a python dictionary
named tuple type for storing results inside CausalAnalysis class;
must be lifted to module level to enable pickling
"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,"
"whether they end up in X or in W.  However, for the continuous columns, we want to scale them all"
"when running the first stage models, but don't want to scale the X columns when running the final model,"
since then our coefficients will have odd units and our trees will also have decisions using those units.
""
"we achieve this by pipelining the X scaling with the Y and T models (with fixed scaling, not refitting)"
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
Controls are all other columns of X
"can't use X[:, feat_ind] when X is a DataFrame"
TODO: we can't currently handle unseen values of the feature column when getting the effect;
we might want to modify OrthoLearner (and other discrete treatment classes)
so that the user can opt-in to allowing unseen treatment values
(and return NaN or something in that case)
HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models
and so we can just peel the first columns off of that combined array for rescaling in the pipeline
TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are
"built, such as model_y_feature_names, model_t_feature_names, model_y_transformer, etc., so that this"
becomes a valid approach to handling this
array checking routines don't accept 0-width arrays
perform model selection
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
convert to NormalInferenceResults for consistency
Set the dictionary values shared between local and global summaries
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
required to fit a discrete DML model
"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular"
sub-cases of models or also integrate with azure autoML. (post-MVP)
"TODO: Add other heterogeneity model options, such as {'automl'} for performing"
"model selection for the causal effect, or {'sparse_linear'} for using a debiased lasso. (post-MVP)"
TODO: Enable multi-class classification (post-MVP)
Validate inputs
TODO: check compatibility of X and Y lengths
"no previous fit, cancel warm start"
"work with numeric feature indices, so that we can easily compare with categorical ones"
"if heterogeneity_inds is 1D, repeat it"
heterogeneity inds should be a 2D list of length same as train_inds
replace None elements of heterogeneity_inds and ensure indices are numeric
"TODO: bail out also if categorical columns, classification, random_state changed?"
TODO: should we also train a new model_y under any circumstances when warm_start is True?
train the Y model
"perform model selection for the Y model using all X, not on a per-column basis"
"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
work with the regression wrapper
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
"note that this needs to happen after wrapping to generalize to the multi-class case,"
since otherwise we'll have too many columns to be able to train a classifier
start with empty results and default shared insights
convert categorical indicators to numeric indices
check for indices over the categorical expansion bound
assume we'll be able to train former failures this time; we'll add them back if not
"can't remove in place while iterating over new_inds, so store in separate list"
"train the model, but warn"
no model can be trained in this case since we need more folds
"don't train a model, but suggest workaround since there are enough instances of least"
populated class
also remove from train_inds so we don't try to access the result later
extract subset of names matching new columns
"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
don't want to cache this failed result
properties to return from effect InferenceResults
properties to return from PopulationSummaryResults
Converts strings to property lookups or method calls as a convenience so that the
_point_props and _summary_props above can be applied to an inference object
Create a summary combining all results into a single output; this is used
by the various causal_effect and causal_effect_dict methods to generate either a dataframe
"or a dictionary, respectively, based on the summary function passed into this method"
"ensure array has shape (m,y,t)"
population summary is missing sample dimension; add it for consistency
outcome dimension is missing; add it for consistency
add singleton treatment dimension if missing
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
"each attr has dimension (m,y) or (m,y,t)"
concatenate along treatment dimension
"for dictionary representation, want to remove unneeded sample dimension"
in cohort and global results
TODO: enrich outcome logic for multi-class classification when that is supported
There is no actual sample level in this data
can't drop only level
should be serialization-ready and contain no numpy arrays
"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
TODO: Note that there's no column metadata for the sample number - should there be?
"need to replicate the column info for each sample, then remove from the shared data"
NOTE: the flattened order has the ouptut dimension before the feature dimension
which may need to be revisited once we support multiclass
get the length of the list corresponding to the first dictionary key
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
a global inference indicates the effect of that one feature on the outcome
need to reshape the output to match the input
we want to offset the inference object by the baseline estimate of y
"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
get the length of the list corresponding to the first dictionary key
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
"NOTE: this calculation is correct only if treatment costs are marginal costs,"
because then scaling the difference between treatment value and treatment costs is the
same as scaling the treatment value and subtracting the scaled treatment cost.
""
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
"continuous treatments, the policy value should include the benefit of decreasing treatments"
(rather than just not treating at all)
""
"We can get the total by seeing that if we restrict attention to units where we would treat,"
2 * policy_value - always_treat
includes exactly their contribution because policy_value and always_treat both include it
"and likewise restricting attention to the units where we want to decrease treatment,"
2 * policy_value - always-treat
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
is zero and the contribution to always_treat is negative
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
get dataframe with all but selected column
apply 10% of a typical treatment for this feature
"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely"
set the effect bounds; for positive treatments these agree with
"the estimates; for negative treatments, we need to invert the interval"
the effect is now always positive since we decrease treatment when negative
"for discrete treatment, stack a zero result in front for control"
we need to call effect_inference to get the correct CI between the two treatment options
we now need to construct the delta in the cost between the two treatments and translate the effect
remove third dimenions potentially added
"find cost of current treatment: equality creates a 2d array with True on each row,"
only if its the location of the current treatment. Then we take the corresponding cost.
construct index of current treatment
add second dimension if needed for broadcasting during translation of effect
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO: conisder working around relying on sklearn implementation details
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
only expose predict_proba if best_model has predict_proba
used because logic elsewhere uses hasattr predict proba to check if model is a classifier
logic copied from check_cv
otherwise we will assume the user already set the cv attribute to something
compatible with splitting with a `groups` argument
"drop groups from arg list, which were already used at the outer level and may not be supported by the model"
the score needs to be compared to another model's
"so we don't need to fit the model itself on all of the data, just get the out-of-sample score"
use _fit_with_groups instead of just fit to handle nested grouping
we need to train the model on the data
copy common parameters
copy common fitted variables
make sure all classes agree on best c/l1 combo
"We need an R^2 score to compare to other models; ElasticNetCV doesn't provide it,"
but we can calculate it ourselves from the MSE plus the variance of the target y
"l1 ratio doesn't apply to Lasso, only ElasticNet"
max R^2 corresponds to min MSE
"constructor takes cv as a positional or kwarg, just pull it out of a new instance"
"If classification methods produce multiple columns of output,"
we need to manually encode classes to ensure consistent column ordering.
We clone the estimator to make sure that all the folds are
"independent, and that it is pickle-able."
verbose was removed from sklearn's non-public _fit_and_predict method in 1.4
`predictions` is a list of method outputs from each fold.
"If each of those is also a list, then treat this as a"
multioutput-multiclass task. We need to separately concatenate
the method outputs for each label into an `n_labels` long list.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
TODO: consider working around relying on sklearn implementation details
local import to avoid circular imports
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Call weighted lasso on reduced design matrix
Weighted tau
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set model to the single-target estimator by default so there's always a model to get and set attributes on
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
(e.g. former has 'positive' and 'precompute' while latter does not)
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
don't proxy special methods
"don't pass get_params through to model, because that will cause sklearn to clone this"
regressor incorrectly
"Note: for known attributes that have been set this method will not be called,"
so we should just throw here because this is an attribute belonging to this class
but which hasn't yet been set on this instance
set default values for None
check freq_weight should be integer and should be accompanied by sample_var
check array shape
weight X and y and sample_var
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
"For aggregation calculations, always treat wy as an array so that einsum expressions don't need to change"
We'll collapse results back down afterwards if necessary
"for federation, we need to store these 5 arrays when using heteroskedasticity-robust inference"
y dimension is always first in the output when present so that broadcasting works correctly
set default values for None
check array shape
check dimension of instruments is more than dimension of treatments
weight X and y
learn point estimate
solve first stage linear regression E[T|Z]
"""that"" means T̂"
solve second stage linear regression E[Y|that]
(T̂.T*T̂)^{-1}
learn cov(theta)
(T̂.T*T̂)^{-1}
sigma^2
reference: http://www.hec.unil.ch/documents/seminars/deep/361.pdf
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
average the outcome dimension if it exists and ensure 2d y_pred
get index of best treatment
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO: consider working around relying on sklearn implementation details
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Convert rows to columns
Require group assignment t to be one-hot-encoded
Get predictions for the 2 splits
Must make sure indices are merged correctly
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Must normalize weights
Override the CATE inference options
Add blb inference to parent's options
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
TODO: ideally the below private attribute logic should be in .fit but is needed in init
for nuisance estimator generation for parent class
should refactor later
Define nuisance estimators
Define parameter estimators
Define
Need to redefine fit here for auto inference to work due to a quirk in how
wrap_fit is defined
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Check that all discrete treatments are represented
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
returns shape-conforming residuals
Copy and/or define models
Define parameter estimators
Define moment and mean gradient estimator
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Call `fit` from parent class
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
override only so that we can exclude treatment featurization verbiage in docstring
Override to flatten output if T is flat
override only so that we can exclude treatment featurization verbiage in docstring
Expand one-hot encoding to include the zero treatment
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Calculate confidence intervals for the parameter (marginal effect)
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
d_t=None here since we measure the effect across all Ts
conditionally expand jacobian dimensions to align with einsum str
Calculate the effects
Calculate the standard deviations for the effects
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/main/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
TODO: enable type aliases
napoleon_preprocess_types = True  # needed for type aliases to work
napoleon_type_aliases = {
"""array_like"": "":term:`array_like`"","
"""ndarray"": ""~numpy.ndarray"","
"""RandomState"": "":class:`~numpy.random.RandomState`"","
"""DataFrame"": "":class:`~pandas.DataFrame`"","
"""Series"": "":class:`~pandas.Series`"","
}
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of strings:
""
"source_suffix = ['.rst', '.md']"
The root toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
def mlasso_model(): return MultiTaskLassoCV(
"cv=3, alphas=alpha_regs, max_iter=200)"
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
"alpha_regs = [5e-3, 1e-2, 5e-2]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
subset of features that are exogenous and create heterogeneity
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
subset of features wrt we estimate heterogeneity
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
introspect the constructor arguments to find the model parameters
to represent
"if the argument is deprecated, ignore it"
Extract and sort argument names excluding 'self'
column names
transfer input to numpy arrays
transfer input to 2d arrays
create dataframe
currently dowhy only support single outcome and single treatment
call dowhy
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
cate estimator but not the effect.
don't proxy special methods
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
For when checking input values is disabled
Type to column extraction function
if not all column names are strings
coerce feature names to be strings
Prefer sklearn 1.0's get_feature_names_out method to deprecated get_feature_names method
"Some featurizers will throw, such as a pipeline with a transformer that doesn't itself support names"
"Get number of arguments, some sklearn featurizer don't accept feature_names"
Handles cases where the passed feature names create issues
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
Get feature names using featurizer
All attempts at retrieving transformed feature names have failed
Delegate handling to downstream logic
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
Normalize weights
This class is mainly derived from statsmodels.iolib.summary.Summary
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
"want to enforce that each bad_arg was either in kwargs,"
or else it was in neither and is just taking its default value
Any access should throw
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
return plain dictionary so that erroneous accesses don't half work (see e.g. #708)
for every dimension of the treatment add some epsilon and observe change in featurized treatment
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
return plain dictionary so that erroneous accesses don't half work (see #708)
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
return plain dictionary so that erroneous accesses don't half work (see #708)
input feature name is already updated by cate_feature_names.
define the index of d_x to filter for each given T
filter X after broadcast with T for each given T
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
return plain dictionary so that erroneous accesses don't half work (see #708)
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
return plain dictionary so that erroneous accesses don't half work (see #708)
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
""
This code contains some snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_export.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
make any access to matplotlib or plt throw an exception
make any access to graphviz or plt throw an exception
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
Initialize saturation & value; calculate chroma & value shift
Calculate some intermediate values
Initialize RGB with same hue & chroma as our color
Shift the initial RGB values to match value and store
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use mean of targets
Write node mean CATE
Write node std of CATE
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
Fetch appropriate color for node
Write node mean CATE
Write node mean CATE
Write recommended treatment and value - cost
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
This flag is true when names are set in a child class instead
"If names are set in a child class, add an attribute reflecting that"
This works only if X is passed as a kwarg
We plan to enforce X as kwarg only in future releases
This checks if names have been set in a child class
"If names were set in a child class, don't do it again"
"Wraps-up fit by setting attributes, cleaning up, etc."
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
apply defaults before calling inference method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"Treatment names is None, default to BaseCateEstimator"
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
Get input names
Summary
add statsmodels to parent's options
add debiasedlasso to parent's options
add blb to parent's options
TODO Share some logic with non-discrete version
Get input names
Note: we do not transform feature names since that is done within summary_frame
Summary
add statsmodels to parent's options
add statsmodels to parent's options
add blb to parent's options
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
fully materialize folds so that they can be reused across models
and precompute fitted indices so that we fail fast if there's an issue with them
NOTE: if any model is missing scores we will just return None even if another model
has scores. this is because we don't know how many scores are missing
"for the models that are missing them, so we don't know how to pad the array"
for convenience we allos a single model to be passed in lieu of a singleton list
"in that case, we will also unwrap the model output"
"when there is more than one model, nuisances from previous models"
come first as positional arguments
"scores entries should be lists of scores, so make each entry a singleton list"
Adding the kwargs to ray object store to be used by remote functions
for each fold to avoid IO overhead
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
generate an instance of the final model
generate an instance of the nuisance model
Define Ray remote function (Ray remote wrapper of the _fit_nuisances function)
Create Ray remote jobs for parallel processing
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
alteration even when we only want to fit_final.
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
upgrade to a GroupKFold or StratiGroupKFold if groups is not None
"we won't have generated a KFold or StratifiedKFold ourselves when groups are passed,"
"but the user might have supplied one, which won't work"
self._models_nuisance will be a list of lists or a list of list of lists
so we use self._ortho_learner_model_nuisance to determine the nesting level
for each mc iteration
for each model under cross fit setting
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"TODO: This could be extended to also work with our sparse and 2SLS estimators,"
if we add an aggregate method to them
Remember to update the docs if this changes
mix in the appropriate inference class
assign all of the attributes from the dummy estimator that would normally be assigned during fitting
TODO: This seems hacky; is there a better abstraction to maintain these?
"This should also include bias_part_of_coef, model_final_, and fitted_models_final above"
Assign treatment expansion attributes
Methods needed to implement the LinearCateEstimator interface
Methods needed to implement the LinearFinalModelCateEstimatorMixin
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Policy Forest
=============================================================================
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base Policy tree
=============================================================================
The values below are required and utilitized by methods in the _SingleTreeExporterMixin
HACK: sklearn 1.3 enforces that the input to plot_tree is a DecisionTreeClassifier or DecisionTreeRegressor
This is a hack to get around that restriction by declaring that PolicyTree inherits from DecisionTreeClassifier
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"Unique treatments (ordered, includes control)"
Number of treatments (excluding control)
Indicator for whether
Get DR outcomes in training sample
Get DR outcomes in validation sample
Get DR outcomes in validation sample
Calculate ATE in the validation sample
Fit propensity in treatment
Predict propensity scores
Possible treatments (need to allow more than 2)
Predict outcomes
T-learner logic
"if CATE is given explicitly or has not been fitted at all previously, fit it now"
Assign units in validation set to groups
Proportion of validations set in group
Group average treatment effect (GATE) -- average of DR outcomes in group
Average of CATE predictions in group
Calculate group calibration score
Calculate overall calibration score
Calculate R-square calibration score
"if CATE is given explicitly or has not been fitted at all previously, fit it now"
treat each treatment as a separate regression
"here, prop_preds should be a matrix"
with rows corresponding to units and columns corresponding to treatment statuses
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"since we only allow single dimensional y, we could flatten the prediction"
override only so that we can exclude treatment featurization verbiage in docstring
override only so that we can exclude treatment featurization verbiage in docstring
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Handles the corner case when X=None but featurizer might be not None
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
TODO: support freq_weight and sample_var in debiased lasso
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
Replacing to remove docstring
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
data is already validated at initial fit time
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE: important to use the rlearner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the rlearner's copy to retain the information from fitting
Handles the corner case when X=None but featurizer might be not None
override only so that we can update the docstring to indicate support for `LinearModelFinalInference`
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support freq_weight and sample_var in debiased lasso
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add blb to parent's options
override only so that we can update the docstring to indicate
support for `GenericSingleTreatmentModelFinalInference`
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
note that groups are not passed to score because they are only used for fitting
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
NOTE: important to get parent's wrapped copy so that
"after training wrapped featurizer is also trained, etc."
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
Fit a doubly robust average effect
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
"If custom param grid, check that only estimator parameters are being altered"
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
override only so that we can update the docstring to indicate support for `blb`
Get input names
Summary
Determine output settings
"Important: This must be the first invocation of the random state at fit time, so that"
train/test splits are re-generatable from an external object simply by knowing the
random_state parameter of the tree. Can be useful in the future if one wants to create local
linear predictions. Currently is also useful for testing.
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
Set min_weight_leaf from min_weight_fraction_leaf
Build tree
We calculate the maximum number of samples from each half-split that any node in the tree can
hold. Used by criterion for memory space savings.
Initialize the criterion object and the criterion_val object if honest.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
""
This code is a fork from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_base.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
Set parameters
Don't instantiate estimators now! Parameters of base_estimator might
"still change. Eg., when grid-searching with the nested object syntax."
self.estimators_ needs to be filled by the derived classes in fit.
Compute the number of jobs
Partition estimators between jobs
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
covariance matrix
get eigen value and eigen vectors
simulate eigen vectors
keep the top 4 eigen value and corresponding eigen vector
replace the negative eigen values
generate a new covariance matrix
get linear approximation of eigen values
coefs
get the indices of each group of features
print(ind_same_proxy)
demo
same proxy
residuals
gmm
log normal on outliers
positive outliers
negative outliers
demean the new residual again
generate data
sample residuals
get prediction for current investment
get prediction for current proxy
get first period prediction
iterate the step ahead contruction
prepare new x
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
get new covariance matrix
get coefs
get residuals
proxy 1 is the outcome
make fixed residuals
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=None here since we measure the effect across all Ts
"y is a vector, rather than a 2D array"
once the estimator has been fit
"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
an intercept even when bias is part of coef.
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
squeeze the first axis
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
squeeze the first axis
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
d_t=None here since we measure the effect across all Ts
d_t=None here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
scale preds
scale std errs
"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
offset preds
"offset the distribution, too"
scale preds
"scale the distribution, too"
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
"if stderr is zero, ppf will return nans and the loop below would never terminate"
so bail out early; note that it might be possible to correct the algorithm for
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
be clean
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
don't proxy special methods
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
second level bootstrap which would be prohibitive computationally?
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
can't import from econml.inference at top level without creating cyclical dependencies
Note that inference results are always methods even if the inference is for a property
(e.g. coef__inference() is a method but coef_ is a property)
Therefore we must insert a lambda if getting inference for a non-callable
"If inference is for a property, create a fresh lambda to avoid passing args through"
"try to get interval/std first if appropriate,"
since we don't prefer a wrapped method with this name
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base GRF tree
=============================================================================
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
=============================================================================
A MultOutputWrapper for GRF classes
=============================================================================
=============================================================================
Instantiations of Generalized Random Forest
=============================================================================
"Append a constant treatment if `fit_intercept=True`, the coefficient"
in front of the constant treatment is the intercept in the moment equation.
"Append a constant treatment and constant instrument if `fit_intercept=True`,"
the coefficient in front of the constant treatment is the intercept in the moment equation.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Base Generalized Random Forest
=============================================================================
TODO: support freq_weight and sample_var
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
We calculate the min eigenvalue proxy that each criterion is considering
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Generating indices a priori before parallelism ended up being orders of magnitude
faster than how sklearn does it. The reason is that random samplers do not release the
gil it seems.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
####################
Variance correction
####################
Subtract the average within bag variance. This ends up being equal to the
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
The negative part is just sq_between.
Objective bayes debiasing for the diagonals where we know a-prior they are positive
"The off diagonals we have no objective prior, so no correction is applied."
Finally correcting the pred_cov or pred_var
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
we have to filter the folds because they contain the indices in the original data not
the indices in the period-filtered data
translate the indices in a fold to the indices in the period-filtered data
"if groups was [3,3,4,4,5,5,6,6,1,1,2,2,0,0] (the group ids can be in any order, but the"
"time periods for each group should be contguous), and we had [10,11,0,1] as the indices in a fold"
(so the fold is taking the entries corresponding to groups 2 and 3)
"then group_period_filter(0) is [0,2,4,6,8,10,12] and gpf(1) is [1,3,5,7,9,11,13]"
"so for period 1, the fold should be [10,0] => [5,0] (the indices that return 10 and 0 in the t=0 data)"
"and for period 2, the fold should be [11,1] => [5,0] again (the indices that return 11,1 in the t=1 data)"
filter to the indices for the time period
"now find their index in the period-filtered data, which is always sorted"
sanity check that the folds are the same no matter the time period
TODO: update docs
"NOTE: sample weight, sample var are not passed in"
Compose final model
Calculate auxiliary quantities
X ⨂ T_res
"sum(model_final.predict(X, T_res))"
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix J"
override only so that we can exclude treatment featurization verbiage in docstring
override only so that we can exclude treatment featurization verbiage in docstring
"we need to set the number of periods before calling super()._prefit, since that will generate the"
"final and nuisance models, which need to have self._n_periods set"
Set _d_t to effective number of treatments
Required for bootstrap inference
for each mc iteration
for each model under cross fit setting
Handles the corner case when X=None but featurizer might be not None
Expand treatments for each time period
NOTE: important to use the _ortho_learner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the _ortho_learner's copy to retain the information from fitting
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
test that the subsampling scheme past to the trees is correct
The sample size is chosen in particular to test rounding based error when subsampling
test that the estimator calcualtes var correctly
test api
test accuracy
test the projection functionality of forests
test that the estimator calcualtes var correctly
test api
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
test that the subsampling scheme past to the trees is correct
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
filter directories by regex if the NOTEBOOK_DIR_PATTERN environment variable is set
omit the lalonde notebook
make sure that coverage outputs reflect notebook contents
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
"remove added coverage cell, then decrement execution_count for other cells to account for it"
create directory if necessary
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
can interpret without uncertainty
can't treat before interpreting
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
for is_discrete in [False]:
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
No heterogeneity
Define indices to test
Heterogeneous effects
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
pass sample weight to final step of pipeline
create data with missing values
model that can handle missing values
"test X, W only"
test W only
dowhy does not support missing values in X
assert that fitting with missing values fails when allow_missing is False
and that setting allow_missing after init still works
assert that we fail with a value error when we pass missing X to a model that doesn't support it
assert that fitting with missing values fails when allow_missing is False
and that setting allow_missing after init still works
metalearners don't support W
metalearners do support missing values in X
dowhy never supports missing values in X
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
identity featurization effect functions
polynomial featurization effect functions
1d polynomial featurization functions
2d-to-1d featurization functions
2d-to-1d vector featurization functions
use LassoCV rather than also selecting over RandomForests to save time
test that treatment names are assigned for the featurized treatment
expected shapes
check effects
ate
loose inference checks
temporarily skip LinearDRIV and SparseLinearDRIV for weird effect shape reasons
effect inference
marginal effect inference
const marginal effect inference
fit a dummy estimator first so the featurizer can be fit to the treatment
edge case with transformer that only takes a vector treatment
so far will always return None for cate_treatment_names
assert proper handling of improper feature names passed to certain transformers
"depending on sklearn version, bad feature names either throws error or only uses first relevant name"
ensure alpha is passed
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
initialize parameters
initialize config wtih base config and overwite some values
predict tree using config parameters and assert
shape of trained tree is the same as y_test
initialize config wtih base honest config and overwite some values
predict tree using config parameters and assert
shape of trained tree is the same as y_test
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
compare with weight
compare with weight
compare with weight
compare with weight
Multi-dimensional output y
1-d y
compare when both sample_var and sample_weight exist
multi-d y
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
dgp
StatsModels2SLS
IV2SLS
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
fixed functions as first stage models
they can be anything as long as fitting doesn't modify the predictions
"that way, it doesn't matter if they are trained on different subsets of the data"
all estimators must have opted in to federation
all estimators must have the same covariance type
test coefficients
test effects
fixed functions as first stage models
they can be anything as long as fitting doesn't modify the predictions
"that way, it doesn't matter if they are trained on different subsets of the data"
test coefficients
test effects
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
test that the subsampling scheme past to the trees is correct
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test inference results when `cate_feature_names` doesn not exist
Test inference results when `cate_feature_names` doesn not exist
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
pvalue for second column should be greater than zero since some points are on either side
of the tested value
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
ensure alpha is passed
only is not None when T1 is a constant or a list of constant
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Generate synthetic data
Run _crossfit with Ray enabled
Run _crossfit without Ray
Compare the results
"Nuisance model has no score method, so nuisance_scores_ should be none"
Test non keyword based calls to fit
test non-array inputs
Test custom splitter
Test incomplete set of test folds
"y scores should be positive, since W predicts Y somewhat"
"t scores might not be, since W and T are uncorrelated"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
creating an instance should warn
using the instance should not warn
using the deprecated method should warn
don't warn if b and c are passed by keyword
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
make any access to matplotlib or plt throw an exception
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
Invert indices to match latest API
Invert indices to match latest API
The feature for heterogeneity stays constant
Auxiliary function for adding xticks and vertical lines when plotting results
for dynamic dml vs ground truth parameters.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
tests that we can recover the right degree of polynomial features
implicitly also tests ability to handle pipelines
since 'poly' uses pipelines containing PolynomialFeatures
"generate larger coefficients in a set of high degree features,"
weighted towards higher degree features
"just test a polynomial T model, since for Y the correct degree also depends on"
the interation of T and X
test corner case with just one model in a list
test corner case with empty list
test selecting between two fixed models
"DGP is a linear model, so linear regression should fit better"
"DGP is now non-linear, so random forest should fit better"
these models only work on multi-output data
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
test at least one estimator from each category
test causal graph
need to set matplotlib backend before viewing model
test refutation estimate
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Generate random Xs
Random covariance matrix of Xs
Effect of Xs on outcome
Effect of treatment on outcomes
Effect of treatment on outcome conditional on X1
Generate treatments based on X and random noise
"Generate Y (based on X, D, and random noise)"
"Simple classifier and regressor for propensity, outcome, and cate"
test the DR outcome difference
"Simple classifier and regressor for propensity, outcome, and cate"
test the DR outcome difference
"Simple classifier and regressor for propensity, outcome, and cate"
test the DR outcome difference
use evaluate_blp to fit on validation only
"Simple classifier and regressor for propensity, outcome, and cate"
test the DR outcome difference
fit nothing
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
accuracy test
"accuracy test, DML"
uncomment when issue #837 is resolved
"NonParamDMLIV(discrete_outcome=discrete_outcome, discrete_treatment=discrete_treatment,"
"discrete_instrument=discrete_instrument, model_final=LinearRegression())"
make sure the auto outcome model is a classifier
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"since we're running so many combinations, just use LassoCV/LogisticRegressionCV"
for the models instead of also selecting over random forest models
ensure we can serialize unfit estimator
ensure we can serialize fit estimator
expected effect size
test effect
test inference
only OrthoIV support inference other than bootstrap
test summary
test can run score
test cate_feature_names
test can run shap values
dgp
no heterogeneity
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"if we aren't fitting on the whole dataset, ensure that the limits are respected"
ensure that the grouping has worked correctly and we get exactly the number of copies
of the items in whichever groups we see
DML nested CV works via a 'cv' attribute
"want to validate the nested grouping, not the outer grouping in the nesting tests"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
parameter combinations to test
"we're running a lot of tests, so use fixed models instead of model selection"
"IntentToTreatDRIV only supports binary treatments and instruments, and doesn't support fit_cov_directly"
TODO: serializing/deserializing for every combination -- is this necessary?
ensure we can serialize unfit estimator
ensure we can serialize fit estimator
expected effect size
assert calculated constant marginal effect shape is expected
const_marginal effect is defined in LinearCateEstimator class
assert calculated marginal effect shape is expected
test inference
test can run score
test cate_feature_names
test can run shap values
"dgp (binary T, binary Z)"
no heterogeneity
with heterogeneity
fitting the covariance directly should be at least as good as computing the covariance from separate models
set the models so that model selection over random forests doesn't take too much time in the repeated trials
directly fitting the covariance should be better than indirectly fitting it
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Create a wrapper around Lasso that doesn't support weights
since Lasso does natively support them starting in sklearn 0.23
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least 6 of every element
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
NOTE: this number may need to change if the default number of folds in
WeightedStratifiedKFold changes
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
make sure we can get out post-fit stuff
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
predetermined splits ensure that all features are seen in each split
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
"with this DGP, since T depends linearly on X, Y depends on X quadratically"
so we should use a quadratic featurizer
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
"we have quadratic terms in y, so we need to pipeline with a quadratic featurizer"
Compare results with and without Ray
create a simple artificial setup where effect of moving from treatment
"a -> b is 2,"
"a -> c is 1, and"
"b -> c is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
should rule out some basic issues.
Note that explicitly specifying the dtype as object is necessary until
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
estimated effects should be identical when treatment is explicitly given
but const_marginal_effect should be reordered based on the explicit cagetories
1-> 2 in original ordering; combination of 3->1 and 3->2
test outer grouping
"with 2 folds, we should get exactly 3 groups per split, each with 10 copies of the y or t value"
test nested grouping
"with 2-fold outer and 2-fold inner grouping, and six total groups,"
should get 1 or 2 groups per split
"Try default, integer, and new user-passed treatment name"
FunctionTransformers are agnostic to passed treatment names
Expected treatment names are the sums of user-passed prefixes and transformer-specific postfixes
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure that we can serialize unfit estimator
ensure that we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef_ and intercept_ inference
verify we can generate the summary
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
test outer grouping
NOTE: StratifiedGroupKFold has a bug when shuffle is True where it doesn't always stratify properly
so we explicitly pass a StratifiedGroupKFold with shuffle=False (the default) rather than letting
cross-fit generate one
"with 2-fold grouping, we should get exactly 3 groups per split"
test nested grouping
"with 2-fold outer and 2-fold inner grouping, we should get 1-2 groups per split"
helper class
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
#########################
WeightedLassoCVWrapper #
#########################
perform 1D fit
perform 2D fit
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
_penalized_inds is only set during fitting
cv exists on penalized model
now we can access _penalized_inds
check that we can read the cv attribute back out from the underlying model
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
continuous treatments have typical treatment values equal to
the mean of the absolute value of non-zero entries
discrete treatments have typical treatment value 1
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
continuous treatments have typical treatment values equal to
the mean of the absolute value of non-zero entries
discrete treatments have typical treatment value 1
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
make sure we don't run into problems dropping every index
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
dgp
model
model
"columns 'd', 'e', 'h' have too many values"
"columns 'd', 'e' have too many values"
lowering bound shouldn't affect already fit columns when warm starting
"column d is now okay, too"
verify that we can use a scalar treatment cost
verify that we can specify per-treatment costs for each sample
verify that using the same state returns the same results each time
set the categories for column 'd' explicitly so that b is default
"first column: 10 ones, this is fine"
"second column: 6 categories, plenty of random instances of each"
this is fine only if we increase the cateogry limit
"third column: nine ones, lots of twos, not enough unless we disable check"
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
forest heterogeneity won't work
"sixth column: just 1 one, not enough even without check"
increase bound on cat expansion
skip checks (reducing folds accordingly)
"Add tests that guarantee that the reliance on DML feature order is not broken, such as"
"Creare a transformer that zeros out all variables after the first n_x variables, so it zeros out W"
Pass an example where W is irrelevant and X is confounder
"As long as DML doesnt change the order of the inputs, then things should be good. Otherwise X would be"
zeroed out and the test will fail
"shouldn't matter if X is scaled much larger or much smaller than W, we should still get good estimates"
rescaling X shouldn't affect the first stage models because they normalize the inputs
"to recover individual coefficients with linear models, we need to be more careful in how we set up X to avoid"
cross terms
scale by 1000 to match the input to this model:
"the scale of X does matter for the final model, which keeps results in user-denominated units"
rescaling X still shouldn't affect the first stage models
TODO: we don't recover the correct values with enough accuracy to enable this assertion
is there a different way to verify that we are learning the correct coefficients?
"np.testing.assert_allclose(loc1.point.values, theta.flatten(), rtol=1e-1)"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
DGP constants
Define data features
Added `_df`to names to be different from the default cate_estimator names
Generate data
################################
Single treatment and outcome #
################################
Test LinearDML
|--> Test featurizers
"ColumnTransformer behaves differently depending on version of sklearn, so we no longer check the names"
|--> Test re-fit
Test SparseLinearDML
Test ForestDML
###################################
Mutiple treatments and outcomes #
###################################
Test LinearDML
Test SparseLinearDML
"Single outcome only, ORF does not support multiple outcomes"
Test DMLOrthoForest
Test DROrthoForest
Test XLearner
Skipping population summary names test because bootstrap inference is too slow
Test SLearner
Test TLearner
Test LinearDRLearner
Test SparseLinearDRLearner
Test ForestDRLearner
Test LinearIntentToTreatDRIV
Test DeepIV
Test categorical treatments
Check refit
Check refit after setting categories
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
import here since otherwise test collection would fail if matplotlib is not installed
Treatment effect function
Outcome support
Treatment support
"Generate controls, covariates, treatments and outcomes"
Heterogeneous treatment effects
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
through shap package.
test shap could generate the plot from the shap_values
"waterfall is broken in this version, fixed by https://github.com/slundberg/shap/pull/2444"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
"Note: unlike other Metalearners, we need the controls' encoded column for training"
"Thus, we append the controls column before the one-hot-encoded T"
"We might want to revisit, though, since it's linearly determined by the others"
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
TODO: prel_model_effect could allow sample_var and freq_weight?
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous"
reshape the predictions
concat W and Z
check nuisances outcome shape
Y_res could be a vector or 1-dimensional 2d-array
"We're projecting, so we're treating E[T|X,Z] as the instrument (ignoring W for simplicity)"
"Then beta(X) = E[T̃ (E[T|X,Z]-E[E[T|X,Z]|X)|X] and we can apply the tower rule several times to get"
"= E[(E[T|X,Z]-E[T|X])^2|X]"
"and also     = E[(E[T|X,Z]-T)^2|X]"
so we can compute it either from (T_proj-T_pred)^2 or from (T_proj-T)^2
The first of these is just Z_res^2
"fit on T*T_proj, covariance will be computed by E[T_res * T_proj] = E[T*T_proj] - E[T]^2"
"return shape (n,)"
we will fit on the covariance (T_res*Z_res) directly
"fit on TZ, covariance will be computed by E[T_res * Z_res] = TZ_pred - T_pred * Z_pred"
"target will be discrete and will be inversed from FirstStageWrapper, shape (n,1)"
"shape (n,)"
"shape (n,)"
"shape(n,)"
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous"
reshape the predictions
"in the projection case, this is a variance and should always be non-negative"
check nuisances outcome shape
"all could be reshaped to vector since Y, T, Z are all single dimensional."
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
A helper class that access all the internal fitted objects of a DRIV Cate Estimator.
Used by both DRIV and IntentToTreatDRIV.
Maggie: I think that would be the case?
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
NOTE: important to use the ortho_learner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
Handles the corner case when X=None but featurizer might be not None
NOTE This is used by the inference methods and is more for internal use to the library
"this is a regression model since the instrument E[T|X,W,Z] is always continuous"
"we're using E[T|X,W,Z] as the instrument"
Define the data generation functions
Define the data generation functions
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
Define the data generation functions
TODO: support freq_weight and sample_var in debiased lasso
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
Define the data generation functions
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
concat W and Z
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
concat W and Z
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
reshape the predictions
"T_res, Z_res, beta expect shape to be (n,1)"
Define the data generation functions
maybe shouldn't expose fit_cate_intercept in this class?
Define the data generation functions
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: do correct adjustment for sample_var
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
concat W and Z
concat W and Z
concat W and Z
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
Define the data generation functions
"train E[T|X,W,Z]"
"train E[Z|X,W]"
note: discrete_instrument rather than discrete_treatment in call to _make_first_stage_selector
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
NOTE: important to use the ortho_learner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
Handles the corner case when X=None but featurizer might be not None
NOTE This is used by the inference methods and is more for internal use to the library
concat W and Z
note that groups are not passed to score because they are only used for fitting
concat W and Z
note that sample_weight and groups are not passed to predict because they are only used for fitting
concat W and Z
A helper class that access all the internal fitted objects of a DMLIV Cate Estimator.
Used by both Parametric and Non Parametric DMLIV.
override only so that we can enforce Z to be required
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Handles the corner case when X=None but featurizer might be not None
Define the data generation functions
Get input names
Summary
coefficient
intercept
Define the data generation functions
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
make T 2D if if was a vector
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO: this utility is documented but internal; reimplement?
TODO: this utility is even less public...
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
use same Cs as would be used by default by LogisticRegressionCV
NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification
which could affect how many times each distinct Y value needs to be present in the data
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
but also supports get_feature_names with expected signature
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
NOTE: we rely on the passthrough columns coming first in the concatenated X;W
"when we pipeline scaling with our first stage models later, so the order here is important"
TODO: remove once older sklearn support is no longer needed
Wrapper to make sure that we get a deep copy of the contents instead of clone returning an untrained copy
Convert python objects to (possibly nested) types that can easily be represented as literals
Convert SingleTreeInterpreter to a python dictionary
named tuple type for storing results inside CausalAnalysis class;
must be lifted to module level to enable pickling
"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,"
"whether they end up in X or in W.  However, for the continuous columns, we want to scale them all"
"when running the first stage models, but don't want to scale the X columns when running the final model,"
since then our coefficients will have odd units and our trees will also have decisions using those units.
""
"we achieve this by pipelining the X scaling with the Y and T models (with fixed scaling, not refitting)"
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
Controls are all other columns of X
"can't use X[:, feat_ind] when X is a DataFrame"
TODO: we can't currently handle unseen values of the feature column when getting the effect;
we might want to modify OrthoLearner (and other discrete treatment classes)
so that the user can opt-in to allowing unseen treatment values
(and return NaN or something in that case)
HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models
and so we can just peel the first columns off of that combined array for rescaling in the pipeline
TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are
"built, such as model_y_feature_names, model_t_feature_names, model_y_transformer, etc., so that this"
becomes a valid approach to handling this
array checking routines don't accept 0-width arrays
perform model selection
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
convert to NormalInferenceResults for consistency
Set the dictionary values shared between local and global summaries
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
required to fit a discrete DML model
"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular"
sub-cases of models or also integrate with azure autoML. (post-MVP)
"TODO: Add other heterogeneity model options, such as {'automl'} for performing"
"model selection for the causal effect, or {'sparse_linear'} for using a debiased lasso. (post-MVP)"
TODO: Enable multi-class classification (post-MVP)
Validate inputs
TODO: check compatibility of X and Y lengths
"no previous fit, cancel warm start"
"work with numeric feature indices, so that we can easily compare with categorical ones"
"if heterogeneity_inds is 1D, repeat it"
heterogeneity inds should be a 2D list of length same as train_inds
replace None elements of heterogeneity_inds and ensure indices are numeric
"TODO: bail out also if categorical columns, classification, random_state changed?"
TODO: should we also train a new model_y under any circumstances when warm_start is True?
train the Y model
"perform model selection for the Y model using all X, not on a per-column basis"
"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
work with the regression wrapper
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
"note that this needs to happen after wrapping to generalize to the multi-class case,"
since otherwise we'll have too many columns to be able to train a classifier
start with empty results and default shared insights
convert categorical indicators to numeric indices
check for indices over the categorical expansion bound
assume we'll be able to train former failures this time; we'll add them back if not
"can't remove in place while iterating over new_inds, so store in separate list"
"train the model, but warn"
no model can be trained in this case since we need more folds
"don't train a model, but suggest workaround since there are enough instances of least"
populated class
also remove from train_inds so we don't try to access the result later
extract subset of names matching new columns
"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
don't want to cache this failed result
properties to return from effect InferenceResults
properties to return from PopulationSummaryResults
Converts strings to property lookups or method calls as a convenience so that the
_point_props and _summary_props above can be applied to an inference object
Create a summary combining all results into a single output; this is used
by the various causal_effect and causal_effect_dict methods to generate either a dataframe
"or a dictionary, respectively, based on the summary function passed into this method"
"ensure array has shape (m,y,t)"
population summary is missing sample dimension; add it for consistency
outcome dimension is missing; add it for consistency
add singleton treatment dimension if missing
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
"each attr has dimension (m,y) or (m,y,t)"
concatenate along treatment dimension
"for dictionary representation, want to remove unneeded sample dimension"
in cohort and global results
TODO: enrich outcome logic for multi-class classification when that is supported
There is no actual sample level in this data
can't drop only level
should be serialization-ready and contain no numpy arrays
"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
TODO: Note that there's no column metadata for the sample number - should there be?
"need to replicate the column info for each sample, then remove from the shared data"
NOTE: the flattened order has the ouptut dimension before the feature dimension
which may need to be revisited once we support multiclass
get the length of the list corresponding to the first dictionary key
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
a global inference indicates the effect of that one feature on the outcome
need to reshape the output to match the input
we want to offset the inference object by the baseline estimate of y
"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
get the length of the list corresponding to the first dictionary key
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
"NOTE: this calculation is correct only if treatment costs are marginal costs,"
because then scaling the difference between treatment value and treatment costs is the
same as scaling the treatment value and subtracting the scaled treatment cost.
""
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
"continuous treatments, the policy value should include the benefit of decreasing treatments"
(rather than just not treating at all)
""
"We can get the total by seeing that if we restrict attention to units where we would treat,"
2 * policy_value - always_treat
includes exactly their contribution because policy_value and always_treat both include it
"and likewise restricting attention to the units where we want to decrease treatment,"
2 * policy_value - always-treat
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
is zero and the contribution to always_treat is negative
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
get dataframe with all but selected column
apply 10% of a typical treatment for this feature
"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely"
set the effect bounds; for positive treatments these agree with
"the estimates; for negative treatments, we need to invert the interval"
the effect is now always positive since we decrease treatment when negative
"for discrete treatment, stack a zero result in front for control"
we need to call effect_inference to get the correct CI between the two treatment options
we now need to construct the delta in the cost between the two treatments and translate the effect
remove third dimenions potentially added
"find cost of current treatment: equality creates a 2d array with True on each row,"
only if its the location of the current treatment. Then we take the corresponding cost.
construct index of current treatment
add second dimension if needed for broadcasting during translation of effect
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO: conisder working around relying on sklearn implementation details
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
only expose predict_proba if best_model has predict_proba
used because logic elsewhere uses hasattr predict proba to check if model is a classifier
logic copied from check_cv
otherwise we will assume the user already set the cv attribute to something
compatible with splitting with a `groups` argument
"drop groups from arg list, which were already used at the outer level and may not be supported by the model"
"since needs_fit is False, is_selecting will only be true if"
the score needs to be compared to another model's
"so we don't need to fit the model itself, just get the out-of-sample score"
use _fit_with_groups instead of just fit to handle nested grouping
we need to train the model on the data
copy common parameters
copy common fitted variables
make sure all classes agree on best c/l1 combo
"We need an R^2 score to compare to other models; ElasticNetCV doesn't provide it,"
but we can calculate it ourselves from the MSE plus the variance of the target y
"l1 ratio doesn't apply to Lasso, only ElasticNet"
max R^2 corresponds to min MSE
"constructor takes cv as a positional or kwarg, just pull it out of a new instance"
but it would be complicated to check that
"technically, if there is just one model and it doesn't need to be fit we don't need to fit it,"
but that complicates the training logic so we don't bother with that case
"If classification methods produce multiple columns of output,"
we need to manually encode classes to ensure consistent column ordering.
We clone the estimator to make sure that all the folds are
"independent, and that it is pickle-able."
verbose was removed from sklearn's non-public _fit_and_predict method in 1.4
`predictions` is a list of method outputs from each fold.
"If each of those is also a list, then treat this as a"
multioutput-multiclass task. We need to separately concatenate
the method outputs for each label into an `n_labels` long list.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
TODO: consider working around relying on sklearn implementation details
local import to avoid circular imports
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Call weighted lasso on reduced design matrix
Weighted tau
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set model to the single-target estimator by default so there's always a model to get and set attributes on
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
(e.g. former has 'positive' and 'precompute' while latter does not)
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
don't proxy special methods
"don't pass get_params through to model, because that will cause sklearn to clone this"
regressor incorrectly
"Note: for known attributes that have been set this method will not be called,"
so we should just throw here because this is an attribute belonging to this class
but which hasn't yet been set on this instance
set default values for None
check freq_weight should be integer and should be accompanied by sample_var
check array shape
weight X and y and sample_var
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
"For aggregation calculations, always treat wy as an array so that einsum expressions don't need to change"
We'll collapse results back down afterwards if necessary
"for federation, we need to store these 5 arrays when using heteroskedasticity-robust inference"
y dimension is always first in the output when present so that broadcasting works correctly
set default values for None
check array shape
check dimension of instruments is more than dimension of treatments
weight X and y
learn point estimate
solve first stage linear regression E[T|Z]
"""that"" means T̂"
solve second stage linear regression E[Y|that]
(T̂.T*T̂)^{-1}
learn cov(theta)
(T̂.T*T̂)^{-1}
sigma^2
reference: http://www.hec.unil.ch/documents/seminars/deep/361.pdf
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
average the outcome dimension if it exists and ensure 2d y_pred
get index of best treatment
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO: consider working around relying on sklearn implementation details
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Convert rows to columns
Require group assignment t to be one-hot-encoded
Get predictions for the 2 splits
Must make sure indices are merged correctly
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Must normalize weights
Override the CATE inference options
Add blb inference to parent's options
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
TODO: ideally the below private attribute logic should be in .fit but is needed in init
for nuisance estimator generation for parent class
should refactor later
Define nuisance estimators
Define parameter estimators
Define
Need to redefine fit here for auto inference to work due to a quirk in how
wrap_fit is defined
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Check that all discrete treatments are represented
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
returns shape-conforming residuals
Copy and/or define models
Define parameter estimators
Define moment and mean gradient estimator
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Call `fit` from parent class
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
override only so that we can exclude treatment featurization verbiage in docstring
Override to flatten output if T is flat
override only so that we can exclude treatment featurization verbiage in docstring
Expand one-hot encoding to include the zero treatment
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Calculate confidence intervals for the parameter (marginal effect)
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
d_t=None here since we measure the effect across all Ts
conditionally expand jacobian dimensions to align with einsum str
Calculate the effects
Calculate the standard deviations for the effects
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/main/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
TODO: enable type aliases
napoleon_preprocess_types = True  # needed for type aliases to work
napoleon_type_aliases = {
"""array_like"": "":term:`array_like`"","
"""ndarray"": ""~numpy.ndarray"","
"""RandomState"": "":class:`~numpy.random.RandomState`"","
"""DataFrame"": "":class:`~pandas.DataFrame`"","
"""Series"": "":class:`~pandas.Series`"","
}
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of strings:
""
"source_suffix = ['.rst', '.md']"
The root toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
def mlasso_model(): return MultiTaskLassoCV(
"cv=3, alphas=alpha_regs, max_iter=200)"
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
"alpha_regs = [5e-3, 1e-2, 5e-2]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
subset of features that are exogenous and create heterogeneity
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
subset of features wrt we estimate heterogeneity
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
introspect the constructor arguments to find the model parameters
to represent
"if the argument is deprecated, ignore it"
Extract and sort argument names excluding 'self'
column names
transfer input to numpy arrays
transfer input to 2d arrays
create dataframe
currently dowhy only support single outcome and single treatment
call dowhy
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
cate estimator but not the effect.
don't proxy special methods
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
For when checking input values is disabled
Type to column extraction function
if not all column names are strings
coerce feature names to be strings
Prefer sklearn 1.0's get_feature_names_out method to deprecated get_feature_names method
"Some featurizers will throw, such as a pipeline with a transformer that doesn't itself support names"
"Get number of arguments, some sklearn featurizer don't accept feature_names"
Handles cases where the passed feature names create issues
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
Get feature names using featurizer
All attempts at retrieving transformed feature names have failed
Delegate handling to downstream logic
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
Normalize weights
This class is mainly derived from statsmodels.iolib.summary.Summary
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
"want to enforce that each bad_arg was either in kwargs,"
or else it was in neither and is just taking its default value
Any access should throw
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
return plain dictionary so that erroneous accesses don't half work (see e.g. #708)
for every dimension of the treatment add some epsilon and observe change in featurized treatment
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
return plain dictionary so that erroneous accesses don't half work (see #708)
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
return plain dictionary so that erroneous accesses don't half work (see #708)
input feature name is already updated by cate_feature_names.
define the index of d_x to filter for each given T
filter X after broadcast with T for each given T
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
return plain dictionary so that erroneous accesses don't half work (see #708)
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
return plain dictionary so that erroneous accesses don't half work (see #708)
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
""
This code contains some snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_export.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
make any access to matplotlib or plt throw an exception
make any access to graphviz or plt throw an exception
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
Initialize saturation & value; calculate chroma & value shift
Calculate some intermediate values
Initialize RGB with same hue & chroma as our color
Shift the initial RGB values to match value and store
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use mean of targets
Write node mean CATE
Write node std of CATE
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
Fetch appropriate color for node
Write node mean CATE
Write node mean CATE
Write recommended treatment and value - cost
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
This flag is true when names are set in a child class instead
"If names are set in a child class, add an attribute reflecting that"
This works only if X is passed as a kwarg
We plan to enforce X as kwarg only in future releases
This checks if names have been set in a child class
"If names were set in a child class, don't do it again"
"Wraps-up fit by setting attributes, cleaning up, etc."
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
apply defaults before calling inference method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"Treatment names is None, default to BaseCateEstimator"
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
Get input names
Summary
add statsmodels to parent's options
add debiasedlasso to parent's options
add blb to parent's options
TODO Share some logic with non-discrete version
Get input names
Note: we do not transform feature names since that is done within summary_frame
Summary
add statsmodels to parent's options
add statsmodels to parent's options
add blb to parent's options
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
remove None arguments
"scores entries should be lists of scores, so make each entry a singleton list"
Adding the kwargs to ray object store to be used by remote functions for each fold to avoid IO overhead
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
generate an instance of the final model
generate an instance of the nuisance model
Define Ray remote function (Ray remote wrapper of the _fit_nuisances function)
Create Ray remote jobs for parallel processing
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
alteration even when we only want to fit_final.
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
upgrade to a GroupKFold or StratiGroupKFold if groups is not None
"we won't have generated a KFold or StratifiedKFold ourselves when groups are passed,"
"but the user might have supplied one, which won't work"
for each mc iteration
for each model under cross fit setting
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"TODO: This could be extended to also work with our sparse and 2SLS estimators,"
if we add an aggregate method to them
Remember to update the docs if this changes
mix in the appropriate inference class
assign all of the attributes from the dummy estimator that would normally be assigned during fitting
TODO: This seems hacky; is there a better abstraction to maintain these?
"This should also include bias_part_of_coef, model_final_, and fitted_models_final above"
Assign treatment expansion attributes
Methods needed to implement the LinearCateEstimator interface
Methods needed to implement the LinearFinalModelCateEstimatorMixin
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Policy Forest
=============================================================================
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base Policy tree
=============================================================================
The values below are required and utilitized by methods in the _SingleTreeExporterMixin
HACK: sklearn 1.3 enforces that the input to plot_tree is a DecisionTreeClassifier or DecisionTreeRegressor
This is a hack to get around that restriction by declaring that PolicyTree inherits from DecisionTreeClassifier
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"Unique treatments (ordered, includes control)"
Number of treatments (excluding control)
Indicator for whether
Get DR outcomes in training sample
Get DR outcomes in validation sample
Get DR outcomes in validation sample
Calculate ATE in the validation sample
Fit propensity in treatment
Predict propensity scores
Possible treatments (need to allow more than 2)
Predict outcomes
T-learner logic
"if CATE is given explicitly or has not been fitted at all previously, fit it now"
Assign units in validation set to groups
Proportion of validations set in group
Group average treatment effect (GATE) -- average of DR outcomes in group
Average of CATE predictions in group
Calculate group calibration score
Calculate overall calibration score
Calculate R-square calibration score
"if CATE is given explicitly or has not been fitted at all previously, fit it now"
treat each treatment as a separate regression
"here, prop_preds should be a matrix"
with rows corresponding to units and columns corresponding to treatment statuses
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"since we only allow single dimensional y, we could flatten the prediction"
override only so that we can exclude treatment featurization verbiage in docstring
override only so that we can exclude treatment featurization verbiage in docstring
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Handles the corner case when X=None but featurizer might be not None
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
TODO: support freq_weight and sample_var in debiased lasso
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
Replacing to remove docstring
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
data is already validated at initial fit time
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE: important to use the rlearner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the rlearner's copy to retain the information from fitting
Handles the corner case when X=None but featurizer might be not None
override only so that we can update the docstring to indicate support for `LinearModelFinalInference`
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support freq_weight and sample_var in debiased lasso
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add blb to parent's options
override only so that we can update the docstring to indicate
support for `GenericSingleTreatmentModelFinalInference`
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
note that groups are not passed to score because they are only used for fitting
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
NOTE: important to get parent's wrapped copy so that
"after training wrapped featurizer is also trained, etc."
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
Fit a doubly robust average effect
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
"If custom param grid, check that only estimator parameters are being altered"
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
override only so that we can update the docstring to indicate support for `blb`
Get input names
Summary
Determine output settings
"Important: This must be the first invocation of the random state at fit time, so that"
train/test splits are re-generatable from an external object simply by knowing the
random_state parameter of the tree. Can be useful in the future if one wants to create local
linear predictions. Currently is also useful for testing.
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
Set min_weight_leaf from min_weight_fraction_leaf
Build tree
We calculate the maximum number of samples from each half-split that any node in the tree can
hold. Used by criterion for memory space savings.
Initialize the criterion object and the criterion_val object if honest.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
""
This code is a fork from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_base.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
Set parameters
Don't instantiate estimators now! Parameters of base_estimator might
"still change. Eg., when grid-searching with the nested object syntax."
self.estimators_ needs to be filled by the derived classes in fit.
Compute the number of jobs
Partition estimators between jobs
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
covariance matrix
get eigen value and eigen vectors
simulate eigen vectors
keep the top 4 eigen value and corresponding eigen vector
replace the negative eigen values
generate a new covariance matrix
get linear approximation of eigen values
coefs
get the indices of each group of features
print(ind_same_proxy)
demo
same proxy
residuals
gmm
log normal on outliers
positive outliers
negative outliers
demean the new residual again
generate data
sample residuals
get prediction for current investment
get prediction for current proxy
get first period prediction
iterate the step ahead contruction
prepare new x
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
get new covariance matrix
get coefs
get residuals
proxy 1 is the outcome
make fixed residuals
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=None here since we measure the effect across all Ts
"y is a vector, rather than a 2D array"
once the estimator has been fit
"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
an intercept even when bias is part of coef.
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
squeeze the first axis
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
squeeze the first axis
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
d_t=None here since we measure the effect across all Ts
d_t=None here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
scale preds
scale std errs
"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
offset preds
"offset the distribution, too"
scale preds
"scale the distribution, too"
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
"if stderr is zero, ppf will return nans and the loop below would never terminate"
so bail out early; note that it might be possible to correct the algorithm for
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
be clean
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
don't proxy special methods
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
second level bootstrap which would be prohibitive computationally?
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
can't import from econml.inference at top level without creating cyclical dependencies
Note that inference results are always methods even if the inference is for a property
(e.g. coef__inference() is a method but coef_ is a property)
Therefore we must insert a lambda if getting inference for a non-callable
"If inference is for a property, create a fresh lambda to avoid passing args through"
"try to get interval/std first if appropriate,"
since we don't prefer a wrapped method with this name
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base GRF tree
=============================================================================
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
=============================================================================
A MultOutputWrapper for GRF classes
=============================================================================
=============================================================================
Instantiations of Generalized Random Forest
=============================================================================
"Append a constant treatment if `fit_intercept=True`, the coefficient"
in front of the constant treatment is the intercept in the moment equation.
"Append a constant treatment and constant instrument if `fit_intercept=True`,"
the coefficient in front of the constant treatment is the intercept in the moment equation.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Base Generalized Random Forest
=============================================================================
TODO: support freq_weight and sample_var
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
We calculate the min eigenvalue proxy that each criterion is considering
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Generating indices a priori before parallelism ended up being orders of magnitude
faster than how sklearn does it. The reason is that random samplers do not release the
gil it seems.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
####################
Variance correction
####################
Subtract the average within bag variance. This ends up being equal to the
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
The negative part is just sq_between.
Objective bayes debiasing for the diagonals where we know a-prior they are positive
"The off diagonals we have no objective prior, so no correction is applied."
Finally correcting the pred_cov or pred_var
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO: update docs
"NOTE: sample weight, sample var are not passed in"
Compose final model
Calculate auxiliary quantities
X ⨂ T_res
"sum(model_final.predict(X, T_res))"
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix J"
override only so that we can exclude treatment featurization verbiage in docstring
override only so that we can exclude treatment featurization verbiage in docstring
"we need to set the number of periods before calling super()._prefit, since that will generate the"
"final and nuisance models, which need to have self._n_periods set"
Set _d_t to effective number of treatments
Required for bootstrap inference
for each mc iteration
for each model under cross fit setting
Handles the corner case when X=None but featurizer might be not None
Expand treatments for each time period
NOTE: important to use the _ortho_learner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the _ortho_learner's copy to retain the information from fitting
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
test that the subsampling scheme past to the trees is correct
The sample size is chosen in particular to test rounding based error when subsampling
test that the estimator calcualtes var correctly
test api
test accuracy
test the projection functionality of forests
test that the estimator calcualtes var correctly
test api
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
test that the subsampling scheme past to the trees is correct
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
filter directories by regex if the NOTEBOOK_DIR_PATTERN environment variable is set
omit the lalonde notebook
make sure that coverage outputs reflect notebook contents
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
"remove added coverage cell, then decrement execution_count for other cells to account for it"
create directory if necessary
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
can interpret without uncertainty
can't treat before interpreting
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
for is_discrete in [False]:
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
No heterogeneity
Define indices to test
Heterogeneous effects
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
pass sample weight to final step of pipeline
create data with missing values
model that can handle missing values
"test X, W only"
test W only
dowhy does not support missing values in X
assert that fitting with missing values fails when allow_missing is False
and that setting allow_missing after init still works
assert that we fail with a value error when we pass missing X to a model that doesn't support it
assert that fitting with missing values fails when allow_missing is False
and that setting allow_missing after init still works
metalearners don't support W
metalearners do support missing values in X
dowhy never supports missing values in X
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
identity featurization effect functions
polynomial featurization effect functions
1d polynomial featurization functions
2d-to-1d featurization functions
2d-to-1d vector featurization functions
use LassoCV rather than also selecting over RandomForests to save time
test that treatment names are assigned for the featurized treatment
expected shapes
check effects
ate
loose inference checks
temporarily skip LinearDRIV and SparseLinearDRIV for weird effect shape reasons
effect inference
marginal effect inference
const marginal effect inference
fit a dummy estimator first so the featurizer can be fit to the treatment
edge case with transformer that only takes a vector treatment
so far will always return None for cate_treatment_names
assert proper handling of improper feature names passed to certain transformers
"depending on sklearn version, bad feature names either throws error or only uses first relevant name"
ensure alpha is passed
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
initialize parameters
initialize config wtih base config and overwite some values
predict tree using config parameters and assert
shape of trained tree is the same as y_test
initialize config wtih base honest config and overwite some values
predict tree using config parameters and assert
shape of trained tree is the same as y_test
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
compare with weight
compare with weight
compare with weight
compare with weight
Multi-dimensional output y
1-d y
compare when both sample_var and sample_weight exist
multi-d y
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
dgp
StatsModels2SLS
IV2SLS
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
fixed functions as first stage models
they can be anything as long as fitting doesn't modify the predictions
"that way, it doesn't matter if they are trained on different subsets of the data"
test coefficients
test effects
fixed functions as first stage models
they can be anything as long as fitting doesn't modify the predictions
"that way, it doesn't matter if they are trained on different subsets of the data"
test coefficients
test effects
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
test that the subsampling scheme past to the trees is correct
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test inference results when `cate_feature_names` doesn not exist
Test inference results when `cate_feature_names` doesn not exist
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
pvalue for second column should be greater than zero since some points are on either side
of the tested value
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
ensure alpha is passed
only is not None when T1 is a constant or a list of constant
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Generate synthetic data
Run _crossfit with Ray enabled
Run _crossfit without Ray
Compare the results
"Nuisance model has no score method, so nuisance_scores_ should be none"
Test non keyword based calls to fit
test non-array inputs
Test custom splitter
Test incomplete set of test folds
"y scores should be positive, since W predicts Y somewhat"
"t scores might not be, since W and T are uncorrelated"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
creating an instance should warn
using the instance should not warn
using the deprecated method should warn
don't warn if b and c are passed by keyword
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
make any access to matplotlib or plt throw an exception
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
Invert indices to match latest API
Invert indices to match latest API
The feature for heterogeneity stays constant
Auxiliary function for adding xticks and vertical lines when plotting results
for dynamic dml vs ground truth parameters.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
test at least one estimator from each category
test causal graph
test refutation estimate
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Generate random Xs
Random covariance matrix of Xs
Effect of Xs on outcome
Effect of treatment on outcomes
Effect of treatment on outcome conditional on X1
Generate treatments based on X and random noise
"Generate Y (based on X, D, and random noise)"
"Simple classifier and regressor for propensity, outcome, and cate"
test the DR outcome difference
"Simple classifier and regressor for propensity, outcome, and cate"
test the DR outcome difference
"Simple classifier and regressor for propensity, outcome, and cate"
test the DR outcome difference
use evaluate_blp to fit on validation only
"Simple classifier and regressor for propensity, outcome, and cate"
test the DR outcome difference
fit nothing
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"since we're running so many combinations, just use LassoCV/LogisticRegressionCV"
for the models instead of also selecting over random forest models
ensure we can serialize unfit estimator
ensure we can serialize fit estimator
expected effect size
test effect
test inference
only OrthoIV support inference other than bootstrap
test summary
test can run score
test cate_feature_names
test can run shap values
dgp
no heterogeneity
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"if we aren't fitting on the whole dataset, ensure that the limits are respected"
ensure that the grouping has worked correctly and we get exactly the number of copies
of the items in whichever groups we see
DML nested CV works via a 'cv' attribute
"want to validate the nested grouping, not the outer grouping in the nesting tests"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
parameter combinations to test
"we're running a lot of tests, so use fixed models instead of model selection"
"IntentToTreatDRIV only supports binary treatments and instruments, and doesn't support fit_cov_directly"
TODO: serializing/deserializing for every combination -- is this necessary?
ensure we can serialize unfit estimator
ensure we can serialize fit estimator
expected effect size
assert calculated constant marginal effect shape is expected
const_marginal effect is defined in LinearCateEstimator class
assert calculated marginal effect shape is expected
test inference
test can run score
test cate_feature_names
test can run shap values
"dgp (binary T, binary Z)"
no heterogeneity
with heterogeneity
fitting the covariance directly should be at least as good as computing the covariance from separate models
set the models so that model selection over random forests doesn't take too much time in the repeated trials
directly fitting the covariance should be better than indirectly fitting it
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Create a wrapper around Lasso that doesn't support weights
since Lasso does natively support them starting in sklearn 0.23
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least 6 of every element
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
NOTE: this number may need to change if the default number of folds in
WeightedStratifiedKFold changes
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
make sure we can get out post-fit stuff
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
predetermined splits ensure that all features are seen in each split
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
"with this DGP, since T depends linearly on X, Y depends on X quadratically"
so we should use a quadratic featurizer
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
"we have quadratic terms in y, so we need to pipeline with a quadratic featurizer"
Compare results with and without Ray
create a simple artificial setup where effect of moving from treatment
"a -> b is 2,"
"a -> c is 1, and"
"b -> c is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
should rule out some basic issues.
Note that explicitly specifying the dtype as object is necessary until
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
estimated effects should be identical when treatment is explicitly given
but const_marginal_effect should be reordered based on the explicit cagetories
1-> 2 in original ordering; combination of 3->1 and 3->2
test outer grouping
"with 2 folds, we should get exactly 3 groups per split, each with 10 copies of the y or t value"
test nested grouping
"with 2-fold outer and 2-fold inner grouping, and six total groups,"
should get 1 or 2 groups per split
"Try default, integer, and new user-passed treatment name"
FunctionTransformers are agnostic to passed treatment names
Expected treatment names are the sums of user-passed prefixes and transformer-specific postfixes
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure that we can serialize unfit estimator
ensure that we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef_ and intercept_ inference
verify we can generate the summary
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
test outer grouping
NOTE: StratifiedGroupKFold has a bug when shuffle is True where it doesn't always stratify properly
so we explicitly pass a StratifiedGroupKFold with shuffle=False (the default) rather than letting
cross-fit generate one
"with 2-fold grouping, we should get exactly 3 groups per split"
test nested grouping
"with 2-fold outer and 2-fold inner grouping, we should get 1-2 groups per split"
helper class
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
#########################
WeightedLassoCVWrapper #
#########################
perform 1D fit
perform 2D fit
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
_penalized_inds is only set during fitting
cv exists on penalized model
now we can access _penalized_inds
check that we can read the cv attribute back out from the underlying model
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
continuous treatments have typical treatment values equal to
the mean of the absolute value of non-zero entries
discrete treatments have typical treatment value 1
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
continuous treatments have typical treatment values equal to
the mean of the absolute value of non-zero entries
discrete treatments have typical treatment value 1
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
make sure we don't run into problems dropping every index
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
dgp
model
model
"columns 'd', 'e', 'h' have too many values"
"columns 'd', 'e' have too many values"
lowering bound shouldn't affect already fit columns when warm starting
"column d is now okay, too"
verify that we can use a scalar treatment cost
verify that we can specify per-treatment costs for each sample
verify that using the same state returns the same results each time
set the categories for column 'd' explicitly so that b is default
"first column: 10 ones, this is fine"
"second column: 6 categories, plenty of random instances of each"
this is fine only if we increase the cateogry limit
"third column: nine ones, lots of twos, not enough unless we disable check"
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
forest heterogeneity won't work
"sixth column: just 1 one, not enough even without check"
increase bound on cat expansion
skip checks (reducing folds accordingly)
"Add tests that guarantee that the reliance on DML feature order is not broken, such as"
"Creare a transformer that zeros out all variables after the first n_x variables, so it zeros out W"
Pass an example where W is irrelevant and X is confounder
"As long as DML doesnt change the order of the inputs, then things should be good. Otherwise X would be"
zeroed out and the test will fail
"shouldn't matter if X is scaled much larger or much smaller than W, we should still get good estimates"
rescaling X shouldn't affect the first stage models because they normalize the inputs
"to recover individual coefficients with linear models, we need to be more careful in how we set up X to avoid"
cross terms
scale by 1000 to match the input to this model:
"the scale of X does matter for the final model, which keeps results in user-denominated units"
rescaling X still shouldn't affect the first stage models
TODO: we don't recover the correct values with enough accuracy to enable this assertion
is there a different way to verify that we are learning the correct coefficients?
"np.testing.assert_allclose(loc1.point.values, theta.flatten(), rtol=1e-1)"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
DGP constants
Define data features
Added `_df`to names to be different from the default cate_estimator names
Generate data
################################
Single treatment and outcome #
################################
Test LinearDML
|--> Test featurizers
"ColumnTransformer behaves differently depending on version of sklearn, so we no longer check the names"
|--> Test re-fit
Test SparseLinearDML
Test ForestDML
###################################
Mutiple treatments and outcomes #
###################################
Test LinearDML
Test SparseLinearDML
"Single outcome only, ORF does not support multiple outcomes"
Test DMLOrthoForest
Test DROrthoForest
Test XLearner
Skipping population summary names test because bootstrap inference is too slow
Test SLearner
Test TLearner
Test LinearDRLearner
Test SparseLinearDRLearner
Test ForestDRLearner
Test LinearIntentToTreatDRIV
Test DeepIV
Test categorical treatments
Check refit
Check refit after setting categories
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
import here since otherwise test collection would fail if matplotlib is not installed
Treatment effect function
Outcome support
Treatment support
"Generate controls, covariates, treatments and outcomes"
Heterogeneous treatment effects
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
through shap package.
test shap could generate the plot from the shap_values
"waterfall is broken in this version, fixed by https://github.com/slundberg/shap/pull/2444"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
"Note: unlike other Metalearners, we need the controls' encoded column for training"
"Thus, we append the controls column before the one-hot-encoded T"
"We might want to revisit, though, since it's linearly determined by the others"
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
"need to fit, too, since we call predict later inside this train method"
"need to fit, too, since we call predict later inside this train method"
"need to fit, too, since we call predict later inside this train method"
"We're projecting, so we're treating E[T|X,Z] as the instrument (ignoring W for simplicity)"
"Then beta(X) = E[T̃ (E[T|X,Z]-E[E[T|X,Z]|X)|X] and we can apply the tower rule several times to get"
"= E[(E[T|X,Z]-E[T|X])^2|X]"
"and also     = E[(E[T|X,Z]-T)^2|X]"
so we can compute it either from (T_proj-T_pred)^2 or from (T_proj-T)^2
"return shape (n,)"
"need to avoid erroneous broadcasting when one of Z_res or T_res is (n,1) and the other is (n,)"
"TODO: if the T and Z models overfit, then this will be biased towards 0;"
consider using nested cross-fitting
a similar comment applies to the projection case
"target will be discrete and will be inversed from FirstStageWrapper, shape (n,1)"
"shape (n,)"
"shape (n,)"
"shape(n,)"
TODO: prel_model_effect could allow sample_var and freq_weight?
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"if discrete, return shape (n,1); if continuous return shape (n,)"
target will be discrete and will be inversed from FirstStageWrapper
"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous"
reshape the predictions
concat W and Z
"in the projection case, this is a variance and should always be non-negative"
check nuisances outcome shape
Y_res could be a vector or 1-dimensional 2d-array
"all could be reshaped to vector since Y, T, Z are all single dimensional."
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
A helper class that access all the internal fitted objects of a DRIV Cate Estimator.
Used by both DRIV and IntentToTreatDRIV.
Maggie: I think that would be the case?
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
NOTE: important to use the ortho_learner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
Handles the corner case when X=None but featurizer might be not None
NOTE This is used by the inference methods and is more for internal use to the library
"this is a regression model since the instrument E[T|X,W,Z] is always continuous"
"we're using E[T|X,W,Z] as the instrument"
Define the data generation functions
Define the data generation functions
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
Define the data generation functions
TODO: support freq_weight and sample_var in debiased lasso
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
Define the data generation functions
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
concat W and Z
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
concat W and Z
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
reshape the predictions
"T_res, Z_res, beta expect shape to be (n,1)"
Define the data generation functions
maybe shouldn't expose fit_cate_intercept in this class?
Define the data generation functions
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: do correct adjustment for sample_var
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
concat W and Z
concat W and Z
concat W and Z
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
Define the data generation functions
"train E[T|X,W,Z]"
"train E[Z|X,W]"
note: discrete_instrument rather than discrete_treatment in call to _make_first_stage_selector
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
NOTE: important to use the ortho_learner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
Handles the corner case when X=None but featurizer might be not None
NOTE This is used by the inference methods and is more for internal use to the library
concat W and Z
note that groups are not passed to score because they are only used for fitting
concat W and Z
note that sample_weight and groups are not passed to predict because they are only used for fitting
concat W and Z
A helper class that access all the internal fitted objects of a DMLIV Cate Estimator.
Used by both Parametric and Non Parametric DMLIV.
override only so that we can enforce Z to be required
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Handles the corner case when X=None but featurizer might be not None
Define the data generation functions
Get input names
Summary
coefficient
intercept
Define the data generation functions
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
make T 2D if if was a vector
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO: this utility is documented but internal; reimplement?
TODO: this utility is even less public...
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
use same Cs as would be used by default by LogisticRegressionCV
NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification
which could affect how many times each distinct Y value needs to be present in the data
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
but also supports get_feature_names with expected signature
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
NOTE: we rely on the passthrough columns coming first in the concatenated X;W
"when we pipeline scaling with our first stage models later, so the order here is important"
TODO: remove once older sklearn support is no longer needed
Wrapper to make sure that we get a deep copy of the contents instead of clone returning an untrained copy
Convert python objects to (possibly nested) types that can easily be represented as literals
Convert SingleTreeInterpreter to a python dictionary
named tuple type for storing results inside CausalAnalysis class;
must be lifted to module level to enable pickling
"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,"
"whether they end up in X or in W.  However, for the continuous columns, we want to scale them all"
"when running the first stage models, but don't want to scale the X columns when running the final model,"
since then our coefficients will have odd units and our trees will also have decisions using those units.
""
"we achieve this by pipelining the X scaling with the Y and T models (with fixed scaling, not refitting)"
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
Controls are all other columns of X
"can't use X[:, feat_ind] when X is a DataFrame"
TODO: we can't currently handle unseen values of the feature column when getting the effect;
we might want to modify OrthoLearner (and other discrete treatment classes)
so that the user can opt-in to allowing unseen treatment values
(and return NaN or something in that case)
HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models
and so we can just peel the first columns off of that combined array for rescaling in the pipeline
TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are
"built, such as model_y_feature_names, model_t_feature_names, model_y_transformer, etc., so that this"
becomes a valid approach to handling this
array checking routines don't accept 0-width arrays
perform model selection
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
convert to NormalInferenceResults for consistency
Set the dictionary values shared between local and global summaries
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
required to fit a discrete DML model
"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular"
sub-cases of models or also integrate with azure autoML. (post-MVP)
"TODO: Add other heterogeneity model options, such as {'automl'} for performing"
"model selection for the causal effect, or {'sparse_linear'} for using a debiased lasso. (post-MVP)"
TODO: Enable multi-class classification (post-MVP)
Validate inputs
TODO: check compatibility of X and Y lengths
"no previous fit, cancel warm start"
"work with numeric feature indices, so that we can easily compare with categorical ones"
"if heterogeneity_inds is 1D, repeat it"
heterogeneity inds should be a 2D list of length same as train_inds
replace None elements of heterogeneity_inds and ensure indices are numeric
"TODO: bail out also if categorical columns, classification, random_state changed?"
TODO: should we also train a new model_y under any circumstances when warm_start is True?
train the Y model
"perform model selection for the Y model using all X, not on a per-column basis"
"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
work with the regression wrapper
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
"note that this needs to happen after wrapping to generalize to the multi-class case,"
since otherwise we'll have too many columns to be able to train a classifier
start with empty results and default shared insights
convert categorical indicators to numeric indices
check for indices over the categorical expansion bound
assume we'll be able to train former failures this time; we'll add them back if not
"can't remove in place while iterating over new_inds, so store in separate list"
"train the model, but warn"
no model can be trained in this case since we need more folds
"don't train a model, but suggest workaround since there are enough instances of least"
populated class
also remove from train_inds so we don't try to access the result later
extract subset of names matching new columns
"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
don't want to cache this failed result
properties to return from effect InferenceResults
properties to return from PopulationSummaryResults
Converts strings to property lookups or method calls as a convenience so that the
_point_props and _summary_props above can be applied to an inference object
Create a summary combining all results into a single output; this is used
by the various causal_effect and causal_effect_dict methods to generate either a dataframe
"or a dictionary, respectively, based on the summary function passed into this method"
"ensure array has shape (m,y,t)"
population summary is missing sample dimension; add it for consistency
outcome dimension is missing; add it for consistency
add singleton treatment dimension if missing
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
"each attr has dimension (m,y) or (m,y,t)"
concatenate along treatment dimension
"for dictionary representation, want to remove unneeded sample dimension"
in cohort and global results
TODO: enrich outcome logic for multi-class classification when that is supported
There is no actual sample level in this data
can't drop only level
should be serialization-ready and contain no numpy arrays
"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
TODO: Note that there's no column metadata for the sample number - should there be?
"need to replicate the column info for each sample, then remove from the shared data"
NOTE: the flattened order has the ouptut dimension before the feature dimension
which may need to be revisited once we support multiclass
get the length of the list corresponding to the first dictionary key
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
a global inference indicates the effect of that one feature on the outcome
need to reshape the output to match the input
we want to offset the inference object by the baseline estimate of y
"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
get the length of the list corresponding to the first dictionary key
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
"NOTE: this calculation is correct only if treatment costs are marginal costs,"
because then scaling the difference between treatment value and treatment costs is the
same as scaling the treatment value and subtracting the scaled treatment cost.
""
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
"continuous treatments, the policy value should include the benefit of decreasing treatments"
(rather than just not treating at all)
""
"We can get the total by seeing that if we restrict attention to units where we would treat,"
2 * policy_value - always_treat
includes exactly their contribution because policy_value and always_treat both include it
"and likewise restricting attention to the units where we want to decrease treatment,"
2 * policy_value - always-treat
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
is zero and the contribution to always_treat is negative
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
get dataframe with all but selected column
apply 10% of a typical treatment for this feature
"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely"
set the effect bounds; for positive treatments these agree with
"the estimates; for negative treatments, we need to invert the interval"
the effect is now always positive since we decrease treatment when negative
"for discrete treatment, stack a zero result in front for control"
we need to call effect_inference to get the correct CI between the two treatment options
we now need to construct the delta in the cost between the two treatments and translate the effect
remove third dimenions potentially added
"find cost of current treatment: equality creates a 2d array with True on each row,"
only if its the location of the current treatment. Then we take the corresponding cost.
construct index of current treatment
add second dimension if needed for broadcasting during translation of effect
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO: conisder working around relying on sklearn implementation details
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
logic copied from check_cv
otherwise we will assume the user already set the cv attribute to something
compatible with splitting with a `groups` argument
"drop groups from arg list, which were already used at the outer level and may not be supported by the model"
"whether selecting or not, need to train the model on the data"
"TODO: we need to alter this to use out-of-sample score here, which"
"will require cross-validation, but should respect grouping, stratifying, etc."
copy common parameters
copy common fitted variables
copy attributes unique to this class
make sure all classes agree on best c/l1 combo
"l1 ratio doesn't apply to Lasso, only ElasticNet"
don't need to use _fit_with_groups here since none of these models support it
"If classification methods produce multiple columns of output,"
we need to manually encode classes to ensure consistent column ordering.
We clone the estimator to make sure that all the folds are
"independent, and that it is pickle-able."
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
`predictions` is a list of method outputs from each fold.
"If each of those is also a list, then treat this as a"
multioutput-multiclass task. We need to separately concatenate
the method outputs for each label into an `n_labels` long list.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
TODO: consider working around relying on sklearn implementation details
local import to avoid circular imports
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Call weighted lasso on reduced design matrix
Weighted tau
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set model to the single-target estimator by default so there's always a model to get and set attributes on
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
(e.g. former has 'positive' and 'precompute' while latter does not)
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
don't proxy special methods
"don't pass get_params through to model, because that will cause sklearn to clone this"
regressor incorrectly
"Note: for known attributes that have been set this method will not be called,"
so we should just throw here because this is an attribute belonging to this class
but which hasn't yet been set on this instance
set default values for None
check freq_weight should be integer and should be accompanied by sample_var
check array shape
weight X and y and sample_var
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
"For aggregation calculations, always treat wy as an array so that einsum expressions don't need to change"
We'll collapse results back down afterwards if necessary
"for federation, we need to store these 5 arrays when using heteroskedasticity-robust inference"
y dimension is always first in the output when present so that broadcasting works correctly
set default values for None
check array shape
check dimension of instruments is more than dimension of treatments
weight X and y
learn point estimate
solve first stage linear regression E[T|Z]
"""that"" means T̂"
solve second stage linear regression E[Y|that]
(T̂.T*T̂)^{-1}
learn cov(theta)
(T̂.T*T̂)^{-1}
sigma^2
reference: http://www.hec.unil.ch/documents/seminars/deep/361.pdf
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
average the outcome dimension if it exists and ensure 2d y_pred
get index of best treatment
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO: consider working around relying on sklearn implementation details
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Convert rows to columns
Require group assignment t to be one-hot-encoded
Get predictions for the 2 splits
Must make sure indices are merged correctly
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Must normalize weights
Override the CATE inference options
Add blb inference to parent's options
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
TODO: ideally the below private attribute logic should be in .fit but is needed in init
for nuisance estimator generation for parent class
should refactor later
Define nuisance estimators
Define parameter estimators
Define
Need to redefine fit here for auto inference to work due to a quirk in how
wrap_fit is defined
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Check that all discrete treatments are represented
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
returns shape-conforming residuals
Copy and/or define models
Define parameter estimators
Define moment and mean gradient estimator
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Call `fit` from parent class
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
override only so that we can exclude treatment featurization verbiage in docstring
Override to flatten output if T is flat
override only so that we can exclude treatment featurization verbiage in docstring
Expand one-hot encoding to include the zero treatment
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Calculate confidence intervals for the parameter (marginal effect)
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
d_t=None here since we measure the effect across all Ts
conditionally expand jacobian dimensions to align with einsum str
Calculate the effects
Calculate the standard deviations for the effects
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/main/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
TODO: enable type aliases
napoleon_preprocess_types = True  # needed for type aliases to work
napoleon_type_aliases = {
"""array_like"": "":term:`array_like`"","
"""ndarray"": ""~numpy.ndarray"","
"""RandomState"": "":class:`~numpy.random.RandomState`"","
"""DataFrame"": "":class:`~pandas.DataFrame`"","
"""Series"": "":class:`~pandas.Series`"","
}
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of strings:
""
"source_suffix = ['.rst', '.md']"
The root toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
def mlasso_model(): return MultiTaskLassoCV(
"cv=3, alphas=alpha_regs, max_iter=200)"
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
"alpha_regs = [5e-3, 1e-2, 5e-2]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
subset of features that are exogenous and create heterogeneity
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
subset of features wrt we estimate heterogeneity
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
introspect the constructor arguments to find the model parameters
to represent
"if the argument is deprecated, ignore it"
Extract and sort argument names excluding 'self'
column names
transfer input to numpy arrays
transfer input to 2d arrays
create dataframe
currently dowhy only support single outcome and single treatment
call dowhy
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
cate estimator but not the effect.
don't proxy special methods
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
For when checking input values is disabled
Type to column extraction function
if not all column names are strings
coerce feature names to be strings
Prefer sklearn 1.0's get_feature_names_out method to deprecated get_feature_names method
"Some featurizers will throw, such as a pipeline with a transformer that doesn't itself support names"
"Get number of arguments, some sklearn featurizer don't accept feature_names"
Handles cases where the passed feature names create issues
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
Get feature names using featurizer
All attempts at retrieving transformed feature names have failed
Delegate handling to downstream logic
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
assume that we should perform nested cross-validation if and only if
the model has a 'cv' attribute; this is a somewhat brittle assumption...
logic copied from check_cv
otherwise we will assume the user already set the cv attribute to something
compatible with splitting with a 'groups' argument
now we have to compute the folds explicitly because some classifiers (like LassoCV)
don't use the groups when calling split internally
Normalize weights
This class is mainly derived from statsmodels.iolib.summary.Summary
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
"want to enforce that each bad_arg was either in kwargs,"
or else it was in neither and is just taking its default value
Any access should throw
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
return plain dictionary so that erroneous accesses don't half work (see e.g. #708)
for every dimension of the treatment add some epsilon and observe change in featurized treatment
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
return plain dictionary so that erroneous accesses don't half work (see #708)
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
return plain dictionary so that erroneous accesses don't half work (see #708)
input feature name is already updated by cate_feature_names.
define the index of d_x to filter for each given T
filter X after broadcast with T for each given T
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
return plain dictionary so that erroneous accesses don't half work (see #708)
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
return plain dictionary so that erroneous accesses don't half work (see #708)
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
""
This code contains some snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_export.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
make any access to matplotlib or plt throw an exception
make any access to graphviz or plt throw an exception
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
Initialize saturation & value; calculate chroma & value shift
Calculate some intermediate values
Initialize RGB with same hue & chroma as our color
Shift the initial RGB values to match value and store
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use mean of targets
Write node mean CATE
Write node std of CATE
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
Fetch appropriate color for node
Write node mean CATE
Write node mean CATE
Write recommended treatment and value - cost
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
This flag is true when names are set in a child class instead
"If names are set in a child class, add an attribute reflecting that"
This works only if X is passed as a kwarg
We plan to enforce X as kwarg only in future releases
This checks if names have been set in a child class
"If names were set in a child class, don't do it again"
"Wraps-up fit by setting attributes, cleaning up, etc."
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"Treatment names is None, default to BaseCateEstimator"
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
Get input names
Summary
add statsmodels to parent's options
add debiasedlasso to parent's options
add blb to parent's options
TODO Share some logic with non-discrete version
Get input names
Note: we do not transform feature names since that is done within summary_frame
Summary
add statsmodels to parent's options
add statsmodels to parent's options
add blb to parent's options
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
remove None arguments
"scores entries should be lists of scores, so make each entry a singleton list"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
generate an instance of the final model
generate an instance of the nuisance model
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
alteration even when we only want to fit_final.
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
"however, sklearn doesn't support both stratifying and grouping (see"
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
their own object that supports grouping if they want to use groups.
for each mc iteration
for each model under cross fit setting
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Policy Forest
=============================================================================
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base Policy tree
=============================================================================
The values below are required and utilitized by methods in the _SingleTreeExporterMixin
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"since we only allow single dimensional y, we could flatten the prediction"
override only so that we can exclude treatment featurization verbiage in docstring
override only so that we can exclude treatment featurization verbiage in docstring
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Handles the corner case when X=None but featurizer might be not None
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
TODO: support freq_weight and sample_var in debiased lasso
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
Replacing to remove docstring
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE: important to use the rlearner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the rlearner's copy to retain the information from fitting
Handles the corner case when X=None but featurizer might be not None
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `LinearModelFinalInference`
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support freq_weight and sample_var in debiased lasso
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add blb to parent's options
override only so that we can update the docstring to indicate
support for `GenericSingleTreatmentModelFinalInference`
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
note that groups are not passed to score because they are only used for fitting
note that groups are not passed to score because they are only used for fitting
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
NOTE: important to get parent's wrapped copy so that
"after training wrapped featurizer is also trained, etc."
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
Fit a doubly robust average effect
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
"If custom param grid, check that only estimator parameters are being altered"
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
override only so that we can update the docstring to indicate support for `blb`
Get input names
Summary
Determine output settings
"Important: This must be the first invocation of the random state at fit time, so that"
train/test splits are re-generatable from an external object simply by knowing the
random_state parameter of the tree. Can be useful in the future if one wants to create local
linear predictions. Currently is also useful for testing.
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
Set min_weight_leaf from min_weight_fraction_leaf
Build tree
We calculate the maximum number of samples from each half-split that any node in the tree can
hold. Used by criterion for memory space savings.
Initialize the criterion object and the criterion_val object if honest.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
""
This code is a fork from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_base.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
Set parameters
Don't instantiate estimators now! Parameters of base_estimator might
"still change. Eg., when grid-searching with the nested object syntax."
self.estimators_ needs to be filled by the derived classes in fit.
Compute the number of jobs
Partition estimators between jobs
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
covariance matrix
get eigen value and eigen vectors
simulate eigen vectors
keep the top 4 eigen value and corresponding eigen vector
replace the negative eigen values
generate a new covariance matrix
get linear approximation of eigen values
coefs
get the indices of each group of features
print(ind_same_proxy)
demo
same proxy
residuals
gmm
log normal on outliers
positive outliers
negative outliers
demean the new residual again
generate data
sample residuals
get prediction for current investment
get prediction for current proxy
get first period prediction
iterate the step ahead contruction
prepare new x
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
get new covariance matrix
get coefs
get residuals
proxy 1 is the outcome
make fixed residuals
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=None here since we measure the effect across all Ts
"y is a vector, rather than a 2D array"
once the estimator has been fit
"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
an intercept even when bias is part of coef.
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
squeeze the first axis
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
squeeze the first axis
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
d_t=None here since we measure the effect across all Ts
d_t=None here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
scale preds
scale std errs
"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
offset preds
"offset the distribution, too"
scale preds
"scale the distribution, too"
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
"if stderr is zero, ppf will return nans and the loop below would never terminate"
so bail out early; note that it might be possible to correct the algorithm for
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
be clean
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
don't proxy special methods
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
second level bootstrap which would be prohibitive computationally?
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
can't import from econml.inference at top level without creating cyclical dependencies
Note that inference results are always methods even if the inference is for a property
(e.g. coef__inference() is a method but coef_ is a property)
Therefore we must insert a lambda if getting inference for a non-callable
"If inference is for a property, create a fresh lambda to avoid passing args through"
"try to get interval/std first if appropriate,"
since we don't prefer a wrapped method with this name
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base GRF tree
=============================================================================
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
=============================================================================
A MultOutputWrapper for GRF classes
=============================================================================
=============================================================================
Instantiations of Generalized Random Forest
=============================================================================
"Append a constant treatment if `fit_intercept=True`, the coefficient"
in front of the constant treatment is the intercept in the moment equation.
"Append a constant treatment and constant instrument if `fit_intercept=True`,"
the coefficient in front of the constant treatment is the intercept in the moment equation.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Base Generalized Random Forest
=============================================================================
TODO: support freq_weight and sample_var
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
We calculate the min eigenvalue proxy that each criterion is considering
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Generating indices a priori before parallelism ended up being orders of magnitude
faster than how sklearn does it. The reason is that random samplers do not release the
gil it seems.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
####################
Variance correction
####################
Subtract the average within bag variance. This ends up being equal to the
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
The negative part is just sq_between.
Objective bayes debiasing for the diagonals where we know a-prior they are positive
"The off diagonals we have no objective prior, so no correction is applied."
Finally correcting the pred_cov or pred_var
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO: update docs
"NOTE: sample weight, sample var are not passed in"
Compose final model
Calculate auxiliary quantities
X ⨂ T_res
"sum(model_final.predict(X, T_res))"
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix J"
override only so that we can exclude treatment featurization verbiage in docstring
override only so that we can exclude treatment featurization verbiage in docstring
"we need to set the number of periods before calling super()._prefit, since that will generate the"
"final and nuisance models, which need to have self._n_periods set"
Set _d_t to effective number of treatments
Required for bootstrap inference
for each mc iteration
for each model under cross fit setting
Handles the corner case when X=None but featurizer might be not None
Expand treatments for each time period
NOTE: important to use the _ortho_learner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the _ortho_learner's copy to retain the information from fitting
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
test that the subsampling scheme past to the trees is correct
The sample size is chosen in particular to test rounding based error when subsampling
test that the estimator calcualtes var correctly
test api
test accuracy
test the projection functionality of forests
test that the estimator calcualtes var correctly
test api
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
test that the subsampling scheme past to the trees is correct
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
filter directories by regex if the NOTEBOOK_DIR_PATTERN environment variable is set
omit the lalonde notebook
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
can interpret without uncertainty
can't treat before interpreting
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
for is_discrete in [False]:
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
No heterogeneity
Define indices to test
Heterogeneous effects
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
identity featurization effect functions
polynomial featurization effect functions
1d polynomial featurization functions
2d-to-1d featurization functions
2d-to-1d vector featurization functions
test that treatment names are assigned for the featurized treatment
expected shapes
check effects
ate
loose inference checks
temporarily skip LinearDRIV and SparseLinearDRIV for weird effect shape reasons
effect inference
marginal effect inference
const marginal effect inference
fit a dummy estimator first so the featurizer can be fit to the treatment
edge case with transformer that only takes a vector treatment
so far will always return None for cate_treatment_names
assert proper handling of improper feature names passed to certain transformers
"depending on sklearn version, bad feature names either throws error or only uses first relevant name"
ensure alpha is passed
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
initialize parameters
initialize config wtih base config and overwite some values
predict tree using config parameters and assert
shape of trained tree is the same as y_test
initialize config wtih base honest config and overwite some values
predict tree using config parameters and assert
shape of trained tree is the same as y_test
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
compare with weight
compare with weight
compare with weight
compare with weight
Multi-dimensional output y
1-d y
compare when both sample_var and sample_weight exist
multi-d y
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
dgp
StatsModels2SLS
IV2SLS
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
test that the subsampling scheme past to the trees is correct
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test inference results when `cate_feature_names` doesn not exist
Test inference results when `cate_feature_names` doesn not exist
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
pvalue for second column should be greater than zero since some points are on either side
of the tested value
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
ensure alpha is passed
only is not None when T1 is a constant or a list of constant
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"Nuisance model has no score method, so nuisance_scores_ should be none"
Test non keyword based calls to fit
test non-array inputs
Test custom splitter
Test incomplete set of test folds
"y scores should be positive, since W predicts Y somewhat"
"t scores might not be, since W and T are uncorrelated"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
creating an instance should warn
using the instance should not warn
using the deprecated method should warn
don't warn if b and c are passed by keyword
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
make any access to matplotlib or plt throw an exception
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
Invert indices to match latest API
Invert indices to match latest API
The feature for heterogeneity stays constant
Auxiliary function for adding xticks and vertical lines when plotting results
for dynamic dml vs ground truth parameters.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
test at least one estimator from each category
test causal graph
test refutation estimate
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
ensure we can serialize unfit estimator
ensure we can serialize fit estimator
expected effect size
test effect
test inference
only OrthoIV support inference other than bootstrap
test summary
test can run score
test cate_feature_names
test can run shap values
dgp
no heterogeneity
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
The __warningregistry__'s need to be in a pristine state for tests
to work properly.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
parameter combinations to test
TODO: serializing/deserializing for every combination -- is this necessary?
ensure we can serialize unfit estimator
ensure we can serialize fit estimator
expected effect size
assert calculated constant marginal effect shape is expected
const_marginal effect is defined in LinearCateEstimator class
assert calculated marginal effect shape is expected
test inference
test can run score
test cate_feature_names
test can run shap values
"dgp (binary T, binary Z)"
no heterogeneity
with heterogeneity
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Create a wrapper around Lasso that doesn't support weights
since Lasso does natively support them starting in sklearn 0.23
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least 6 of every element
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
NOTE: this number may need to change if the default number of folds in
WeightedStratifiedKFold changes
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
make sure we can get out post-fit stuff
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
predetermined splits ensure that all features are seen in each split
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
create a simple artificial setup where effect of moving from treatment
"a -> b is 2,"
"a -> c is 1, and"
"b -> c is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
should rule out some basic issues.
Note that explicitly specifying the dtype as object is necessary until
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
estimated effects should be identical when treatment is explicitly given
but const_marginal_effect should be reordered based on the explicit cagetories
1-> 2 in original ordering; combination of 3->1 and 3->2
test outer grouping
test nested grouping
DML nested CV works via a 'cv' attribute
"with 2-fold outer and 2-fold inner grouping, and six total groups,"
should get 1 or 2 groups per split
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we see
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
"Try default, integer, and new user-passed treatment name"
FunctionTransformers are agnostic to passed treatment names
Expected treatment names are the sums of user-passed prefixes and transformer-specific postfixes
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure that we can serialize unfit estimator
ensure that we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef_ and intercept_ inference
verify we can generate the summary
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
test outer grouping
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
test nested grouping
DML nested CV works via a 'cv' attribute
"with 2-fold outer and 2-fold inner grouping, and six total groups,"
should get 1 or 2 groups per split
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we see
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
helper class
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
#########################
WeightedLassoCVWrapper #
#########################
perform 1D fit
perform 2D fit
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
_penalized_inds is only set during fitting
cv exists on penalized model
now we can access _penalized_inds
check that we can read the cv attribute back out from the underlying model
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
continuous treatments have typical treatment values equal to
the mean of the absolute value of non-zero entries
discrete treatments have typical treatment value 1
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
continuous treatments have typical treatment values equal to
the mean of the absolute value of non-zero entries
discrete treatments have typical treatment value 1
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
make sure we don't run into problems dropping every index
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
dgp
model
model
"columns 'd', 'e', 'h' have too many values"
"columns 'd', 'e' have too many values"
lowering bound shouldn't affect already fit columns when warm starting
"column d is now okay, too"
verify that we can use a scalar treatment cost
verify that we can specify per-treatment costs for each sample
verify that using the same state returns the same results each time
set the categories for column 'd' explicitly so that b is default
"first column: 10 ones, this is fine"
"second column: 6 categories, plenty of random instances of each"
this is fine only if we increase the cateogry limit
"third column: nine ones, lots of twos, not enough unless we disable check"
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
forest heterogeneity won't work
"sixth column: just 1 one, not enough even without check"
increase bound on cat expansion
skip checks (reducing folds accordingly)
"Add tests that guarantee that the reliance on DML feature order is not broken, such as"
"Creare a transformer that zeros out all variables after the first n_x variables, so it zeros out W"
Pass an example where W is irrelevant and X is confounder
"As long as DML doesnt change the order of the inputs, then things should be good. Otherwise X would be"
zeroed out and the test will fail
"shouldn't matter if X is scaled much larger or much smaller than W, we should still get good estimates"
rescaling X shouldn't affect the first stage models because they normalize the inputs
"to recover individual coefficients with linear models, we need to be more careful in how we set up X to avoid"
cross terms
scale by 1000 to match the input to this model:
"the scale of X does matter for the final model, which keeps results in user-denominated units"
rescaling X still shouldn't affect the first stage models
TODO: we don't recover the correct values with enough accuracy to enable this assertion
is there a different way to verify that we are learning the correct coefficients?
"np.testing.assert_allclose(loc1.point.values, theta.flatten(), rtol=1e-1)"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
DGP constants
Define data features
Added `_df`to names to be different from the default cate_estimator names
Generate data
################################
Single treatment and outcome #
################################
Test LinearDML
|--> Test featurizers
"ColumnTransformer behaves differently depending on version of sklearn, so we no longer check the names"
|--> Test re-fit
Test SparseLinearDML
Test ForestDML
###################################
Mutiple treatments and outcomes #
###################################
Test LinearDML
Test SparseLinearDML
"Single outcome only, ORF does not support multiple outcomes"
Test DMLOrthoForest
Test DROrthoForest
Test XLearner
Skipping population summary names test because bootstrap inference is too slow
Test SLearner
Test TLearner
Test LinearDRLearner
Test SparseLinearDRLearner
Test ForestDRLearner
Test LinearIntentToTreatDRIV
Test DeepIV
Test categorical treatments
Check refit
Check refit after setting categories
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
Treatment effect function
Outcome support
Treatment support
"Generate controls, covariates, treatments and outcomes"
Heterogeneous treatment effects
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
through shap package.
test shap could generate the plot from the shap_values
"waterfall is broken in this version, fixed by https://github.com/slundberg/shap/pull/2444"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
"Note: unlike other Metalearners, we need the controls' encoded column for training"
"Thus, we append the controls column before the one-hot-encoded T"
"We might want to revisit, though, since it's linearly determined by the others"
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
"fit on projected Z: E[T * E[T|X,Z]|X]"
"if discrete, return shape (n,1); if continuous return shape (n,)"
"target will be discrete and will be inversed from FirstStageWrapper, shape (n,1)"
"shape (n,)"
"shape (n,)"
"shape(n,)"
TODO: prel_model_effect could allow sample_var and freq_weight?
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"if discrete, return shape (n,1); if continuous return shape (n,)"
target will be discrete and will be inversed from FirstStageWrapper
"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous"
reshape the predictions
concat W and Z
check nuisances outcome shape
Y_res could be a vector or 1-dimensional 2d-array
"all could be reshaped to vector since Y, T, Z are all single dimensional."
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
A helper class that access all the internal fitted objects of a DRIV Cate Estimator.
Used by both DRIV and IntentToTreatDRIV.
Maggie: I think that would be the case?
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
NOTE: important to use the ortho_learner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
Handles the corner case when X=None but featurizer might be not None
NOTE This is used by the inference methods and is more for internal use to the library
this is a regression model since proj_t is probability
outcome is continuous since proj_t is probability
Define the data generation functions
Define the data generation functions
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
Define the data generation functions
TODO: support freq_weight and sample_var in debiased lasso
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
Define the data generation functions
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
concat W and Z
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
concat W and Z
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
reshape the predictions
"T_res, Z_res, beta expect shape to be (n,1)"
Define the data generation functions
maybe shouldn't expose fit_cate_intercept in this class?
Define the data generation functions
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: do correct adjustment for sample_var
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
concat W and Z
concat W and Z
concat W and Z
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
Define the data generation functions
"train E[T|X,W,Z]"
"train [Z|X,W]"
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
NOTE: important to use the ortho_learner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
Handles the corner case when X=None but featurizer might be not None
NOTE This is used by the inference methods and is more for internal use to the library
concat W and Z
note that groups are not passed to score because they are only used for fitting
concat W and Z
note that sample_weight and groups are not passed to predict because they are only used for fitting
concat W and Z
A helper class that access all the internal fitted objects of a DMLIV Cate Estimator.
Used by both Parametric and Non Parametric DMLIV.
override only so that we can enforce Z to be required
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Handles the corner case when X=None but featurizer might be not None
Define the data generation functions
Get input names
Summary
coefficient
intercept
Define the data generation functions
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
make T 2D if if was a vector
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO: this utility is documented but internal; reimplement?
TODO: this utility is even less public...
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
use same Cs as would be used by default by LogisticRegressionCV
NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification
which could affect how many times each distinct Y value needs to be present in the data
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
but also supports get_feature_names with expected signature
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
NOTE: we rely on the passthrough columns coming first in the concatenated X;W
"when we pipeline scaling with our first stage models later, so the order here is important"
TODO: remove once older sklearn support is no longer needed
Wrapper to make sure that we get a deep copy of the contents instead of clone returning an untrained copy
Convert python objects to (possibly nested) types that can easily be represented as literals
Convert SingleTreeInterpreter to a python dictionary
named tuple type for storing results inside CausalAnalysis class;
must be lifted to module level to enable pickling
"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,"
"whether they end up in X or in W.  However, for the continuous columns, we want to scale them all"
"when running the first stage models, but don't want to scale the X columns when running the final model,"
since then our coefficients will have odd units and our trees will also have decisions using those units.
""
"we achieve this by pipelining the X scaling with the Y and T models (with fixed scaling, not refitting)"
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
Controls are all other columns of X
"can't use X[:, feat_ind] when X is a DataFrame"
TODO: we can't currently handle unseen values of the feature column when getting the effect;
we might want to modify OrthoLearner (and other discrete treatment classes)
so that the user can opt-in to allowing unseen treatment values
(and return NaN or something in that case)
HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models
and so we can just peel the first columns off of that combined array for rescaling in the pipeline
TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are
"built, such as model_y_feature_names, model_t_feature_names, model_y_transformer, etc., so that this"
becomes a valid approach to handling this
array checking routines don't accept 0-width arrays
perform model selection
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
convert to NormalInferenceResults for consistency
Set the dictionary values shared between local and global summaries
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
required to fit a discrete DML model
"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular"
sub-cases of models or also integrate with azure autoML. (post-MVP)
"TODO: Add other heterogeneity model options, such as {'automl'} for performing"
"model selection for the causal effect, or {'sparse_linear'} for using a debiased lasso. (post-MVP)"
TODO: Enable multi-class classification (post-MVP)
Validate inputs
TODO: check compatibility of X and Y lengths
"no previous fit, cancel warm start"
"work with numeric feature indices, so that we can easily compare with categorical ones"
"if heterogeneity_inds is 1D, repeat it"
heterogeneity inds should be a 2D list of length same as train_inds
replace None elements of heterogeneity_inds and ensure indices are numeric
"TODO: bail out also if categorical columns, classification, random_state changed?"
TODO: should we also train a new model_y under any circumstances when warm_start is True?
train the Y model
"perform model selection for the Y model using all X, not on a per-column basis"
"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
work with the regression wrapper
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
"note that this needs to happen after wrapping to generalize to the multi-class case,"
since otherwise we'll have too many columns to be able to train a classifier
start with empty results and default shared insights
convert categorical indicators to numeric indices
check for indices over the categorical expansion bound
assume we'll be able to train former failures this time; we'll add them back if not
"can't remove in place while iterating over new_inds, so store in separate list"
"train the model, but warn"
no model can be trained in this case since we need more folds
"don't train a model, but suggest workaround since there are enough instances of least"
populated class
also remove from train_inds so we don't try to access the result later
extract subset of names matching new columns
"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
don't want to cache this failed result
properties to return from effect InferenceResults
properties to return from PopulationSummaryResults
Converts strings to property lookups or method calls as a convenience so that the
_point_props and _summary_props above can be applied to an inference object
Create a summary combining all results into a single output; this is used
by the various causal_effect and causal_effect_dict methods to generate either a dataframe
"or a dictionary, respectively, based on the summary function passed into this method"
"ensure array has shape (m,y,t)"
population summary is missing sample dimension; add it for consistency
outcome dimension is missing; add it for consistency
add singleton treatment dimension if missing
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
"each attr has dimension (m,y) or (m,y,t)"
concatenate along treatment dimension
"for dictionary representation, want to remove unneeded sample dimension"
in cohort and global results
TODO: enrich outcome logic for multi-class classification when that is supported
There is no actual sample level in this data
can't drop only level
should be serialization-ready and contain no numpy arrays
"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
TODO: Note that there's no column metadata for the sample number - should there be?
"need to replicate the column info for each sample, then remove from the shared data"
NOTE: the flattened order has the ouptut dimension before the feature dimension
which may need to be revisited once we support multiclass
get the length of the list corresponding to the first dictionary key
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
a global inference indicates the effect of that one feature on the outcome
need to reshape the output to match the input
we want to offset the inference object by the baseline estimate of y
"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
get the length of the list corresponding to the first dictionary key
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
"NOTE: this calculation is correct only if treatment costs are marginal costs,"
because then scaling the difference between treatment value and treatment costs is the
same as scaling the treatment value and subtracting the scaled treatment cost.
""
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
"continuous treatments, the policy value should include the benefit of decreasing treatments"
(rather than just not treating at all)
""
"We can get the total by seeing that if we restrict attention to units where we would treat,"
2 * policy_value - always_treat
includes exactly their contribution because policy_value and always_treat both include it
"and likewise restricting attention to the units where we want to decrease treatment,"
2 * policy_value - always-treat
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
is zero and the contribution to always_treat is negative
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
get dataframe with all but selected column
apply 10% of a typical treatment for this feature
"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely"
set the effect bounds; for positive treatments these agree with
"the estimates; for negative treatments, we need to invert the interval"
the effect is now always positive since we decrease treatment when negative
"for discrete treatment, stack a zero result in front for control"
we need to call effect_inference to get the correct CI between the two treatment options
we now need to construct the delta in the cost between the two treatments and translate the effect
remove third dimenions potentially added
"find cost of current treatment: equality creates a 2d array with True on each row,"
only if its the location of the current treatment. Then we take the corresponding cost.
construct index of current treatment
add second dimension if needed for broadcasting during translation of effect
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO: conisder working around relying on sklearn implementation details
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
"If classification methods produce multiple columns of output,"
we need to manually encode classes to ensure consistent column ordering.
We clone the estimator to make sure that all the folds are
"independent, and that it is pickle-able."
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
`predictions` is a list of method outputs from each fold.
"If each of those is also a list, then treat this as a"
multioutput-multiclass task. We need to separately concatenate
the method outputs for each label into an `n_labels` long list.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
TODO: consider working around relying on sklearn implementation details
"TODO: once we drop support for sklearn < 1.0, we can remove this"
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
normalize was deprecated or removed; don't need to do anything
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Call weighted lasso on reduced design matrix
Weighted tau
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set model to WeightedLassoCV by default so there's always a model to get and set attributes on
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
(e.g. former has 'positive' and 'precompute' while latter does not)
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
don't proxy special methods
"don't pass get_params through to model, because that will cause sklearn to clone this"
regressor incorrectly
"Note: for known attributes that have been set this method will not be called,"
so we should just throw here because this is an attribute belonging to this class
but which hasn't yet been set on this instance
set default values for None
check freq_weight should be integer and should be accompanied by sample_var
check array shape
weight X and y and sample_var
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
set default values for None
check array shape
check dimension of instruments is more than dimension of treatments
weight X and y
learn point estimate
solve first stage linear regression E[T|Z]
"""that"" means T̂"
solve second stage linear regression E[Y|that]
(T̂.T*T̂)^{-1}
learn cov(theta)
(T̂.T*T̂)^{-1}
sigma^2
reference: http://www.hec.unil.ch/documents/seminars/deep/361.pdf
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
average the outcome dimension if it exists and ensure 2d y_pred
get index of best treatment
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
TODO: consider working around relying on sklearn implementation details
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Convert rows to columns
Require group assignment t to be one-hot-encoded
Get predictions for the 2 splits
Must make sure indices are merged correctly
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Must normalize weights
Override the CATE inference options
Add blb inference to parent's options
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Need to redefine fit here for auto inference to work due to a quirk in how
wrap_fit is defined
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Check that all discrete treatments are represented
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
returns shape-conforming residuals
Copy and/or define models
Define parameter estimators
Define moment and mean gradient estimator
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Call `fit` from parent class
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
override only so that we can exclude treatment featurization verbiage in docstring
Override to flatten output if T is flat
override only so that we can exclude treatment featurization verbiage in docstring
Expand one-hot encoding to include the zero treatment
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Calculate confidence intervals for the parameter (marginal effect)
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
d_t=None here since we measure the effect across all Ts
conditionally expand jacobian dimensions to align with einsum str
Calculate the effects
Calculate the standard deviations for the effects
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
Copyright (c) PyWhy contributors. All rights reserved.
Licensed under the MIT License.
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/main/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
TODO: enable type aliases
napoleon_preprocess_types = True  # needed for type aliases to work
napoleon_type_aliases = {
"""array_like"": "":term:`array_like`"","
"""ndarray"": ""~numpy.ndarray"","
"""RandomState"": "":class:`~numpy.random.RandomState`"","
"""DataFrame"": "":class:`~pandas.DataFrame`"","
"""Series"": "":class:`~pandas.Series`"","
}
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of strings:
""
"source_suffix = ['.rst', '.md']"
The root toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
def mlasso_model(): return MultiTaskLassoCV(
"cv=3, alphas=alpha_regs, max_iter=200)"
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
"alpha_regs = [5e-3, 1e-2, 5e-2]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
subset of features that are exogenous and create heterogeneity
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
subset of features wrt we estimate heterogeneity
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
introspect the constructor arguments to find the model parameters
to represent
"if the argument is deprecated, ignore it"
Extract and sort argument names excluding 'self'
column names
transfer input to numpy arrays
transfer input to 2d arrays
create dataframe
currently dowhy only support single outcome and single treatment
call dowhy
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
cate estimator but not the effect.
don't proxy special methods
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
For when checking input values is disabled
Type to column extraction function
if not all column names are strings
coerce feature names to be strings
Prefer sklearn 1.0's get_feature_names_out method to deprecated get_feature_names method
"Some featurizers will throw, such as a pipeline with a transformer that doesn't itself support names"
"Get number of arguments, some sklearn featurizer don't accept feature_names"
Handles cases where the passed feature names create issues
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
Get feature names using featurizer
All attempts at retrieving transformed feature names have failed
Delegate handling to downstream logic
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
assume that we should perform nested cross-validation if and only if
the model has a 'cv' attribute; this is a somewhat brittle assumption...
logic copied from check_cv
otherwise we will assume the user already set the cv attribute to something
compatible with splitting with a 'groups' argument
now we have to compute the folds explicitly because some classifiers (like LassoCV)
don't use the groups when calling split internally
Normalize weights
This class is mainly derived from statsmodels.iolib.summary.Summary
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
"want to enforce that each bad_arg was either in kwargs,"
or else it was in neither and is just taking its default value
Any access should throw
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
for every dimension of the treatment add some epsilon and observe change in featurized treatment
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
input feature name is already updated by cate_feature_names.
define the index of d_x to filter for each given T
filter X after broadcast with T for each given T
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains some snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_export.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
make any access to matplotlib or plt throw an exception
make any access to graphviz or plt throw an exception
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
Initialize saturation & value; calculate chroma & value shift
Calculate some intermediate values
Initialize RGB with same hue & chroma as our color
Shift the initial RGB values to match value and store
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use mean of targets
Write node mean CATE
Write node std of CATE
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
Fetch appropriate color for node
Write node mean CATE
Write node mean CATE
Write recommended treatment and value - cost
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
This flag is true when names are set in a child class instead
"If names are set in a child class, add an attribute reflecting that"
This works only if X is passed as a kwarg
We plan to enforce X as kwarg only in future releases
This checks if names have been set in a child class
"If names were set in a child class, don't do it again"
"Wraps-up fit by setting attributes, cleaning up, etc."
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"Treatment names is None, default to BaseCateEstimator"
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
Get input names
Summary
add statsmodels to parent's options
add debiasedlasso to parent's options
add blb to parent's options
TODO Share some logic with non-discrete version
Get input names
Note: we do not transform feature names since that is done within summary_frame
Summary
add statsmodels to parent's options
add statsmodels to parent's options
add blb to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
remove None arguments
"scores entries should be lists of scores, so make each entry a singleton list"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
generate an instance of the final model
generate an instance of the nuisance model
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
alteration even when we only want to fit_final.
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
"however, sklearn doesn't support both stratifying and grouping (see"
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
their own object that supports grouping if they want to use groups.
for each mc iteration
for each model under cross fit setting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Policy Forest
=============================================================================
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base Policy tree
=============================================================================
The values below are required and utilitized by methods in the _SingleTreeExporterMixin
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"since we only allow single dimensional y, we could flatten the prediction"
override only so that we can exclude treatment featurization verbiage in docstring
override only so that we can exclude treatment featurization verbiage in docstring
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Handles the corner case when X=None but featurizer might be not None
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
TODO: support freq_weight and sample_var in debiased lasso
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
Replacing to remove docstring
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE: important to use the rlearner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the rlearner's copy to retain the information from fitting
Handles the corner case when X=None but featurizer might be not None
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `LinearModelFinalInference`
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support freq_weight and sample_var in debiased lasso
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add blb to parent's options
override only so that we can update the docstring to indicate
support for `GenericSingleTreatmentModelFinalInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
note that groups are not passed to score because they are only used for fitting
note that groups are not passed to score because they are only used for fitting
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
NOTE: important to get parent's wrapped copy so that
"after training wrapped featurizer is also trained, etc."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
Fit a doubly robust average effect
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
"If custom param grid, check that only estimator parameters are being altered"
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
override only so that we can update the docstring to indicate support for `blb`
Get input names
Summary
Determine output settings
"Important: This must be the first invocation of the random state at fit time, so that"
train/test splits are re-generatable from an external object simply by knowing the
random_state parameter of the tree. Can be useful in the future if one wants to create local
linear predictions. Currently is also useful for testing.
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
Set min_weight_leaf from min_weight_fraction_leaf
Build tree
We calculate the maximum number of samples from each half-split that any node in the tree can
hold. Used by criterion for memory space savings.
Initialize the criterion object and the criterion_val object if honest.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code is a fork from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_base.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
Set parameters
Don't instantiate estimators now! Parameters of base_estimator might
"still change. Eg., when grid-searching with the nested object syntax."
self.estimators_ needs to be filled by the derived classes in fit.
Compute the number of jobs
Partition estimators between jobs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
covariance matrix
get eigen value and eigen vectors
simulate eigen vectors
keep the top 4 eigen value and corresponding eigen vector
replace the negative eigen values
generate a new covariance matrix
get linear approximation of eigen values
coefs
get the indices of each group of features
print(ind_same_proxy)
demo
same proxy
residuals
gmm
log normal on outliers
positive outliers
negative outliers
demean the new residual again
generate data
sample residuals
get prediction for current investment
get prediction for current proxy
get first period prediction
iterate the step ahead contruction
prepare new x
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
get new covariance matrix
get coefs
get residuals
proxy 1 is the outcome
make fixed residuals
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=None here since we measure the effect across all Ts
"y is a vector, rather than a 2D array"
once the estimator has been fit
"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
an intercept even when bias is part of coef.
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
squeeze the first axis
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
squeeze the first axis
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
d_t=None here since we measure the effect across all Ts
d_t=None here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
scale preds
scale std errs
"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
offset preds
"offset the distribution, too"
scale preds
"scale the distribution, too"
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
"if stderr is zero, ppf will return nans and the loop below would never terminate"
so bail out early; note that it might be possible to correct the algorithm for
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
be clean
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
don't proxy special methods
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
second level bootstrap which would be prohibitive computationally?
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
can't import from econml.inference at top level without creating cyclical dependencies
Note that inference results are always methods even if the inference is for a property
(e.g. coef__inference() is a method but coef_ is a property)
Therefore we must insert a lambda if getting inference for a non-callable
"If inference is for a property, create a fresh lambda to avoid passing args through"
"try to get interval/std first if appropriate,"
since we don't prefer a wrapped method with this name
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base GRF tree
=============================================================================
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
=============================================================================
A MultOutputWrapper for GRF classes
=============================================================================
=============================================================================
Instantiations of Generalized Random Forest
=============================================================================
"Append a constant treatment if `fit_intercept=True`, the coefficient"
in front of the constant treatment is the intercept in the moment equation.
"Append a constant treatment and constant instrument if `fit_intercept=True`,"
the coefficient in front of the constant treatment is the intercept in the moment equation.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Base Generalized Random Forest
=============================================================================
TODO: support freq_weight and sample_var
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
We calculate the min eigenvalue proxy that each criterion is considering
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Generating indices a priori before parallelism ended up being orders of magnitude
faster than how sklearn does it. The reason is that random samplers do not release the
gil it seems.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
####################
Variance correction
####################
Subtract the average within bag variance. This ends up being equal to the
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
The negative part is just sq_between.
Objective bayes debiasing for the diagonals where we know a-prior they are positive
"The off diagonals we have no objective prior, so no correction is applied."
Finally correcting the pred_cov or pred_var
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: update docs
"NOTE: sample weight, sample var are not passed in"
Compose final model
Calculate auxiliary quantities
X ⨂ T_res
"sum(model_final.predict(X, T_res))"
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix J"
override only so that we can exclude treatment featurization verbiage in docstring
override only so that we can exclude treatment featurization verbiage in docstring
generate an instance of the final model
generate an instance of the nuisance model
Set _d_t to effective number of treatments
Required for bootstrap inference
for each mc iteration
for each model under cross fit setting
Handles the corner case when X=None but featurizer might be not None
Expand treatments for each time period
NOTE: important to use the _ortho_learner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the _ortho_learner's copy to retain the information from fitting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
test that the subsampling scheme past to the trees is correct
The sample size is chosen in particular to test rounding based error when subsampling
test that the estimator calcualtes var correctly
test api
test accuracy
test the projection functionality of forests
test that the estimator calcualtes var correctly
test api
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
test that the subsampling scheme past to the trees is correct
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
filter directories by regex if the NOTEBOOK_DIR_PATTERN environment variable is set
omit the lalonde notebook
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
can interpret without uncertainty
can't treat before interpreting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
for is_discrete in [False]:
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
No heterogeneity
Define indices to test
Heterogeneous effects
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
identity featurization effect functions
polynomial featurization effect functions
1d polynomial featurization functions
2d-to-1d featurization functions
2d-to-1d vector featurization functions
test that treatment names are assigned for the featurized treatment
expected shapes
check effects
ate
loose inference checks
temporarily skip LinearDRIV and SparseLinearDRIV for weird effect shape reasons
effect inference
marginal effect inference
const marginal effect inference
fit a dummy estimator first so the featurizer can be fit to the treatment
edge case with transformer that only takes a vector treatment
so far will always return None for cate_treatment_names
assert proper handling of improper feature names passed to certain transformers
"depending on sklearn version, bad feature names either throws error or only uses first relevant name"
ensure alpha is passed
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
initialize parameters
initialize config wtih base config and overwite some values
predict tree using config parameters and assert
shape of trained tree is the same as y_test
initialize config wtih base honest config and overwite some values
predict tree using config parameters and assert
shape of trained tree is the same as y_test
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
compare with weight
compare with weight
compare with weight
compare with weight
Multi-dimensional output y
1-d y
compare when both sample_var and sample_weight exist
multi-d y
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
dgp
StatsModels2SLS
IV2SLS
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that the subsampling scheme past to the trees is correct
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test inference results when `cate_feature_names` doesn not exist
Test inference results when `cate_feature_names` doesn not exist
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
pvalue for second column should be greater than zero since some points are on either side
of the tested value
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
ensure alpha is passed
only is not None when T1 is a constant or a list of constant
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Nuisance model has no score method, so nuisance_scores_ should be none"
Test non keyword based calls to fit
test non-array inputs
Test custom splitter
Test incomplete set of test folds
"y scores should be positive, since W predicts Y somewhat"
"t scores might not be, since W and T are uncorrelated"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
creating an instance should warn
using the instance should not warn
using the deprecated method should warn
don't warn if b and c are passed by keyword
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make any access to matplotlib or plt throw an exception
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
Invert indices to match latest API
Invert indices to match latest API
The feature for heterogeneity stays constant
Auxiliary function for adding xticks and vertical lines when plotting results
for dynamic dml vs ground truth parameters.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test at least one estimator from each category
test causal graph
test refutation estimate
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
ensure we can serialize unfit estimator
ensure we can serialize fit estimator
expected effect size
test effect
test inference
only OrthoIV support inference other than bootstrap
test summary
test can run score
test cate_feature_names
test can run shap values
dgp
no heterogeneity
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
The __warningregistry__'s need to be in a pristine state for tests
to work properly.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
parameter combinations to test
TODO: serializing/deserializing for every combination -- is this necessary?
ensure we can serialize unfit estimator
ensure we can serialize fit estimator
expected effect size
assert calculated constant marginal effect shape is expected
const_marginal effect is defined in LinearCateEstimator class
assert calculated marginal effect shape is expected
test inference
test can run score
test cate_feature_names
test can run shap values
"dgp (binary T, binary Z)"
no heterogeneity
with heterogeneity
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Create a wrapper around Lasso that doesn't support weights
since Lasso does natively support them starting in sklearn 0.23
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least 6 of every element
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
NOTE: this number may need to change if the default number of folds in
WeightedStratifiedKFold changes
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
make sure we can get out post-fit stuff
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
predetermined splits ensure that all features are seen in each split
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
create a simple artificial setup where effect of moving from treatment
"a -> b is 2,"
"a -> c is 1, and"
"b -> c is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
should rule out some basic issues.
Note that explicitly specifying the dtype as object is necessary until
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
estimated effects should be identical when treatment is explicitly given
but const_marginal_effect should be reordered based on the explicit cagetories
1-> 2 in original ordering; combination of 3->1 and 3->2
test outer grouping
test nested grouping
DML nested CV works via a 'cv' attribute
"with 2-fold outer and 2-fold inner grouping, and six total groups,"
should get 1 or 2 groups per split
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we see
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
"Try default, integer, and new user-passed treatment name"
FunctionTransformers are agnostic to passed treatment names
Expected treatment names are the sums of user-passed prefixes and transformer-specific postfixes
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure that we can serialize unfit estimator
ensure that we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef_ and intercept_ inference
verify we can generate the summary
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
test outer grouping
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
test nested grouping
DML nested CV works via a 'cv' attribute
"with 2-fold outer and 2-fold inner grouping, and six total groups,"
should get 1 or 2 groups per split
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we see
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
helper class
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
#########################
WeightedLassoCVWrapper #
#########################
perform 1D fit
perform 2D fit
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
_penalized_inds is only set during fitting
cv exists on penalized model
now we can access _penalized_inds
check that we can read the cv attribute back out from the underlying model
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
continuous treatments have typical treatment values equal to
the mean of the absolute value of non-zero entries
discrete treatments have typical treatment value 1
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
continuous treatments have typical treatment values equal to
the mean of the absolute value of non-zero entries
discrete treatments have typical treatment value 1
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
make sure we don't run into problems dropping every index
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
dgp
model
model
"columns 'd', 'e', 'h' have too many values"
"columns 'd', 'e' have too many values"
lowering bound shouldn't affect already fit columns when warm starting
"column d is now okay, too"
verify that we can use a scalar treatment cost
verify that we can specify per-treatment costs for each sample
verify that using the same state returns the same results each time
set the categories for column 'd' explicitly so that b is default
"first column: 10 ones, this is fine"
"second column: 6 categories, plenty of random instances of each"
this is fine only if we increase the cateogry limit
"third column: nine ones, lots of twos, not enough unless we disable check"
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
forest heterogeneity won't work
"sixth column: just 1 one, not enough even without check"
increase bound on cat expansion
skip checks (reducing folds accordingly)
"Add tests that guarantee that the reliance on DML feature order is not broken, such as"
"Creare a transformer that zeros out all variables after the first n_x variables, so it zeros out W"
Pass an example where W is irrelevant and X is confounder
"As long as DML doesnt change the order of the inputs, then things should be good. Otherwise X would be"
zeroed out and the test will fail
"shouldn't matter if X is scaled much larger or much smaller than W, we should still get good estimates"
rescaling X shouldn't affect the first stage models because they normalize the inputs
"to recover individual coefficients with linear models, we need to be more careful in how we set up X to avoid"
cross terms
scale by 1000 to match the input to this model:
"the scale of X does matter for the final model, which keeps results in user-denominated units"
rescaling X still shouldn't affect the first stage models
TODO: we don't recover the correct values with enough accuracy to enable this assertion
is there a different way to verify that we are learning the correct coefficients?
"np.testing.assert_allclose(loc1.point.values, theta.flatten(), rtol=1e-1)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Define data features
Added `_df`to names to be different from the default cate_estimator names
Generate data
################################
Single treatment and outcome #
################################
Test LinearDML
|--> Test featurizers
"ColumnTransformer behaves differently depending on version of sklearn, so we no longer check the names"
|--> Test re-fit
Test SparseLinearDML
Test ForestDML
###################################
Mutiple treatments and outcomes #
###################################
Test LinearDML
Test SparseLinearDML
"Single outcome only, ORF does not support multiple outcomes"
Test DMLOrthoForest
Test DROrthoForest
Test XLearner
Skipping population summary names test because bootstrap inference is too slow
Test SLearner
Test TLearner
Test LinearDRLearner
Test SparseLinearDRLearner
Test ForestDRLearner
Test LinearIntentToTreatDRIV
Test DeepIV
Test categorical treatments
Check refit
Check refit after setting categories
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
Treatment effect function
Outcome support
Treatment support
"Generate controls, covariates, treatments and outcomes"
Heterogeneous treatment effects
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
through shap package.
test shap could generate the plot from the shap_values
"waterfall is broken in this version, fixed by https://github.com/slundberg/shap/pull/2444"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
"Note: unlike other Metalearners, we need the controls' encoded column for training"
"Thus, we append the controls column before the one-hot-encoded T"
"We might want to revisit, though, since it's linearly determined by the others"
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
"fit on projected Z: E[T * E[T|X,Z]|X]"
"if discrete, return shape (n,1); if continuous return shape (n,)"
"target will be discrete and will be inversed from FirstStageWrapper, shape (n,1)"
"shape (n,)"
"shape (n,)"
"shape(n,)"
TODO: prel_model_effect could allow sample_var and freq_weight?
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"if discrete, return shape (n,1); if continuous return shape (n,)"
target will be discrete and will be inversed from FirstStageWrapper
"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous"
reshape the predictions
concat W and Z
check nuisances outcome shape
Y_res could be a vector or 1-dimensional 2d-array
"all could be reshaped to vector since Y, T, Z are all single dimensional."
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
A helper class that access all the internal fitted objects of a DRIV Cate Estimator.
Used by both DRIV and IntentToTreatDRIV.
Maggie: I think that would be the case?
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
NOTE: important to use the ortho_learner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
Handles the corner case when X=None but featurizer might be not None
NOTE This is used by the inference methods and is more for internal use to the library
this is a regression model since proj_t is probability
outcome is continuous since proj_t is probability
Define the data generation functions
Define the data generation functions
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
Define the data generation functions
TODO: support freq_weight and sample_var in debiased lasso
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
Define the data generation functions
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
concat W and Z
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
concat W and Z
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
reshape the predictions
"T_res, Z_res, beta expect shape to be (n,1)"
Define the data generation functions
maybe shouldn't expose fit_cate_intercept in this class?
Define the data generation functions
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: do correct adjustment for sample_var
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
concat W and Z
concat W and Z
concat W and Z
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
Define the data generation functions
"train E[T|X,W,Z]"
"train [Z|X,W]"
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
NOTE: important to use the ortho_learner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
Handles the corner case when X=None but featurizer might be not None
NOTE This is used by the inference methods and is more for internal use to the library
concat W and Z
note that groups are not passed to score because they are only used for fitting
concat W and Z
note that sample_weight and groups are not passed to predict because they are only used for fitting
concat W and Z
A helper class that access all the internal fitted objects of a DMLIV Cate Estimator.
Used by both Parametric and Non Parametric DMLIV.
override only so that we can enforce Z to be required
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Handles the corner case when X=None but featurizer might be not None
Define the data generation functions
Get input names
Summary
coefficient
intercept
Define the data generation functions
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
make T 2D if if was a vector
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: this utility is documented but internal; reimplement?
TODO: this utility is even less public...
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
use same Cs as would be used by default by LogisticRegressionCV
NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification
which could affect how many times each distinct Y value needs to be present in the data
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
but also supports get_feature_names with expected signature
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
NOTE: we rely on the passthrough columns coming first in the concatenated X;W
"when we pipeline scaling with our first stage models later, so the order here is important"
Wrapper to make sure that we get a deep copy of the contents instead of clone returning an untrained copy
Convert python objects to (possibly nested) types that can easily be represented as literals
Convert SingleTreeInterpreter to a python dictionary
named tuple type for storing results inside CausalAnalysis class;
must be lifted to module level to enable pickling
"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,"
"whether they end up in X or in W.  However, for the continuous columns, we want to scale them all"
"when running the first stage models, but don't want to scale the X columns when running the final model,"
since then our coefficients will have odd units and our trees will also have decisions using those units.
""
"we achieve this by pipelining the X scaling with the Y and T models (with fixed scaling, not refitting)"
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
Controls are all other columns of X
"can't use X[:, feat_ind] when X is a DataFrame"
TODO: we can't currently handle unseen values of the feature column when getting the effect;
we might want to modify OrthoLearner (and other discrete treatment classes)
so that the user can opt-in to allowing unseen treatment values
(and return NaN or something in that case)
HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models
and so we can just peel the first columns off of that combined array for rescaling in the pipeline
TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are
"built, such as model_y_feature_names, model_t_feature_names, model_y_transformer, etc., so that this"
becomes a valid approach to handling this
array checking routines don't accept 0-width arrays
perform model selection
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
convert to NormalInferenceResults for consistency
Set the dictionary values shared between local and global summaries
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
required to fit a discrete DML model
"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular"
sub-cases of models or also integrate with azure autoML. (post-MVP)
"TODO: Add other heterogeneity model options, such as {'automl'} for performing"
"model selection for the causal effect, or {'sparse_linear'} for using a debiased lasso. (post-MVP)"
TODO: Enable multi-class classification (post-MVP)
Validate inputs
TODO: check compatibility of X and Y lengths
"no previous fit, cancel warm start"
"work with numeric feature indices, so that we can easily compare with categorical ones"
"if heterogeneity_inds is 1D, repeat it"
heterogeneity inds should be a 2D list of length same as train_inds
replace None elements of heterogeneity_inds and ensure indices are numeric
"TODO: bail out also if categorical columns, classification, random_state changed?"
TODO: should we also train a new model_y under any circumstances when warm_start is True?
train the Y model
"perform model selection for the Y model using all X, not on a per-column basis"
"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
work with the regression wrapper
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
"note that this needs to happen after wrapping to generalize to the multi-class case,"
since otherwise we'll have too many columns to be able to train a classifier
start with empty results and default shared insights
convert categorical indicators to numeric indices
check for indices over the categorical expansion bound
assume we'll be able to train former failures this time; we'll add them back if not
"can't remove in place while iterating over new_inds, so store in separate list"
"train the model, but warn"
no model can be trained in this case since we need more folds
"don't train a model, but suggest workaround since there are enough instances of least"
populated class
also remove from train_inds so we don't try to access the result later
extract subset of names matching new columns
"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
don't want to cache this failed result
properties to return from effect InferenceResults
properties to return from PopulationSummaryResults
Converts strings to property lookups or method calls as a convenience so that the
_point_props and _summary_props above can be applied to an inference object
Create a summary combining all results into a single output; this is used
by the various causal_effect and causal_effect_dict methods to generate either a dataframe
"or a dictionary, respectively, based on the summary function passed into this method"
"ensure array has shape (m,y,t)"
population summary is missing sample dimension; add it for consistency
outcome dimension is missing; add it for consistency
add singleton treatment dimension if missing
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
"each attr has dimension (m,y) or (m,y,t)"
concatenate along treatment dimension
"for dictionary representation, want to remove unneeded sample dimension"
in cohort and global results
TODO: enrich outcome logic for multi-class classification when that is supported
There is no actual sample level in this data
can't drop only level
should be serialization-ready and contain no numpy arrays
"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
TODO: Note that there's no column metadata for the sample number - should there be?
"need to replicate the column info for each sample, then remove from the shared data"
NOTE: the flattened order has the ouptut dimension before the feature dimension
which may need to be revisited once we support multiclass
get the length of the list corresponding to the first dictionary key
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
a global inference indicates the effect of that one feature on the outcome
need to reshape the output to match the input
we want to offset the inference object by the baseline estimate of y
"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
get the length of the list corresponding to the first dictionary key
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
"NOTE: this calculation is correct only if treatment costs are marginal costs,"
because then scaling the difference between treatment value and treatment costs is the
same as scaling the treatment value and subtracting the scaled treatment cost.
""
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
"continuous treatments, the policy value should include the benefit of decreasing treatments"
(rather than just not treating at all)
""
"We can get the total by seeing that if we restrict attention to units where we would treat,"
2 * policy_value - always_treat
includes exactly their contribution because policy_value and always_treat both include it
"and likewise restricting attention to the units where we want to decrease treatment,"
2 * policy_value - always-treat
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
is zero and the contribution to always_treat is negative
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
get dataframe with all but selected column
apply 10% of a typical treatment for this feature
"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely"
set the effect bounds; for positive treatments these agree with
"the estimates; for negative treatments, we need to invert the interval"
the effect is now always positive since we decrease treatment when negative
"for discrete treatment, stack a zero result in front for control"
we need to call effect_inference to get the correct CI between the two treatment options
we now need to construct the delta in the cost between the two treatments and translate the effect
remove third dimenions potentially added
"find cost of current treatment: equality creates a 2d array with True on each row,"
only if its the location of the current treatment. Then we take the corresponding cost.
construct index of current treatment
add second dimension if needed for broadcasting during translation of effect
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: conisder working around relying on sklearn implementation details
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
"If classification methods produce multiple columns of output,"
we need to manually encode classes to ensure consistent column ordering.
We clone the estimator to make sure that all the folds are
"independent, and that it is pickle-able."
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
`predictions` is a list of method outputs from each fold.
"If each of those is also a list, then treat this as a"
multioutput-multiclass task. We need to separately concatenate
the method outputs for each label into an `n_labels` long list.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
TODO: consider working around relying on sklearn implementation details
"TODO: once we drop support for sklearn < 1.0, we can remove this"
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
normalize was deprecated or removed; don't need to do anything
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Call weighted lasso on reduced design matrix
Weighted tau
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set model to WeightedLassoCV by default so there's always a model to get and set attributes on
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
(e.g. former has 'positive' and 'precompute' while latter does not)
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
don't proxy special methods
"don't pass get_params through to model, because that will cause sklearn to clone this"
regressor incorrectly
"Note: for known attributes that have been set this method will not be called,"
so we should just throw here because this is an attribute belonging to this class
but which hasn't yet been set on this instance
set default values for None
check freq_weight should be integer and should be accompanied by sample_var
check array shape
weight X and y and sample_var
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
set default values for None
check array shape
check dimension of instruments is more than dimension of treatments
weight X and y
learn point estimate
solve first stage linear regression E[T|Z]
"""that"" means T̂"
solve second stage linear regression E[Y|that]
(T̂.T*T̂)^{-1}
learn cov(theta)
(T̂.T*T̂)^{-1}
sigma^2
reference: http://www.hec.unil.ch/documents/seminars/deep/361.pdf
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
average the outcome dimension if it exists and ensure 2d y_pred
get index of best treatment
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: consider working around relying on sklearn implementation details
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Convert rows to columns
Require group assignment t to be one-hot-encoded
Get predictions for the 2 splits
Must make sure indices are merged correctly
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Must normalize weights
Override the CATE inference options
Add blb inference to parent's options
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Need to redefine fit here for auto inference to work due to a quirk in how
wrap_fit is defined
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Check that all discrete treatments are represented
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
returns shape-conforming residuals
Copy and/or define models
Define parameter estimators
Define moment and mean gradient estimator
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Call `fit` from parent class
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
override only so that we can exclude treatment featurization verbiage in docstring
Override to flatten output if T is flat
override only so that we can exclude treatment featurization verbiage in docstring
Expand one-hot encoding to include the zero treatment
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Calculate confidence intervals for the parameter (marginal effect)
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
d_t=None here since we measure the effect across all Ts
conditionally expand jacobian dimensions to align with einsum str
Calculate the effects
Calculate the standard deviations for the effects
"conditionally index multiple dimensions depending on shapes of T, Y and feat_T"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/main/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The root toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
def mlasso_model(): return MultiTaskLassoCV(
"cv=3, alphas=alpha_regs, max_iter=200)"
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
"alpha_regs = [5e-3, 1e-2, 5e-2]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
subset of features that are exogenous and create heterogeneity
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
subset of features wrt we estimate heterogeneity
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
introspect the constructor arguments to find the model parameters
to represent
"if the argument is deprecated, ignore it"
Extract and sort argument names excluding 'self'
column names
transfer input to numpy arrays
transfer input to 2d arrays
create dataframe
currently dowhy only support single outcome and single treatment
call dowhy
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
cate estimator but not the effect.
don't proxy special methods
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
For when checking input values is disabled
Type to column extraction function
Prefer sklearn 1.0's get_feature_names_out method to deprecated get_feature_names method
"Some featurizers will throw, such as a pipeline with a transformer that doesn't itself support names"
"Get number of arguments, some sklearn featurizer don't accept feature_names"
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
Get feature names using featurizer
All attempts at retrieving transformed feature names have failed
Delegate handling to downstream logic
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
assume that we should perform nested cross-validation if and only if
the model has a 'cv' attribute; this is a somewhat brittle assumption...
logic copied from check_cv
otherwise we will assume the user already set the cv attribute to something
compatible with splitting with a 'groups' argument
now we have to compute the folds explicitly because some classifiers (like LassoCV)
don't use the groups when calling split internally
Normalize weights
This class is mainly derived from statsmodels.iolib.summary.Summary
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
"want to enforce that each bad_arg was either in kwargs,"
or else it was in neither and is just taking its default value
Any access should throw
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
input feature name is already updated by cate_feature_names.
define the index of d_x to filter for each given T
filter X after broadcast with T for each given T
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains some snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_export.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
make any access to matplotlib or plt throw an exception
make any access to graphviz or plt throw an exception
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
Initialize saturation & value; calculate chroma & value shift
Calculate some intermediate values
Initialize RGB with same hue & chroma as our color
Shift the initial RGB values to match value and store
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use mean of targets
Write node mean CATE
Write node std of CATE
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
Fetch appropriate color for node
Write node mean CATE
Write node mean CATE
Write recommended treatment and value - cost
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
This flag is true when names are set in a child class instead
"If names are set in a child class, add an attribute reflecting that"
This works only if X is passed as a kwarg
We plan to enforce X as kwarg only in future releases
This checks if names have been set in a child class
"If names were set in a child class, don't do it again"
"Wraps-up fit by setting attributes, cleaning up, etc."
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"Treatment names is None, default to BaseCateEstimator"
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
Get input names
Summary
add statsmodels to parent's options
add debiasedlasso to parent's options
add blb to parent's options
TODO Share some logic with non-discrete version
Get input names
Note: we do not transform feature names since that is done within summary_frame
Summary
add statsmodels to parent's options
add statsmodels to parent's options
add blb to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
remove None arguments
"scores entries should be lists of scores, so make each entry a singleton list"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
generate an instance of the final model
generate an instance of the nuisance model
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
alteration even when we only want to fit_final.
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
"however, sklearn doesn't support both stratifying and grouping (see"
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
their own object that supports grouping if they want to use groups.
for each mc iteration
for each model under cross fit setting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Policy Forest
=============================================================================
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base Policy tree
=============================================================================
The values below are required and utilitized by methods in the _SingleTreeExporterMixin
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"since we only allow single dimensional y, we could flatten the prediction"
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Handles the corner case when X=None but featurizer might be not None
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
TODO: support freq_weight and sample_var in debiased lasso
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
Replacing to remove docstring
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE: important to use the rlearner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the rlearner's copy to retain the information from fitting
Handles the corner case when X=None but featurizer might be not None
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `LinearModelFinalInference`
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support freq_weight and sample_var in debiased lasso
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add blb to parent's options
override only so that we can update the docstring to indicate
support for `GenericSingleTreatmentModelFinalInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
note that groups are not passed to score because they are only used for fitting
note that groups are not passed to score because they are only used for fitting
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
NOTE: important to get parent's wrapped copy so that
"after training wrapped featurizer is also trained, etc."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
Fit a doubly robust average effect
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
"If custom param grid, check that only estimator parameters are being altered"
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
override only so that we can update the docstring to indicate support for `blb`
Get input names
Summary
Determine output settings
"Important: This must be the first invocation of the random state at fit time, so that"
train/test splits are re-generatable from an external object simply by knowing the
random_state parameter of the tree. Can be useful in the future if one wants to create local
linear predictions. Currently is also useful for testing.
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
Set min_weight_leaf from min_weight_fraction_leaf
Build tree
We calculate the maximum number of samples from each half-split that any node in the tree can
hold. Used by criterion for memory space savings.
Initialize the criterion object and the criterion_val object if honest.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code is a fork from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_base.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
Set parameters
Don't instantiate estimators now! Parameters of base_estimator might
"still change. Eg., when grid-searching with the nested object syntax."
self.estimators_ needs to be filled by the derived classes in fit.
Compute the number of jobs
Partition estimators between jobs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
covariance matrix
get eigen value and eigen vectors
simulate eigen vectors
keep the top 4 eigen value and corresponding eigen vector
replace the negative eigen values
generate a new covariance matrix
get linear approximation of eigen values
coefs
get the indices of each group of features
print(ind_same_proxy)
demo
same proxy
residuals
gmm
log normal on outliers
positive outliers
negative outliers
demean the new residual again
generate data
sample residuals
get prediction for current investment
get prediction for current proxy
get first period prediction
iterate the step ahead contruction
prepare new x
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
get new covariance matrix
get coefs
get residuals
proxy 1 is the outcome
make fixed residuals
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=None here since we measure the effect across all Ts
once the estimator has been fit
"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
an intercept even when bias is part of coef.
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
squeeze the first axis
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
d_t=None here since we measure the effect across all Ts
d_t=None here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
scale preds
scale std errs
"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
offset preds
"offset the distribution, too"
scale preds
"scale the distribution, too"
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
"if stderr is zero, ppf will return nans and the loop below would never terminate"
so bail out early; note that it might be possible to correct the algorithm for
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
be clean
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
don't proxy special methods
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
second level bootstrap which would be prohibitive computationally?
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
can't import from econml.inference at top level without creating cyclical dependencies
Note that inference results are always methods even if the inference is for a property
(e.g. coef__inference() is a method but coef_ is a property)
Therefore we must insert a lambda if getting inference for a non-callable
"If inference is for a property, create a fresh lambda to avoid passing args through"
"try to get interval/std first if appropriate,"
since we don't prefer a wrapped method with this name
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base GRF tree
=============================================================================
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
=============================================================================
A MultOutputWrapper for GRF classes
=============================================================================
=============================================================================
Instantiations of Generalized Random Forest
=============================================================================
"Append a constant treatment if `fit_intercept=True`, the coefficient"
in front of the constant treatment is the intercept in the moment equation.
"Append a constant treatment and constant instrument if `fit_intercept=True`,"
the coefficient in front of the constant treatment is the intercept in the moment equation.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Base Generalized Random Forest
=============================================================================
TODO: support freq_weight and sample_var
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
We calculate the min eigenvalue proxy that each criterion is considering
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Generating indices a priori before parallelism ended up being orders of magnitude
faster than how sklearn does it. The reason is that random samplers do not release the
gil it seems.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
####################
Variance correction
####################
Subtract the average within bag variance. This ends up being equal to the
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
The negative part is just sq_between.
Objective bayes debiasing for the diagonals where we know a-prior they are positive
"The off diagonals we have no objective prior, so no correction is applied."
Finally correcting the pred_cov or pred_var
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
test that the subsampling scheme past to the trees is correct
The sample size is chosen in particular to test rounding based error when subsampling
test that the estimator calcualtes var correctly
test api
test accuracy
test the projection functionality of forests
test that the estimator calcualtes var correctly
test api
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
test that the subsampling scheme past to the trees is correct
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
filter directories by regex if the NOTEBOOK_DIR_PATTERN environment variable is set
omit the lalonde notebook
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
can interpret without uncertainty
can't treat before interpreting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
for is_discrete in [False]:
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
No heterogeneity
Define indices to test
Heterogeneous effects
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
initialize parameters
initialize config wtih base config and overwite some values
predict tree using config parameters and assert
shape of trained tree is the same as y_test
initialize config wtih base honest config and overwite some values
predict tree using config parameters and assert
shape of trained tree is the same as y_test
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
compare with weight
compare with weight
compare with weight
compare with weight
Multi-dimensional output y
1-d y
compare when both sample_var and sample_weight exist
multi-d y
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
dgp
StatsModels2SLS
IV2SLS
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that the subsampling scheme past to the trees is correct
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test inference results when `cate_feature_names` doesn not exist
Test inference results when `cate_feature_names` doesn not exist
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
pvalue for second column should be greater than zero since some points are on either side
of the tested value
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
ensure alpha is passed
only is not None when T1 is a constant or a list of constant
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Nuisance model has no score method, so nuisance_scores_ should be none"
Test non keyword based calls to fit
test non-array inputs
Test custom splitter
Test incomplete set of test folds
"y scores should be positive, since W predicts Y somewhat"
"t scores might not be, since W and T are uncorrelated"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
creating an instance should warn
using the instance should not warn
using the deprecated method should warn
don't warn if b and c are passed by keyword
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make any access to matplotlib or plt throw an exception
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
Invert indices to match latest API
Invert indices to match latest API
The feature for heterogeneity stays constant
Auxiliary function for adding xticks and vertical lines when plotting results
for dynamic dml vs ground truth parameters.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test at least one estimator from each category
test causal graph
test refutation estimate
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
ensure we can serialize unfit estimator
ensure we can serialize fit estimator
expected effect size
test effect
test inference
only OrthoIV support inference other than bootstrap
test summary
test can run score
test cate_feature_names
test can run shap values
dgp
no heterogeneity
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
The __warningregistry__'s need to be in a pristine state for tests
to work properly.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
parameter combinations to test
TODO: serializing/deserializing for every combination -- is this necessary?
ensure we can serialize unfit estimator
ensure we can serialize fit estimator
expected effect size
assert calculated constant marginal effect shape is expected
const_marginal effect is defined in LinearCateEstimator class
assert calculated marginal effect shape is expected
test inference
test can run score
test cate_feature_names
test can run shap values
"dgp (binary T, binary Z)"
no heterogeneity
with heterogeneity
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Create a wrapper around Lasso that doesn't support weights
since Lasso does natively support them starting in sklearn 0.23
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least 6 of every element
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
NOTE: this number may need to change if the default number of folds in
WeightedStratifiedKFold changes
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
make sure we can get out post-fit stuff
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
predetermined splits ensure that all features are seen in each split
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
create a simple artificial setup where effect of moving from treatment
"a -> b is 2,"
"a -> c is 1, and"
"b -> c is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
should rule out some basic issues.
Note that explicitly specifying the dtype as object is necessary until
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
estimated effects should be identical when treatment is explicitly given
but const_marginal_effect should be reordered based on the explicit cagetories
1-> 2 in original ordering; combination of 3->1 and 3->2
test outer grouping
test nested grouping
DML nested CV works via a 'cv' attribute
"with 2-fold outer and 2-fold inner grouping, and six total groups,"
should get 1 or 2 groups per split
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we see
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure that we can serialize unfit estimator
ensure that we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef_ and intercept_ inference
verify we can generate the summary
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
test outer grouping
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
test nested grouping
DML nested CV works via a 'cv' attribute
"with 2-fold outer and 2-fold inner grouping, and six total groups,"
should get 1 or 2 groups per split
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we see
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
helper class
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
#########################
WeightedLassoCVWrapper #
#########################
perform 1D fit
perform 2D fit
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
_penalized_inds is only set during fitting
cv exists on penalized model
now we can access _penalized_inds
check that we can read the cv attribute back out from the underlying model
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
continuous treatments have typical treatment values equal to
the mean of the absolute value of non-zero entries
discrete treatments have typical treatment value 1
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
continuous treatments have typical treatment values equal to
the mean of the absolute value of non-zero entries
discrete treatments have typical treatment value 1
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
make sure we don't run into problems dropping every index
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
dgp
model
model
"columns 'd', 'e', 'h' have too many values"
"columns 'd', 'e' have too many values"
lowering bound shouldn't affect already fit columns when warm starting
"column d is now okay, too"
verify that we can use a scalar treatment cost
verify that we can specify per-treatment costs for each sample
verify that using the same state returns the same results each time
set the categories for column 'd' explicitly so that b is default
"first column: 10 ones, this is fine"
"second column: 6 categories, plenty of random instances of each"
this is fine only if we increase the cateogry limit
"third column: nine ones, lots of twos, not enough unless we disable check"
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
forest heterogeneity won't work
"sixth column: just 1 one, not enough even without check"
increase bound on cat expansion
skip checks (reducing folds accordingly)
"Add tests that guarantee that the reliance on DML feature order is not broken, such as"
"Creare a transformer that zeros out all variables after the first n_x variables, so it zeros out W"
Pass an example where W is irrelevant and X is confounder
"As long as DML doesnt change the order of the inputs, then things should be good. Otherwise X would be"
zeroed out and the test will fail
"shouldn't matter if X is scaled much larger or much smaller than W, we should still get good estimates"
rescaling X shouldn't affect the first stage models because they normalize the inputs
"to recover individual coefficients with linear models, we need to be more careful in how we set up X to avoid"
cross terms
scale by 1000 to match the input to this model:
"the scale of X does matter for the final model, which keeps results in user-denominated units"
rescaling X still shouldn't affect the first stage models
TODO: we don't recover the correct values with enough accuracy to enable this assertion
is there a different way to verify that we are learning the correct coefficients?
"np.testing.assert_allclose(loc1.point.values, theta.flatten(), rtol=1e-1)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Define data features
Added `_df`to names to be different from the default cate_estimator names
Generate data
################################
Single treatment and outcome #
################################
Test LinearDML
|--> Test featurizers
"ColumnTransformer behaves differently depending on version of sklearn, so we no longer check the names"
|--> Test re-fit
Test SparseLinearDML
Test ForestDML
###################################
Mutiple treatments and outcomes #
###################################
Test LinearDML
Test SparseLinearDML
"Single outcome only, ORF does not support multiple outcomes"
Test DMLOrthoForest
Test DROrthoForest
Test XLearner
Skipping population summary names test because bootstrap inference is too slow
Test SLearner
Test TLearner
Test LinearDRLearner
Test SparseLinearDRLearner
Test ForestDRLearner
Test LinearIntentToTreatDRIV
Test DeepIV
Test categorical treatments
Check refit
Check refit after setting categories
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
Treatment effect function
Outcome support
Treatment support
"Generate controls, covariates, treatments and outcomes"
Heterogeneous treatment effects
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
through shap package.
test shap could generate the plot from the shap_values
"waterfall is broken in this version, fixed by https://github.com/slundberg/shap/pull/2444"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
"Note: unlike other Metalearners, we need the controls' encoded column for training"
"Thus, we append the controls column before the one-hot-encoded T"
"We might want to revisit, though, since it's linearly determined by the others"
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
"fit on projected Z: E[T * E[T|X,Z]|X]"
"if discrete, return shape (n,1); if continuous return shape (n,)"
"target will be discrete and will be inversed from FirstStageWrapper, shape (n,1)"
"shape (n,)"
"shape (n,)"
"shape(n,)"
TODO: prel_model_effect could allow sample_var and freq_weight?
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"if discrete, return shape (n,1); if continuous return shape (n,)"
target will be discrete and will be inversed from FirstStageWrapper
"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous"
reshape the predictions
concat W and Z
check nuisances outcome shape
Y_res could be a vector or 1-dimensional 2d-array
"all could be reshaped to vector since Y, T, Z are all single dimensional."
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
A helper class that access all the internal fitted objects of a DRIV Cate Estimator.
Used by both DRIV and IntentToTreatDRIV.
Maggie: I think that would be the case?
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
NOTE: important to use the ortho_learner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
Handles the corner case when X=None but featurizer might be not None
NOTE This is used by the inference methods and is more for internal use to the library
this is a regression model since proj_t is probability
outcome is continuous since proj_t is probability
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
TODO: support freq_weight and sample_var in debiased lasso
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
concat W and Z
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
concat W and Z
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
reshape the predictions
"T_res, Z_res, beta expect shape to be (n,1)"
maybe shouldn't expose fit_cate_intercept in this class?
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: do correct adjustment for sample_var
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
concat W and Z
concat W and Z
concat W and Z
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
"train E[T|X,W,Z]"
"train [Z|X,W]"
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
NOTE: important to use the ortho_learner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
Handles the corner case when X=None but featurizer might be not None
NOTE This is used by the inference methods and is more for internal use to the library
concat W and Z
note that groups are not passed to score because they are only used for fitting
concat W and Z
note that sample_weight and groups are not passed to predict because they are only used for fitting
concat W and Z
A helper class that access all the internal fitted objects of a DMLIV Cate Estimator.
Used by both Parametric and Non Parametric DMLIV.
override only so that we can enforce Z to be required
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Handles the corner case when X=None but featurizer might be not None
Get input names
Summary
coefficient
intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
make T 2D if if was a vector
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: this utility is documented but internal; reimplement?
TODO: this utility is even less public...
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
use same Cs as would be used by default by LogisticRegressionCV
NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification
which could affect how many times each distinct Y value needs to be present in the data
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
but also supports get_feature_names with expected signature
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
NOTE: we rely on the passthrough columns coming first in the concatenated X;W
"when we pipeline scaling with our first stage models later, so the order here is important"
Wrapper to make sure that we get a deep copy of the contents instead of clone returning an untrained copy
Convert python objects to (possibly nested) types that can easily be represented as literals
Convert SingleTreeInterpreter to a python dictionary
named tuple type for storing results inside CausalAnalysis class;
must be lifted to module level to enable pickling
"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,"
"whether they end up in X or in W.  However, for the continuous columns, we want to scale them all"
"when running the first stage models, but don't want to scale the X columns when running the final model,"
since then our coefficients will have odd units and our trees will also have decisions using those units.
""
"we achieve this by pipelining the X scaling with the Y and T models (with fixed scaling, not refitting)"
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
Controls are all other columns of X
"can't use X[:, feat_ind] when X is a DataFrame"
TODO: we can't currently handle unseen values of the feature column when getting the effect;
we might want to modify OrthoLearner (and other discrete treatment classes)
so that the user can opt-in to allowing unseen treatment values
(and return NaN or something in that case)
HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models
and so we can just peel the first columns off of that combined array for rescaling in the pipeline
TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are
"built, such as model_y_feature_names, model_t_feature_names, model_y_transformer, etc., so that this"
becomes a valid approach to handling this
array checking routines don't accept 0-width arrays
perform model selection
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
convert to NormalInferenceResults for consistency
Set the dictionary values shared between local and global summaries
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
required to fit a discrete DML model
"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular"
sub-cases of models or also integrate with azure autoML. (post-MVP)
"TODO: Add other heterogeneity model options, such as {'automl'} for performing"
"model selection for the causal effect, or {'sparse_linear'} for using a debiased lasso. (post-MVP)"
TODO: Enable multi-class classification (post-MVP)
Validate inputs
TODO: check compatibility of X and Y lengths
"no previous fit, cancel warm start"
"work with numeric feature indices, so that we can easily compare with categorical ones"
"if heterogeneity_inds is 1D, repeat it"
heterogeneity inds should be a 2D list of length same as train_inds
replace None elements of heterogeneity_inds and ensure indices are numeric
"TODO: bail out also if categorical columns, classification, random_state changed?"
TODO: should we also train a new model_y under any circumstances when warm_start is True?
train the Y model
"perform model selection for the Y model using all X, not on a per-column basis"
"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
work with the regression wrapper
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
"note that this needs to happen after wrapping to generalize to the multi-class case,"
since otherwise we'll have too many columns to be able to train a classifier
start with empty results and default shared insights
convert categorical indicators to numeric indices
check for indices over the categorical expansion bound
assume we'll be able to train former failures this time; we'll add them back if not
"can't remove in place while iterating over new_inds, so store in separate list"
"train the model, but warn"
no model can be trained in this case since we need more folds
"don't train a model, but suggest workaround since there are enough instances of least"
populated class
also remove from train_inds so we don't try to access the result later
extract subset of names matching new columns
"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
don't want to cache this failed result
properties to return from effect InferenceResults
properties to return from PopulationSummaryResults
Converts strings to property lookups or method calls as a convenience so that the
_point_props and _summary_props above can be applied to an inference object
Create a summary combining all results into a single output; this is used
by the various causal_effect and causal_effect_dict methods to generate either a dataframe
"or a dictionary, respectively, based on the summary function passed into this method"
"ensure array has shape (m,y,t)"
population summary is missing sample dimension; add it for consistency
outcome dimension is missing; add it for consistency
add singleton treatment dimension if missing
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
"each attr has dimension (m,y) or (m,y,t)"
concatenate along treatment dimension
"for dictionary representation, want to remove unneeded sample dimension"
in cohort and global results
TODO: enrich outcome logic for multi-class classification when that is supported
There is no actual sample level in this data
can't drop only level
should be serialization-ready and contain no numpy arrays
"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
TODO: Note that there's no column metadata for the sample number - should there be?
"need to replicate the column info for each sample, then remove from the shared data"
NOTE: the flattened order has the ouptut dimension before the feature dimension
which may need to be revisited once we support multiclass
get the length of the list corresponding to the first dictionary key
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
a global inference indicates the effect of that one feature on the outcome
need to reshape the output to match the input
we want to offset the inference object by the baseline estimate of y
"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
get the length of the list corresponding to the first dictionary key
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
"NOTE: this calculation is correct only if treatment costs are marginal costs,"
because then scaling the difference between treatment value and treatment costs is the
same as scaling the treatment value and subtracting the scaled treatment cost.
""
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
"continuous treatments, the policy value should include the benefit of decreasing treatments"
(rather than just not treating at all)
""
"We can get the total by seeing that if we restrict attention to units where we would treat,"
2 * policy_value - always_treat
includes exactly their contribution because policy_value and always_treat both include it
"and likewise restricting attention to the units where we want to decrease treatment,"
2 * policy_value - always-treat
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
is zero and the contribution to always_treat is negative
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
get dataframe with all but selected column
apply 10% of a typical treatment for this feature
"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely"
set the effect bounds; for positive treatments these agree with
"the estimates; for negative treatments, we need to invert the interval"
the effect is now always positive since we decrease treatment when negative
"for discrete treatment, stack a zero result in front for control"
we need to call effect_inference to get the correct CI between the two treatment options
we now need to construct the delta in the cost between the two treatments and translate the effect
remove third dimenions potentially added
"find cost of current treatment: equality creates a 2d array with True on each row,"
only if its the location of the current treatment. Then we take the corresponding cost.
construct index of current treatment
add second dimension if needed for broadcasting during translation of effect
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: conisder working around relying on sklearn implementation details
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
"If classification methods produce multiple columns of output,"
we need to manually encode classes to ensure consistent column ordering.
We clone the estimator to make sure that all the folds are
"independent, and that it is pickle-able."
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
`predictions` is a list of method outputs from each fold.
"If each of those is also a list, then treat this as a"
multioutput-multiclass task. We need to separately concatenate
the method outputs for each label into an `n_labels` long list.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
TODO: consider working around relying on sklearn implementation details
"TODO: once we drop support for sklearn < 1.0, we can remove this"
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
normalize was deprecated or removed; don't need to do anything
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Call weighted lasso on reduced design matrix
Weighted tau
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set model to WeightedLassoCV by default so there's always a model to get and set attributes on
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
(e.g. former has 'positive' and 'precompute' while latter does not)
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
don't proxy special methods
"don't pass get_params through to model, because that will cause sklearn to clone this"
regressor incorrectly
"Note: for known attributes that have been set this method will not be called,"
so we should just throw here because this is an attribute belonging to this class
but which hasn't yet been set on this instance
set default values for None
check freq_weight should be integer and should be accompanied by sample_var
check array shape
weight X and y and sample_var
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
set default values for None
check array shape
check dimension of instruments is more than dimension of treatments
weight X and y
learn point estimate
solve first stage linear regression E[T|Z]
"""that"" means T̂"
solve second stage linear regression E[Y|that]
(T̂.T*T̂)^{-1}
learn cov(theta)
(T̂.T*T̂)^{-1}
sigma^2
reference: http://www.hec.unil.ch/documents/seminars/deep/361.pdf
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
average the outcome dimension if it exists and ensure 2d y_pred
get index of best treatment
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: consider working around relying on sklearn implementation details
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Convert rows to columns
Require group assignment t to be one-hot-encoded
Get predictions for the 2 splits
Must make sure indices are merged correctly
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Must normalize weights
Override the CATE inference options
Add blb inference to parent's options
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Need to redefine fit here for auto inference to work due to a quirk in how
wrap_fit is defined
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Check that all discrete treatments are represented
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
returns shape-conforming residuals
Copy and/or define models
Define parameter estimators
Define moment and mean gradient estimator
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Call `fit` from parent class
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Expand one-hot encoding to include the zero treatment
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Calculate confidence intervals for the parameter (marginal effect)
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
d_t=None here since we measure the effect across all Ts
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: update docs
"NOTE: sample weight, sample var are not passed in"
Compose final model
Calculate auxiliary quantities
X ⨂ T_res
"sum(model_final.predict(X, T_res))"
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix J"
generate an instance of the final model
generate an instance of the nuisance model
Set _d_t to effective number of treatments
Required for bootstrap inference
for each mc iteration
for each model under cross fit setting
Handles the corner case when X=None but featurizer might be not None
Expand treatments for each time period
NOTE: important to use the _ortho_learner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the _ortho_learner's copy to retain the information from fitting
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/main/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The root toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
def mlasso_model(): return MultiTaskLassoCV(
"cv=3, alphas=alpha_regs, max_iter=200)"
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
"alpha_regs = [5e-3, 1e-2, 5e-2]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
subset of features that are exogenous and create heterogeneity
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
subset of features wrt we estimate heterogeneity
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
introspect the constructor arguments to find the model parameters
to represent
"if the argument is deprecated, ignore it"
Extract and sort argument names excluding 'self'
column names
transfer input to numpy arrays
transfer input to 2d arrays
create dataframe
currently dowhy only support single outcome and single treatment
call dowhy
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
cate estimator but not the effect.
don't proxy special methods
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
For when checking input values is disabled
Type to column extraction function
Prefer sklearn 1.0's get_feature_names_out method to deprecated get_feature_names method
"Some featurizers will throw, such as a pipeline with a transformer that doesn't itself support names"
"Get number of arguments, some sklearn featurizer don't accept feature_names"
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
Get feature names using featurizer
All attempts at retrieving transformed feature names have failed
Delegate handling to downstream logic
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
assume that we should perform nested cross-validation if and only if
the model has a 'cv' attribute; this is a somewhat brittle assumption...
logic copied from check_cv
otherwise we will assume the user already set the cv attribute to something
compatible with splitting with a 'groups' argument
now we have to compute the folds explicitly because some classifiers (like LassoCV)
don't use the groups when calling split internally
Normalize weights
This class is mainly derived from statsmodels.iolib.summary.Summary
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
"want to enforce that each bad_arg was either in kwargs,"
or else it was in neither and is just taking its default value
Any access should throw
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
input feature name is already updated by cate_feature_names.
define the index of d_x to filter for each given T
filter X after broadcast with T for each given T
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains some snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_export.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
make any access to matplotlib or plt throw an exception
make any access to graphviz or plt throw an exception
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
Initialize saturation & value; calculate chroma & value shift
Calculate some intermediate values
Initialize RGB with same hue & chroma as our color
Shift the initial RGB values to match value and store
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use mean of targets
Write node mean CATE
Write node std of CATE
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
Fetch appropriate color for node
Write node mean CATE
Write node mean CATE
Write recommended treatment and value - cost
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
This flag is true when names are set in a child class instead
"If names are set in a child class, add an attribute reflecting that"
This works only if X is passed as a kwarg
We plan to enforce X as kwarg only in future releases
This checks if names have been set in a child class
"If names were set in a child class, don't do it again"
"Wraps-up fit by setting attributes, cleaning up, etc."
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"Treatment names is None, default to BaseCateEstimator"
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
Get input names
Summary
add statsmodels to parent's options
add debiasedlasso to parent's options
add blb to parent's options
TODO Share some logic with non-discrete version
Get input names
Note: we do not transform feature names since that is done within summary_frame
Summary
add statsmodels to parent's options
add statsmodels to parent's options
add blb to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
remove None arguments
"scores entries should be lists of scores, so make each entry a singleton list"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
generate an instance of the final model
generate an instance of the nuisance model
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
alteration even when we only want to fit_final.
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
"however, sklearn doesn't support both stratifying and grouping (see"
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
their own object that supports grouping if they want to use groups.
for each mc iteration
for each model under cross fit setting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Policy Forest
=============================================================================
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base Policy tree
=============================================================================
The values below are required and utilitized by methods in the _SingleTreeExporterMixin
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"since we only allow single dimensional y, we could flatten the prediction"
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Handles the corner case when X=None but featurizer might be not None
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
TODO: support freq_weight and sample_var in debiased lasso
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
Replacing to remove docstring
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE: important to use the rlearner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the rlearner's copy to retain the information from fitting
Handles the corner case when X=None but featurizer might be not None
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `LinearModelFinalInference`
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support freq_weight and sample_var in debiased lasso
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add blb to parent's options
override only so that we can update the docstring to indicate
support for `GenericSingleTreatmentModelFinalInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
note that groups are not passed to score because they are only used for fitting
note that groups are not passed to score because they are only used for fitting
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
NOTE: important to get parent's wrapped copy so that
"after training wrapped featurizer is also trained, etc."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
Fit a doubly robust average effect
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
"If custom param grid, check that only estimator parameters are being altered"
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
override only so that we can update the docstring to indicate support for `blb`
Get input names
Summary
Determine output settings
"Important: This must be the first invocation of the random state at fit time, so that"
train/test splits are re-generatable from an external object simply by knowing the
random_state parameter of the tree. Can be useful in the future if one wants to create local
linear predictions. Currently is also useful for testing.
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
Set min_weight_leaf from min_weight_fraction_leaf
Build tree
We calculate the maximum number of samples from each half-split that any node in the tree can
hold. Used by criterion for memory space savings.
Initialize the criterion object and the criterion_val object if honest.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code is a fork from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_base.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
Set parameters
Don't instantiate estimators now! Parameters of base_estimator might
"still change. Eg., when grid-searching with the nested object syntax."
self.estimators_ needs to be filled by the derived classes in fit.
Compute the number of jobs
Partition estimators between jobs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
covariance matrix
get eigen value and eigen vectors
simulate eigen vectors
keep the top 4 eigen value and corresponding eigen vector
replace the negative eigen values
generate a new covariance matrix
get linear approximation of eigen values
coefs
get the indices of each group of features
print(ind_same_proxy)
demo
same proxy
residuals
gmm
log normal on outliers
positive outliers
negative outliers
demean the new residual again
generate data
sample residuals
get prediction for current investment
get prediction for current proxy
get first period prediction
iterate the step ahead contruction
prepare new x
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
get new covariance matrix
get coefs
get residuals
proxy 1 is the outcome
make fixed residuals
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=None here since we measure the effect across all Ts
once the estimator has been fit
"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
an intercept even when bias is part of coef.
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
squeeze the first axis
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
d_t=None here since we measure the effect across all Ts
d_t=None here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
scale preds
scale std errs
"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
offset preds
"offset the distribution, too"
scale preds
"scale the distribution, too"
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
"if stderr is zero, ppf will return nans and the loop below would never terminate"
so bail out early; note that it might be possible to correct the algorithm for
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
be clean
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
don't proxy special methods
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
second level bootstrap which would be prohibitive computationally?
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
can't import from econml.inference at top level without creating cyclical dependencies
Note that inference results are always methods even if the inference is for a property
(e.g. coef__inference() is a method but coef_ is a property)
Therefore we must insert a lambda if getting inference for a non-callable
"If inference is for a property, create a fresh lambda to avoid passing args through"
"try to get interval/std first if appropriate,"
since we don't prefer a wrapped method with this name
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base GRF tree
=============================================================================
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
=============================================================================
A MultOutputWrapper for GRF classes
=============================================================================
=============================================================================
Instantiations of Generalized Random Forest
=============================================================================
"Append a constant treatment if `fit_intercept=True`, the coefficient"
in front of the constant treatment is the intercept in the moment equation.
"Append a constant treatment and constant instrument if `fit_intercept=True`,"
the coefficient in front of the constant treatment is the intercept in the moment equation.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Base Generalized Random Forest
=============================================================================
TODO: support freq_weight and sample_var
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
We calculate the min eigenvalue proxy that each criterion is considering
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Generating indices a priori before parallelism ended up being orders of magnitude
faster than how sklearn does it. The reason is that random samplers do not release the
gil it seems.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
####################
Variance correction
####################
Subtract the average within bag variance. This ends up being equal to the
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
The negative part is just sq_between.
Objective bayes debiasing for the diagonals where we know a-prior they are positive
"The off diagonals we have no objective prior, so no correction is applied."
Finally correcting the pred_cov or pred_var
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
test that the subsampling scheme past to the trees is correct
The sample size is chosen in particular to test rounding based error when subsampling
test that the estimator calcualtes var correctly
test api
test accuracy
test the projection functionality of forests
test that the estimator calcualtes var correctly
test api
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
test that the subsampling scheme past to the trees is correct
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
filter directories by regex if the NOTEBOOK_DIR_PATTERN environment variable is set
omit the lalonde notebook
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
can interpret without uncertainty
can't treat before interpreting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
for is_discrete in [False]:
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
No heterogeneity
Define indices to test
Heterogeneous effects
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
compare with weight
compare with weight
compare with weight
compare with weight
Multi-dimensional output y
1-d y
compare when both sample_var and sample_weight exist
multi-d y
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
dgp
StatsModels2SLS
IV2SLS
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that the subsampling scheme past to the trees is correct
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test inference results when `cate_feature_names` doesn not exist
Test inference results when `cate_feature_names` doesn not exist
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
pvalue for second column should be greater than zero since some points are on either side
of the tested value
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
ensure alpha is passed
only is not None when T1 is a constant or a list of constant
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Nuisance model has no score method, so nuisance_scores_ should be none"
Test non keyword based calls to fit
test non-array inputs
Test custom splitter
Test incomplete set of test folds
"y scores should be positive, since W predicts Y somewhat"
"t scores might not be, since W and T are uncorrelated"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
creating an instance should warn
using the instance should not warn
using the deprecated method should warn
don't warn if b and c are passed by keyword
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make any access to matplotlib or plt throw an exception
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
Invert indices to match latest API
Invert indices to match latest API
The feature for heterogeneity stays constant
Auxiliary function for adding xticks and vertical lines when plotting results
for dynamic dml vs ground truth parameters.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test at least one estimator from each category
test causal graph
test refutation estimate
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
ensure we can serialize unfit estimator
ensure we can serialize fit estimator
expected effect size
test effect
test inference
only OrthoIV support inference other than bootstrap
test summary
test can run score
test cate_feature_names
test can run shap values
dgp
no heterogeneity
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
The __warningregistry__'s need to be in a pristine state for tests
to work properly.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
ensure we can serialize unfit estimator
ensure we can serialize fit estimator
expected effect size
test effect
test inference
test can run score
test cate_feature_names
test can run shap values
"dgp (binary T, binary Z)"
no heterogeneity
with heterogeneity
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Create a wrapper around Lasso that doesn't support weights
since Lasso does natively support them starting in sklearn 0.23
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least 6 of every element
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
NOTE: this number may need to change if the default number of folds in
WeightedStratifiedKFold changes
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
make sure we can get out post-fit stuff
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
predetermined splits ensure that all features are seen in each split
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
create a simple artificial setup where effect of moving from treatment
"a -> b is 2,"
"a -> c is 1, and"
"b -> c is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
should rule out some basic issues.
Note that explicitly specifying the dtype as object is necessary until
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
estimated effects should be identical when treatment is explicitly given
but const_marginal_effect should be reordered based on the explicit cagetories
1-> 2 in original ordering; combination of 3->1 and 3->2
test outer grouping
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure that we can serialize unfit estimator
ensure that we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef_ and intercept_ inference
verify we can generate the summary
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
test outer grouping
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
helper class
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
#########################
WeightedLassoCVWrapper #
#########################
perform 1D fit
perform 2D fit
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
_penalized_inds is only set during fitting
cv exists on penalized model
now we can access _penalized_inds
check that we can read the cv attribute back out from the underlying model
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
continuous treatments have typical treatment values equal to
the mean of the absolute value of non-zero entries
discrete treatments have typical treatment value 1
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
continuous treatments have typical treatment values equal to
the mean of the absolute value of non-zero entries
discrete treatments have typical treatment value 1
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
make sure we don't run into problems dropping every index
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
dgp
model
model
"columns 'd', 'e', 'h' have too many values"
"columns 'd', 'e' have too many values"
lowering bound shouldn't affect already fit columns when warm starting
"column d is now okay, too"
verify that we can use a scalar treatment cost
verify that we can specify per-treatment costs for each sample
verify that using the same state returns the same results each time
set the categories for column 'd' explicitly so that b is default
"first column: 10 ones, this is fine"
"second column: 6 categories, plenty of random instances of each"
this is fine only if we increase the cateogry limit
"third column: nine ones, lots of twos, not enough unless we disable check"
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
forest heterogeneity won't work
"sixth column: just 1 one, not enough even without check"
increase bound on cat expansion
skip checks (reducing folds accordingly)
"Add tests that guarantee that the reliance on DML feature order is not broken, such as"
"Creare a transformer that zeros out all variables after the first n_x variables, so it zeros out W"
Pass an example where W is irrelevant and X is confounder
"As long as DML doesnt change the order of the inputs, then things should be good. Otherwise X would be"
zeroed out and the test will fail
"shouldn't matter if X is scaled much larger or much smaller than W, we should still get good estimates"
rescaling X shouldn't affect the first stage models because they normalize the inputs
"to recover individual coefficients with linear models, we need to be more careful in how we set up X to avoid"
cross terms
scale by 1000 to match the input to this model:
"the scale of X does matter for the final model, which keeps results in user-denominated units"
rescaling X still shouldn't affect the first stage models
TODO: we don't recover the correct values with enough accuracy to enable this assertion
is there a different way to verify that we are learning the correct coefficients?
"np.testing.assert_allclose(loc1.point.values, theta.flatten(), rtol=1e-1)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Define data features
Added `_df`to names to be different from the default cate_estimator names
Generate data
################################
Single treatment and outcome #
################################
Test LinearDML
|--> Test featurizers
"ColumnTransformer behaves differently depending on version of sklearn, so we no longer check the names"
|--> Test re-fit
Test SparseLinearDML
Test ForestDML
###################################
Mutiple treatments and outcomes #
###################################
Test LinearDML
Test SparseLinearDML
"Single outcome only, ORF does not support multiple outcomes"
Test DMLOrthoForest
Test DROrthoForest
Test XLearner
Skipping population summary names test because bootstrap inference is too slow
Test SLearner
Test TLearner
Test LinearDRLearner
Test SparseLinearDRLearner
Test ForestDRLearner
Test LinearIntentToTreatDRIV
Test DeepIV
Test categorical treatments
Check refit
Check refit after setting categories
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
Treatment effect function
Outcome support
Treatment support
"Generate controls, covariates, treatments and outcomes"
Heterogeneous treatment effects
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
through shap package.
test shap could generate the plot from the shap_values
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
"Note: unlike other Metalearners, we need the controls' encoded column for training"
"Thus, we append the controls column before the one-hot-encoded T"
"We might want to revisit, though, since it's linearly determined by the others"
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
"fit on projected Z: E[T * E[T|X,Z]|X]"
"if discrete, return shape (n,1); if continuous return shape (n,)"
"target will be discrete and will be inversed from FirstStageWrapper, shape (n,1)"
"shape (n,)"
"shape (n,)"
"shape(n,)"
TODO: prel_model_effect could allow sample_var and freq_weight?
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"if discrete, return shape (n,1); if continuous return shape (n,)"
target will be discrete and will be inversed from FirstStageWrapper
"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous"
reshape the predictions
concat W and Z
check nuisances outcome shape
Y_res could be a vector or 1-dimensional 2d-array
"all could be reshaped to vector since Y, T, Z are all single dimensional."
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
A helper class that access all the internal fitted objects of a DRIV Cate Estimator.
Used by both DRIV and IntentToTreatDRIV.
Maggie: I think that would be the case?
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
NOTE: important to use the ortho_learner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
Handles the corner case when X=None but featurizer might be not None
NOTE This is used by the inference methods and is more for internal use to the library
this is a regression model since proj_t is probability
outcome is continuous since proj_t is probability
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
TODO: support freq_weight and sample_var in debiased lasso
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
concat W and Z
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
concat W and Z
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
reshape the predictions
"T_res, Z_res, beta expect shape to be (n,1)"
maybe shouldn't expose fit_cate_intercept in this class?
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: do correct adjustment for sample_var
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
concat W and Z
concat W and Z
concat W and Z
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
"train E[T|X,W,Z]"
"train [Z|X,W]"
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
NOTE: important to use the ortho_learner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
Handles the corner case when X=None but featurizer might be not None
NOTE This is used by the inference methods and is more for internal use to the library
concat W and Z
note that groups are not passed to score because they are only used for fitting
concat W and Z
note that sample_weight and groups are not passed to predict because they are only used for fitting
concat W and Z
A helper class that access all the internal fitted objects of a DMLIV Cate Estimator.
Used by both Parametric and Non Parametric DMLIV.
override only so that we can enforce Z to be required
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Handles the corner case when X=None but featurizer might be not None
Get input names
Summary
coefficient
intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
make T 2D if if was a vector
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: this utility is documented but internal; reimplement?
TODO: this utility is even less public...
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
use same Cs as would be used by default by LogisticRegressionCV
NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification
which could affect how many times each distinct Y value needs to be present in the data
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
but also supports get_feature_names with expected signature
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
NOTE: we rely on the passthrough columns coming first in the concatenated X;W
"when we pipeline scaling with our first stage models later, so the order here is important"
Wrapper to make sure that we get a deep copy of the contents instead of clone returning an untrained copy
Convert python objects to (possibly nested) types that can easily be represented as literals
Convert SingleTreeInterpreter to a python dictionary
named tuple type for storing results inside CausalAnalysis class;
must be lifted to module level to enable pickling
"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,"
"whether they end up in X or in W.  However, for the continuous columns, we want to scale them all"
"when running the first stage models, but don't want to scale the X columns when running the final model,"
since then our coefficients will have odd units and our trees will also have decisions using those units.
""
"we achieve this by pipelining the X scaling with the Y and T models (with fixed scaling, not refitting)"
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
Controls are all other columns of X
"can't use X[:, feat_ind] when X is a DataFrame"
TODO: we can't currently handle unseen values of the feature column when getting the effect;
we might want to modify OrthoLearner (and other discrete treatment classes)
so that the user can opt-in to allowing unseen treatment values
(and return NaN or something in that case)
HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models
and so we can just peel the first columns off of that combined array for rescaling in the pipeline
TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are
"built, such as model_y_feature_names, model_t_feature_names, model_y_transformer, etc., so that this"
becomes a valid approach to handling this
array checking routines don't accept 0-width arrays
perform model selection
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
convert to NormalInferenceResults for consistency
Set the dictionary values shared between local and global summaries
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
required to fit a discrete DML model
"TODO: Add other nuisance model options, such as {'azure_automl', 'forests', 'boosting'} that will use particular"
sub-cases of models or also integrate with azure autoML. (post-MVP)
"TODO: Add other heterogeneity model options, such as {'automl'} for performing"
"model selection for the causal effect, or {'sparse_linear'} for using a debiased lasso. (post-MVP)"
TODO: Enable multi-class classification (post-MVP)
Validate inputs
TODO: check compatibility of X and Y lengths
"no previous fit, cancel warm start"
"work with numeric feature indices, so that we can easily compare with categorical ones"
"if heterogeneity_inds is 1D, repeat it"
heterogeneity inds should be a 2D list of length same as train_inds
replace None elements of heterogeneity_inds and ensure indices are numeric
"TODO: bail out also if categorical columns, classification, random_state changed?"
TODO: should we also train a new model_y under any circumstances when warm_start is True?
train the Y model
"perform model selection for the Y model using all X, not on a per-column basis"
"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
work with the regression wrapper
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
"note that this needs to happen after wrapping to generalize to the multi-class case,"
since otherwise we'll have too many columns to be able to train a classifier
start with empty results and default shared insights
convert categorical indicators to numeric indices
check for indices over the categorical expansion bound
assume we'll be able to train former failures this time; we'll add them back if not
"can't remove in place while iterating over new_inds, so store in separate list"
"train the model, but warn"
no model can be trained in this case since we need more folds
"don't train a model, but suggest workaround since there are enough instances of least"
populated class
also remove from train_inds so we don't try to access the result later
extract subset of names matching new columns
"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
don't want to cache this failed result
properties to return from effect InferenceResults
properties to return from PopulationSummaryResults
Converts strings to property lookups or method calls as a convenience so that the
_point_props and _summary_props above can be applied to an inference object
Create a summary combining all results into a single output; this is used
by the various causal_effect and causal_effect_dict methods to generate either a dataframe
"or a dictionary, respectively, based on the summary function passed into this method"
"ensure array has shape (m,y,t)"
population summary is missing sample dimension; add it for consistency
outcome dimension is missing; add it for consistency
add singleton treatment dimension if missing
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
"each attr has dimension (m,y) or (m,y,t)"
concatenate along treatment dimension
"for dictionary representation, want to remove unneeded sample dimension"
in cohort and global results
TODO: enrich outcome logic for multi-class classification when that is supported
There is no actual sample level in this data
can't drop only level
should be serialization-ready and contain no numpy arrays
"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
TODO: Note that there's no column metadata for the sample number - should there be?
"need to replicate the column info for each sample, then remove from the shared data"
NOTE: the flattened order has the ouptut dimension before the feature dimension
which may need to be revisited once we support multiclass
get the length of the list corresponding to the first dictionary key
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
a global inference indicates the effect of that one feature on the outcome
need to reshape the output to match the input
we want to offset the inference object by the baseline estimate of y
"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
get the length of the list corresponding to the first dictionary key
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
"NOTE: this calculation is correct only if treatment costs are marginal costs,"
because then scaling the difference between treatment value and treatment costs is the
same as scaling the treatment value and subtracting the scaled treatment cost.
""
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
"continuous treatments, the policy value should include the benefit of decreasing treatments"
(rather than just not treating at all)
""
"We can get the total by seeing that if we restrict attention to units where we would treat,"
2 * policy_value - always_treat
includes exactly their contribution because policy_value and always_treat both include it
"and likewise restricting attention to the units where we want to decrease treatment,"
2 * policy_value - always-treat
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
is zero and the contribution to always_treat is negative
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
get dataframe with all but selected column
apply 10% of a typical treatment for this feature
"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely"
set the effect bounds; for positive treatments these agree with
"the estimates; for negative treatments, we need to invert the interval"
the effect is now always positive since we decrease treatment when negative
"for discrete treatment, stack a zero result in front for control"
we need to call effect_inference to get the correct CI between the two treatment options
we now need to construct the delta in the cost between the two treatments and translate the effect
remove third dimenions potentially added
"find cost of current treatment: equality creates a 2d array with True on each row,"
only if its the location of the current treatment. Then we take the corresponding cost.
construct index of current treatment
add second dimension if needed for broadcasting during translation of effect
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: conisder working around relying on sklearn implementation details
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
"If classification methods produce multiple columns of output,"
we need to manually encode classes to ensure consistent column ordering.
We clone the estimator to make sure that all the folds are
"independent, and that it is pickle-able."
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
`predictions` is a list of method outputs from each fold.
"If each of those is also a list, then treat this as a"
multioutput-multiclass task. We need to separately concatenate
the method outputs for each label into an `n_labels` long list.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
TODO: consider working around relying on sklearn implementation details
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Call weighted lasso on reduced design matrix
Weighted tau
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set model to WeightedLassoCV by default so there's always a model to get and set attributes on
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
(e.g. former has 'positive' and 'precompute' while latter does not)
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
don't proxy special methods
"don't pass get_params through to model, because that will cause sklearn to clone this"
regressor incorrectly
"Note: for known attributes that have been set this method will not be called,"
so we should just throw here because this is an attribute belonging to this class
but which hasn't yet been set on this instance
set default values for None
check freq_weight should be integer and should be accompanied by sample_var
check array shape
weight X and y and sample_var
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
set default values for None
check array shape
check dimension of instruments is more than dimension of treatments
weight X and y
learn point estimate
solve first stage linear regression E[T|Z]
"""that"" means T̂"
solve second stage linear regression E[Y|that]
(T̂.T*T̂)^{-1}
learn cov(theta)
(T̂.T*T̂)^{-1}
sigma^2
reference: http://www.hec.unil.ch/documents/seminars/deep/361.pdf
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
average the outcome dimension if it exists and ensure 2d y_pred
get index of best treatment
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: consider working around relying on sklearn implementation details
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Convert rows to columns
Require group assignment t to be one-hot-encoded
Get predictions for the 2 splits
Must make sure indices are merged correctly
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Must normalize weights
Override the CATE inference options
Add blb inference to parent's options
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Need to redefine fit here for auto inference to work due to a quirk in how
wrap_fit is defined
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Check that all discrete treatments are represented
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
returns shape-conforming residuals
Copy and/or define models
Define parameter estimators
Define moment and mean gradient estimator
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Call `fit` from parent class
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Expand one-hot encoding to include the zero treatment
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Calculate confidence intervals for the parameter (marginal effect)
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
d_t=None here since we measure the effect across all Ts
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: update docs
"NOTE: sample weight, sample var are not passed in"
Compose final model
Calculate auxiliary quantities
X ⨂ T_res
"sum(model_final.predict(X, T_res))"
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix J"
generate an instance of the final model
generate an instance of the nuisance model
Set _d_t to effective number of treatments
Required for bootstrap inference
for each mc iteration
for each model under cross fit setting
Handles the corner case when X=None but featurizer might be not None
Expand treatments for each time period
NOTE: important to use the _ortho_learner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the _ortho_learner's copy to retain the information from fitting
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
def mlasso_model(): return MultiTaskLassoCV(
"cv=3, alphas=alpha_regs, max_iter=200)"
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
"alpha_regs = [5e-3, 1e-2, 5e-2]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
subset of features that are exogenous and create heterogeneity
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
subset of features wrt we estimate heterogeneity
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
introspect the constructor arguments to find the model parameters
to represent
"if the argument is deprecated, ignore it"
Extract and sort argument names excluding 'self'
column names
transfer input to numpy arrays
transfer input to 2d arrays
create dataframe
currently dowhy only support single outcome and single treatment
call dowhy
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
cate estimator but not the effect.
don't proxy special methods
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
For when checking input values is disabled
Type to column extraction function
"Get number of arguments, some sklearn featurizer don't accept feature_names"
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
Get feature names using featurizer
All attempts at retrieving transformed feature names have failed
Delegate handling to downstream logic
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
assume that we should perform nested cross-validation if and only if
the model has a 'cv' attribute; this is a somewhat brittle assumption...
logic copied from check_cv
otherwise we will assume the user already set the cv attribute to something
compatible with splitting with a 'groups' argument
now we have to compute the folds explicitly because some classifiers (like LassoCV)
don't use the groups when calling split internally
Normalize weights
This class is mainly derived from statsmodels.iolib.summary.Summary
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
"want to enforce that each bad_arg was either in kwargs,"
or else it was in neither and is just taking its default value
Any access should throw
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
input feature name is already updated by cate_feature_names.
define the index of d_x to filter for each given T
filter X after broadcast with T for each given T
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains some snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
make any access to matplotlib or plt throw an exception
make any access to graphviz or plt throw an exception
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
Initialize saturation & value; calculate chroma & value shift
Calculate some intermediate values
Initialize RGB with same hue & chroma as our color
Shift the initial RGB values to match value and store
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use mean of targets
Write node mean CATE
Write node std of CATE
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
Fetch appropriate color for node
Write node mean CATE
Write node mean CATE
Write recommended treatment and value - cost
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
This flag is true when names are set in a child class instead
"If names are set in a child class, add an attribute reflecting that"
This works only if X is passed as a kwarg
We plan to enforce X as kwarg only in future releases
This checks if names have been set in a child class
"If names were set in a child class, don't do it again"
"Wraps-up fit by setting attributes, cleaning up, etc."
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"Treatment names is None, default to BaseCateEstimator"
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
Get input names
Summary
add statsmodels to parent's options
add debiasedlasso to parent's options
add blb to parent's options
TODO Share some logic with non-discrete version
Get input names
Summary
add statsmodels to parent's options
add statsmodels to parent's options
add blb to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
remove None arguments
"scores entries should be lists of scores, so make each entry a singleton list"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
generate an instance of the final model
generate an instance of the nuisance model
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
alteration even when we only want to fit_final.
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
"however, sklearn doesn't support both stratifying and grouping (see"
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
their own object that supports grouping if they want to use groups.
for each mc iteration
for each model under cross fit setting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Policy Forest
=============================================================================
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base Policy tree
=============================================================================
The values below are required and utilitized by methods in the _SingleTreeExporterMixin
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"since we only allow single dimensional y, we could flatten the prediction"
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Handles the corner case when X=None but featurizer might be not None
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
TODO: support freq_weight and sample_var in debiased lasso
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
Replacing to remove docstring
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE: important to use the rlearner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the rlearner's copy to retain the information from fitting
Handles the corner case when X=None but featurizer might be not None
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `LinearModelFinalInference`
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support freq_weight and sample_var in debiased lasso
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add blb to parent's options
override only so that we can update the docstring to indicate
support for `GenericSingleTreatmentModelFinalInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
note that groups are not passed to score because they are only used for fitting
note that groups are not passed to score because they are only used for fitting
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
NOTE: important to get parent's wrapped copy so that
"after training wrapped featurizer is also trained, etc."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
Fit a doubly robust average effect
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
"If custom param grid, check that only estimator parameters are being altered"
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
override only so that we can update the docstring to indicate support for `blb`
Get input names
Summary
Determine output settings
"Important: This must be the first invocation of the random state at fit time, so that"
train/test splits are re-generatable from an external object simply by knowing the
random_state parameter of the tree. Can be useful in the future if one wants to create local
linear predictions. Currently is also useful for testing.
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
Set min_weight_leaf from min_weight_fraction_leaf
Build tree
We calculate the maximum number of samples from each half-split that any node in the tree can
hold. Used by criterion for memory space savings.
Initialize the criterion object and the criterion_val object if honest.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code is a fork from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
Set parameters
Don't instantiate estimators now! Parameters of base_estimator might
"still change. Eg., when grid-searching with the nested object syntax."
self.estimators_ needs to be filled by the derived classes in fit.
Compute the number of jobs
Partition estimators between jobs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
covariance matrix
get eigen value and eigen vectors
simulate eigen vectors
keep the top 4 eigen value and corresponding eigen vector
replace the negative eigen values
generate a new covariance matrix
get linear approximation of eigen values
coefs
get the indices of each group of features
print(ind_same_proxy)
demo
same proxy
residuals
gmm
log normal on outliers
positive outliers
negative outliers
demean the new residual again
generate data
sample residuals
get prediction for current investment
get prediction for current proxy
get first period prediction
iterate the step ahead contruction
prepare new x
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
get new covariance matrix
get coefs
get residuals
proxy 1 is the outcome
make fixed residuals
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=None here since we measure the effect across all Ts
once the estimator has been fit
"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
an intercept even when bias is part of coef.
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
squeeze the first axis
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
d_t=None here since we measure the effect across all Ts
d_t=None here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
scale preds
scale std errs
"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
offset preds
"offset the distribution, too"
scale preds
"scale the distribution, too"
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
"if stderr is zero, ppf will return nans and the loop below would never terminate"
so bail out early; note that it might be possible to correct the algorithm for
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
be clean
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
don't proxy special methods
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
second level bootstrap which would be prohibitive computationally?
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
can't import from econml.inference at top level without creating cyclical dependencies
Note that inference results are always methods even if the inference is for a property
(e.g. coef__inference() is a method but coef_ is a property)
Therefore we must insert a lambda if getting inference for a non-callable
"If inference is for a property, create a fresh lambda to avoid passing args through"
"try to get interval/std first if appropriate,"
since we don't prefer a wrapped method with this name
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base GRF tree
=============================================================================
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
=============================================================================
A MultOutputWrapper for GRF classes
=============================================================================
=============================================================================
Instantiations of Generalized Random Forest
=============================================================================
"Append a constant treatment if `fit_intercept=True`, the coefficient"
in front of the constant treatment is the intercept in the moment equation.
"Append a constant treatment and constant instrument if `fit_intercept=True`,"
the coefficient in front of the constant treatment is the intercept in the moment equation.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Base Generalized Random Forest
=============================================================================
TODO: support freq_weight and sample_var
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
We calculate the min eigenvalue proxy that each criterion is considering
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Generating indices a priori before parallelism ended up being orders of magnitude
faster than how sklearn does it. The reason is that random samplers do not release the
gil it seems.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
####################
Variance correction
####################
Subtract the average within bag variance. This ends up being equal to the
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
The negative part is just sq_between.
Objective bayes debiasing for the diagonals where we know a-prior they are positive
"The off diagonals we have no objective prior, so no correction is applied."
Finally correcting the pred_cov or pred_var
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
test that the subsampling scheme past to the trees is correct
The sample size is chosen in particular to test rounding based error when subsampling
test that the estimator calcualtes var correctly
test api
test accuracy
test the projection functionality of forests
test that the estimator calcualtes var correctly
test api
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
test that the subsampling scheme past to the trees is correct
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
omit the lalonde notebook
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
can interpret without uncertainty
can't treat before interpreting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
for is_discrete in [False]:
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
No heterogeneity
Define indices to test
Heterogeneous effects
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
compare with weight
compare with weight
compare with weight
compare with weight
Multi-dimensional output y
1-d y
compare when both sample_var and sample_weight exist
multi-d y
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
dgp
StatsModels2SLS
IV2SLS
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that the subsampling scheme past to the trees is correct
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test inference results when `cate_feature_names` doesn not exist
Test inference results when `cate_feature_names` doesn not exist
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
pvalue for second column should be greater than zero since some points are on either side
of the tested value
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
ensure alpha is passed
only is not None when T1 is a constant or a list of constant
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Nuisance model has no score method, so nuisance_scores_ should be none"
Test non keyword based calls to fit
test non-array inputs
Test custom splitter
Test incomplete set of test folds
"y scores should be positive, since W predicts Y somewhat"
"t scores might not be, since W and T are uncorrelated"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
creating an instance should warn
using the instance should not warn
using the deprecated method should warn
don't warn if b and c are passed by keyword
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make any access to matplotlib or plt throw an exception
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
Invert indices to match latest API
Invert indices to match latest API
The feature for heterogeneity stays constant
Auxiliary function for adding xticks and vertical lines when plotting results
for dynamic dml vs ground truth parameters.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test at least one estimator from each category
test causal graph
test refutation estimate
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
ensure we can serialize unfit estimator
ensure we can serialize fit estimator
expected effect size
test effect
test inference
only OrthoIV support inference other than bootstrap
test summary
test can run score
test cate_feature_names
test can run shap values
dgp
no heterogeneity
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
The __warningregistry__'s need to be in a pristine state for tests
to work properly.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
ensure we can serialize unfit estimator
ensure we can serialize fit estimator
expected effect size
test effect
test inference
test can run score
test cate_feature_names
test can run shap values
"dgp (binary T, binary Z)"
no heterogeneity
with heterogeneity
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Create a wrapper around Lasso that doesn't support weights
since Lasso does natively support them starting in sklearn 0.23
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least 6 of every element
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
NOTE: this number may need to change if the default number of folds in
WeightedStratifiedKFold changes
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
make sure we can get out post-fit stuff
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
predetermined splits ensure that all features are seen in each split
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
create a simple artificial setup where effect of moving from treatment
"a -> b is 2,"
"a -> c is 1, and"
"b -> c is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
should rule out some basic issues.
Note that explicitly specifying the dtype as object is necessary until
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
estimated effects should be identical when treatment is explicitly given
but const_marginal_effect should be reordered based on the explicit cagetories
1-> 2 in original ordering; combination of 3->1 and 3->2
test outer grouping
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure that we can serialize unfit estimator
ensure that we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef_ and intercept_ inference
verify we can generate the summary
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
test outer grouping
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
helper class
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
#########################
WeightedLassoCVWrapper #
#########################
perform 1D fit
perform 2D fit
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
_penalized_inds is only set during fitting
cv exists on penalized model
now we can access _penalized_inds
check that we can read the cv attribute back out from the underlying model
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
continuous treatments have typical treatment values equal to
the mean of the absolute value of non-zero entries
discrete treatments have typical treatment value 1
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
continuous treatments have typical treatment values equal to
the mean of the absolute value of non-zero entries
discrete treatments have typical treatment value 1
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
make sure we don't run into problems dropping every index
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
dgp
model
model
"columns 'd', 'e', 'h' have too many values"
"columns 'd', 'e' have too many values"
lowering bound shouldn't affect already fit columns when warm starting
"column d is now okay, too"
verify that we can use a scalar treatment cost
verify that we can specify per-treatment costs for each sample
verify that using the same state returns the same results each time
set the categories for column 'd' explicitly so that b is default
"first column: 10 ones, this is fine"
"second column: 6 categories, plenty of random instances of each"
this is fine only if we increase the cateogry limit
"third column: nine ones, lots of twos, not enough unless we disable check"
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
forest heterogeneity won't work
"sixth column: just 1 one, not enough even without check"
increase bound on cat expansion
skip checks (reducing folds accordingly)
"Add tests that guarantee that the reliance on DML feature order is not broken, such as"
"Creare a transformer that zeros out all variables after the first n_x variables, so it zeros out W"
Pass an example where W is irrelevant and X is confounder
"As long as DML doesnt change the order of the inputs, then things should be good. Otherwise X would be"
zeroed out and the test will fail
"shouldn't matter if X is scaled much larger or much smaller than W, we should still get good estimates"
rescaling X shouldn't affect the first stage models because they normalize the inputs
"to recover individual coefficients with linear models, we need to be more careful in how we set up X to avoid"
cross terms
scale by 1000 to match the input to this model:
"the scale of X does matter for the final model, which keeps results in user-denominated units"
rescaling X still shouldn't affect the first stage models
TODO: we don't recover the correct values with enough accuracy to enable this assertion
is there a different way to verify that we are learning the correct coefficients?
"np.testing.assert_allclose(loc1.point.values, theta.flatten(), rtol=1e-1)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Define data features
Added `_df`to names to be different from the default cate_estimator names
Generate data
################################
Single treatment and outcome #
################################
Test LinearDML
|--> Test featurizers
ColumnTransformer doesn't propagate column names
|--> Test re-fit
Test SparseLinearDML
Test ForestDML
###################################
Mutiple treatments and outcomes #
###################################
Test LinearDML
Test SparseLinearDML
"Single outcome only, ORF does not support multiple outcomes"
Test DMLOrthoForest
Test DROrthoForest
Test XLearner
Skipping population summary names test because bootstrap inference is too slow
Test SLearner
Test TLearner
Test LinearDRLearner
Test SparseLinearDRLearner
Test ForestDRLearner
Test LinearIntentToTreatDRIV
Test DeepIV
Test categorical treatments
Check refit
Check refit after setting categories
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
Treatment effect function
Outcome support
Treatment support
"Generate controls, covariates, treatments and outcomes"
Heterogeneous treatment effects
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
through shap package.
test shap could generate the plot from the shap_values
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
"Note: unlike other Metalearners, we need the controls' encoded column for training"
"Thus, we append the controls column before the one-hot-encoded T"
"We might want to revisit, though, since it's linearly determined by the others"
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
"fit on projected Z: E[T * E[T|X,Z]|X]"
"if discrete, return shape (n,1); if continuous return shape (n,)"
"target will be discrete and will be inversed from FirstStageWrapper, shape (n,1)"
"shape (n,)"
"shape (n,)"
"shape(n,)"
TODO: prel_model_effect could allow sample_var and freq_weight?
"T and Z only allow single continuous or binary, keep the shape of (n,) for continuous and (n,1) for binary"
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"if discrete, return shape (n,1); if continuous return shape (n,)"
target will be discrete and will be inversed from FirstStageWrapper
"for convenience, reshape Z,T to a vector since they are either binary or single dimensional continuous"
reshape the predictions
concat W and Z
check nuisances outcome shape
Y_res could be a vector or 1-dimensional 2d-array
"all could be reshaped to vector since Y, T, Z are all single dimensional."
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
A helper class that access all the internal fitted objects of a DRIV Cate Estimator.
Used by both DRIV and IntentToTreatDRIV.
Maggie: I think that would be the case?
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
NOTE: important to use the ortho_learner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
Handles the corner case when X=None but featurizer might be not None
NOTE This is used by the inference methods and is more for internal use to the library
this is a regression model since proj_t is probability
outcome is continuous since proj_t is probability
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
TODO: support freq_weight and sample_var in debiased lasso
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
concat W and Z
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
concat W and Z
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
reshape the predictions
"T_res, Z_res, beta expect shape to be (n,1)"
maybe shouldn't expose fit_cate_intercept in this class?
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: do correct adjustment for sample_var
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
concat W and Z
concat W and Z
concat W and Z
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
"train E[T|X,W,Z]"
"train [Z|X,W]"
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
NOTE: important to use the ortho_learner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
Handles the corner case when X=None but featurizer might be not None
NOTE This is used by the inference methods and is more for internal use to the library
concat W and Z
note that groups are not passed to score because they are only used for fitting
concat W and Z
note that sample_weight and groups are not passed to predict because they are only used for fitting
concat W and Z
A helper class that access all the internal fitted objects of a DMLIV Cate Estimator.
Used by both Parametric and Non Parametric DMLIV.
override only so that we can enforce Z to be required
"Replacing score from _OrthoLearner, to enforce Z to be required and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Handles the corner case when X=None but featurizer might be not None
Get input names
Summary
coefficient
intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
make T 2D if if was a vector
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: this utility is documented but internal; reimplement?
TODO: this utility is even less public...
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
use same Cs as would be used by default by LogisticRegressionCV
NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification
which could affect how many times each distinct Y value needs to be present in the data
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
but also supports get_feature_names with expected signature
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
NOTE: we rely on the passthrough columns coming first in the concatenated X;W
"when we pipeline scaling with our first stage models later, so the order here is important"
Wrapper to make sure that we get a deep copy of the contents instead of clone returning an untrained copy
Convert python objects to (possibly nested) types that can easily be represented as literals
Convert SingleTreeInterpreter to a python dictionary
named tuple type for storing results inside CausalAnalysis class;
must be lifted to module level to enable pickling
"the transformation logic here is somewhat tricky; we always need to encode the categorical columns,"
"whether they end up in X or in W.  However, for the continuous columns, we want to scale them all"
"when running the first stage models, but don't want to scale the X columns when running the final model,"
since then our coefficients will have odd units and our trees will also have decisions using those units.
""
"we achieve this by pipelining the X scaling with the Y and T models (with fixed scaling, not refitting)"
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
Controls are all other columns of X
"can't use X[:, feat_ind] when X is a DataFrame"
TODO: we can't currently handle unseen values of the feature column when getting the effect;
we might want to modify OrthoLearner (and other discrete treatment classes)
so that the user can opt-in to allowing unseen treatment values
(and return NaN or something in that case)
HACK: this is slightly ugly because we rely on the fact that DML passes [X;W] to the first stage models
and so we can just peel the first columns off of that combined array for rescaling in the pipeline
TODO: consider addding an API to DML that allows for better understanding of how the nuisance inputs are
"built, such as model_y_feature_names, model_t_feature_names, model_y_transformer, etc., so that this"
becomes a valid approach to handling this
array checking routines don't accept 0-width arrays
perform model selection
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
convert to NormalInferenceResults for consistency
Set the dictionary values shared between local and global summaries
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
required to fit a discrete DML model
Validate inputs
TODO: check compatibility of X and Y lengths
"no previous fit, cancel warm start"
"work with numeric feature indices, so that we can easily compare with categorical ones"
"if heterogeneity_inds is 1D, repeat it"
heterogeneity inds should be a 2D list of length same as train_inds
replace None elements of heterogeneity_inds and ensure indices are numeric
"TODO: bail out also if categorical columns, classification, random_state changed?"
TODO: should we also train a new model_y under any circumstances when warm_start is True?
train the Y model
"perform model selection for the Y model using all X, not on a per-column basis"
"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
work with the regression wrapper
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
"note that this needs to happen after wrapping to generalize to the multi-class case,"
since otherwise we'll have too many columns to be able to train a classifier
start with empty results and default shared insights
convert categorical indicators to numeric indices
check for indices over the categorical expansion bound
assume we'll be able to train former failures this time; we'll add them back if not
"can't remove in place while iterating over new_inds, so store in separate list"
"train the model, but warn"
no model can be trained in this case since we need more folds
"don't train a model, but suggest workaround since there are enough instances of least"
populated class
also remove from train_inds so we don't try to access the result later
extract subset of names matching new columns
"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
don't want to cache this failed result
properties to return from effect InferenceResults
properties to return from PopulationSummaryResults
Converts strings to property lookups or method calls as a convenience so that the
_point_props and _summary_props above can be applied to an inference object
Create a summary combining all results into a single output; this is used
by the various causal_effect and causal_effect_dict methods to generate either a dataframe
"or a dictionary, respectively, based on the summary function passed into this method"
"ensure array has shape (m,y,t)"
population summary is missing sample dimension; add it for consistency
outcome dimension is missing; add it for consistency
add singleton treatment dimension if missing
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
"each attr has dimension (m,y) or (m,y,t)"
concatenate along treatment dimension
"for dictionary representation, want to remove unneeded sample dimension"
in cohort and global results
TODO: enrich outcome logic for multi-class classification when that is supported
There is no actual sample level in this data
can't drop only level
should be serialization-ready and contain no numpy arrays
"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
TODO: Note that there's no column metadata for the sample number - should there be?
"need to replicate the column info for each sample, then remove from the shared data"
NOTE: the flattened order has the ouptut dimension before the feature dimension
which may need to be revisited once we support multiclass
get the length of the list corresponding to the first dictionary key
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
a global inference indicates the effect of that one feature on the outcome
need to reshape the output to match the input
we want to offset the inference object by the baseline estimate of y
"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
get the length of the list corresponding to the first dictionary key
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
"NOTE: this calculation is correct only if treatment costs are marginal costs,"
because then scaling the difference between treatment value and treatment costs is the
same as scaling the treatment value and subtracting the scaled treatment cost.
""
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
"continuous treatments, the policy value should include the benefit of decreasing treatments"
(rather than just not treating at all)
""
"We can get the total by seeing that if we restrict attention to units where we would treat,"
2 * policy_value - always_treat
includes exactly their contribution because policy_value and always_treat both include it
"and likewise restricting attention to the units where we want to decrease treatment,"
2 * policy_value - always-treat
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
is zero and the contribution to always_treat is negative
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
get dataframe with all but selected column
apply 10% of a typical treatment for this feature
"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely"
set the effect bounds; for positive treatments these agree with
"the estimates; for negative treatments, we need to invert the interval"
the effect is now always positive since we decrease treatment when negative
"for discrete treatment, stack a zero result in front for control"
we need to call effect_inference to get the correct CI between the two treatment options
we now need to construct the delta in the cost between the two treatments and translate the effect
remove third dimenions potentially added
"find cost of current treatment: equality creates a 2d array with True on each row,"
only if its the location of the current treatment. Then we take the corresponding cost.
construct index of current treatment
add second dimension if needed for broadcasting during translation of effect
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: conisder working around relying on sklearn implementation details
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
"If classification methods produce multiple columns of output,"
we need to manually encode classes to ensure consistent column ordering.
We clone the estimator to make sure that all the folds are
"independent, and that it is pickle-able."
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
`predictions` is a list of method outputs from each fold.
"If each of those is also a list, then treat this as a"
multioutput-multiclass task. We need to separately concatenate
the method outputs for each label into an `n_labels` long list.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
TODO: consider working around relying on sklearn implementation details
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Call weighted lasso on reduced design matrix
Weighted tau
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set model to WeightedLassoCV by default so there's always a model to get and set attributes on
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
(e.g. former has 'positive' and 'precompute' while latter does not)
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
don't proxy special methods
"don't pass get_params through to model, because that will cause sklearn to clone this"
regressor incorrectly
"Note: for known attributes that have been set this method will not be called,"
so we should just throw here because this is an attribute belonging to this class
but which hasn't yet been set on this instance
set default values for None
check freq_weight should be integer and should be accompanied by sample_var
check array shape
weight X and y and sample_var
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
set default values for None
check array shape
check dimension of instruments is more than dimension of treatments
weight X and y
learn point estimate
solve first stage linear regression E[T|Z]
"""that"" means T̂"
solve second stage linear regression E[Y|that]
(T̂.T*T̂)^{-1}
learn cov(theta)
(T̂.T*T̂)^{-1}
sigma^2
reference: http://www.hec.unil.ch/documents/seminars/deep/361.pdf
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
average the outcome dimension if it exists and ensure 2d y_pred
get index of best treatment
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: consider working around relying on sklearn implementation details
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Convert rows to columns
Require group assignment t to be one-hot-encoded
Get predictions for the 2 splits
Must make sure indices are merged correctly
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Must normalize weights
Override the CATE inference options
Add blb inference to parent's options
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Need to redefine fit here for auto inference to work due to a quirk in how
wrap_fit is defined
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Check that all discrete treatments are represented
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
returns shape-conforming residuals
Copy and/or define models
Define parameter estimators
Define moment and mean gradient estimator
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Call `fit` from parent class
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Expand one-hot encoding to include the zero treatment
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Calculate confidence intervals for the parameter (marginal effect)
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
d_t=None here since we measure the effect across all Ts
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: update docs
"NOTE: sample weight, sample var are not passed in"
Compose final model
Calculate auxiliary quantities
X ⨂ T_res
"sum(model_final.predict(X, T_res))"
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (t, j) block entry (of size n_treatments x n_treatments) of matrix J"
generate an instance of the final model
generate an instance of the nuisance model
Set _d_t to effective number of treatments
Required for bootstrap inference
for each mc iteration
for each model under cross fit setting
Handles the corner case when X=None but featurizer might be not None
Expand treatments for each time period
NOTE: important to use the _ortho_learner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the _ortho_learner's copy to retain the information from fitting
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
def mlasso_model(): return MultiTaskLassoCV(
"cv=3, alphas=alpha_regs, max_iter=200)"
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
"alpha_regs = [5e-3, 1e-2, 5e-2]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
subset of features that are exogenous and create heterogeneity
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
subset of features wrt we estimate heterogeneity
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
introspect the constructor arguments to find the model parameters
to represent
"if the argument is deprecated, ignore it"
Extract and sort argument names excluding 'self'
column names
transfer input to numpy arrays
transfer input to 2d arrays
create dataframe
currently dowhy only support single outcome and single treatment
call dowhy
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
cate estimator but not the effect.
don't proxy special methods
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
For when checking input values is disabled
Type to column extraction function
"Get number of arguments, some sklearn featurizer don't accept feature_names"
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
Get feature names using featurizer
All attempts at retrieving transformed feature names have failed
Delegate handling to downstream logic
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
assume that we should perform nested cross-validation if and only if
the model has a 'cv' attribute; this is a somewhat brittle assumption...
logic copied from check_cv
otherwise we will assume the user already set the cv attribute to something
compatible with splitting with a 'groups' argument
now we have to compute the folds explicitly because some classifiers (like LassoCV)
don't use the groups when calling split internally
Normalize weights
This class is mainly derived from statsmodels.iolib.summary.Summary
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
"want to enforce that each bad_arg was either in kwargs,"
or else it was in neither and is just taking its default value
Any access should throw
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
input feature name is already updated by cate_feature_names.
define the index of d_x to filter for each given T
filter X after broadcast with T for each given T
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains some snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
make any access to matplotlib or plt throw an exception
make any access to graphviz or plt throw an exception
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
Initialize saturation & value; calculate chroma & value shift
Calculate some intermediate values
Initialize RGB with same hue & chroma as our color
Shift the initial RGB values to match value and store
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use mean of targets
Write node mean CATE
Write node std of CATE
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
Fetch appropriate color for node
Write node mean CATE
Write node mean CATE
Write recommended treatment and value - cost
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
This flag is true when names are set in a child class instead
"If names are set in a child class, add an attribute reflecting that"
This works only if X is passed as a kwarg
We plan to enforce X as kwarg only in future releases
This checks if names have been set in a child class
"If names were set in a child class, don't do it again"
"Wraps-up fit by setting attributes, cleaning up, etc."
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"Treatment names is None, default to BaseCateEstimator"
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
Get input names
Summary
add statsmodels to parent's options
add debiasedlasso to parent's options
add blb to parent's options
TODO Share some logic with non-discrete version
Get input names
Summary
add statsmodels to parent's options
add statsmodels to parent's options
add blb to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
remove None arguments
"scores entries should be lists of scores, so make each entry a singleton list"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
generate an instance of the final model
generate an instance of the nuisance model
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
alteration even when we only want to fit_final.
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
"however, sklearn doesn't support both stratifying and grouping (see"
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
their own object that supports grouping if they want to use groups.
for each mc iteration
for each model under cross fit setting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Policy Forest
=============================================================================
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base Policy tree
=============================================================================
The values below are required and utilitized by methods in the _SingleTreeExporterMixin
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"since we only allow single dimensional y, we could flatten the prediction"
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Handles the corner case when X=None but featurizer might be not None
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
TODO: support freq_weight and sample_var in debiased lasso
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
Replacing to remove docstring
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE: important to use the rlearner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the rlearner's copy to retain the information from fitting
Handles the corner case when X=None but featurizer might be not None
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `StatsModelsInference`
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support freq_weight and sample_var in debiased lasso
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add blb to parent's options
override only so that we can update the docstring to indicate
support for `GenericSingleTreatmentModelFinalInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
note that groups are not passed to score because they are only used for fitting
note that groups are not passed to score because they are only used for fitting
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
NOTE: important to get parent's wrapped copy so that
"after training wrapped featurizer is also trained, etc."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
Fit a doubly robust average effect
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
"If custom param grid, check that only estimator parameters are being altered"
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
override only so that we can update the docstring to indicate support for `blb`
Get input names
Summary
Determine output settings
"Important: This must be the first invocation of the random state at fit time, so that"
train/test splits are re-generatable from an external object simply by knowing the
random_state parameter of the tree. Can be useful in the future if one wants to create local
linear predictions. Currently is also useful for testing.
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
Set min_weight_leaf from min_weight_fraction_leaf
Build tree
We calculate the maximum number of samples from each half-split that any node in the tree can
hold. Used by criterion for memory space savings.
Initialize the criterion object and the criterion_val object if honest.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code is a fork from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
Set parameters
Don't instantiate estimators now! Parameters of base_estimator might
"still change. Eg., when grid-searching with the nested object syntax."
self.estimators_ needs to be filled by the derived classes in fit.
Compute the number of jobs
Partition estimators between jobs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=None here since we measure the effect across all Ts
once the estimator has been fit
"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
an intercept even when bias is part of coef.
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
squeeze the first axis
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
d_t=None here since we measure the effect across all Ts
d_t=None here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
scale preds
scale std errs
"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
offset preds
"offset the distribution, too"
scale preds
"scale the distribution, too"
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
"if stderr is zero, ppf will return nans and the loop below would never terminate"
so bail out early; note that it might be possible to correct the algorithm for
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
be clean
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
don't proxy special methods
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
second level bootstrap which would be prohibitive computationally?
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
can't import from econml.inference at top level without creating cyclical dependencies
Note that inference results are always methods even if the inference is for a property
(e.g. coef__inference() is a method but coef_ is a property)
Therefore we must insert a lambda if getting inference for a non-callable
"If inference is for a property, create a fresh lambda to avoid passing args through"
"try to get interval/std first if appropriate,"
since we don't prefer a wrapped method with this name
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base GRF tree
=============================================================================
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
=============================================================================
A MultOutputWrapper for GRF classes
=============================================================================
=============================================================================
Instantiations of Generalized Random Forest
=============================================================================
"Append a constant treatment if `fit_intercept=True`, the coefficient"
in front of the constant treatment is the intercept in the moment equation.
"Append a constant treatment and constant instrument if `fit_intercept=True`,"
the coefficient in front of the constant treatment is the intercept in the moment equation.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Base Generalized Random Forest
=============================================================================
TODO: support freq_weight and sample_var
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
We calculate the min eigenvalue proxy that each criterion is considering
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Generating indices a priori before parallelism ended up being orders of magnitude
faster than how sklearn does it. The reason is that random samplers do not release the
gil it seems.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
####################
Variance correction
####################
Subtract the average within bag variance. This ends up being equal to the
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
The negative part is just sq_between.
Objective bayes debiasing for the diagonals where we know a-prior they are positive
"The off diagonals we have no objective prior, so no correction is applied."
Finally correcting the pred_cov or pred_var
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
test that the subsampling scheme past to the trees is correct
The sample size is chosen in particular to test rounding based error when subsampling
test that the estimator calcualtes var correctly
test api
test accuracy
test the projection functionality of forests
test that the estimator calcualtes var correctly
test api
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
test that the subsampling scheme past to the trees is correct
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
omit the lalonde notebook
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
can interpret without uncertainty
can't treat before interpreting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
compare with weight
compare with weight
compare with weight
compare with weight
Multi-dimensional output y
1-d y
compare when both sample_var and sample_weight exist
multi-d y
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that the subsampling scheme past to the trees is correct
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test inference results when `cate_feature_names` doesn not exist
Test inference results when `cate_feature_names` doesn not exist
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
pvalue for second column should be greater than zero since some points are on either side
of the tested value
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
only is not None when T1 is a constant or a list of constant
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Nuisance model has no score method, so nuisance_scores_ should be none"
Test non keyword based calls to fit
test non-array inputs
Test custom splitter
Test incomplete set of test folds
"y scores should be positive, since W predicts Y somewhat"
"t scores might not be, since W and T are uncorrelated"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
creating an instance should warn
using the instance should not warn
using the deprecated method should warn
don't warn if b and c are passed by keyword
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test at least one estimator from each category
test causal graph
test refutation estimate
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
ensure that we've got at least two of every row
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
need to make sure we get all *joint* combinations
IntentToTreat only supports binary treatments/instruments
IntentToTreat only supports binary treatments/instruments
IntentToTreat requires X
ensure we can serialize unfit estimator
these support only W but not X
"these support only binary, not general discrete T and Z"
ensure we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
TODO: add tests for extra properties like coef_ where they exist
TODO: add tests for extra properties like coef_ where they exist
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
TODO: ideally we could also test whether Z and X are jointly okay when both discrete
"however, with custom splits the checking happens in the first stage wrapper"
where we don't have all of the required information to do this;
we'd probably need to add it to _crossfit instead
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
The __warningregistry__'s need to be in a pristine state for tests
to work properly.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Create a wrapper around Lasso that doesn't support weights
since Lasso does natively support them starting in sklearn 0.23
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least 6 of every element
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
NOTE: this number may need to change if the default number of folds in
WeightedStratifiedKFold changes
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
make sure we can get out post-fit stuff
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
predetermined splits ensure that all features are seen in each split
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
create a simple artificial setup where effect of moving from treatment
"a -> b is 2,"
"a -> c is 1, and"
"b -> c is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
should rule out some basic issues.
Note that explicitly specifying the dtype as object is necessary until
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
estimated effects should be identical when treatment is explicitly given
but const_marginal_effect should be reordered based on the explicit cagetories
1-> 2 in original ordering; combination of 3->1 and 3->2
test outer grouping
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure that we can serialize unfit estimator
ensure that we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef_ and intercept_ inference
verify we can generate the summary
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
test outer grouping
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
helper class
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
#########################
WeightedLassoCVWrapper #
#########################
perform 1D fit
perform 2D fit
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
_penalized_inds is only set during fitting
cv exists on penalized model
now we can access _penalized_inds
check that we can read the cv attribute back out from the underlying model
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
continuous treatments have typical treatment values equal to
the mean of the absolute value of non-zero entries
discrete treatments have typical treatment value 1
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
continuous treatments have typical treatment values equal to
the mean of the absolute value of non-zero entries
discrete treatments have typical treatment value 1
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
make sure we don't run into problems dropping every index
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
dgp
model
model
"columns 'd', 'e', 'h' have too many values"
"columns 'd', 'e' have too many values"
lowering bound shouldn't affect already fit columns when warm starting
"column d is now okay, too"
verify that we can use a scalar treatment cost
verify that we can specify per-treatment costs for each sample
verify that using the same state returns the same results each time
set the categories for column 'd' explicitly so that b is default
"first column: 10 ones, this is fine"
"second column: 6 categories, plenty of random instances of each"
this is fine only if we increase the cateogry limit
"third column: nine ones, lots of twos, not enough unless we disable check"
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
forest heterogeneity won't work
"sixth column: just 1 one, not enough even without check"
increase bound on cat expansion
skip checks (reducing folds accordingly)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Define data features
Added `_df`to names to be different from the default cate_estimator names
Generate data
################################
Single treatment and outcome #
################################
Test LinearDML
|--> Test featurizers
ColumnTransformer doesn't propagate column names
|--> Test re-fit
Test SparseLinearDML
Test ForestDML
###################################
Mutiple treatments and outcomes #
###################################
Test LinearDML
Test SparseLinearDML
"Single outcome only, ORF does not support multiple outcomes"
Test DMLOrthoForest
Test DROrthoForest
Test XLearner
Skipping population summary names test because bootstrap inference is too slow
Test SLearner
Test TLearner
Test LinearDRLearner
Test SparseLinearDRLearner
Test ForestDRLearner
Test LinearIntentToTreatDRIV
Test DeepIV
Test categorical treatments
Check refit
Check refit after setting categories
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
Treatment effect function
Outcome support
Treatment support
"Generate controls, covariates, treatments and outcomes"
Heterogeneous treatment effects
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
through shap package.
test shap could generate the plot from the shap_values
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
"Note: unlike other Metalearners, we need the controls' encoded column for training"
"Thus, we append the controls column before the one-hot-encoded T"
"We might want to revisit, though, since it's linearly determined by the others"
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
output is
"* a column of ones if X, W, and Z are all None"
* just X or W or Z if both of the others are None
* hstack([arrs]) for whatever subset are not None otherwise
ensure Z is 2D
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
"instruments, and outcomes"
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"TODO: check that Y, T, Z do not have multiple columns"
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: do correct adjustment for sample_var
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
TODO: allow the final model to actually use X? Then we'd need to rename the class
since we would actually be calculating a CATE rather than ATE.
TODO: allow the final model to actually use X?
TODO: allow the final model to actually use X?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
TODO: would it be useful to extend to handle controls ala vanilla DML?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
make T 2D if if was a vector
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: this utility is documented but internal; reimplement?
TODO: this utility is even less public...
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
use same Cs as would be used by default by LogisticRegressionCV
NOTE: we don't use LogisticRegressionCV inside the grid search because of the nested stratification
which could affect how many times each distinct Y value needs to be present in the data
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
but also supports get_feature_names with expected signature
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
Convert python objects to (possibly nested) types that can easily be represented as literals
Convert SingleTreeInterpreter to a python dictionary
named tuple type for storing results inside CausalAnalysis class;
must be lifted to module level to enable pickling
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
Controls are all other columns of X
"can't use X[:, feat_ind] when X is a DataFrame"
TODO: we can't currently handle unseen values of the feature column when getting the effect;
we might want to modify OrthoLearner (and other discrete treatment classes)
so that the user can opt-in to allowing unseen treatment values
(and return NaN or something in that case)
array checking routines don't accept 0-width arrays
perform model selection
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
convert to NormalInferenceResults for consistency
Set the dictionary values shared between local and global summaries
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
required to fit a discrete DML model
Validate inputs
TODO: check compatibility of X and Y lengths
"no previous fit, cancel warm start"
"work with numeric feature indices, so that we can easily compare with categorical ones"
"if heterogeneity_inds is 1D, repeat it"
heterogeneity inds should be a 2D list of length same as train_inds
replace None elements of heterogeneity_inds and ensure indices are numeric
"TODO: bail out also if categorical columns, classification, random_state changed?"
TODO: should we also train a new model_y under any circumstances when warm_start is True?
train the Y model
"perform model selection for the Y model using all X, not on a per-column basis"
"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
work with the regression wrapper
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
"note that this needs to happen after wrapping to generalize to the multi-class case,"
since otherwise we'll have too many columns to be able to train a classifier
start with empty results and default shared insights
convert categorical indicators to numeric indices
check for indices over the categorical expansion bound
assume we'll be able to train former failures this time; we'll add them back if not
"can't remove in place while iterating over new_inds, so store in separate list"
"train the model, but warn"
no model can be trained in this case since we need more folds
"don't train a model, but suggest workaround since there are enough instances of least"
populated class
also remove from train_inds so we don't try to access the result later
extract subset of names matching new columns
"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
don't want to cache this failed result
properties to return from effect InferenceResults
properties to return from PopulationSummaryResults
Converts strings to property lookups or method calls as a convenience so that the
_point_props and _summary_props above can be applied to an inference object
Create a summary combining all results into a single output; this is used
by the various causal_effect and causal_effect_dict methods to generate either a dataframe
"or a dictionary, respectively, based on the summary function passed into this method"
"ensure array has shape (m,y,t)"
population summary is missing sample dimension; add it for consistency
outcome dimension is missing; add it for consistency
add singleton treatment dimension if missing
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
"each attr has dimension (m,y) or (m,y,t)"
concatenate along treatment dimension
"for dictionary representation, want to remove unneeded sample dimension"
in cohort and global results
TODO: enrich outcome logic for multi-class classification when that is supported
There is no actual sample level in this data
can't drop only level
should be serialization-ready and contain no numpy arrays
"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
TODO: Note that there's no column metadata for the sample number - should there be?
"need to replicate the column info for each sample, then remove from the shared data"
NOTE: the flattened order has the ouptut dimension before the feature dimension
which may need to be revisited once we support multiclass
get the length of the list corresponding to the first dictionary key
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
a global inference indicates the effect of that one feature on the outcome
need to reshape the output to match the input
we want to offset the inference object by the baseline estimate of y
"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
get the length of the list corresponding to the first dictionary key
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
"NOTE: this calculation is correct only if treatment costs are marginal costs,"
because then scaling the difference between treatment value and treatment costs is the
same as scaling the treatment value and subtracting the scaled treatment cost.
""
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
"continuous treatments, the policy value should include the benefit of decreasing treatments"
(rather than just not treating at all)
""
"We can get the total by seeing that if we restrict attention to units where we would treat,"
2 * policy_value - always_treat
includes exactly their contribution because policy_value and always_treat both include it
"and likewise restricting attention to the units where we want to decrease treatment,"
2 * policy_value - always-treat
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
is zero and the contribution to always_treat is negative
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
get dataframe with all but selected column
apply 10% of a typical treatment for this feature
"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely"
set the effect bounds; for positive treatments these agree with
"the estimates; for negative treatments, we need to invert the interval"
the effect is now always positive since we decrease treatment when negative
"for discrete treatment, stack a zero result in front for control"
we need to call effect_inference to get the correct CI between the two treatment options
we now need to construct the delta in the cost between the two treatments and translate the effect
remove third dimenions potentially added
"find cost of current treatment: equality creates a 2d array with True on each row,"
only if its the location of the current treatment. Then we take the corresponding cost.
construct index of current treatment
add second dimension if needed for broadcasting during translation of effect
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: conisder working around relying on sklearn implementation details
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
"If classification methods produce multiple columns of output,"
we need to manually encode classes to ensure consistent column ordering.
We clone the estimator to make sure that all the folds are
"independent, and that it is pickle-able."
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
`predictions` is a list of method outputs from each fold.
"If each of those is also a list, then treat this as a"
multioutput-multiclass task. We need to separately concatenate
the method outputs for each label into an `n_labels` long list.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
TODO: consider working around relying on sklearn implementation details
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Call weighted lasso on reduced design matrix
Weighted tau
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set model to WeightedLassoCV by default so there's always a model to get and set attributes on
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
(e.g. former has 'positive' and 'precompute' while latter does not)
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
don't proxy special methods
"don't pass get_params through to model, because that will cause sklearn to clone this"
regressor incorrectly
"Note: for known attributes that have been set this method will not be called,"
so we should just throw here because this is an attribute belonging to this class
but which hasn't yet been set on this instance
set default values for None
check freq_weight should be integer and should be accompanied by sample_var
check array shape
weight X and y and sample_var
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
average the outcome dimension if it exists and ensure 2d y_pred
get index of best treatment
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: consider working around relying on sklearn implementation details
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Convert rows to columns
Require group assignment t to be one-hot-encoded
Get predictions for the 2 splits
Must make sure indices are merged correctly
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Must normalize weights
Override the CATE inference options
Add blb inference to parent's options
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Need to redefine fit here for auto inference to work due to a quirk in how
wrap_fit is defined
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Check that all discrete treatments are represented
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
returns shape-conforming residuals
Copy and/or define models
Define parameter estimators
Define moment and mean gradient estimator
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Call `fit` from parent class
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Expand one-hot encoding to include the zero treatment
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Calculate confidence intervals for the parameter (marginal effect)
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
d_t=None here since we measure the effect across all Ts
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
def mlasso_model(): return MultiTaskLassoCV(
"cv=3, alphas=alpha_regs, max_iter=200)"
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
"alpha_regs = [5e-3, 1e-2, 5e-2]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
subset of features that are exogenous and create heterogeneity
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
subset of features wrt we estimate heterogeneity
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
introspect the constructor arguments to find the model parameters
to represent
"if the argument is deprecated, ignore it"
Extract and sort argument names excluding 'self'
column names
transfer input to numpy arrays
transfer input to 2d arrays
create dataframe
currently dowhy only support single outcome and single treatment
call dowhy
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
cate estimator but not the effect.
don't proxy special methods
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
For when checking input values is disabled
Type to column extraction function
"Get number of arguments, some sklearn featurizer don't accept feature_names"
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
Get feature names using featurizer
All attempts at retrieving transformed feature names have failed
Delegate handling to downstream logic
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
assume that we should perform nested cross-validation if and only if
the model has a 'cv' attribute; this is a somewhat brittle assumption...
logic copied from check_cv
otherwise we will assume the user already set the cv attribute to something
compatible with splitting with a 'groups' argument
now we have to compute the folds explicitly because some classifiers (like LassoCV)
don't use the groups when calling split internally
Normalize weights
This class is mainly derived from statsmodels.iolib.summary.Summary
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
"want to enforce that each bad_arg was either in kwargs,"
or else it was in neither and is just taking its default value
Any access should throw
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
input feature name is already updated by cate_feature_names.
define the index of d_x to filter for each given T
filter X after broadcast with T for each given T
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains some snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
make any access to matplotlib or plt throw an exception
make any access to graphviz or plt throw an exception
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
Initialize saturation & value; calculate chroma & value shift
Calculate some intermediate values
Initialize RGB with same hue & chroma as our color
Shift the initial RGB values to match value and store
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use mean of targets
Write node mean CATE
Write node std of CATE
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
Fetch appropriate color for node
Write node mean CATE
Write node mean CATE
Write recommended treatment and value - cost
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
This flag is true when names are set in a child class instead
"If names are set in a child class, add an attribute reflecting that"
This works only if X is passed as a kwarg
We plan to enforce X as kwarg only in future releases
This checks if names have been set in a child class
"If names were set in a child class, don't do it again"
"Wraps-up fit by setting attributes, cleaning up, etc."
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"Treatment names is None, default to BaseCateEstimator"
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
Get input names
Summary
add statsmodels to parent's options
add debiasedlasso to parent's options
add blb to parent's options
TODO Share some logic with non-discrete version
Get input names
Summary
add statsmodels to parent's options
add statsmodels to parent's options
add blb to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
remove None arguments
"scores entries should be lists of scores, so make each entry a singleton list"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
generate an instance of the final model
generate an instance of the nuisance model
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
alteration even when we only want to fit_final.
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
"however, sklearn doesn't support both stratifying and grouping (see"
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
their own object that supports grouping if they want to use groups.
for each mc iteration
for each model under cross fit setting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Policy Forest
=============================================================================
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base Policy tree
=============================================================================
The values below are required and utilitized by methods in the _SingleTreeExporterMixin
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"since we only allow single dimensional y, we could flatten the prediction"
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Handles the corner case when X=None but featurizer might be not None
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
TODO: support freq_weight and sample_var in debiased lasso
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
Replacing to remove docstring
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE: important to use the rlearner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the rlearner's copy to retain the information from fitting
Handles the corner case when X=None but featurizer might be not None
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `StatsModelsInference`
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support freq_weight and sample_var in debiased lasso
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add blb to parent's options
override only so that we can update the docstring to indicate
support for `GenericSingleTreatmentModelFinalInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
note that groups are not passed to score because they are only used for fitting
note that groups are not passed to score because they are only used for fitting
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
NOTE: important to get parent's wrapped copy so that
"after training wrapped featurizer is also trained, etc."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
Fit a doubly robust average effect
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
"If custom param grid, check that only estimator parameters are being altered"
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
override only so that we can update the docstring to indicate support for `blb`
Get input names
Summary
Determine output settings
"Important: This must be the first invocation of the random state at fit time, so that"
train/test splits are re-generatable from an external object simply by knowing the
random_state parameter of the tree. Can be useful in the future if one wants to create local
linear predictions. Currently is also useful for testing.
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
Set min_weight_leaf from min_weight_fraction_leaf
Build tree
We calculate the maximum number of samples from each half-split that any node in the tree can
hold. Used by criterion for memory space savings.
Initialize the criterion object and the criterion_val object if honest.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code is a fork from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
Set parameters
Don't instantiate estimators now! Parameters of base_estimator might
"still change. Eg., when grid-searching with the nested object syntax."
self.estimators_ needs to be filled by the derived classes in fit.
Compute the number of jobs
Partition estimators between jobs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=None here since we measure the effect across all Ts
once the estimator has been fit
"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
an intercept even when bias is part of coef.
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
squeeze the first axis
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
d_t=None here since we measure the effect across all Ts
d_t=None here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
scale preds
scale std errs
"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
offset preds
"offset the distribution, too"
scale preds
"scale the distribution, too"
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
"if stderr is zero, ppf will return nans and the loop below would never terminate"
so bail out early; note that it might be possible to correct the algorithm for
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
be clean
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
don't proxy special methods
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
second level bootstrap which would be prohibitive computationally?
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
can't import from econml.inference at top level without creating cyclical dependencies
Note that inference results are always methods even if the inference is for a property
(e.g. coef__inference() is a method but coef_ is a property)
Therefore we must insert a lambda if getting inference for a non-callable
"If inference is for a property, create a fresh lambda to avoid passing args through"
"try to get interval/std first if appropriate,"
since we don't prefer a wrapped method with this name
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base GRF tree
=============================================================================
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
=============================================================================
A MultOutputWrapper for GRF classes
=============================================================================
=============================================================================
Instantiations of Generalized Random Forest
=============================================================================
"Append a constant treatment if `fit_intercept=True`, the coefficient"
in front of the constant treatment is the intercept in the moment equation.
"Append a constant treatment and constant instrument if `fit_intercept=True`,"
the coefficient in front of the constant treatment is the intercept in the moment equation.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Base Generalized Random Forest
=============================================================================
TODO: support freq_weight and sample_var
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
We calculate the min eigenvalue proxy that each criterion is considering
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Generating indices a priori before parallelism ended up being orders of magnitude
faster than how sklearn does it. The reason is that random samplers do not release the
gil it seems.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
####################
Variance correction
####################
Subtract the average within bag variance. This ends up being equal to the
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
The negative part is just sq_between.
Objective bayes debiasing for the diagonals where we know a-prior they are positive
"The off diagonals we have no objective prior, so no correction is applied."
Finally correcting the pred_cov or pred_var
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
test that the subsampling scheme past to the trees is correct
The sample size is chosen in particular to test rounding based error when subsampling
test that the estimator calcualtes var correctly
test api
test accuracy
test the projection functionality of forests
test that the estimator calcualtes var correctly
test api
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
test that the subsampling scheme past to the trees is correct
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
omit the lalonde notebook
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
can interpret without uncertainty
can't treat before interpreting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
compare with weight
compare with weight
compare with weight
compare with weight
Multi-dimensional output y
1-d y
compare when both sample_var and sample_weight exist
multi-d y
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that the subsampling scheme past to the trees is correct
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test inference results when `cate_feature_names` doesn not exist
Test inference results when `cate_feature_names` doesn not exist
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
pvalue for second column should be greater than zero since some points are on either side
of the tested value
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
only is not None when T1 is a constant or a list of constant
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Nuisance model has no score method, so nuisance_scores_ should be none"
Test non keyword based calls to fit
test non-array inputs
Test custom splitter
Test incomplete set of test folds
"y scores should be positive, since W predicts Y somewhat"
"t scores might not be, since W and T are uncorrelated"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
creating an instance should warn
using the instance should not warn
using the deprecated method should warn
don't warn if b and c are passed by keyword
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test at least one estimator from each category
test causal graph
test refutation estimate
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
ensure that we've got at least two of every row
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
need to make sure we get all *joint* combinations
IntentToTreat only supports binary treatments/instruments
IntentToTreat only supports binary treatments/instruments
IntentToTreat requires X
ensure we can serialize unfit estimator
these support only W but not X
"these support only binary, not general discrete T and Z"
ensure we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
TODO: add tests for extra properties like coef_ where they exist
TODO: add tests for extra properties like coef_ where they exist
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
TODO: ideally we could also test whether Z and X are jointly okay when both discrete
"however, with custom splits the checking happens in the first stage wrapper"
where we don't have all of the required information to do this;
we'd probably need to add it to _crossfit instead
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
The __warningregistry__'s need to be in a pristine state for tests
to work properly.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Create a wrapper around Lasso that doesn't support weights
since Lasso does natively support them starting in sklearn 0.23
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least 6 of every element
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
NOTE: this number may need to change if the default number of folds in
WeightedStratifiedKFold changes
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
make sure we can get out post-fit stuff
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
predetermined splits ensure that all features are seen in each split
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
create a simple artificial setup where effect of moving from treatment
"a -> b is 2,"
"a -> c is 1, and"
"b -> c is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
should rule out some basic issues.
Note that explicitly specifying the dtype as object is necessary until
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
estimated effects should be identical when treatment is explicitly given
but const_marginal_effect should be reordered based on the explicit cagetories
1-> 2 in original ordering; combination of 3->1 and 3->2
test outer grouping
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure that we can serialize unfit estimator
ensure that we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef_ and intercept_ inference
verify we can generate the summary
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
test outer grouping
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
helper class
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
#########################
WeightedLassoCVWrapper #
#########################
perform 1D fit
perform 2D fit
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
_penalized_inds is only set during fitting
cv exists on penalized model
now we can access _penalized_inds
check that we can read the cv attribute back out from the underlying model
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
make sure we don't run into problems dropping every index
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
dgp
model
model
"columns 'd', 'e', 'h' have too many values"
"columns 'd', 'e' have too many values"
lowering bound shouldn't affect already fit columns when warm starting
"column d is now okay, too"
verify that we can use a scalar treatment cost
verify that we can specify per-treatment costs for each sample
verify that using the same state returns the same results each time
set the categories for column 'd' explicitly so that b is default
"first column: 10 ones, this is fine"
"second column: 6 categories, plenty of random instances of each"
this is fine only if we increase the cateogry limit
"third column: nine ones, lots of twos, not enough unless we disable check"
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
forest heterogeneity won't work
"sixth column: just 1 one, not enough even without check"
increase bound on cat expansion
skip checks (reducing folds accordingly)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Define data features
Added `_df`to names to be different from the default cate_estimator names
Generate data
################################
Single treatment and outcome #
################################
Test LinearDML
|--> Test featurizers
ColumnTransformer doesn't propagate column names
|--> Test re-fit
Test SparseLinearDML
Test ForestDML
###################################
Mutiple treatments and outcomes #
###################################
Test LinearDML
Test SparseLinearDML
"Single outcome only, ORF does not support multiple outcomes"
Test DMLOrthoForest
Test DROrthoForest
Test XLearner
Skipping population summary names test because bootstrap inference is too slow
Test SLearner
Test TLearner
Test LinearDRLearner
Test SparseLinearDRLearner
Test ForestDRLearner
Test LinearIntentToTreatDRIV
Test DeepIV
Test categorical treatments
Check refit
Check refit after setting categories
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
Treatment effect function
Outcome support
Treatment support
"Generate controls, covariates, treatments and outcomes"
Heterogeneous treatment effects
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
through shap package.
test shap could generate the plot from the shap_values
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
"Note: unlike other Metalearners, we need the controls' encoded column for training"
"Thus, we append the controls column before the one-hot-encoded T"
"We might want to revisit, though, since it's linearly determined by the others"
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
output is
"* a column of ones if X, W, and Z are all None"
* just X or W or Z if both of the others are None
* hstack([arrs]) for whatever subset are not None otherwise
ensure Z is 2D
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
"instruments, and outcomes"
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"TODO: check that Y, T, Z do not have multiple columns"
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: do correct adjustment for sample_var
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
TODO: allow the final model to actually use X? Then we'd need to rename the class
since we would actually be calculating a CATE rather than ATE.
TODO: allow the final model to actually use X?
TODO: allow the final model to actually use X?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
TODO: would it be useful to extend to handle controls ala vanilla DML?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
make T 2D if if was a vector
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: this utility is documented but internal; reimplement?
TODO: this utility is even less public...
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
but also supports get_feature_names with expected signature
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
Convert python objects to (possibly nested) types that can easily be represented as literals
Convert SingleTreeInterpreter to a python dictionary
named tuple type for storing results inside CausalAnalysis class;
must be lifted to module level to enable pickling
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
Controls are all other columns of X
"can't use X[:, feat_ind] when X is a DataFrame"
TODO: we can't currently handle unseen values of the feature column when getting the effect;
we might want to modify OrthoLearner (and other discrete treatment classes)
so that the user can opt-in to allowing unseen treatment values
(and return NaN or something in that case)
array checking routines don't accept 0-width arrays
perform model selection
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
convert to NormalInferenceResults for consistency
Set the dictionary values shared between local and global summaries
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
required to fit a discrete DML model
Validate inputs
TODO: check compatibility of X and Y lengths
"no previous fit, cancel warm start"
"work with numeric feature indices, so that we can easily compare with categorical ones"
"if heterogeneity_inds is 1D, repeat it"
heterogeneity inds should be a 2D list of length same as train_inds
replace None elements of heterogeneity_inds and ensure indices are numeric
"TODO: bail out also if categorical columns, classification, random_state changed?"
TODO: should we also train a new model_y under any circumstances when warm_start is True?
train the Y model
"perform model selection for the Y model using all X, not on a per-column basis"
"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
work with the regression wrapper
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
"note that this needs to happen after wrapping to generalize to the multi-class case,"
since otherwise we'll have too many columns to be able to train a classifier
start with empty results and default shared insights
convert categorical indicators to numeric indices
check for indices over the categorical expansion bound
assume we'll be able to train former failures this time; we'll add them back if not
"can't remove in place while iterating over new_inds, so store in separate list"
"train the model, but warn"
no model can be trained in this case since we need more folds
"don't train a model, but suggest workaround since there are enough instances of least"
populated class
also remove from train_inds so we don't try to access the result later
extract subset of names matching new columns
"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
don't want to cache this failed result
properties to return from effect InferenceResults
properties to return from PopulationSummaryResults
Converts strings to property lookups or method calls as a convenience so that the
_point_props and _summary_props above can be applied to an inference object
Create a summary combining all results into a single output; this is used
by the various causal_effect and causal_effect_dict methods to generate either a dataframe
"or a dictionary, respectively, based on the summary function passed into this method"
"ensure array has shape (m,y,t)"
population summary is missing sample dimension; add it for consistency
outcome dimension is missing; add it for consistency
add singleton treatment dimension if missing
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
"each attr has dimension (m,y) or (m,y,t)"
concatenate along treatment dimension
"for dictionary representation, want to remove unneeded sample dimension"
in cohort and global results
TODO: enrich outcome logic for multi-class classification when that is supported
There is no actual sample level in this data
can't drop only level
should be serialization-ready and contain no numpy arrays
"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
TODO: Note that there's no column metadata for the sample number - should there be?
"need to replicate the column info for each sample, then remove from the shared data"
NOTE: the flattened order has the ouptut dimension before the feature dimension
which may need to be revisited once we support multiclass
get the length of the list corresponding to the first dictionary key
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
a global inference indicates the effect of that one feature on the outcome
need to reshape the output to match the input
we want to offset the inference object by the baseline estimate of y
"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
get the length of the list corresponding to the first dictionary key
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
"NOTE: this calculation is correct only if treatment costs are marginal costs,"
because then scaling the difference between treatment value and treatment costs is the
same as scaling the treatment value and subtracting the scaled treatment cost.
""
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
"continuous treatments, the policy value should include the benefit of decreasing treatments"
(rather than just not treating at all)
""
"We can get the total by seeing that if we restrict attention to units where we would treat,"
2 * policy_value - always_treat
includes exactly their contribution because policy_value and always_treat both include it
"and likewise restricting attention to the units where we want to decrease treatment,"
2 * policy_value - always-treat
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
is zero and the contribution to always_treat is negative
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
get dataframe with all but selected column
apply 10% of a typical treatment for this feature
"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely"
set the effect bounds; for positive treatments these agree with
"the estimates; for negative treatments, we need to invert the interval"
the effect is now always positive since we decrease treatment when negative
"for discrete treatment, stack a zero result in front for control"
we need to call effect_inference to get the correct CI between the two treatment options
we now need to construct the delta in the cost between the two treatments and translate the effect
remove third dimenions potentially added
"find cost of current treatment: equality creates a 2d array with True on each row,"
only if its the location of the current treatment. Then we take the corresponding cost.
construct index of current treatment
add second dimension if needed for broadcasting during translation of effect
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: conisder working around relying on sklearn implementation details
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
"If classification methods produce multiple columns of output,"
we need to manually encode classes to ensure consistent column ordering.
We clone the estimator to make sure that all the folds are
"independent, and that it is pickle-able."
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
`predictions` is a list of method outputs from each fold.
"If each of those is also a list, then treat this as a"
multioutput-multiclass task. We need to separately concatenate
the method outputs for each label into an `n_labels` long list.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
TODO: consider working around relying on sklearn implementation details
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Call weighted lasso on reduced design matrix
Weighted tau
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set model to WeightedLassoCV by default so there's always a model to get and set attributes on
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
(e.g. former has 'positive' and 'precompute' while latter does not)
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
don't proxy special methods
"don't pass get_params through to model, because that will cause sklearn to clone this"
regressor incorrectly
"Note: for known attributes that have been set this method will not be called,"
so we should just throw here because this is an attribute belonging to this class
but which hasn't yet been set on this instance
set default values for None
check freq_weight should be integer and should be accompanied by sample_var
check array shape
weight X and y and sample_var
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
average the outcome dimension if it exists and ensure 2d y_pred
get index of best treatment
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: consider working around relying on sklearn implementation details
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Convert rows to columns
Require group assignment t to be one-hot-encoded
Get predictions for the 2 splits
Must make sure indices are merged correctly
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Must normalize weights
Override the CATE inference options
Add blb inference to parent's options
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Need to redefine fit here for auto inference to work due to a quirk in how
wrap_fit is defined
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Check that all discrete treatments are represented
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
returns shape-conforming residuals
Copy and/or define models
Define parameter estimators
Define moment and mean gradient estimator
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Call `fit` from parent class
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Expand one-hot encoding to include the zero treatment
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Calculate confidence intervals for the parameter (marginal effect)
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
d_t=None here since we measure the effect across all Ts
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
def mlasso_model(): return MultiTaskLassoCV(
"cv=3, alphas=alpha_regs, max_iter=200)"
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
"alpha_regs = [5e-3, 1e-2, 5e-2]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
subset of features that are exogenous and create heterogeneity
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
subset of features wrt we estimate heterogeneity
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
introspect the constructor arguments to find the model parameters
to represent
"if the argument is deprecated, ignore it"
Extract and sort argument names excluding 'self'
column names
transfer input to numpy arrays
transfer input to 2d arrays
create dataframe
currently dowhy only support single outcome and single treatment
call dowhy
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
cate estimator but not the effect.
don't proxy special methods
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
For when checking input values is disabled
Type to column extraction function
"Get number of arguments, some sklearn featurizer don't accept feature_names"
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
Get feature names using featurizer
All attempts at retrieving transformed feature names have failed
Delegate handling to downstream logic
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
assume that we should perform nested cross-validation if and only if
the model has a 'cv' attribute; this is a somewhat brittle assumption...
logic copied from check_cv
otherwise we will assume the user already set the cv attribute to something
compatible with splitting with a 'groups' argument
now we have to compute the folds explicitly because some classifiers (like LassoCV)
don't use the groups when calling split internally
Normalize weights
This class is mainly derived from statsmodels.iolib.summary.Summary
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
"want to enforce that each bad_arg was either in kwargs,"
or else it was in neither and is just taking its default value
Any access should throw
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
input feature name is already updated by cate_feature_names.
define the index of d_x to filter for each given T
filter X after broadcast with T for each given T
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains some snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
make any access to matplotlib or plt throw an exception
make any access to graphviz or plt throw an exception
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
Initialize saturation & value; calculate chroma & value shift
Calculate some intermediate values
Initialize RGB with same hue & chroma as our color
Shift the initial RGB values to match value and store
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use mean of targets
Write node mean CATE
Write node std of CATE
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
Fetch appropriate color for node
Write node mean CATE
Write node mean CATE
Write recommended treatment and value - cost
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
This flag is true when names are set in a child class instead
"If names are set in a child class, add an attribute reflecting that"
This works only if X is passed as a kwarg
We plan to enforce X as kwarg only in future releases
This checks if names have been set in a child class
"If names were set in a child class, don't do it again"
"Wraps-up fit by setting attributes, cleaning up, etc."
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"Treatment names is None, default to BaseCateEstimator"
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
Get input names
Summary
add statsmodels to parent's options
add debiasedlasso to parent's options
add blb to parent's options
TODO Share some logic with non-discrete version
Get input names
Summary
add statsmodels to parent's options
add statsmodels to parent's options
add blb to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
remove None arguments
"scores entries should be lists of scores, so make each entry a singleton list"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
generate an instance of the final model
generate an instance of the nuisance model
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
alteration even when we only want to fit_final.
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
"however, sklearn doesn't support both stratifying and grouping (see"
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
their own object that supports grouping if they want to use groups.
for each mc iteration
for each model under cross fit setting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Policy Forest
=============================================================================
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base Policy tree
=============================================================================
The values below are required and utilitized by methods in the _SingleTreeExporterMixin
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"since we only allow single dimensional y, we could flatten the prediction"
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Handles the corner case when X=None but featurizer might be not None
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
TODO: support freq_weight and sample_var in debiased lasso
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
Replacing to remove docstring
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE: important to use the rlearner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the rlearner's copy to retain the information from fitting
Handles the corner case when X=None but featurizer might be not None
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `StatsModelsInference`
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support freq_weight and sample_var in debiased lasso
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add blb to parent's options
override only so that we can update the docstring to indicate
support for `GenericSingleTreatmentModelFinalInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
note that groups are not passed to score because they are only used for fitting
note that groups are not passed to score because they are only used for fitting
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
NOTE: important to get parent's wrapped copy so that
"after training wrapped featurizer is also trained, etc."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
Fit a doubly robust average effect
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
"If custom param grid, check that only estimator parameters are being altered"
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
override only so that we can update the docstring to indicate support for `blb`
Get input names
Summary
Determine output settings
"Important: This must be the first invocation of the random state at fit time, so that"
train/test splits are re-generatable from an external object simply by knowing the
random_state parameter of the tree. Can be useful in the future if one wants to create local
linear predictions. Currently is also useful for testing.
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
Set min_weight_leaf from min_weight_fraction_leaf
Build tree
We calculate the maximum number of samples from each half-split that any node in the tree can
hold. Used by criterion for memory space savings.
Initialize the criterion object and the criterion_val object if honest.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code is a fork from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
Set parameters
Don't instantiate estimators now! Parameters of base_estimator might
"still change. Eg., when grid-searching with the nested object syntax."
self.estimators_ needs to be filled by the derived classes in fit.
Compute the number of jobs
Partition estimators between jobs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=None here since we measure the effect across all Ts
once the estimator has been fit
"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
an intercept even when bias is part of coef.
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
squeeze the first axis
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
d_t=None here since we measure the effect across all Ts
d_t=None here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
scale preds
scale std errs
"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
offset preds
"offset the distribution, too"
scale preds
"scale the distribution, too"
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
"if stderr is zero, ppf will return nans and the loop below would never terminate"
so bail out early; note that it might be possible to correct the algorithm for
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
be clean
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
don't proxy special methods
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
second level bootstrap which would be prohibitive computationally?
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
can't import from econml.inference at top level without creating cyclical dependencies
Note that inference results are always methods even if the inference is for a property
(e.g. coef__inference() is a method but coef_ is a property)
Therefore we must insert a lambda if getting inference for a non-callable
"If inference is for a property, create a fresh lambda to avoid passing args through"
"try to get interval/std first if appropriate,"
since we don't prefer a wrapped method with this name
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base GRF tree
=============================================================================
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
=============================================================================
A MultOutputWrapper for GRF classes
=============================================================================
=============================================================================
Instantiations of Generalized Random Forest
=============================================================================
"Append a constant treatment if `fit_intercept=True`, the coefficient"
in front of the constant treatment is the intercept in the moment equation.
"Append a constant treatment and constant instrument if `fit_intercept=True`,"
the coefficient in front of the constant treatment is the intercept in the moment equation.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Base Generalized Random Forest
=============================================================================
TODO: support freq_weight and sample_var
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
We calculate the min eigenvalue proxy that each criterion is considering
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Generating indices a priori before parallelism ended up being orders of magnitude
faster than how sklearn does it. The reason is that random samplers do not release the
gil it seems.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
####################
Variance correction
####################
Subtract the average within bag variance. This ends up being equal to the
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
The negative part is just sq_between.
Objective bayes debiasing for the diagonals where we know a-prior they are positive
"The off diagonals we have no objective prior, so no correction is applied."
Finally correcting the pred_cov or pred_var
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
test that the subsampling scheme past to the trees is correct
The sample size is chosen in particular to test rounding based error when subsampling
test that the estimator calcualtes var correctly
test api
test accuracy
test the projection functionality of forests
test that the estimator calcualtes var correctly
test api
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
test that the subsampling scheme past to the trees is correct
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
omit the lalonde notebook
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
can interpret without uncertainty
can't treat before interpreting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
compare with weight
compare with weight
compare with weight
compare with weight
Multi-dimensional output y
1-d y
compare when both sample_var and sample_weight exist
multi-d y
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that the subsampling scheme past to the trees is correct
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test inference results when `cate_feature_names` doesn not exist
Test inference results when `cate_feature_names` doesn not exist
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
pvalue for second column should be greater than zero since some points are on either side
of the tested value
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
only is not None when T1 is a constant or a list of constant
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Nuisance model has no score method, so nuisance_scores_ should be none"
Test non keyword based calls to fit
test non-array inputs
Test custom splitter
Test incomplete set of test folds
"y scores should be positive, since W predicts Y somewhat"
"t scores might not be, since W and T are uncorrelated"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
creating an instance should warn
using the instance should not warn
using the deprecated method should warn
don't warn if b and c are passed by keyword
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test at least one estimator from each category
test causal graph
test refutation estimate
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
ensure that we've got at least two of every row
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
need to make sure we get all *joint* combinations
IntentToTreat only supports binary treatments/instruments
IntentToTreat only supports binary treatments/instruments
IntentToTreat requires X
ensure we can serialize unfit estimator
these support only W but not X
"these support only binary, not general discrete T and Z"
ensure we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
TODO: add tests for extra properties like coef_ where they exist
TODO: add tests for extra properties like coef_ where they exist
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
TODO: ideally we could also test whether Z and X are jointly okay when both discrete
"however, with custom splits the checking happens in the first stage wrapper"
where we don't have all of the required information to do this;
we'd probably need to add it to _crossfit instead
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
The __warningregistry__'s need to be in a pristine state for tests
to work properly.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Create a wrapper around Lasso that doesn't support weights
since Lasso does natively support them starting in sklearn 0.23
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least 6 of every element
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
NOTE: this number may need to change if the default number of folds in
WeightedStratifiedKFold changes
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
make sure we can get out post-fit stuff
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
predetermined splits ensure that all features are seen in each split
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
create a simple artificial setup where effect of moving from treatment
"a -> b is 2,"
"a -> c is 1, and"
"b -> c is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
should rule out some basic issues.
Note that explicitly specifying the dtype as object is necessary until
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
estimated effects should be identical when treatment is explicitly given
but const_marginal_effect should be reordered based on the explicit cagetories
1-> 2 in original ordering; combination of 3->1 and 3->2
test outer grouping
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure that we can serialize unfit estimator
ensure that we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef_ and intercept_ inference
verify we can generate the summary
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
test outer grouping
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
helper class
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
#########################
WeightedLassoCVWrapper #
#########################
perform 1D fit
perform 2D fit
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
_penalized_inds is only set during fitting
cv exists on penalized model
now we can access _penalized_inds
check that we can read the cv attribute back out from the underlying model
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
make sure we don't run into problems dropping every index
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
global and cohort row-wise dicts have d_y * d_t entries
local dictionary is flattened to n_rows * d_y * d_t
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
dgp
model
model
"columns 'd', 'e', 'h' have too many values"
"columns 'd', 'e' have too many values"
lowering bound shouldn't affect already fit columns when warm starting
"column d is now okay, too"
verify that we can use a scalar treatment cost
verify that we can specify per-treatment costs for each sample
verify that using the same state returns the same results each time
set the categories for column 'd' explicitly so that b is default
"first column: 10 ones, this is fine"
"second column: 6 categories, plenty of random instances of each"
this is fine only if we increase the cateogry limit
"third column: nine ones, lots of twos, not enough unless we disable check"
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
forest heterogeneity won't work
"sixth column: just 1 one, not enough even without check"
increase bound on cat expansion
skip checks (reducing folds accordingly)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Define data features
Added `_df`to names to be different from the default cate_estimator names
Generate data
################################
Single treatment and outcome #
################################
Test LinearDML
|--> Test featurizers
ColumnTransformer doesn't propagate column names
|--> Test re-fit
Test SparseLinearDML
Test ForestDML
###################################
Mutiple treatments and outcomes #
###################################
Test LinearDML
Test SparseLinearDML
"Single outcome only, ORF does not support multiple outcomes"
Test DMLOrthoForest
Test DROrthoForest
Test XLearner
Skipping population summary names test because bootstrap inference is too slow
Test SLearner
Test TLearner
Test LinearDRLearner
Test SparseLinearDRLearner
Test ForestDRLearner
Test LinearIntentToTreatDRIV
Test DeepIV
Test categorical treatments
Check refit
Check refit after setting categories
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
Treatment effect function
Outcome support
Treatment support
"Generate controls, covariates, treatments and outcomes"
Heterogeneous treatment effects
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
through shap package.
test shap could generate the plot from the shap_values
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
"Note: unlike other Metalearners, we need the controls' encoded column for training"
"Thus, we append the controls column before the one-hot-encoded T"
"We might want to revisit, though, since it's linearly determined by the others"
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
output is
"* a column of ones if X, W, and Z are all None"
* just X or W or Z if both of the others are None
* hstack([arrs]) for whatever subset are not None otherwise
ensure Z is 2D
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
"instruments, and outcomes"
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"TODO: check that Y, T, Z do not have multiple columns"
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: do correct adjustment for sample_var
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
TODO: allow the final model to actually use X? Then we'd need to rename the class
since we would actually be calculating a CATE rather than ATE.
TODO: allow the final model to actually use X?
TODO: allow the final model to actually use X?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
TODO: would it be useful to extend to handle controls ala vanilla DML?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
make T 2D if if was a vector
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: this utility is documented but internal; reimplement?
TODO: this utility is even less public...
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
but also supports get_feature_names with expected signature
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
Convert python objects to (possibly nested) types that can easily be represented as literals
Convert SingleTreeInterpreter to a python dictionary
named tuple type for storing results inside CausalAnalysis class;
must be lifted to module level to enable pickling
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
Controls are all other columns of X
"can't use X[:, feat_ind] when X is a DataFrame"
TODO: we can't currently handle unseen values of the feature column when getting the effect;
we might want to modify OrthoLearner (and other discrete treatment classes)
so that the user can opt-in to allowing unseen treatment values
(and return NaN or something in that case)
array checking routines don't accept 0-width arrays
perform model selection
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
convert to NormalInferenceResults for consistency
Set the dictionary values shared between local and global summaries
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
required to fit a discrete DML model
Validate inputs
TODO: check compatibility of X and Y lengths
"no previous fit, cancel warm start"
"work with numeric feature indices, so that we can easily compare with categorical ones"
"if heterogeneity_inds is 1D, repeat it"
heterogeneity inds should be a 2D list of length same as train_inds
replace None elements of heterogeneity_inds and ensure indices are numeric
"TODO: bail out also if categorical columns, classification, random_state changed?"
TODO: should we also train a new model_y under any circumstances when warm_start is True?
train the Y model
"perform model selection for the Y model using all X, not on a per-column basis"
"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
work with the regression wrapper
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
"note that this needs to happen after wrapping to generalize to the multi-class case,"
since otherwise we'll have too many columns to be able to train a classifier
start with empty results and default shared insights
convert categorical indicators to numeric indices
check for indices over the categorical expansion bound
assume we'll be able to train former failures this time; we'll add them back if not
"can't remove in place while iterating over new_inds, so store in separate list"
"train the model, but warn"
no model can be trained in this case since we need more folds
"don't train a model, but suggest workaround since there are enough instances of least"
populated class
also remove from train_inds so we don't try to access the result later
extract subset of names matching new columns
"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
don't want to cache this failed result
properties to return from effect InferenceResults
properties to return from PopulationSummaryResults
Converts strings to property lookups or method calls as a convenience so that the
_point_props and _summary_props above can be applied to an inference object
Create a summary combining all results into a single output; this is used
by the various causal_effect and causal_effect_dict methods to generate either a dataframe
"or a dictionary, respectively, based on the summary function passed into this method"
"ensure array has shape (m,y,t)"
population summary is missing sample dimension; add it for consistency
outcome dimension is missing; add it for consistency
add singleton treatment dimension if missing
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
"each attr has dimension (m,y) or (m,y,t)"
concatenate along treatment dimension
"for dictionary representation, want to remove unneeded sample dimension"
in cohort and global results
TODO: enrich outcome logic for multi-class classification when that is supported
There is no actual sample level in this data
can't drop only level
should be serialization-ready and contain no numpy arrays
"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
TODO: Note that there's no column metadata for the sample number - should there be?
"need to replicate the column info for each sample, then remove from the shared data"
NOTE: the flattened order has the ouptut dimension before the feature dimension
which may need to be revisited once we support multiclass
get the length of the list corresponding to the first dictionary key
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
a global inference indicates the effect of that one feature on the outcome
need to reshape the output to match the input
we want to offset the inference object by the baseline estimate of y
"remove entries belonging to row data, since we're including them in the list of nested dictionaries"
get the length of the list corresponding to the first dictionary key
"`list(row_data)` gets the keys as a list, since `row_data.keys()` can't be indexed into"
"NOTE: this calculation is correct only if treatment costs are marginal costs,"
because then scaling the difference between treatment value and treatment costs is the
same as scaling the treatment value and subtracting the scaled treatment cost.
""
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
"continuous treatments, the policy value should include the benefit of decreasing treatments"
(rather than just not treating at all)
""
"We can get the total by seeing that if we restrict attention to units where we would treat,"
2 * policy_value - always_treat
includes exactly their contribution because policy_value and always_treat both include it
"and likewise restricting attention to the units where we want to decrease treatment,"
2 * policy_value - always-treat
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
is zero and the contribution to always_treat is negative
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
get dataframe with all but selected column
apply 10% of a typical treatment for this feature
"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely"
set the effect bounds; for positive treatments these agree with
"the estimates; for negative treatments, we need to invert the interval"
the effect is now always positive since we decrease treatment when negative
"for discrete treatment, stack a zero result in front for control"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: conisder working around relying on sklearn implementation details
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
"If classification methods produce multiple columns of output,"
we need to manually encode classes to ensure consistent column ordering.
We clone the estimator to make sure that all the folds are
"independent, and that it is pickle-able."
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
`predictions` is a list of method outputs from each fold.
"If each of those is also a list, then treat this as a"
multioutput-multiclass task. We need to separately concatenate
the method outputs for each label into an `n_labels` long list.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
TODO: consider working around relying on sklearn implementation details
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Call weighted lasso on reduced design matrix
Weighted tau
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set model to WeightedLassoCV by default so there's always a model to get and set attributes on
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
(e.g. former has 'positive' and 'precompute' while latter does not)
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
don't proxy special methods
"don't pass get_params through to model, because that will cause sklearn to clone this"
regressor incorrectly
"Note: for known attributes that have been set this method will not be called,"
so we should just throw here because this is an attribute belonging to this class
but which hasn't yet been set on this instance
set default values for None
check freq_weight should be integer and should be accompanied by sample_var
check array shape
weight X and y and sample_var
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
average the outcome dimension if it exists and ensure 2d y_pred
get index of best treatment
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: consider working around relying on sklearn implementation details
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Convert rows to columns
Require group assignment t to be one-hot-encoded
Get predictions for the 2 splits
Must make sure indices are merged correctly
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Must normalize weights
Override the CATE inference options
Add blb inference to parent's options
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Need to redefine fit here for auto inference to work due to a quirk in how
wrap_fit is defined
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Check that all discrete treatments are represented
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
returns shape-conforming residuals
Copy and/or define models
Define parameter estimators
Define moment and mean gradient estimator
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Call `fit` from parent class
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Expand one-hot encoding to include the zero treatment
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Calculate confidence intervals for the parameter (marginal effect)
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
d_t=None here since we measure the effect across all Ts
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
def mlasso_model(): return MultiTaskLassoCV(
"cv=3, alphas=alpha_regs, max_iter=200)"
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
"alpha_regs = [5e-3, 1e-2, 5e-2]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
subset of features that are exogenous and create heterogeneity
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
subset of features wrt we estimate heterogeneity
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
introspect the constructor arguments to find the model parameters
to represent
"if the argument is deprecated, ignore it"
Extract and sort argument names excluding 'self'
column names
transfer input to numpy arrays
transfer input to 2d arrays
create dataframe
currently dowhy only support single outcome and single treatment
call dowhy
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
cate estimator but not the effect.
don't proxy special methods
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
For when checking input values is disabled
Type to column extraction function
"Get number of arguments, some sklearn featurizer don't accept feature_names"
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
Get feature names using featurizer
All attempts at retrieving transformed feature names have failed
Delegate handling to downstream logic
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
assume that we should perform nested cross-validation if and only if
the model has a 'cv' attribute; this is a somewhat brittle assumption...
logic copied from check_cv
otherwise we will assume the user already set the cv attribute to something
compatible with splitting with a 'groups' argument
now we have to compute the folds explicitly because some classifiers (like LassoCV)
don't use the groups when calling split internally
Normalize weights
This class is mainly derived from statsmodels.iolib.summary.Summary
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
"want to enforce that each bad_arg was either in kwargs,"
or else it was in neither and is just taking its default value
Any access should throw
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
input feature name is already updated by cate_feature_names.
define the index of d_x to filter for each given T
filter X after broadcast with T for each given T
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains some snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
make any access to matplotlib or plt throw an exception
make any access to graphviz or plt throw an exception
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
Initialize saturation & value; calculate chroma & value shift
Calculate some intermediate values
Initialize RGB with same hue & chroma as our color
Shift the initial RGB values to match value and store
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use mean of targets
Write node mean CATE
Write node std of CATE
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
Fetch appropriate color for node
Write node mean CATE
Write node mean CATE
Write recommended treatment and value - cost
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
This flag is true when names are set in a child class instead
"If names are set in a child class, add an attribute reflecting that"
This works only if X is passed as a kwarg
We plan to enforce X as kwarg only in future releases
This checks if names have been set in a child class
"If names were set in a child class, don't do it again"
"Wraps-up fit by setting attributes, cleaning up, etc."
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"Treatment names is None, default to BaseCateEstimator"
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
Get input names
Summary
add statsmodels to parent's options
add debiasedlasso to parent's options
add blb to parent's options
TODO Share some logic with non-discrete version
Get input names
Summary
add statsmodels to parent's options
add statsmodels to parent's options
add blb to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
remove None arguments
"scores entries should be lists of scores, so make each entry a singleton list"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
generate an instance of the final model
generate an instance of the nuisance model
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
alteration even when we only want to fit_final.
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
"however, sklearn doesn't support both stratifying and grouping (see"
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
their own object that supports grouping if they want to use groups.
for each mc iteration
for each model under cross fit setting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Policy Forest
=============================================================================
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base Policy tree
=============================================================================
The values below are required and utilitized by methods in the _SingleTreeExporterMixin
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"since we only allow single dimensional y, we could flatten the prediction"
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Handles the corner case when X=None but featurizer might be not None
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
TODO: support freq_weight and sample_var in debiased lasso
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
Replacing to remove docstring
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE: important to use the rlearner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the rlearner's copy to retain the information from fitting
Handles the corner case when X=None but featurizer might be not None
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `StatsModelsInference`
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support freq_weight and sample_var in debiased lasso
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add blb to parent's options
override only so that we can update the docstring to indicate
support for `GenericSingleTreatmentModelFinalInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
note that groups are not passed to score because they are only used for fitting
note that groups are not passed to score because they are only used for fitting
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
NOTE: important to get parent's wrapped copy so that
"after training wrapped featurizer is also trained, etc."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
Fit a doubly robust average effect
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
"If custom param grid, check that only estimator parameters are being altered"
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
override only so that we can update the docstring to indicate support for `blb`
Get input names
Summary
Determine output settings
"Important: This must be the first invocation of the random state at fit time, so that"
train/test splits are re-generatable from an external object simply by knowing the
random_state parameter of the tree. Can be useful in the future if one wants to create local
linear predictions. Currently is also useful for testing.
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
Set min_weight_leaf from min_weight_fraction_leaf
Build tree
We calculate the maximum number of samples from each half-split that any node in the tree can
hold. Used by criterion for memory space savings.
Initialize the criterion object and the criterion_val object if honest.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code is a fork from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
Set parameters
Don't instantiate estimators now! Parameters of base_estimator might
"still change. Eg., when grid-searching with the nested object syntax."
self.estimators_ needs to be filled by the derived classes in fit.
Compute the number of jobs
Partition estimators between jobs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=None here since we measure the effect across all Ts
once the estimator has been fit
"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
an intercept even when bias is part of coef.
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
squeeze the first axis
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
d_t=None here since we measure the effect across all Ts
d_t=None here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
Use broadcast to ensure that the shape of pred isn't being changed due to broadcasting the other direction
scale preds
scale std errs
"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
offset preds
"offset the distribution, too"
scale preds
"scale the distribution, too"
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
"if stderr is zero, ppf will return nans and the loop below would never terminate"
so bail out early; note that it might be possible to correct the algorithm for
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
be clean
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
don't proxy special methods
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
second level bootstrap which would be prohibitive computationally?
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
can't import from econml.inference at top level without creating cyclical dependencies
Note that inference results are always methods even if the inference is for a property
(e.g. coef__inference() is a method but coef_ is a property)
Therefore we must insert a lambda if getting inference for a non-callable
"If inference is for a property, create a fresh lambda to avoid passing args through"
"try to get interval/std first if appropriate,"
since we don't prefer a wrapped method with this name
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base GRF tree
=============================================================================
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
=============================================================================
A MultOutputWrapper for GRF classes
=============================================================================
=============================================================================
Instantiations of Generalized Random Forest
=============================================================================
"Append a constant treatment if `fit_intercept=True`, the coefficient"
in front of the constant treatment is the intercept in the moment equation.
"Append a constant treatment and constant instrument if `fit_intercept=True`,"
the coefficient in front of the constant treatment is the intercept in the moment equation.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Base Generalized Random Forest
=============================================================================
TODO: support freq_weight and sample_var
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
We calculate the min eigenvalue proxy that each criterion is considering
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Generating indices a priori before parallelism ended up being orders of magnitude
faster than how sklearn does it. The reason is that random samplers do not release the
gil it seems.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
####################
Variance correction
####################
Subtract the average within bag variance. This ends up being equal to the
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
The negative part is just sq_between.
Objective bayes debiasing for the diagonals where we know a-prior they are positive
"The off diagonals we have no objective prior, so no correction is applied."
Finally correcting the pred_cov or pred_var
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
test that the subsampling scheme past to the trees is correct
The sample size is chosen in particular to test rounding based error when subsampling
test that the estimator calcualtes var correctly
test api
test accuracy
test the projection functionality of forests
test that the estimator calcualtes var correctly
test api
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
test that the subsampling scheme past to the trees is correct
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
omit the lalonde notebook
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
can interpret without uncertainty
can't treat before interpreting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
compare with weight
compare with weight
compare with weight
compare with weight
Multi-dimensional output y
1-d y
compare when both sample_var and sample_weight exist
multi-d y
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that the subsampling scheme past to the trees is correct
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test inference results when `cate_feature_names` doesn not exist
Test inference results when `cate_feature_names` doesn not exist
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
pvalue for second column should be greater than zero since some points are on either side
of the tested value
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
only is not None when T1 is a constant or a list of constant
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Nuisance model has no score method, so nuisance_scores_ should be none"
Test non keyword based calls to fit
test non-array inputs
Test custom splitter
Test incomplete set of test folds
"y scores should be positive, since W predicts Y somewhat"
"t scores might not be, since W and T are uncorrelated"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
creating an instance should warn
using the instance should not warn
using the deprecated method should warn
don't warn if b and c are passed by keyword
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test at least one estimator from each category
test causal graph
test refutation estimate
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
ensure that we've got at least two of every row
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
need to make sure we get all *joint* combinations
IntentToTreat only supports binary treatments/instruments
IntentToTreat only supports binary treatments/instruments
IntentToTreat requires X
ensure we can serialize unfit estimator
these support only W but not X
"these support only binary, not general discrete T and Z"
ensure we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
TODO: add tests for extra properties like coef_ where they exist
TODO: add tests for extra properties like coef_ where they exist
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
TODO: ideally we could also test whether Z and X are jointly okay when both discrete
"however, with custom splits the checking happens in the first stage wrapper"
where we don't have all of the required information to do this;
we'd probably need to add it to _crossfit instead
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
The __warningregistry__'s need to be in a pristine state for tests
to work properly.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Create a wrapper around Lasso that doesn't support weights
since Lasso does natively support them starting in sklearn 0.23
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least 6 of every element
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
NOTE: this number may need to change if the default number of folds in
WeightedStratifiedKFold changes
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
make sure we can get out post-fit stuff
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
predetermined splits ensure that all features are seen in each split
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
create a simple artificial setup where effect of moving from treatment
"a -> b is 2,"
"a -> c is 1, and"
"b -> c is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
should rule out some basic issues.
Note that explicitly specifying the dtype as object is necessary until
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
estimated effects should be identical when treatment is explicitly given
but const_marginal_effect should be reordered based on the explicit cagetories
1-> 2 in original ordering; combination of 3->1 and 3->2
test outer grouping
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure that we can serialize unfit estimator
ensure that we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef_ and intercept_ inference
verify we can generate the summary
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
test outer grouping
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
helper class
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
#########################
WeightedLassoCVWrapper #
#########################
perform 1D fit
perform 2D fit
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
_penalized_inds is only set during fitting
cv exists on penalized model
now we can access _penalized_inds
check that we can read the cv attribute back out from the underlying model
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
make sure we don't run into problems dropping every index
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"global shape is (d_y, sum(d_t))"
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
dgp
model
model
"columns 'd', 'e', 'h' have too many values"
"columns 'd', 'e' have too many values"
lowering bound shouldn't affect already fit columns when warm starting
"column d is now okay, too"
verify that we can use a scalar treatment cost
verify that we can specify per-treatment costs for each sample
verify that using the same state returns the same results each time
set the categories for column 'd' explicitly so that b is default
"first column: 10 ones, this is fine"
"second column: 6 categories, plenty of random instances of each"
this is fine only if we increase the cateogry limit
"third column: nine ones, lots of twos, not enough unless we disable check"
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
forest heterogeneity won't work
"sixth column: just 1 one, not enough even without check"
increase bound on cat expansion
skip checks (reducing folds accordingly)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Define data features
Added `_df`to names to be different from the default cate_estimator names
Generate data
################################
Single treatment and outcome #
################################
Test LinearDML
|--> Test featurizers
ColumnTransformer doesn't propagate column names
|--> Test re-fit
Test SparseLinearDML
Test ForestDML
###################################
Mutiple treatments and outcomes #
###################################
Test LinearDML
Test SparseLinearDML
"Single outcome only, ORF does not support multiple outcomes"
Test DMLOrthoForest
Test DROrthoForest
Test XLearner
Skipping population summary names test because bootstrap inference is too slow
Test SLearner
Test TLearner
Test LinearDRLearner
Test SparseLinearDRLearner
Test ForestDRLearner
Test LinearIntentToTreatDRIV
Test DeepIV
Test categorical treatments
Check refit
Check refit after setting categories
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
Treatment effect function
Outcome support
Treatment support
"Generate controls, covariates, treatments and outcomes"
Heterogeneous treatment effects
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
through shap package.
test shap could generate the plot from the shap_values
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
"Note: unlike other Metalearners, we need the controls' encoded column for training"
"Thus, we append the controls column before the one-hot-encoded T"
"We might want to revisit, though, since it's linearly determined by the others"
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
output is
"* a column of ones if X, W, and Z are all None"
* just X or W or Z if both of the others are None
* hstack([arrs]) for whatever subset are not None otherwise
ensure Z is 2D
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
"instruments, and outcomes"
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"TODO: check that Y, T, Z do not have multiple columns"
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: do correct adjustment for sample_var
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
TODO: allow the final model to actually use X? Then we'd need to rename the class
since we would actually be calculating a CATE rather than ATE.
TODO: allow the final model to actually use X?
TODO: allow the final model to actually use X?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
TODO: would it be useful to extend to handle controls ala vanilla DML?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
make T 2D if if was a vector
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: this utility is documented but internal; reimplement?
TODO: this utility is even less public...
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
but also supports get_feature_names with expected signature
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
Convert python objects to (possibly nested) types that can easily be represented as literals
Convert SingleTreeInterpreter to a python dictionary
named tuple type for storing results inside CausalAnalysis class;
must be lifted to module level to enable pickling
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
Controls are all other columns of X
"can't use X[:, feat_ind] when X is a DataFrame"
TODO: we can't currently handle unseen values of the feature column when getting the effect;
we might want to modify OrthoLearner (and other discrete treatment classes)
so that the user can opt-in to allowing unseen treatment values
(and return NaN or something in that case)
array checking routines don't accept 0-width arrays
perform model selection
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
convert to NormalInferenceResults for consistency
Set the dictionary values shared between local and global summaries
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
required to fit a discrete DML model
Validate inputs
TODO: check compatibility of X and Y lengths
"no previous fit, cancel warm start"
"work with numeric feature indices, so that we can easily compare with categorical ones"
"if heterogeneity_inds is 1D, repeat it"
heterogeneity inds should be a 2D list of length same as train_inds
replace None elements of heterogeneity_inds and ensure indices are numeric
"TODO: bail out also if categorical columns, classification, random_state changed?"
TODO: should we also train a new model_y under any circumstances when warm_start is True?
train the Y model
"perform model selection for the Y model using all X, not on a per-column basis"
"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
work with the regression wrapper
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
"note that this needs to happen after wrapping to generalize to the multi-class case,"
since otherwise we'll have too many columns to be able to train a classifier
start with empty results and default shared insights
convert categorical indicators to numeric indices
check for indices over the categorical expansion bound
assume we'll be able to train former failures this time; we'll add them back if not
"can't remove in place while iterating over new_inds, so store in separate list"
"train the model, but warn"
no model can be trained in this case since we need more folds
"don't train a model, but suggest workaround since there are enough instances of least"
populated class
also remove from train_inds so we don't try to access the result later
extract subset of names matching new columns
"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
don't want to cache this failed result
properties to return from effect InferenceResults
properties to return from PopulationSummaryResults
Converts strings to property lookups or method calls as a convenience so that the
_point_props and _summary_props above can be applied to an inference object
Create a summary combining all results into a single output; this is used
by the various causal_effect and causal_effect_dict methods to generate either a dataframe
"or a dictionary, respectively, based on the summary function passed into this method"
"ensure array has shape (m,y,t)"
population summary is missing sample dimension; add it for consistency
outcome dimension is missing; add it for consistency
add singleton treatment dimension if missing
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
"each attr has dimension (m,y) or (m,y,t)"
concatenate along treatment dimension
"for dictionary representation, want to remove unneeded sample dimension"
in cohort and global results
TODO: enrich outcome logic for multi-class classification when that is supported
can't drop only level
should be serialization-ready and contain no numpy arrays
a global inference indicates the effect of that one feature on the outcome
need to reshape the output to match the input
we want to offset the inference object by the baseline estimate of y
"NOTE: this calculation is correct only if treatment costs are marginal costs,"
because then scaling the difference between treatment value and treatment costs is the
same as scaling the treatment value and subtracting the scaled treatment cost.
""
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
"continuous treatments, the policy value should include the benefit of decreasing treatments"
(rather than just not treating at all)
""
"We can get the total by seeing that if we restrict attention to units where we would treat,"
2 * policy_value - always_treat
includes exactly their contribution because policy_value and always_treat both include it
"and likewise restricting attention to the units where we want to decrease treatment,"
2 * policy_value - always-treat
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
is zero and the contribution to always_treat is negative
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
get dataframe with all but selected column
apply 10% of a typical treatment for this feature
"we've got treatment costs of shape (n, d_t-1) so we need to add a y dimension to broadcast safely"
set the effect bounds; for positive treatments these agree with
"the estimates; for negative treatments, we need to invert the interval"
the effect is now always positive since we decrease treatment when negative
"for discrete treatment, stack a zero result in front for control"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: conisder working around relying on sklearn implementation details
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
"If classification methods produce multiple columns of output,"
we need to manually encode classes to ensure consistent column ordering.
We clone the estimator to make sure that all the folds are
"independent, and that it is pickle-able."
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
`predictions` is a list of method outputs from each fold.
"If each of those is also a list, then treat this as a"
multioutput-multiclass task. We need to separately concatenate
the method outputs for each label into an `n_labels` long list.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
TODO: consider working around relying on sklearn implementation details
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Call weighted lasso on reduced design matrix
Weighted tau
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set model to WeightedLassoCV by default so there's always a model to get and set attributes on
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
(e.g. former has 'positive' and 'precompute' while latter does not)
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
don't proxy special methods
"don't pass get_params through to model, because that will cause sklearn to clone this"
regressor incorrectly
"Note: for known attributes that have been set this method will not be called,"
so we should just throw here because this is an attribute belonging to this class
but which hasn't yet been set on this instance
set default values for None
check freq_weight should be integer and should be accompanied by sample_var
check array shape
weight X and y and sample_var
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
average the outcome dimension if it exists and ensure 2d y_pred
get index of best treatment
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: consider working around relying on sklearn implementation details
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Convert rows to columns
Require group assignment t to be one-hot-encoded
Get predictions for the 2 splits
Must make sure indices are merged correctly
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Must normalize weights
Override the CATE inference options
Add blb inference to parent's options
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Need to redefine fit here for auto inference to work due to a quirk in how
wrap_fit is defined
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Check that all discrete treatments are represented
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
returns shape-conforming residuals
Copy and/or define models
Define parameter estimators
Define moment and mean gradient estimator
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Call `fit` from parent class
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Expand one-hot encoding to include the zero treatment
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Calculate confidence intervals for the parameter (marginal effect)
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
d_t=None here since we measure the effect across all Ts
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
def mlasso_model(): return MultiTaskLassoCV(
"cv=3, alphas=alpha_regs, max_iter=200)"
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
"alpha_regs = [5e-3, 1e-2, 5e-2]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
subset of features that are exogenous and create heterogeneity
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
subset of features wrt we estimate heterogeneity
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
introspect the constructor arguments to find the model parameters
to represent
"if the argument is deprecated, ignore it"
Extract and sort argument names excluding 'self'
column names
transfer input to numpy arrays
transfer input to 2d arrays
create dataframe
currently dowhy only support single outcome and single treatment
call dowhy
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
cate estimator but not the effect.
don't proxy special methods
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
For when checking input values is disabled
Type to column extraction function
"Get number of arguments, some sklearn featurizer don't accept feature_names"
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
Get feature names using featurizer
All attempts at retrieving transformed feature names have failed
Delegate handling to downstream logic
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
assume that we should perform nested cross-validation if and only if
the model has a 'cv' attribute; this is a somewhat brittle assumption...
logic copied from check_cv
otherwise we will assume the user already set the cv attribute to something
compatible with splitting with a 'groups' argument
now we have to compute the folds explicitly because some classifiers (like LassoCV)
don't use the groups when calling split internally
Normalize weights
This class is mainly derived from statsmodels.iolib.summary.Summary
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
"want to enforce that each bad_arg was either in kwargs,"
or else it was in neither and is just taking its default value
Any access should throw
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
input feature name is already updated by cate_feature_names.
define the index of d_x to filter for each given T
filter X after broadcast with T for each given T
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains some snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
make any access to matplotlib or plt throw an exception
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
Initialize saturation & value; calculate chroma & value shift
Calculate some intermediate values
Initialize RGB with same hue & chroma as our color
Shift the initial RGB values to match value and store
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use mean of targets
Write node mean CATE
Write node std of CATE
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
Fetch appropriate color for node
Write node mean CATE
Write node mean CATE
Write recommended treatment and value - cost
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
This flag is true when names are set in a child class instead
"If names are set in a child class, add an attribute reflecting that"
This works only if X is passed as a kwarg
We plan to enforce X as kwarg only in future releases
This checks if names have been set in a child class
"If names were set in a child class, don't do it again"
"Wraps-up fit by setting attributes, cleaning up, etc."
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"Treatment names is None, default to BaseCateEstimator"
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
Get input names
Summary
add statsmodels to parent's options
add debiasedlasso to parent's options
add blb to parent's options
TODO Share some logic with non-discrete version
Get input names
Summary
add statsmodels to parent's options
add statsmodels to parent's options
add blb to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
remove None arguments
"scores entries should be lists of scores, so make each entry a singleton list"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
generate an instance of the final model
generate an instance of the nuisance model
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
alteration even when we only want to fit_final.
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
"however, sklearn doesn't support both stratifying and grouping (see"
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
their own object that supports grouping if they want to use groups.
for each mc iteration
for each model under cross fit setting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Policy Forest
=============================================================================
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base Policy tree
=============================================================================
The values below are required and utilitized by methods in the _SingleTreeExporterMixin
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"since we only allow single dimensional y, we could flatten the prediction"
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Handles the corner case when X=None but featurizer might be not None
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
TODO: support freq_weight and sample_var in debiased lasso
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
Replacing to remove docstring
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE: important to use the rlearner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the rlearner's copy to retain the information from fitting
Handles the corner case when X=None but featurizer might be not None
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `StatsModelsInference`
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support freq_weight and sample_var in debiased lasso
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add blb to parent's options
override only so that we can update the docstring to indicate
support for `GenericSingleTreatmentModelFinalInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
note that groups are not passed to score because they are only used for fitting
note that groups are not passed to score because they are only used for fitting
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
NOTE: important to get parent's wrapped copy so that
"after training wrapped featurizer is also trained, etc."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
Fit a doubly robust average effect
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
"If custom param grid, check that only estimator parameters are being altered"
"use 0.699 instead of 0.7 as train size so that if there are 5 examples in a stratum, we get 2 in test"
override only so that we can update the docstring to indicate support for `blb`
Get input names
Summary
Determine output settings
"Important: This must be the first invocation of the random state at fit time, so that"
train/test splits are re-generatable from an external object simply by knowing the
random_state parameter of the tree. Can be useful in the future if one wants to create local
linear predictions. Currently is also useful for testing.
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
Set min_weight_leaf from min_weight_fraction_leaf
Build tree
We calculate the maximum number of samples from each half-split that any node in the tree can
hold. Used by criterion for memory space savings.
Initialize the criterion object and the criterion_val object if honest.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code is a fork from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
Set parameters
Don't instantiate estimators now! Parameters of base_estimator might
"still change. Eg., when grid-searching with the nested object syntax."
self.estimators_ needs to be filled by the derived classes in fit.
Compute the number of jobs
Partition estimators between jobs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=None here since we measure the effect across all Ts
once the estimator has been fit
"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
an intercept even when bias is part of coef.
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
squeeze the first axis
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
d_t=None here since we measure the effect across all Ts
d_t=None here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
NOTE: use np.asarray(offset) because if offset is a pd.Series direct addition would make the sum
"a Series as well, which would subsequently break summary_frame because flatten isn't supported"
NOTE: use np.asarray(factor) because if offset is a pd.Series direct addition would make the product
"a Series as well, which would subsequently break summary_frame because flatten isn't supported"
scale preds
scale std errs
"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
offset preds
"offset the distribution, too"
scale preds
"scale the distribution, too"
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
"if stderr is zero, ppf will return nans and the loop below would never terminate"
so bail out early; note that it might be possible to correct the algorithm for
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
be clean
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
don't proxy special methods
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
second level bootstrap which would be prohibitive computationally?
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
can't import from econml.inference at top level without creating cyclical dependencies
Note that inference results are always methods even if the inference is for a property
(e.g. coef__inference() is a method but coef_ is a property)
Therefore we must insert a lambda if getting inference for a non-callable
"If inference is for a property, create a fresh lambda to avoid passing args through"
"try to get interval/std first if appropriate,"
since we don't prefer a wrapped method with this name
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base GRF tree
=============================================================================
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
=============================================================================
A MultOutputWrapper for GRF classes
=============================================================================
=============================================================================
Instantiations of Generalized Random Forest
=============================================================================
"Append a constant treatment if `fit_intercept=True`, the coefficient"
in front of the constant treatment is the intercept in the moment equation.
"Append a constant treatment and constant instrument if `fit_intercept=True`,"
the coefficient in front of the constant treatment is the intercept in the moment equation.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Base Generalized Random Forest
=============================================================================
TODO: support freq_weight and sample_var
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
We calculate the min eigenvalue proxy that each criterion is considering
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Generating indices a priori before parallelism ended up being orders of magnitude
faster than how sklearn does it. The reason is that random samplers do not release the
gil it seems.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
####################
Variance correction
####################
Subtract the average within bag variance. This ends up being equal to the
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
The negative part is just sq_between.
Objective bayes debiasing for the diagonals where we know a-prior they are positive
"The off diagonals we have no objective prior, so no correction is applied."
Finally correcting the pred_cov or pred_var
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
test that the subsampling scheme past to the trees is correct
The sample size is chosen in particular to test rounding based error when subsampling
test that the estimator calcualtes var correctly
test api
test accuracy
test the projection functionality of forests
test that the estimator calcualtes var correctly
test api
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
test that the subsampling scheme past to the trees is correct
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
omit the lalonde notebook
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
can interpret without uncertainty
can't treat before interpreting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
compare with weight
compare with weight
compare with weight
compare with weight
Multi-dimensional output y
1-d y
compare when both sample_var and sample_weight exist
multi-d y
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that the subsampling scheme past to the trees is correct
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test inference results when `cate_feature_names` doesn not exist
Test inference results when `cate_feature_names` doesn not exist
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
pvalue for second column should be greater than zero since some points are on either side
of the tested value
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
only is not None when T1 is a constant or a list of constant
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Nuisance model has no score method, so nuisance_scores_ should be none"
Test non keyword based calls to fit
test non-array inputs
Test custom splitter
Test incomplete set of test folds
"y scores should be positive, since W predicts Y somewhat"
"t scores might not be, since W and T are uncorrelated"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
creating an instance should warn
using the instance should not warn
using the deprecated method should warn
don't warn if b and c are passed by keyword
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test at least one estimator from each category
test causal graph
test refutation estimate
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
ensure that we've got at least two of every row
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
need to make sure we get all *joint* combinations
IntentToTreat only supports binary treatments/instruments
IntentToTreat only supports binary treatments/instruments
IntentToTreat requires X
ensure we can serialize unfit estimator
these support only W but not X
"these support only binary, not general discrete T and Z"
ensure we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
TODO: add tests for extra properties like coef_ where they exist
TODO: add tests for extra properties like coef_ where they exist
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
TODO: ideally we could also test whether Z and X are jointly okay when both discrete
"however, with custom splits the checking happens in the first stage wrapper"
where we don't have all of the required information to do this;
we'd probably need to add it to _crossfit instead
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
The __warningregistry__'s need to be in a pristine state for tests
to work properly.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Create a wrapper around Lasso that doesn't support weights
since Lasso does natively support them starting in sklearn 0.23
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least 6 of every element
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
NOTE: this number may need to change if the default number of folds in
WeightedStratifiedKFold changes
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
make sure we can get out post-fit stuff
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
predetermined splits ensure that all features are seen in each split
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
create a simple artificial setup where effect of moving from treatment
"a -> b is 2,"
"a -> c is 1, and"
"b -> c is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
should rule out some basic issues.
Note that explicitly specifying the dtype as object is necessary until
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
estimated effects should be identical when treatment is explicitly given
but const_marginal_effect should be reordered based on the explicit cagetories
1-> 2 in original ordering; combination of 3->1 and 3->2
test outer grouping
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure that we can serialize unfit estimator
ensure that we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef_ and intercept_ inference
verify we can generate the summary
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
test outer grouping
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
helper class
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
#########################
WeightedLassoCVWrapper #
#########################
perform 1D fit
perform 2D fit
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
_penalized_inds is only set during fitting
cv exists on penalized model
now we can access _penalized_inds
check that we can read the cv attribute back out from the underlying model
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
make sure we don't run into problems dropping every index
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"global shape is (d_y, sum(d_t))"
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
dgp
model
model
"columns 'd', 'e', 'h' have too many values"
"columns 'd', 'e' have too many values"
lowering bound shouldn't affect already fit columns when warm starting
"column d is now okay, too"
verify that we can use a scalar treatment cost
verify that we can specify per-treatment costs for each sample
verify that using the same state returns the same results each time
set the categories for column 'd' explicitly so that b is default
"first column: 10 ones, this is fine"
"second column: 6 categories, plenty of random instances of each"
this is fine only if we increase the cateogry limit
"third column: nine ones, lots of twos, not enough unless we disable check"
"fourth column: 5 ones, also not enough but barely works even with forest heterogeneity"
"fifth column: 2 ones, ensures that we will change number of folds for linear heterogeneity"
forest heterogeneity won't work
"sixth column: just 1 one, not enough even without check"
increase bound on cat expansion
skip checks (reducing folds accordingly)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Define data features
Added `_df`to names to be different from the default cate_estimator names
Generate data
################################
Single treatment and outcome #
################################
Test LinearDML
|--> Test featurizers
ColumnTransformer doesn't propagate column names
|--> Test re-fit
Test SparseLinearDML
Test ForestDML
###################################
Mutiple treatments and outcomes #
###################################
Test LinearDML
Test SparseLinearDML
"Single outcome only, ORF does not support multiple outcomes"
Test DMLOrthoForest
Test DROrthoForest
Test XLearner
Skipping population summary names test because bootstrap inference is too slow
Test SLearner
Test TLearner
Test LinearDRLearner
Test SparseLinearDRLearner
Test ForestDRLearner
Test LinearIntentToTreatDRIV
Test DeepIV
Test categorical treatments
Check refit
Check refit after setting categories
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
Treatment effect function
Outcome support
Treatment support
"Generate controls, covariates, treatments and outcomes"
Heterogeneous treatment effects
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
through shap package.
test shap could generate the plot from the shap_values
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
"Note: unlike other Metalearners, we need the controls' encoded column for training"
"Thus, we append the controls column before the one-hot-encoded T"
"We might want to revisit, though, since it's linearly determined by the others"
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
output is
"* a column of ones if X, W, and Z are all None"
* just X or W or Z if both of the others are None
* hstack([arrs]) for whatever subset are not None otherwise
ensure Z is 2D
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
"instruments, and outcomes"
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"TODO: check that Y, T, Z do not have multiple columns"
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: do correct adjustment for sample_var
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
TODO: allow the final model to actually use X? Then we'd need to rename the class
since we would actually be calculating a CATE rather than ATE.
TODO: allow the final model to actually use X?
TODO: allow the final model to actually use X?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
TODO: would it be useful to extend to handle controls ala vanilla DML?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
make T 2D if if was a vector
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: this utility is documented but internal; reimplement?
TODO: this utility is even less public...
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
but also supports get_feature_names with expected signature
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
Convert python objects to (possibly nested) types that can easily be represented as literals
Convert SingleTreeInterpreter to a python dictionary
named tuple type for storing results inside CausalAnalysis class;
must be lifted to module level to enable pickling
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
Controls are all other columns of X
"can't use X[:, feat_ind] when X is a DataFrame"
TODO: we can't currently handle unseen values of the feature column when getting the effect;
we might want to modify OrthoLearner (and other discrete treatment classes)
so that the user can opt-in to allowing unseen treatment values
(and return NaN or something in that case)
array checking routines don't accept 0-width arrays
perform model selection
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
convert to NormalInferenceResults for consistency
Set the dictionary values shared between local and global summaries
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
"Unless we're opting into minimal cross-fitting, this is the minimum number of instances of each category"
required to fit a discrete DML model
Validate inputs
TODO: check compatibility of X and Y lengths
"no previous fit, cancel warm start"
"work with numeric feature indices, so that we can easily compare with categorical ones"
"if heterogeneity_inds is 1D, repeat it"
heterogeneity inds should be a 2D list of length same as train_inds
replace None elements of heterogeneity_inds and ensure indices are numeric
"TODO: bail out also if categorical columns, classification, random_state changed?"
TODO: should we also train a new model_y under any circumstances when warm_start is True?
train the Y model
"perform model selection for the Y model using all X, not on a per-column basis"
"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
work with the regression wrapper
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
"note that this needs to happen after wrapping to generalize to the multi-class case,"
since otherwise we'll have too many columns to be able to train a classifier
start with empty results and default shared insights
convert categorical indicators to numeric indices
check for indices over the categorical expansion bound
assume we'll be able to train former failures this time; we'll add them back if not
"can't remove in place while iterating over new_inds, so store in separate list"
"train the model, but warn"
no model can be trained in this case since we need more folds
"don't train a model, but suggest workaround since there are enough instances of least"
populated class
also remove from train_inds so we don't try to access the result later
extract subset of names matching new columns
"track indices where an exception was thrown, since we can't remove from dictionary while iterating"
don't want to cache this failed result
properties to return from effect InferenceResults
properties to return from PopulationSummaryResults
Converts strings to property lookups or method calls as a convenience so that the
_point_props and _summary_props above can be applied to an inference object
Create a summary combining all results into a single output; this is used
by the various causal_effect and causal_effect_dict methods to generate either a dataframe
"or a dictionary, respectively, based on the summary function passed into this method"
"ensure array has shape (m,y,t)"
population summary is missing sample dimension; add it for consistency
outcome dimension is missing; add it for consistency
add singleton treatment dimension if missing
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
"each attr has dimension (m,y) or (m,y,t)"
concatenate along treatment dimension
"for dictionary representation, want to remove unneeded sample dimension"
in cohort and global results
TODO: enrich outcome logic for multi-class classification when that is supported
can't drop only level
should be serialization-ready and contain no numpy arrays
a global inference indicates the effect of that one feature on the outcome
need to reshape the output to match the input
we want to offset the inference object by the baseline estimate of y
"NOTE: this calculation is correct only if treatment costs are marginal costs,"
because then scaling the difference between treatment value and treatment costs is the
same as scaling the treatment value and subtracting the scaled treatment cost.
""
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
"continuous treatments, the policy value should include the benefit of decreasing treatments"
(rather than just not treating at all)
""
"We can get the total by seeing that if we restrict attention to units where we would treat,"
2 * policy_value - always_treat
includes exactly their contribution because policy_value and always_treat both include it
"and likewise restricting attention to the units where we want to decrease treatment,"
2 * policy_value - always-treat
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
is zero and the contribution to always_treat is negative
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
get dataframe with all but selected column
apply 10% of a typical treatment for this feature
set the effect bounds; for positive treatments these agree with
"the estimates; for negative treatments, we need to invert the interval"
the effect is now always positive since we decrease treatment when negative
"for discrete treatment, stack a zero result in front for control"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: conisder working around relying on sklearn implementation details
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
"If classification methods produce multiple columns of output,"
we need to manually encode classes to ensure consistent column ordering.
We clone the estimator to make sure that all the folds are
"independent, and that it is pickle-able."
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
`predictions` is a list of method outputs from each fold.
"If each of those is also a list, then treat this as a"
multioutput-multiclass task. We need to separately concatenate
the method outputs for each label into an `n_labels` long list.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
TODO: consider working around relying on sklearn implementation details
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Call weighted lasso on reduced design matrix
Weighted tau
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set model to WeightedLassoCV by default so there's always a model to get and set attributes on
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
(e.g. former has 'positive' and 'precompute' while latter does not)
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
don't proxy special methods
"don't pass get_params through to model, because that will cause sklearn to clone this"
regressor incorrectly
"Note: for known attributes that have been set this method will not be called,"
so we should just throw here because this is an attribute belonging to this class
but which hasn't yet been set on this instance
set default values for None
check freq_weight should be integer and should be accompanied by sample_var
check array shape
weight X and y and sample_var
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
average the outcome dimension if it exists and ensure 2d y_pred
get index of best treatment
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: consider working around relying on sklearn implementation details
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Convert rows to columns
Require group assignment t to be one-hot-encoded
Get predictions for the 2 splits
Must make sure indices are merged correctly
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Must normalize weights
Override the CATE inference options
Add blb inference to parent's options
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Need to redefine fit here for auto inference to work due to a quirk in how
wrap_fit is defined
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Check that all discrete treatments are represented
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
returns shape-conforming residuals
Copy and/or define models
Define parameter estimators
Define moment and mean gradient estimator
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Call `fit` from parent class
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Expand one-hot encoding to include the zero treatment
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Calculate confidence intervals for the parameter (marginal effect)
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
d_t=None here since we measure the effect across all Ts
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
def mlasso_model(): return MultiTaskLassoCV(
"cv=3, alphas=alpha_regs, max_iter=200)"
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
"alpha_regs = [5e-3, 1e-2, 5e-2]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
subset of features that are exogenous and create heterogeneity
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
subset of features wrt we estimate heterogeneity
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
introspect the constructor arguments to find the model parameters
to represent
"if the argument is deprecated, ignore it"
Extract and sort argument names excluding 'self'
column names
transfer input to numpy arrays
transfer input to 2d arrays
create dataframe
currently dowhy only support single outcome and single treatment
call dowhy
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
cate estimator but not the effect.
don't proxy special methods
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
For when checking input values is disabled
Type to column extraction function
"Get number of arguments, some sklearn featurizer don't accept feature_names"
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
Get feature names using featurizer
All attempts at retrieving transformed feature names have failed
Delegate handling to downstream logic
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
assume that we should perform nested cross-validation if and only if
the model has a 'cv' attribute; this is a somewhat brittle assumption...
logic copied from check_cv
otherwise we will assume the user already set the cv attribute to something
compatible with splitting with a 'groups' argument
now we have to compute the folds explicitly because some classifiers (like LassoCV)
don't use the groups when calling split internally
Normalize weights
This class is mainly derived from statsmodels.iolib.summary.Summary
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
"want to enforce that each bad_arg was either in kwargs,"
or else it was in neither and is just taking its default value
Any access should throw
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
input feature name is already updated by cate_feature_names.
define the index of d_x to filter for each given T
filter X after broadcast with T for each given T
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains some snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
make any access to matplotlib or plt throw an exception
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
Initialize saturation & value; calculate chroma & value shift
Calculate some intermediate values
Initialize RGB with same hue & chroma as our color
Shift the initial RGB values to match value and store
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use mean of targets
Write node mean CATE
Write node std of CATE
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
Fetch appropriate color for node
Write node mean CATE
Write node mean CATE
Write recommended treatment and value - cost
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
This flag is true when names are set in a child class instead
"If names are set in a child class, add an attribute reflecting that"
This works only if X is passed as a kwarg
We plan to enforce X as kwarg only in future releases
This checks if names have been set in a child class
"If names were set in a child class, don't do it again"
"Wraps-up fit by setting attributes, cleaning up, etc."
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"Treatment names is None, default to BaseCateEstimator"
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
Get input names
Summary
add statsmodels to parent's options
add debiasedlasso to parent's options
add blb to parent's options
TODO Share some logic with non-discrete version
Get input names
Summary
add statsmodels to parent's options
add statsmodels to parent's options
add blb to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
remove None arguments
"scores entries should be lists of scores, so make each entry a singleton list"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
generate an instance of the final model
generate an instance of the nuisance model
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
alteration even when we only want to fit_final.
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
"however, sklearn doesn't support both stratifying and grouping (see"
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
their own object that supports grouping if they want to use groups.
for each mc iteration
for each model under cross fit setting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Policy Forest
=============================================================================
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base Policy tree
=============================================================================
The values below are required and utilitized by methods in the _SingleTreeExporterMixin
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"since we only allow single dimensional y, we could flatten the prediction"
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Handles the corner case when X=None but featurizer might be not None
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
TODO: support freq_weight and sample_var in debiased lasso
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
Replacing to remove docstring
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE: important to use the rlearner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the rlearner's copy to retain the information from fitting
Handles the corner case when X=None but featurizer might be not None
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `StatsModelsInference`
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support freq_weight and sample_var in debiased lasso
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add blb to parent's options
override only so that we can update the docstring to indicate
support for `GenericSingleTreatmentModelFinalInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
note that groups are not passed to score because they are only used for fitting
note that groups are not passed to score because they are only used for fitting
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
NOTE: important to get parent's wrapped copy so that
"after training wrapped featurizer is also trained, etc."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
Fit a doubly robust average effect
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
"If custom param grid, check that only estimator parameters are being altered"
override only so that we can update the docstring to indicate support for `blb`
Get input names
Summary
Determine output settings
"Important: This must be the first invocation of the random state at fit time, so that"
train/test splits are re-generatable from an external object simply by knowing the
random_state parameter of the tree. Can be useful in the future if one wants to create local
linear predictions. Currently is also useful for testing.
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
Set min_weight_leaf from min_weight_fraction_leaf
Build tree
We calculate the maximum number of samples from each half-split that any node in the tree can
hold. Used by criterion for memory space savings.
Initialize the criterion object and the criterion_val object if honest.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code is a fork from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
Set parameters
Don't instantiate estimators now! Parameters of base_estimator might
"still change. Eg., when grid-searching with the nested object syntax."
self.estimators_ needs to be filled by the derived classes in fit.
Compute the number of jobs
Partition estimators between jobs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=None here since we measure the effect across all Ts
once the estimator has been fit
"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
an intercept even when bias is part of coef.
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
squeeze the first axis
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
d_t=None here since we measure the effect across all Ts
d_t=None here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
NOTE: use np.asarray(offset) because if offset is a pd.Series direct addition would make the sum
"a Series as well, which would subsequently break summary_frame because flatten isn't supported"
NOTE: use np.asarray(factor) because if offset is a pd.Series direct addition would make the product
"a Series as well, which would subsequently break summary_frame because flatten isn't supported"
scale preds
scale std errs
"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
offset preds
"offset the distribution, too"
scale preds
"scale the distribution, too"
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
"if stderr is zero, ppf will return nans and the loop below would never terminate"
so bail out early; note that it might be possible to correct the algorithm for
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
be clean
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
don't proxy special methods
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
second level bootstrap which would be prohibitive computationally?
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
can't import from econml.inference at top level without creating cyclical dependencies
Note that inference results are always methods even if the inference is for a property
(e.g. coef__inference() is a method but coef_ is a property)
Therefore we must insert a lambda if getting inference for a non-callable
"If inference is for a property, create a fresh lambda to avoid passing args through"
"try to get interval/std first if appropriate,"
since we don't prefer a wrapped method with this name
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base GRF tree
=============================================================================
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
=============================================================================
A MultOutputWrapper for GRF classes
=============================================================================
=============================================================================
Instantiations of Generalized Random Forest
=============================================================================
"Append a constant treatment if `fit_intercept=True`, the coefficient"
in front of the constant treatment is the intercept in the moment equation.
"Append a constant treatment and constant instrument if `fit_intercept=True`,"
the coefficient in front of the constant treatment is the intercept in the moment equation.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Base Generalized Random Forest
=============================================================================
TODO: support freq_weight and sample_var
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
We calculate the min eigenvalue proxy that each criterion is considering
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Generating indices a priori before parallelism ended up being orders of magnitude
faster than how sklearn does it. The reason is that random samplers do not release the
gil it seems.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
####################
Variance correction
####################
Subtract the average within bag variance. This ends up being equal to the
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
The negative part is just sq_between.
Objective bayes debiasing for the diagonals where we know a-prior they are positive
"The off diagonals we have no objective prior, so no correction is applied."
Finally correcting the pred_cov or pred_var
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
test that the subsampling scheme past to the trees is correct
The sample size is chosen in particular to test rounding based error when subsampling
test that the estimator calcualtes var correctly
test api
test accuracy
test the projection functionality of forests
test that the estimator calcualtes var correctly
test api
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
test that the subsampling scheme past to the trees is correct
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
omit the lalonde notebook
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
can interpret without uncertainty
can't treat before interpreting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
compare with weight
compare with weight
compare with weight
compare with weight
Multi-dimensional output y
1-d y
compare when both sample_var and sample_weight exist
multi-d y
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that the subsampling scheme past to the trees is correct
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test inference results when `cate_feature_names` doesn not exist
Test inference results when `cate_feature_names` doesn not exist
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
pvalue for second column should be greater than zero since some points are on either side
of the tested value
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
only is not None when T1 is a constant or a list of constant
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Nuisance model has no score method, so nuisance_scores_ should be none"
Test non keyword based calls to fit
test non-array inputs
Test custom splitter
Test incomplete set of test folds
"y scores should be positive, since W predicts Y somewhat"
"t scores might not be, since W and T are uncorrelated"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
creating an instance should warn
using the instance should not warn
using the deprecated method should warn
don't warn if b and c are passed by keyword
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test at least one estimator from each category
test causal graph
test refutation estimate
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
ensure that we've got at least two of every row
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
need to make sure we get all *joint* combinations
IntentToTreat only supports binary treatments/instruments
IntentToTreat only supports binary treatments/instruments
IntentToTreat requires X
ensure we can serialize unfit estimator
these support only W but not X
"these support only binary, not general discrete T and Z"
ensure we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
TODO: add tests for extra properties like coef_ where they exist
TODO: add tests for extra properties like coef_ where they exist
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
TODO: ideally we could also test whether Z and X are jointly okay when both discrete
"however, with custom splits the checking happens in the first stage wrapper"
where we don't have all of the required information to do this;
we'd probably need to add it to _crossfit instead
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
The __warningregistry__'s need to be in a pristine state for tests
to work properly.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Create a wrapper around Lasso that doesn't support weights
since Lasso does natively support them starting in sklearn 0.23
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least 6 of every element
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
NOTE: this number may need to change if the default number of folds in
WeightedStratifiedKFold changes
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
make sure we can get out post-fit stuff
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
predetermined splits ensure that all features are seen in each split
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
create a simple artificial setup where effect of moving from treatment
"a -> b is 2,"
"a -> c is 1, and"
"b -> c is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
should rule out some basic issues.
Note that explicitly specifying the dtype as object is necessary until
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
estimated effects should be identical when treatment is explicitly given
but const_marginal_effect should be reordered based on the explicit cagetories
1-> 2 in original ordering; combination of 3->1 and 3->2
test outer grouping
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure that we can serialize unfit estimator
ensure that we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef_ and intercept_ inference
verify we can generate the summary
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
test outer grouping
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
helper class
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
#########################
WeightedLassoCVWrapper #
#########################
perform 1D fit
perform 2D fit
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
_penalized_inds is only set during fitting
cv exists on penalized model
now we can access _penalized_inds
check that we can read the cv attribute back out from the underlying model
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"global shape is (d_y, sum(d_t))"
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
make sure we don't run into problems dropping every index
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"global shape is (d_y, sum(d_t))"
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
"Make sure we handle continuous, binary, and multi-class treatments"
"For multiple discrete treatments, one ""always treat"" value per non-default treatment"
policy value should exceed always treating with any treatment
dgp
model
model
"columns 'd', 'e', 'h' have too many values"
"columns 'd', 'e' have too many values"
lowering bound shouldn't affect already fit columns when warm starting
"column d is now okay, too"
verify that we can use a scalar treatment cost
verify that we can specify per-treatment costs for each sample
verify that using the same state returns the same results each time
set the categories for column 'd' explicitly so that b is default
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Define data features
Added `_df`to names to be different from the default cate_estimator names
Generate data
################################
Single treatment and outcome #
################################
Test LinearDML
|--> Test featurizers
ColumnTransformer doesn't propagate column names
|--> Test re-fit
Test SparseLinearDML
Test ForestDML
###################################
Mutiple treatments and outcomes #
###################################
Test LinearDML
Test SparseLinearDML
"Single outcome only, ORF does not support multiple outcomes"
Test DMLOrthoForest
Test DROrthoForest
Test XLearner
Skipping population summary names test because bootstrap inference is too slow
Test SLearner
Test TLearner
Test LinearDRLearner
Test SparseLinearDRLearner
Test ForestDRLearner
Test LinearIntentToTreatDRIV
Test DeepIV
Test categorical treatments
Check refit
Check refit after setting categories
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
Treatment effect function
Outcome support
Treatment support
"Generate controls, covariates, treatments and outcomes"
Heterogeneous treatment effects
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
through shap package.
test shap could generate the plot from the shap_values
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
"Note: unlike other Metalearners, we need the controls' encoded column for training"
"Thus, we append the controls column before the one-hot-encoded T"
"We might want to revisit, though, since it's linearly determined by the others"
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
output is
"* a column of ones if X, W, and Z are all None"
* just X or W or Z if both of the others are None
* hstack([arrs]) for whatever subset are not None otherwise
ensure Z is 2D
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
"instruments, and outcomes"
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"TODO: check that Y, T, Z do not have multiple columns"
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: do correct adjustment for sample_var
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
TODO: allow the final model to actually use X? Then we'd need to rename the class
since we would actually be calculating a CATE rather than ATE.
TODO: allow the final model to actually use X?
TODO: allow the final model to actually use X?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
TODO: would it be useful to extend to handle controls ala vanilla DML?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
make T 2D if if was a vector
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: this utility is documented but internal; reimplement?
TODO: this utility is even less public...
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
but also supports get_feature_names with expected signature
NOTE: set handle_unknown to 'ignore' so that we don't throw at runtime if given a novel value
Convert python objects to (possibly nested) types that can easily be represented as literals
Convert SingleTreeInterpreter to a python dictionary
named tuple type for storing results inside CausalAnalysis class;
must be lifted to module level to enable pickling
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
Controls are all other columns of X
"can't use X[:, feat_ind] when X is a DataFrame"
TODO: we can't currently handle unseen values of the feature column when getting the effect;
we might want to modify OrthoLearner (and other discrete treatment classes)
so that the user can opt-in to allowing unseen treatment values
(and return NaN or something in that case)
array checking routines don't accept 0-width arrays
perform model selection
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
convert to NormalInferenceResults for consistency
Set the dictionary values shared between local and global summaries
"calculate a ""typical"" treatment value, using the mean of the absolute value of non-zero treatments"
Validate inputs
TODO: check compatibility of X and Y lengths
"no previous fit, cancel warm start"
"work with numeric feature indices, so that we can easily compare with categorical ones"
"if heterogeneity_inds is 1D, repeat it"
heterogeneity inds should be a 2D list of length same as train_inds
replace None elements of heterogeneity_inds and ensure indices are numeric
"TODO: bail out also if categorical columns, classification, random_state changed?"
TODO: should we also train a new model_y under any circumstances when warm_start is True?
train the Y model
"perform model selection for the Y model using all X, not on a per-column basis"
"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
work with the regression wrapper
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
"note that this needs to happen after wrapping to generalize to the multi-class case,"
since otherwise we'll have too many columns to be able to train a classifier
start with empty results and default shared insights
convert categorical indicators to numeric indices
check for indices over the categorical expansion bound
"can't remove in place while iterating over new_inds, so store in separate list"
also remove from train_inds so we don't try to access the result later
extract subset of names matching new columns
properties to return from effect InferenceResults
properties to return from PopulationSummaryResults
Converts strings to property lookups or method calls as a convenience so that the
_point_props and _summary_props above can be applied to an inference object
Create a summary combining all results into a single output; this is used
by the various causal_effect and causal_effect_dict methods to generate either a dataframe
"or a dictionary, respectively, based on the summary function passed into this method"
"ensure array has shape (m,y,t)"
population summary is missing sample dimension; add it for consistency
outcome dimension is missing; add it for consistency
add singleton treatment dimension if missing
store set of inference results so we don't need to recompute per-attribute below in summary/coalesce
"each attr has dimension (m,y) or (m,y,t)"
concatenate along treatment dimension
"for dictionary representation, want to remove unneeded sample dimension"
in cohort and global results
TODO: enrich outcome logic for multi-class classification when that is supported
can't drop only level
should be serialization-ready and contain no numpy arrays
a global inference indicates the effect of that one feature on the outcome
need to reshape the output to match the input
we want to offset the inference object by the baseline estimate of y
"NOTE: this calculation is correct only if treatment costs are marginal costs,"
because then scaling the difference between treatment value and treatment costs is the
same as scaling the treatment value and subtracting the scaled treatment cost.
""
"Note also that unlike the standard outputs of the SinglePolicyTreeInterpreter, for"
"continuous treatments, the policy value should include the benefit of decreasing treatments"
(rather than just not treating at all)
""
"We can get the total by seeing that if we restrict attention to units where we would treat,"
2 * policy_value - always_treat
includes exactly their contribution because policy_value and always_treat both include it
"and likewise restricting attention to the units where we want to decrease treatment,"
2 * policy_value - always-treat
"also computes the *benefit* of decreasing treatment, because their contribution to policy_value"
is zero and the contribution to always_treat is negative
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
get dataframe with all but selected column
apply 10% of a typical treatment for this feature
set the effect bounds; for positive treatments these agree with
"the estimates; for negative treatments, we need to invert the interval"
the effect is now always positive since we decrease treatment when negative
"for discrete treatment, stack a zero result in front for control"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: conisder working around relying on sklearn implementation details
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
"If classification methods produce multiple columns of output,"
we need to manually encode classes to ensure consistent column ordering.
We clone the estimator to make sure that all the folds are
"independent, and that it is pickle-able."
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
`predictions` is a list of method outputs from each fold.
"If each of those is also a list, then treat this as a"
multioutput-multiclass task. We need to separately concatenate
the method outputs for each label into an `n_labels` long list.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
TODO: consider working around relying on sklearn implementation details
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Call weighted lasso on reduced design matrix
Weighted tau
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set model to WeightedLassoCV by default so there's always a model to get and set attributes on
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
(e.g. former has 'positive' and 'precompute' while latter does not)
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
don't proxy special methods
"don't pass get_params through to model, because that will cause sklearn to clone this"
regressor incorrectly
"Note: for known attributes that have been set this method will not be called,"
so we should just throw here because this is an attribute belonging to this class
but which hasn't yet been set on this instance
set default values for None
check freq_weight should be integer and should be accompanied by sample_var
check array shape
weight X and y and sample_var
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
average the outcome dimension if it exists and ensure 2d y_pred
get index of best treatment
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: consider working around relying on sklearn implementation details
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Convert rows to columns
Require group assignment t to be one-hot-encoded
Get predictions for the 2 splits
Must make sure indices are merged correctly
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Must normalize weights
Override the CATE inference options
Add blb inference to parent's options
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Need to redefine fit here for auto inference to work due to a quirk in how
wrap_fit is defined
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Check that all discrete treatments are represented
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
returns shape-conforming residuals
Copy and/or define models
Define parameter estimators
Define moment and mean gradient estimator
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Call `fit` from parent class
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Expand one-hot encoding to include the zero treatment
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Calculate confidence intervals for the parameter (marginal effect)
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
d_t=None here since we measure the effect across all Ts
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
def mlasso_model(): return MultiTaskLassoCV(
"cv=3, alphas=alpha_regs, max_iter=200)"
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
"alpha_regs = [5e-3, 1e-2, 5e-2]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
subset of features that are exogenous and create heterogeneity
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
subset of features wrt we estimate heterogeneity
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
introspect the constructor arguments to find the model parameters
to represent
"if the argument is deprecated, ignore it"
Extract and sort argument names excluding 'self'
column names
transfer input to numpy arrays
transfer input to 2d arrays
create dataframe
currently dowhy only support single outcome and single treatment
call dowhy
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
cate estimator but not the effect.
don't proxy special methods
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
For when checking input values is disabled
Type to column extraction function
"Get number of arguments, some sklearn featurizer don't accept feature_names"
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
Get feature names using featurizer
All attempts at retrieving transformed feature names have failed
Delegate handling to downstream logic
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
assume that we should perform nested cross-validation if and only if
the model has a 'cv' attribute; this is a somewhat brittle assumption...
logic copied from check_cv
otherwise we will assume the user already set the cv attribute to something
compatible with splitting with a 'groups' argument
now we have to compute the folds explicitly because some classifiers (like LassoCV)
don't use the groups when calling split internally
Normalize weights
This class is mainly derived from statsmodels.iolib.summary.Summary
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
"want to enforce that each bad_arg was either in kwargs,"
or else it was in neither and is just taking its default value
Any access should throw
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
input feature name is already updated by cate_feature_names.
define the index of d_x to filter for each given T
filter X after broadcast with T for each given T
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains some snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
make any access to matplotlib or plt throw an exception
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
Initialize saturation & value; calculate chroma & value shift
Calculate some intermediate values
Initialize RGB with same hue & chroma as our color
Shift the initial RGB values to match value and store
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use mean of targets
Write node mean CATE
Write node std of CATE
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
Fetch appropriate color for node
Write node mean CATE
Write node mean CATE
Write recommended treatment and value - cost
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
This flag is true when names are set in a child class instead
"If names are set in a child class, add an attribute reflecting that"
This works only if X is passed as a kwarg
We plan to enforce X as kwarg only in future releases
This checks if names have been set in a child class
"If names were set in a child class, don't do it again"
"Wraps-up fit by setting attributes, cleaning up, etc."
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"Treatment names is None, default to BaseCateEstimator"
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
Get input names
Summary
add statsmodels to parent's options
add debiasedlasso to parent's options
add blb to parent's options
TODO Share some logic with non-discrete version
Get input names
Summary
add statsmodels to parent's options
add statsmodels to parent's options
add blb to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
remove None arguments
"scores entries should be lists of scores, so make each entry a singleton list"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
generate an instance of the final model
generate an instance of the nuisance model
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
alteration even when we only want to fit_final.
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
"however, sklearn doesn't support both stratifying and grouping (see"
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
their own object that supports grouping if they want to use groups.
for each mc iteration
for each model under cross fit setting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Policy Forest
=============================================================================
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base Policy tree
=============================================================================
The values below are required and utilitized by methods in the _SingleTreeExporterMixin
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"since we only allow single dimensional y, we could flatten the prediction"
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Handles the corner case when X=None but featurizer might be not None
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
TODO: support freq_weight and sample_var in debiased lasso
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
Replacing to remove docstring
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE: important to use the rlearner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the rlearner's copy to retain the information from fitting
Handles the corner case when X=None but featurizer might be not None
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `StatsModelsInference`
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support freq_weight and sample_var in debiased lasso
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add blb to parent's options
override only so that we can update the docstring to indicate
support for `GenericSingleTreatmentModelFinalInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
note that groups are not passed to score because they are only used for fitting
note that groups are not passed to score because they are only used for fitting
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
NOTE: important to get parent's wrapped copy so that
"after training wrapped featurizer is also trained, etc."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
Fit a doubly robust average effect
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
"If custom param grid, check that only estimator parameters are being altered"
override only so that we can update the docstring to indicate support for `blb`
Get input names
Summary
Determine output settings
"Important: This must be the first invocation of the random state at fit time, so that"
train/test splits are re-generatable from an external object simply by knowing the
random_state parameter of the tree. Can be useful in the future if one wants to create local
linear predictions. Currently is also useful for testing.
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
Set min_weight_leaf from min_weight_fraction_leaf
Build tree
We calculate the maximum number of samples from each half-split that any node in the tree can
hold. Used by criterion for memory space savings.
Initialize the criterion object and the criterion_val object if honest.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code is a fork from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
Set parameters
Don't instantiate estimators now! Parameters of base_estimator might
"still change. Eg., when grid-searching with the nested object syntax."
self.estimators_ needs to be filled by the derived classes in fit.
Compute the number of jobs
Partition estimators between jobs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=None here since we measure the effect across all Ts
once the estimator has been fit
"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
an intercept even when bias is part of coef.
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
squeeze the first axis
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
d_t=None here since we measure the effect across all Ts
d_t=None here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
NOTE: use np.asarray(offset) becuase if offset is a pd.Series direct addition would make the sum
"a Series as well, which would subsequently break summary_frame because flatten isn't supported"
"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
offset preds
"offset the distribution, too"
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
"if stderr is zero, ppf will return nans and the loop below would never terminate"
so bail out early; note that it might be possible to correct the algorithm for
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
be clean
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
don't proxy special methods
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
second level bootstrap which would be prohibitive computationally?
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
can't import from econml.inference at top level without creating cyclical dependencies
Note that inference results are always methods even if the inference is for a property
(e.g. coef__inference() is a method but coef_ is a property)
Therefore we must insert a lambda if getting inference for a non-callable
"If inference is for a property, create a fresh lambda to avoid passing args through"
"try to get interval/std first if appropriate,"
since we don't prefer a wrapped method with this name
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base GRF tree
=============================================================================
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
=============================================================================
A MultOutputWrapper for GRF classes
=============================================================================
=============================================================================
Instantiations of Generalized Random Forest
=============================================================================
"Append a constant treatment if `fit_intercept=True`, the coefficient"
in front of the constant treatment is the intercept in the moment equation.
"Append a constant treatment and constant instrument if `fit_intercept=True`,"
the coefficient in front of the constant treatment is the intercept in the moment equation.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Base Generalized Random Forest
=============================================================================
TODO: support freq_weight and sample_var
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
We calculate the min eigenvalue proxy that each criterion is considering
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Generating indices a priori before parallelism ended up being orders of magnitude
faster than how sklearn does it. The reason is that random samplers do not release the
gil it seems.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
####################
Variance correction
####################
Subtract the average within bag variance. This ends up being equal to the
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
The negative part is just sq_between.
Objective bayes debiasing for the diagonals where we know a-prior they are positive
"The off diagonals we have no objective prior, so no correction is applied."
Finally correcting the pred_cov or pred_var
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
test that the subsampling scheme past to the trees is correct
The sample size is chosen in particular to test rounding based error when subsampling
test that the estimator calcualtes var correctly
test api
test accuracy
test the projection functionality of forests
test that the estimator calcualtes var correctly
test api
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
test that the subsampling scheme past to the trees is correct
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
omit the lalonde notebook
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
can interpret without uncertainty
can't treat before interpreting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
compare with weight
compare with weight
compare with weight
compare with weight
Multi-dimensional output y
1-d y
compare when both sample_var and sample_weight exist
multi-d y
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that the subsampling scheme past to the trees is correct
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test inference results when `cate_feature_names` doesn not exist
Test inference results when `cate_feature_names` doesn not exist
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
pvalue for second column should be greater than zero since some points are on either side
of the tested value
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
only is not None when T1 is a constant or a list of constant
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Nuisance model has no score method, so nuisance_scores_ should be none"
Test non keyword based calls to fit
test non-array inputs
Test custom splitter
Test incomplete set of test folds
"y scores should be positive, since W predicts Y somewhat"
"t scores might not be, since W and T are uncorrelated"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
creating an instance should warn
using the instance should not warn
using the deprecated method should warn
don't warn if b and c are passed by keyword
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test at least one estimator from each category
test causal graph
test refutation estimate
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
ensure that we've got at least two of every row
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
need to make sure we get all *joint* combinations
IntentToTreat only supports binary treatments/instruments
IntentToTreat only supports binary treatments/instruments
IntentToTreat requires X
ensure we can serialize unfit estimator
these support only W but not X
"these support only binary, not general discrete T and Z"
ensure we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
TODO: add tests for extra properties like coef_ where they exist
TODO: add tests for extra properties like coef_ where they exist
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
TODO: ideally we could also test whether Z and X are jointly okay when both discrete
"however, with custom splits the checking happens in the first stage wrapper"
where we don't have all of the required information to do this;
we'd probably need to add it to _crossfit instead
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
The __warningregistry__'s need to be in a pristine state for tests
to work properly.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Create a wrapper around Lasso that doesn't support weights
since Lasso does natively support them starting in sklearn 0.23
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least 6 of every element
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
NOTE: this number may need to change if the default number of folds in
WeightedStratifiedKFold changes
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
make sure we can get out post-fit stuff
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
predetermined splits ensure that all features are seen in each split
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
create a simple artificial setup where effect of moving from treatment
"a -> b is 2,"
"a -> c is 1, and"
"b -> c is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
should rule out some basic issues.
Note that explicitly specifying the dtype as object is necessary until
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
estimated effects should be identical when treatment is explicitly given
but const_marginal_effect should be reordered based on the explicit cagetories
1-> 2 in original ordering; combination of 3->1 and 3->2
test outer grouping
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure that we can serialize unfit estimator
ensure that we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef_ and intercept_ inference
verify we can generate the summary
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
test outer grouping
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
helper class
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
#########################
WeightedLassoCVWrapper #
#########################
perform 1D fit
perform 2D fit
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
_penalized_inds is only set during fitting
cv exists on penalized model
now we can access _penalized_inds
check that we can read the cv attribute back out from the underlying model
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
Can't handle multi-dimensional treatments
"global shape is (d_y, sum(d_t))"
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
Can't handle multi-dimensional treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
Can't handle multi-dimensional treatments
"global shape is (d_y, sum(d_t))"
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
make sure we don't run into problems dropping every index
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"global shape is (d_y, sum(d_t))"
Can't handle multi-dimensional treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
Can't handle multi-dimensional treatments
dgp
model
model
"columns 'd', 'e', 'h' have too many values"
"columns 'd', 'e' have too many values"
lowering bound shouldn't affect already fit columns when warm starting
"column d is now okay, too"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Define data features
Added `_df`to names to be different from the default cate_estimator names
Generate data
################################
Single treatment and outcome #
################################
Test LinearDML
|--> Test featurizers
ColumnTransformer doesn't propagate column names
|--> Test re-fit
Test SparseLinearDML
Test ForestDML
###################################
Mutiple treatments and outcomes #
###################################
Test LinearDML
Test SparseLinearDML
"Single outcome only, ORF does not support multiple outcomes"
Test DMLOrthoForest
Test DROrthoForest
Test XLearner
Skipping population summary names test because bootstrap inference is too slow
Test SLearner
Test TLearner
Test LinearDRLearner
Test SparseLinearDRLearner
Test ForestDRLearner
Test LinearIntentToTreatDRIV
Test DeepIV
Test categorical treatments
Check refit
Check refit after setting categories
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
Treatment effect function
Outcome support
Treatment support
"Generate controls, covariates, treatments and outcomes"
Heterogeneous treatment effects
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
through shap package.
test shap could generate the plot from the shap_values
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
"Note: unlike other Metalearners, we need the controls' encoded column for training"
"Thus, we append the controls column before the one-hot-encoded T"
"We might want to revisit, though, since it's linearly determined by the others"
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
output is
"* a column of ones if X, W, and Z are all None"
* just X or W or Z if both of the others are None
* hstack([arrs]) for whatever subset are not None otherwise
ensure Z is 2D
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
"instruments, and outcomes"
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"TODO: check that Y, T, Z do not have multiple columns"
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: do correct adjustment for sample_var
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
TODO: allow the final model to actually use X? Then we'd need to rename the class
since we would actually be calculating a CATE rather than ATE.
TODO: allow the final model to actually use X?
TODO: allow the final model to actually use X?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
TODO: would it be useful to extend to handle controls ala vanilla DML?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
make T 2D if if was a vector
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: this utility is documented but internal; reimplement?
TODO: this utility is even less public...
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
but also supports get_feature_names with expected signature
Convert python objects to (possibly nested) types that can easily be represented as literals
named tuple type for storing results inside CausalAnalysis class;
must be lifted to module level to enable pickling
Validate inputs
TODO: check compatibility of X and Y lengths
"no previous fit, cancel warm start"
"work with numeric feature indices, so that we can easily compare with categorical ones"
"if heterogeneity_inds is 1D, repeat it"
heterogeneity inds should be a 2D list of length same as train_inds
replace None elements of heterogeneity_inds and ensure indices are numeric
"TODO: bail out also if categorical columns, classification changed?"
TODO: should we also train a new model_y under any circumstances when warm_start is True?
train the Y model
"perform model selection for the Y model using all X, not on a per-column basis"
"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
work with the regression wrapper
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
"note that this needs to happen after wrapping to generalize to the multi-class case,"
since otherwise we'll have too many columns to be able to train a classifier
start with empty results and default shared insights
convert categorical indicators to numeric indices
check for indices over the categorical expansion bound
"can't remove in place while iterating over new_inds, so store in separate list"
also remove from train_inds so we don't try to access the result later
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
Controls are all other columns of X
"can't use X[:, feat_ind] when X is a DataFrame"
array checking routines don't accept 0-width arrays
perform model selection
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
convert to NormalInferenceResults for consistency
Set the dictionary values shared between local and global summaries
extract subset of names matching new columns
properties to return from effect InferenceResults
properties to return from PopulationSummaryResults
Converts strings to property lookups or method calls as a convenience so that the
_point_props and _summary_props above can be applied to an inference object
Create a summary combining all results into a single output; this is used
by the various causal_effect and causal_effect_dict methods to generate either a dataframe
"or a dictionary, respectively, based on the summary function passed into this method"
"ensure array has shape (m,y,t)"
population summary is missing sample dimension; add it for consistency
outcome dimension is missing; add it for consistency
add singleton treatment dimension if missing
"each attr has dimension (m,y) or (m,y,t)"
concatenate along treatment dimension
"for dictionary representation, want to remove unneeded sample dimension"
in cohort and global results
TODO: enrich outcome logic for multi-class classification when that is supported
can't drop only level
should be serialization-ready and contain no numpy arrays
a global inference indicates the effect of that one feature on the outcome
need to reshape the output to match the input
we want to offset the inference object by the baseline estimate of y
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: conisder working around relying on sklearn implementation details
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
"If classification methods produce multiple columns of output,"
we need to manually encode classes to ensure consistent column ordering.
We clone the estimator to make sure that all the folds are
"independent, and that it is pickle-able."
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
`predictions` is a list of method outputs from each fold.
"If each of those is also a list, then treat this as a"
multioutput-multiclass task. We need to separately concatenate
the method outputs for each label into an `n_labels` long list.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
TODO: consider working around relying on sklearn implementation details
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Call weighted lasso on reduced design matrix
Weighted tau
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set model to WeightedLassoCV by default so there's always a model to get and set attributes on
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
(e.g. former has 'positive' and 'precompute' while latter does not)
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
don't proxy special methods
"don't pass get_params through to model, because that will cause sklearn to clone this"
regressor incorrectly
"Note: for known attributes that have been set this method will not be called,"
so we should just throw here because this is an attribute belonging to this class
but which hasn't yet been set on this instance
set default values for None
check freq_weight should be integer and should be accompanied by sample_var
check array shape
weight X and y and sample_var
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
average the outcome dimension if it exists and ensure 2d y_pred
get index of best treatment
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: consider working around relying on sklearn implementation details
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Convert rows to columns
Require group assignment t to be one-hot-encoded
Get predictions for the 2 splits
Must make sure indices are merged correctly
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Must normalize weights
Override the CATE inference options
Add blb inference to parent's options
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Need to redefine fit here for auto inference to work due to a quirk in how
wrap_fit is defined
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Check that all discrete treatments are represented
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
returns shape-conforming residuals
Copy and/or define models
Define parameter estimators
Define moment and mean gradient estimator
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Call `fit` from parent class
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Expand one-hot encoding to include the zero treatment
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Calculate confidence intervals for the parameter (marginal effect)
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
d_t=None here since we measure the effect across all Ts
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
def mlasso_model(): return MultiTaskLassoCV(
"cv=3, alphas=alpha_regs, max_iter=200)"
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
"alpha_regs = [5e-3, 1e-2, 5e-2]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
subset of features that are exogenous and create heterogeneity
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
subset of features wrt we estimate heterogeneity
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
introspect the constructor arguments to find the model parameters
to represent
"if the argument is deprecated, ignore it"
Extract and sort argument names excluding 'self'
column names
transfer input to numpy arrays
transfer input to 2d arrays
create dataframe
currently dowhy only support single outcome and single treatment
call dowhy
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
cate estimator but not the effect.
don't proxy special methods
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
For when checking input values is disabled
Type to column extraction function
"Get number of arguments, some sklearn featurizer don't accept feature_names"
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
Get feature names using featurizer
All attempts at retrieving transformed feature names have failed
Delegate handling to downstream logic
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
assume that we should perform nested cross-validation if and only if
the model has a 'cv' attribute; this is a somewhat brittle assumption...
logic copied from check_cv
otherwise we will assume the user already set the cv attribute to something
compatible with splitting with a 'groups' argument
now we have to compute the folds explicitly because some classifiers (like LassoCV)
don't use the groups when calling split internally
Normalize weights
This class is mainly derived from statsmodels.iolib.summary.Summary
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
"want to enforce that each bad_arg was either in kwargs,"
or else it was in neither and is just taking its default value
Any access should throw
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
input feature name is already updated by cate_feature_names.
define the index of d_x to filter for each given T
filter X after broadcast with T for each given T
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains some snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
make any access to matplotlib or plt throw an exception
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
Initialize saturation & value; calculate chroma & value shift
Calculate some intermediate values
Initialize RGB with same hue & chroma as our color
Shift the initial RGB values to match value and store
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use mean of targets
Write node mean CATE
Write node std of CATE
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
Fetch appropriate color for node
Write node mean CATE
Write node mean CATE
Write recommended treatment and value - cost
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
This flag is true when names are set in a child class instead
"If names are set in a child class, add an attribute reflecting that"
This works only if X is passed as a kwarg
We plan to enforce X as kwarg only in future releases
This checks if names have been set in a child class
"If names were set in a child class, don't do it again"
"Wraps-up fit by setting attributes, cleaning up, etc."
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"Treatment names is None, default to BaseCateEstimator"
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
Get input names
Summary
add statsmodels to parent's options
add debiasedlasso to parent's options
add blb to parent's options
TODO Share some logic with non-discrete version
Get input names
Summary
add statsmodels to parent's options
add statsmodels to parent's options
add blb to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
remove None arguments
"scores entries should be lists of scores, so make each entry a singleton list"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
generate an instance of the final model
generate an instance of the nuisance model
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
alteration even when we only want to fit_final.
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
"however, sklearn doesn't support both stratifying and grouping (see"
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
their own object that supports grouping if they want to use groups.
for each mc iteration
for each model under cross fit setting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Policy Forest
=============================================================================
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base Policy tree
=============================================================================
The values below are required and utilitized by methods in the _SingleTreeExporterMixin
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"since we only allow single dimensional y, we could flatten the prediction"
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Handles the corner case when X=None but featurizer might be not None
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
TODO: support freq_weight and sample_var in debiased lasso
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
Replacing to remove docstring
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE: important to use the rlearner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the rlearner's copy to retain the information from fitting
Handles the corner case when X=None but featurizer might be not None
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `StatsModelsInference`
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support freq_weight and sample_var in debiased lasso
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add blb to parent's options
override only so that we can update the docstring to indicate
support for `GenericSingleTreatmentModelFinalInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
note that groups are not passed to score because they are only used for fitting
note that groups are not passed to score because they are only used for fitting
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
NOTE: important to get parent's wrapped copy so that
"after training wrapped featurizer is also trained, etc."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
Fit a doubly robust average effect
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
"If custom param grid, check that only estimator parameters are being altered"
override only so that we can update the docstring to indicate support for `blb`
Get input names
Summary
Determine output settings
"Important: This must be the first invocation of the random state at fit time, so that"
train/test splits are re-generatable from an external object simply by knowing the
random_state parameter of the tree. Can be useful in the future if one wants to create local
linear predictions. Currently is also useful for testing.
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
Set min_weight_leaf from min_weight_fraction_leaf
Build tree
We calculate the maximum number of samples from each half-split that any node in the tree can
hold. Used by criterion for memory space savings.
Initialize the criterion object and the criterion_val object if honest.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code is a fork from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
Set parameters
Don't instantiate estimators now! Parameters of base_estimator might
"still change. Eg., when grid-searching with the nested object syntax."
self.estimators_ needs to be filled by the derived classes in fit.
Compute the number of jobs
Partition estimators between jobs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=None here since we measure the effect across all Ts
once the estimator has been fit
"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
an intercept even when bias is part of coef.
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
squeeze the first axis
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
d_t=None here since we measure the effect across all Ts
d_t=None here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
NOTE: use np.asarray(offset) becuase if offset is a pd.Series direct addition would make the sum
"a Series as well, which would subsequently break summary_frame because flatten isn't supported"
"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
offset preds
"offset the distribution, too"
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
"if stderr is zero, ppf will return nans and the loop below would never terminate"
so bail out early; note that it might be possible to correct the algorithm for
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
be clean
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
don't proxy special methods
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
second level bootstrap which would be prohibitive computationally?
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
can't import from econml.inference at top level without creating cyclical dependencies
Note that inference results are always methods even if the inference is for a property
(e.g. coef__inference() is a method but coef_ is a property)
Therefore we must insert a lambda if getting inference for a non-callable
"If inference is for a property, create a fresh lambda to avoid passing args through"
"try to get interval/std first if appropriate,"
since we don't prefer a wrapped method with this name
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base GRF tree
=============================================================================
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
=============================================================================
A MultOutputWrapper for GRF classes
=============================================================================
=============================================================================
Instantiations of Generalized Random Forest
=============================================================================
"Append a constant treatment if `fit_intercept=True`, the coefficient"
in front of the constant treatment is the intercept in the moment equation.
"Append a constant treatment and constant instrument if `fit_intercept=True`,"
the coefficient in front of the constant treatment is the intercept in the moment equation.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Base Generalized Random Forest
=============================================================================
TODO: support freq_weight and sample_var
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
We calculate the min eigenvalue proxy that each criterion is considering
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Generating indices a priori before parallelism ended up being orders of magnitude
faster than how sklearn does it. The reason is that random samplers do not release the
gil it seems.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
####################
Variance correction
####################
Subtract the average within bag variance. This ends up being equal to the
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
The negative part is just sq_between.
Objective bayes debiasing for the diagonals where we know a-prior they are positive
"The off diagonals we have no objective prior, so no correction is applied."
Finally correcting the pred_cov or pred_var
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
test that the subsampling scheme past to the trees is correct
test that the estimator calcualtes var correctly
test api
test accuracy
test the projection functionality of forests
test that the estimator calcualtes var correctly
test api
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
test that the subsampling scheme past to the trees is correct
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
omit the lalonde notebook
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
can interpret without uncertainty
can't treat before interpreting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
compare with weight
compare with weight
compare with weight
compare with weight
Multi-dimensional output y
1-d y
compare when both sample_var and sample_weight exist
multi-d y
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
compare when both sample_var and sample_weight exist
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that the subsampling scheme past to the trees is correct
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test inference results when `cate_feature_names` doesn not exist
Test inference results when `cate_feature_names` doesn not exist
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
pvalue for second column should be greater than zero since some points are on either side
of the tested value
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
only is not None when T1 is a constant or a list of constant
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Nuisance model has no score method, so nuisance_scores_ should be none"
Test non keyword based calls to fit
test non-array inputs
Test custom splitter
Test incomplete set of test folds
"y scores should be positive, since W predicts Y somewhat"
"t scores might not be, since W and T are uncorrelated"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
creating an instance should warn
using the instance should not warn
using the deprecated method should warn
don't warn if b and c are passed by keyword
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test at least one estimator from each category
test causal graph
test refutation estimate
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
ensure that we've got at least two of every row
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
need to make sure we get all *joint* combinations
IntentToTreat only supports binary treatments/instruments
IntentToTreat only supports binary treatments/instruments
IntentToTreat requires X
ensure we can serialize unfit estimator
these support only W but not X
"these support only binary, not general discrete T and Z"
ensure we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
TODO: add tests for extra properties like coef_ where they exist
TODO: add tests for extra properties like coef_ where they exist
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
TODO: ideally we could also test whether Z and X are jointly okay when both discrete
"however, with custom splits the checking happens in the first stage wrapper"
where we don't have all of the required information to do this;
we'd probably need to add it to _crossfit instead
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
The __warningregistry__'s need to be in a pristine state for tests
to work properly.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Create a wrapper around Lasso that doesn't support weights
since Lasso does natively support them starting in sklearn 0.23
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least 6 of every element
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
NOTE: this number may need to change if the default number of folds in
WeightedStratifiedKFold changes
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
make sure we can get out post-fit stuff
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
predetermined splits ensure that all features are seen in each split
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
create a simple artificial setup where effect of moving from treatment
"a -> b is 2,"
"a -> c is 1, and"
"b -> c is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
should rule out some basic issues.
Note that explicitly specifying the dtype as object is necessary until
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
estimated effects should be identical when treatment is explicitly given
but const_marginal_effect should be reordered based on the explicit cagetories
1-> 2 in original ordering; combination of 3->1 and 3->2
test outer grouping
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure that we can serialize unfit estimator
ensure that we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef_ and intercept_ inference
verify we can generate the summary
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
test outer grouping
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
helper class
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
#########################
WeightedLassoCVWrapper #
#########################
perform 1D fit
perform 2D fit
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
_penalized_inds is only set during fitting
cv exists on penalized model
now we can access _penalized_inds
check that we can read the cv attribute back out from the underlying model
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
Can't handle multi-dimensional treatments
"global shape is (d_y, sum(d_t))"
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
Can't handle multi-dimensional treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
Can't handle multi-dimensional treatments
"global shape is (d_y, sum(d_t))"
"ExitStack can be used as a ""do nothing"" ContextManager"
features; for categoricals they should appear #cats-1 times each
make sure we don't run into problems dropping every index
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
"global shape is (d_y, sum(d_t))"
Can't handle multi-dimensional treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
"global and cohort data should have exactly the same structure, but different values"
local index should have as many times entries as global as there were rows passed in
features; for categoricals they should appear #cats-1 times each
"global shape is (d_y, sum(d_t))"
Can't handle multi-dimensional treatments
dgp
model
model
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Define data features
Added `_df`to names to be different from the default cate_estimator names
Generate data
################################
Single treatment and outcome #
################################
Test LinearDML
|--> Test featurizers
ColumnTransformer doesn't propagate column names
|--> Test re-fit
Test SparseLinearDML
Test ForestDML
###################################
Mutiple treatments and outcomes #
###################################
Test LinearDML
Test SparseLinearDML
"Single outcome only, ORF does not support multiple outcomes"
Test DMLOrthoForest
Test DROrthoForest
Test XLearner
Skipping population summary names test because bootstrap inference is too slow
Test SLearner
Test TLearner
Test LinearDRLearner
Test SparseLinearDRLearner
Test ForestDRLearner
Test LinearIntentToTreatDRIV
Test DeepIV
Test categorical treatments
Check refit
Check refit after setting categories
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
Treatment effect function
Outcome support
Treatment support
"Generate controls, covariates, treatments and outcomes"
Heterogeneous treatment effects
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
through shap package.
test shap could generate the plot from the shap_values
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
"Note: unlike other Metalearners, we need the controls' encoded column for training"
"Thus, we append the controls column before the one-hot-encoded T"
"We might want to revisit, though, since it's linearly determined by the others"
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
output is
"* a column of ones if X, W, and Z are all None"
* just X or W or Z if both of the others are None
* hstack([arrs]) for whatever subset are not None otherwise
ensure Z is 2D
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
"instruments, and outcomes"
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"TODO: check that Y, T, Z do not have multiple columns"
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: do correct adjustment for sample_var
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
TODO: allow the final model to actually use X? Then we'd need to rename the class
since we would actually be calculating a CATE rather than ATE.
TODO: allow the final model to actually use X?
TODO: allow the final model to actually use X?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
TODO: would it be useful to extend to handle controls ala vanilla DML?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
make T 2D if if was a vector
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: this utility is documented but internal; reimplement?
TODO: this utility is even less public...
"keys should be mutually exclusive with shared keys, so that the dictionaries can be cleanly merged"
simplification of sklearn's ColumnTransformer that encodes categoricals and passes through selected other columns
but also supports get_feature_names with expected signature
Validate inputs
TODO: check compatibility of X and Y lengths
"no previous fit, cancel warm start"
TODO: implement check for upper bound on categoricals
"work with numeric feature indices, so that we can easily compare with categorical ones"
"if heterogeneity_inds is 1D, repeat it"
heterogeneity inds should be a 2D list of length same as train_inds
replace None elements of heterogeneity_inds and ensure indices are numeric
"TODO: bail out also if categorical columns, classification changed?"
TODO: should we also train a new model_y under any circumstances when warm_start is True?
train the Y model
"perform model selection for the Y model using all X, not on a per-column basis"
"now that we've trained the classifier and wrapped it, ensure that y is transformed to"
work with the regression wrapper
we use column_or_1d to treat pd.Series and pd.DataFrame objects the same way as arrays
"note that this needs to happen after wrapping to generalize to the multi-class case,"
since otherwise we'll have too many columns to be able to train a classifier
start with empty results and default shared insights
convert categorical indicators to numeric indices
Use _ColumnTransformer instead of ColumnTransformer so we can get feature names
Controls are all other columns of X
"can't use X[:, feat_ind] when X is a DataFrame"
array checking routines don't accept 0-width arrays
perform model selection
Prefer ate__inference to const_marginal_ate_inference(X) because it is doubly-robust and not conservative
convert to NormalInferenceResults for consistency
Set the dictionary values shared between local and global summaries
extract subset of names matching new columns
properties to return from effect InferenceResults
properties to return from PopulationSummaryResults
Converts strings to property lookups or method calls as a convenience so that the
_point_props and _summary_props above can be applied to an inference object
Create a summary combining all results into a single output; this is used
by the various causal_effect and causal_effect_dict methods to generate either a dataframe
"or a dictionary, respectively, based on the summary function passed into this method"
"ensure array has shape (m,y,t)"
population summary is missing sample dimension; add it for consistency
outcome dimension is missing; add it for consistency
add singleton treatment dimension if missing
"each attr has dimension (m,y) or (m,y,t)"
concatenate along treatment dimension
"for dictionary representation, want to remove unneeded sample dimension"
in cohort and global results
TODO: enrich outcome logic for multi-class classification when that is supported
can't drop only level
should be serialization-ready and contain no numpy arrays
a global inference indicates the effect of that one feature on the outcome
need to reshape the output to match the input
we want to offset the inference object by the baseline estimate of y
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
TODO: it seems like it would be better to just return the tree itself rather than plot it;
"however, the tree can't store the feature and treatment names we compute here..."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: conisder working around relying on sklearn implementation details
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
"If classification methods produce multiple columns of output,"
we need to manually encode classes to ensure consistent column ordering.
We clone the estimator to make sure that all the folds are
"independent, and that it is pickle-able."
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
`predictions` is a list of method outputs from each fold.
"If each of those is also a list, then treat this as a"
multioutput-multiclass task. We need to separately concatenate
the method outputs for each label into an `n_labels` long list.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
TODO: consider working around relying on sklearn implementation details
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Call weighted lasso on reduced design matrix
Weighted tau
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set model to WeightedLassoCV by default so there's always a model to get and set attributes on
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
(e.g. former has 'positive' and 'precompute' while latter does not)
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
don't proxy special methods
"don't pass get_params through to model, because that will cause sklearn to clone this"
regressor incorrectly
"Note: for known attributes that have been set this method will not be called,"
so we should just throw here because this is an attribute belonging to this class
but which hasn't yet been set on this instance
set default values for None
check freq_weight should be integer and should be accompanied by sample_var
check array shape
weight X and y and sample_var
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
average the outcome dimension if it exists and ensure 2d y_pred
get index of best treatment
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: consider working around relying on sklearn implementation details
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Convert rows to columns
Require group assignment t to be one-hot-encoded
Get predictions for the 2 splits
Must make sure indices are merged correctly
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Must normalize weights
Override the CATE inference options
Add blb inference to parent's options
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Need to redefine fit here for auto inference to work due to a quirk in how
wrap_fit is defined
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Check that all discrete treatments are represented
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
returns shape-conforming residuals
Copy and/or define models
Define parameter estimators
Define moment and mean gradient estimator
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Call `fit` from parent class
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Expand one-hot encoding to include the zero treatment
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Calculate confidence intervals for the parameter (marginal effect)
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
d_t=None here since we measure the effect across all Ts
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
def mlasso_model(): return MultiTaskLassoCV(
"cv=3, alphas=alpha_regs, max_iter=200)"
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
"alpha_regs = [5e-3, 1e-2, 5e-2]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
subset of features that are exogenous and create heterogeneity
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
subset of features wrt we estimate heterogeneity
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
introspect the constructor arguments to find the model parameters
to represent
"if the argument is deprecated, ignore it"
Extract and sort argument names excluding 'self'
column names
transfer input to numpy arrays
transfer input to 2d arrays
create dataframe
currently dowhy only support single outcome and single treatment
call dowhy
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
cate estimator but not the effect.
don't proxy special methods
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
For when checking input values is disabled
Type to column extraction function
"Get number of arguments, some sklearn featurizer don't accept feature_names"
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
Get feature names using featurizer
All attempts at retrieving transformed feature names have failed
Delegate handling to downstream logic
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
assume that we should perform nested cross-validation if and only if
the model has a 'cv' attribute; this is a somewhat brittle assumption...
logic copied from check_cv
otherwise we will assume the user already set the cv attribute to something
compatible with splitting with a 'groups' argument
now we have to compute the folds explicitly because some classifiers (like LassoCV)
don't use the groups when calling split internally
Normalize weights
This class is mainly derived from statsmodels.iolib.summary.Summary
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
"want to enforce that each bad_arg was either in kwargs,"
or else it was in neither and is just taking its default value
Any access should throw
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
input feature name is already updated by cate_feature_names.
define the index of d_x to filter for each given T
filter X after broadcast with T for each given T
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains some snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
make any access to matplotlib or plt throw an exception
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
Initialize saturation & value; calculate chroma & value shift
Calculate some intermediate values
Initialize RGB with same hue & chroma as our color
Shift the initial RGB values to match value and store
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use mean of targets
Write node mean CATE
Write node std of CATE
TODO. Create our own color pallete for multiple treatments. The one below is for binary treatments.
Fetch appropriate color for node
Write node mean CATE
Write node mean CATE
Write recommended treatment and value - cost
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
This flag is true when names are set in a child class instead
"If names are set in a child class, add an attribute reflecting that"
This works only if X is passed as a kwarg
We plan to enforce X as kwarg only in future releases
This checks if names have been set in a child class
"If names were set in a child class, don't do it again"
"Wraps-up fit by setting attributes, cleaning up, etc."
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"Treatment names is None, default to BaseCateEstimator"
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
Get input names
Summary
add statsmodels to parent's options
add debiasedlasso to parent's options
add blb to parent's options
TODO Share some logic with non-discrete version
Get input names
Summary
add statsmodels to parent's options
add statsmodels to parent's options
add blb to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
remove None arguments
"scores entries should be lists of scores, so make each entry a singleton list"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
generate an instance of the final model
generate an instance of the nuisance model
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
alteration even when we only want to fit_final.
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
"however, sklearn doesn't support both stratifying and grouping (see"
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
their own object that supports grouping if they want to use groups.
for each mc iteration
for each model under cross fit setting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Policy Forest
=============================================================================
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base Policy tree
=============================================================================
The values below are required and utilitized by methods in the _SingleTreeExporterMixin
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"since we only allow single dimensional y, we could flatten the prediction"
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Handles the corner case when X=None but featurizer might be not None
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
TODO: support sample_var
Replacing to remove docstring
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE: important to use the rlearner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the rlearner's copy to retain the information from fitting
Handles the corner case when X=None but featurizer might be not None
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `StatsModelsInference`
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support sample_var
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add blb to parent's options
override only so that we can update the docstring to indicate
support for `GenericSingleTreatmentModelFinalInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
note that groups are not passed to score because they are only used for fitting
note that groups are not passed to score because they are only used for fitting
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
NOTE: important to get parent's wrapped copy so that
"after training wrapped featurizer is also trained, etc."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
Fit a doubly robust average effect
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
"If custom param grid, check that only estimator parameters are being altered"
override only so that we can update the docstring to indicate support for `blb`
Get input names
Summary
Determine output settings
"Important: This must be the first invocation of the random state at fit time, so that"
train/test splits are re-generatable from an external object simply by knowing the
random_state parameter of the tree. Can be useful in the future if one wants to create local
linear predictions. Currently is also useful for testing.
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
Set min_weight_leaf from min_weight_fraction_leaf
Build tree
We calculate the maximum number of samples from each half-split that any node in the tree can
hold. Used by criterion for memory space savings.
Initialize the criterion object and the criterion_val object if honest.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code is a fork from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
Set parameters
Don't instantiate estimators now! Parameters of base_estimator might
"still change. Eg., when grid-searching with the nested object syntax."
self.estimators_ needs to be filled by the derived classes in fit.
Compute the number of jobs
Partition estimators between jobs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=None here since we measure the effect across all Ts
once the estimator has been fit
"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
an intercept even when bias is part of coef.
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
squeeze the first axis
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
d_t=None here since we measure the effect across all Ts
d_t=None here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
"if stderr is zero, ppf will return nans and the loop below would never terminate"
so bail out early; note that it might be possible to correct the algorithm for
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
be clean
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
don't proxy special methods
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
second level bootstrap which would be prohibitive computationally?
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
can't import from econml.inference at top level without creating cyclical dependencies
Note that inference results are always methods even if the inference is for a property
(e.g. coef__inference() is a method but coef_ is a property)
Therefore we must insert a lambda if getting inference for a non-callable
"If inference is for a property, create a fresh lambda to avoid passing args through"
"try to get interval/std first if appropriate,"
since we don't prefer a wrapped method with this name
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base GRF tree
=============================================================================
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
=============================================================================
A MultOutputWrapper for GRF classes
=============================================================================
=============================================================================
Instantiations of Generalized Random Forest
=============================================================================
"Append a constant treatment if `fit_intercept=True`, the coefficient"
in front of the constant treatment is the intercept in the moment equation.
"Append a constant treatment and constant instrument if `fit_intercept=True`,"
the coefficient in front of the constant treatment is the intercept in the moment equation.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Base Generalized Random Forest
=============================================================================
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
We calculate the min eigenvalue proxy that each criterion is considering
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Generating indices a priori before parallelism ended up being orders of magnitude
faster than how sklearn does it. The reason is that random samplers do not release the
gil it seems.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
####################
Variance correction
####################
Subtract the average within bag variance. This ends up being equal to the
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
The negative part is just sq_between.
Objective bayes debiasing for the diagonals where we know a-prior they are positive
"The off diagonals we have no objective prior, so no correction is applied."
Finally correcting the pred_cov or pred_var
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
test that the subsampling scheme past to the trees is correct
test that the estimator calcualtes var correctly
test api
test accuracy
test the projection functionality of forests
test that the estimator calcualtes var correctly
test api
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
test that the subsampling scheme past to the trees is correct
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
can interpret without uncertainty
can't treat before interpreting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
Multi-dimensional output y
1-d y
multi-d y
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that the subsampling scheme past to the trees is correct
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test inference results when `cate_feature_names` doesn not exist
Test inference results when `cate_feature_names` doesn not exist
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
pvalue for second column should be greater than zero since some points are on either side
of the tested value
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
only is not None when T1 is a constant or a list of constant
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Nuisance model has no score method, so nuisance_scores_ should be none"
Test non keyword based calls to fit
test non-array inputs
Test custom splitter
Test incomplete set of test folds
"y scores should be positive, since W predicts Y somewhat"
"t scores might not be, since W and T are uncorrelated"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
creating an instance should warn
using the instance should not warn
using the deprecated method should warn
don't warn if b and c are passed by keyword
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test at least one estimator from each category
test causal graph
test refutation estimate
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
ensure that we've got at least two of every row
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
need to make sure we get all *joint* combinations
IntentToTreat only supports binary treatments/instruments
IntentToTreat only supports binary treatments/instruments
IntentToTreat requires X
ensure we can serialize unfit estimator
these support only W but not X
"these support only binary, not general discrete T and Z"
ensure we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
TODO: add tests for extra properties like coef_ where they exist
TODO: add tests for extra properties like coef_ where they exist
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
TODO: ideally we could also test whether Z and X are jointly okay when both discrete
"however, with custom splits the checking happens in the first stage wrapper"
where we don't have all of the required information to do this;
we'd probably need to add it to _crossfit instead
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
The __warningregistry__'s need to be in a pristine state for tests
to work properly.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Create a wrapper around Lasso that doesn't support weights
since Lasso does natively support them starting in sklearn 0.23
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least 6 of every element
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
NOTE: this number may need to change if the default number of folds in
WeightedStratifiedKFold changes
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
predetermined splits ensure that all features are seen in each split
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
create a simple artificial setup where effect of moving from treatment
"a -> b is 2,"
"a -> c is 1, and"
"b -> c is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
should rule out some basic issues.
Note that explicitly specifying the dtype as object is necessary until
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
estimated effects should be identical when treatment is explicitly given
but const_marginal_effect should be reordered based on the explicit cagetories
1-> 2 in original ordering; combination of 3->1 and 3->2
test outer grouping
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure that we can serialize unfit estimator
ensure that we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef_ and intercept_ inference
verify we can generate the summary
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
test outer grouping
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
helper class
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
#########################
WeightedLassoCVWrapper #
#########################
perform 1D fit
perform 2D fit
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
_penalized_inds is only set during fitting
cv exists on penalized model
now we can access _penalized_inds
check that we can read the cv attribute back out from the underlying model
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Define data features
Added `_df`to names to be different from the default cate_estimator names
Generate data
################################
Single treatment and outcome #
################################
Test LinearDML
|--> Test featurizers
ColumnTransformer doesn't propagate column names
|--> Test re-fit
Test SparseLinearDML
Test ForestDML
###################################
Mutiple treatments and outcomes #
###################################
Test LinearDML
Test SparseLinearDML
"Single outcome only, ORF does not support multiple outcomes"
Test DMLOrthoForest
Test DROrthoForest
Test XLearner
Skipping population summary names test because bootstrap inference is too slow
Test SLearner
Test TLearner
Test LinearDRLearner
Test SparseLinearDRLearner
Test ForestDRLearner
Test LinearIntentToTreatDRIV
Test DeepIV
Test categorical treatments
Check refit
Check refit after setting categories
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
Treatment effect function
Outcome support
Treatment support
"Generate controls, covariates, treatments and outcomes"
Heterogeneous treatment effects
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
through shap package.
test shap could generate the plot from the shap_values
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
"Note: unlike other Metalearners, we need the controls' encoded column for training"
"Thus, we append the controls column before the one-hot-encoded T"
"We might want to revisit, though, since it's linearly determined by the others"
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
output is
"* a column of ones if X, W, and Z are all None"
* just X or W or Z if both of the others are None
* hstack([arrs]) for whatever subset are not None otherwise
ensure Z is 2D
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
"instruments, and outcomes"
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"TODO: check that Y, T, Z do not have multiple columns"
override only so that we can update the docstring to indicate support for `StatsModelsInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
TODO: allow the final model to actually use X? Then we'd need to rename the class
since we would actually be calculating a CATE rather than ATE.
TODO: allow the final model to actually use X?
TODO: allow the final model to actually use X?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
TODO: would it be useful to extend to handle controls ala vanilla DML?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
make T 2D if if was a vector
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: conisder working around relying on sklearn implementation details
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
"If classification methods produce multiple columns of output,"
we need to manually encode classes to ensure consistent column ordering.
We clone the estimator to make sure that all the folds are
"independent, and that it is pickle-able."
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
`predictions` is a list of method outputs from each fold.
"If each of those is also a list, then treat this as a"
multioutput-multiclass task. We need to separately concatenate
the method outputs for each label into an `n_labels` long list.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
TODO: consider working around relying on sklearn implementation details
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Call weighted lasso on reduced design matrix
Weighted tau
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set model to WeightedLassoCV by default so there's always a model to get and set attributes on
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
(e.g. former has 'positive' and 'precompute' while latter does not)
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
don't proxy special methods
"don't pass get_params through to model, because that will cause sklearn to clone this"
regressor incorrectly
"Note: for known attributes that have been set this method will not be called,"
so we should just throw here because this is an attribute belonging to this class
but which hasn't yet been set on this instance
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
average the outcome dimension if it exists and ensure 2d y_pred
get index of best treatment
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: consider working around relying on sklearn implementation details
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Convert rows to columns
Require group assignment t to be one-hot-encoded
Get predictions for the 2 splits
Must make sure indices are merged correctly
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Must normalize weights
Override the CATE inference options
Add blb inference to parent's options
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Need to redefine fit here for auto inference to work due to a quirk in how
wrap_fit is defined
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Check that all discrete treatments are represented
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
returns shape-conforming residuals
Copy and/or define models
Define parameter estimators
Define moment and mean gradient estimator
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Call `fit` from parent class
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Expand one-hot encoding to include the zero treatment
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Calculate confidence intervals for the parameter (marginal effect)
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
d_t=None here since we measure the effect across all Ts
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
def mlasso_model(): return MultiTaskLassoCV(
"cv=3, alphas=alpha_regs, max_iter=200)"
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
"alpha_regs = [5e-3, 1e-2, 5e-2]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
subset of features that are exogenous and create heterogeneity
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
subset of features wrt we estimate heterogeneity
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
introspect the constructor arguments to find the model parameters
to represent
Extract and sort argument names excluding 'self'
create dataframe
currently dowhy only support single outcome and single treatment
column names
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
cate estimator but not the effect.
don't proxy special methods
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
For when checking input values is disabled
Type to column extraction function
"Get number of arguments, some sklearn featurizer don't accept feature_names"
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
Get feature names using featurizer
All attempts at retrieving transformed feature names have failed
Delegate handling to downstream logic
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
assume that we should perform nested cross-validation if and only if
the model has a 'cv' attribute; this is a somewhat brittle assumption...
logic copied from check_cv
otherwise we will assume the user already set the cv attribute to something
compatible with splitting with a 'groups' argument
now we have to compute the folds explicitly because some classifiers (like LassoCV)
don't use the groups when calling split internally
Normalize weights
This class is mainly derived from statsmodels.iolib.summary.Summary
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
"want to enforce that each bad_arg was either in kwargs,"
or else it was in neither and is just taking its default value
Any access should throw
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
input feature name is already updated by cate_feature_names.
define the index of d_x to filter for each given T
filter X after broadcast with T for each given T
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
This flag is true when names are set in a child class instead
"If names are set in a child class, add an attribute reflecting that"
This works only if X is passed as a kwarg
We plan to enforce X as kwarg only in future releases
This checks if names have been set in a child class
"If names were set in a child class, don't do it again"
"Wraps-up fit by setting attributes, cleaning up, etc."
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"Treatment names is None, default to BaseCateEstimator"
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
Get input names
Summary
add statsmodels to parent's options
add debiasedlasso to parent's options
add blb to parent's options
TODO Share some logic with non-discrete version
Get input names
Summary
add statsmodels to parent's options
add statsmodels to parent's options
add blb to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
remove None arguments
"scores entries should be lists of scores, so make each entry a singleton list"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
generate an instance of the final model
generate an instance of the nuisance model
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
alteration even when we only want to fit_final.
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
"however, sklearn doesn't support both stratifying and grouping (see"
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
their own object that supports grouping if they want to use groups.
######################################################
These should be removed once `n_splits` is deprecated
######################################################
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"since we only allow single dimensional y, we could flatten the prediction"
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Handles the corner case when X=None but featurizer might be not None
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
TODO: support sample_var
Replacing to remove docstring
###################################################################
Everything below should be removed once parameters are deprecated
###################################################################
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE: important to use the rlearner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the rlearner's copy to retain the information from fitting
Handles the corner case when X=None but featurizer might be not None
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `StatsModelsInference`
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support sample_var
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add blb to parent's options
override only so that we can update the docstring to indicate
support for `GenericSingleTreatmentModelFinalInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
note that groups are not passed to score because they are only used for fitting
note that groups are not passed to score because they are only used for fitting
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
NOTE: important to get parent's wrapped copy so that
"after training wrapped featurizer is also trained, etc."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
Fit a doubly robust average effect
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
"If custom param grid, check that only estimator parameters are being altered"
override only so that we can update the docstring to indicate support for `blb`
Get input names
Summary
######################################################
These should be removed once `n_splits` is deprecated
######################################################
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=None here since we measure the effect across all Ts
once the estimator has been fit
"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
an intercept even when bias is part of coef.
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
squeeze the first axis
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
d_t=None here since we measure the effect across all Ts
d_t=None here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
"if stderr is zero, ppf will return nans and the loop below would never terminate"
so bail out early; note that it might be possible to correct the algorithm for
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
be clean
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
don't proxy special methods
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
second level bootstrap which would be prohibitive computationally?
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
can't import from econml.inference at top level without creating cyclical dependencies
Note that inference results are always methods even if the inference is for a property
(e.g. coef__inference() is a method but coef_ is a property)
Therefore we must insert a lambda if getting inference for a non-callable
"If inference is for a property, create a fresh lambda to avoid passing args through"
"try to get interval/std first if appropriate,"
since we don't prefer a wrapped method with this name
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code is a fork from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
Set parameters
Don't instantiate estimators now! Parameters of base_estimator might
"still change. Eg., when grid-searching with the nested object syntax."
self.estimators_ needs to be filled by the derived classes in fit.
Compute the number of jobs
Partition estimators between jobs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base GRF tree
=============================================================================
Determine output settings
"Important: This must be the first invocation of the random state at fit time, so that"
train/test splits are re-generatable from an external object simply by knowing the
random_state parameter of the tree. Can be useful in the future if one wants to create local
linear predictions. Currently is also useful for testing.
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
Set min_weight_leaf from min_weight_fraction_leaf
Build tree
We calculate the maximum number of samples from each half-split that any node in the tree can
hold. Used by criterion for memory space savings.
Initialize the criterion object and the criterion_val object if honest.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
=============================================================================
A MultOutputWrapper for GRF classes
=============================================================================
=============================================================================
Instantiations of Generalized Random Forest
=============================================================================
"Append a constant treatment if `fit_intercept=True`, the coefficient"
in front of the constant treatment is the intercept in the moment equation.
"Append a constant treatment and constant instrument if `fit_intercept=True`,"
the coefficient in front of the constant treatment is the intercept in the moment equation.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Base Generalized Random Forest
=============================================================================
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
We calculate the min eigenvalue proxy that each criterion is considering
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Generating indices a priori before parallelism ended up being orders of magnitude
faster than how sklearn does it. The reason is that random samplers do not release the
gil it seems.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
####################
Variance correction
####################
Subtract the average within bag variance. This ends up being equal to the
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
The negative part is just sq_between.
Objective bayes debiasing for the diagonals where we know a-prior they are positive
"The off diagonals we have no objective prior, so no correction is applied."
Finally correcting the pred_cov or pred_var
avoid storing the output of every estimator by summing them here
Parallel loop
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
test that the subsampling scheme past to the trees is correct
test that the estimator calcualtes var correctly
test api
test accuracy
test the projection functionality of forests
test that the estimator calcualtes var correctly
test api
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
test that the subsampling scheme past to the trees is correct
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
can interpret without uncertainty
can't treat before interpreting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
Multi-dimensional output y
1-d y
multi-d y
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test inference results when `cate_feature_names` doesn not exist
Test inference results when `cate_feature_names` doesn not exist
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
pvalue for second column should be greater than zero since some points are on either side
of the tested value
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
only is not None when T1 is a constant or a list of constant
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Nuisance model has no score method, so nuisance_scores_ should be none"
Test non keyword based calls to fit
test non-array inputs
Test custom splitter
Test incomplete set of test folds
"y scores should be positive, since W predicts Y somewhat"
"t scores might not be, since W and T are uncorrelated"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
creating an instance should warn
using the instance should not warn
using the deprecated method should warn
don't warn if b and c are passed by keyword
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test at least one estimator from each category
test causal graph
test refutation estimate
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
ensure that we've got at least two of every row
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
need to make sure we get all *joint* combinations
IntentToTreat only supports binary treatments/instruments
IntentToTreat only supports binary treatments/instruments
IntentToTreat requires X
ensure we can serialize unfit estimator
these support only W but not X
"these support only binary, not general discrete T and Z"
ensure we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
TODO: add tests for extra properties like coef_ where they exist
TODO: add tests for extra properties like coef_ where they exist
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
TODO: ideally we could also test whether Z and X are jointly okay when both discrete
"however, with custom splits the checking happens in the first stage wrapper"
where we don't have all of the required information to do this;
we'd probably need to add it to _crossfit instead
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
The __warningregistry__'s need to be in a pristine state for tests
to work properly.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Create a wrapper around Lasso that doesn't support weights
since Lasso does natively support them starting in sklearn 0.23
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least 6 of every element
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
NOTE: this number may need to change if the default number of folds in
WeightedStratifiedKFold changes
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
predetermined splits ensure that all features are seen in each split
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
create a simple artificial setup where effect of moving from treatment
"a -> b is 2,"
"a -> c is 1, and"
"b -> c is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
should rule out some basic issues.
Note that explicitly specifying the dtype as object is necessary until
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
estimated effects should be identical when treatment is explicitly given
but const_marginal_effect should be reordered based on the explicit cagetories
1-> 2 in original ordering; combination of 3->1 and 3->2
test outer grouping
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure that we can serialize unfit estimator
ensure that we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef_ and intercept_ inference
verify we can generate the summary
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
test outer grouping
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
helper class
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
#########################
WeightedLassoCVWrapper #
#########################
perform 1D fit
perform 2D fit
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
_penalized_inds is only set during fitting
cv exists on penalized model
now we can access _penalized_inds
check that we can read the cv attribute back out from the underlying model
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Define data features
Added `_df`to names to be different from the default cate_estimator names
Generate data
################################
Single treatment and outcome #
################################
Test LinearDML
|--> Test featurizers
ColumnTransformer doesn't propagate column names
|--> Test re-fit
Test SparseLinearDML
Test ForestDML
###################################
Mutiple treatments and outcomes #
###################################
Test LinearDML
Test SparseLinearDML
"Single outcome only, ORF does not support multiple outcomes"
Test DMLOrthoForest
Test DROrthoForest
Test XLearner
Skipping population summary names test because bootstrap inference is too slow
Test SLearner
Test TLearner
Test LinearDRLearner
Test SparseLinearDRLearner
Test ForestDRLearner
Test LinearIntentToTreatDRIV
Test DeepIV
Test categorical treatments
Check refit
Check refit after setting categories
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
Treatment effect function
Outcome support
Treatment support
"Generate controls, covariates, treatments and outcomes"
Heterogeneous treatment effects
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
through shap package.
test shap could generate the plot from the shap_values
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
"Note: unlike other Metalearners, we need the controls' encoded column for training"
"Thus, we append the controls column before the one-hot-encoded T"
"We might want to revisit, though, since it's linearly determined by the others"
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
output is
"* a column of ones if X, W, and Z are all None"
* just X or W or Z if both of the others are None
* hstack([arrs]) for whatever subset are not None otherwise
ensure Z is 2D
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
"instruments, and outcomes"
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"TODO: check that Y, T, Z do not have multiple columns"
override only so that we can update the docstring to indicate support for `StatsModelsInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
TODO: allow the final model to actually use X? Then we'd need to rename the class
since we would actually be calculating a CATE rather than ATE.
TODO: allow the final model to actually use X?
TODO: allow the final model to actually use X?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
TODO: would it be useful to extend to handle controls ala vanilla DML?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
make T 2D if if was a vector
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: conisder working around relying on sklearn implementation details
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
"If classification methods produce multiple columns of output,"
we need to manually encode classes to ensure consistent column ordering.
We clone the estimator to make sure that all the folds are
"independent, and that it is pickle-able."
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
`predictions` is a list of method outputs from each fold.
"If each of those is also a list, then treat this as a"
multioutput-multiclass task. We need to separately concatenate
the method outputs for each label into an `n_labels` long list.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
TODO: consider working around relying on sklearn implementation details
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Call weighted lasso on reduced design matrix
Weighted tau
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set model to WeightedLassoCV by default so there's always a model to get and set attributes on
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
(e.g. former has 'positive' and 'precompute' while latter does not)
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
don't proxy special methods
"don't pass get_params through to model, because that will cause sklearn to clone this"
regressor incorrectly
"Note: for known attributes that have been set this method will not be called,"
so we should just throw here because this is an attribute belonging to this class
but which hasn't yet been set on this instance
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: generalize to multiple treatment case?
get index of best treatment
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make any access to matplotlib or plt throw an exception
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use first target
Write node mean CATE
Write node std of CATE
Fetch appropriate color for node
"red for negative, green for positive"
Write node mean CATE
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: consider working around relying on sklearn implementation details
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Convert rows to columns
Require group assignment t to be one-hot-encoded
Get predictions for the 2 splits
Must make sure indices are merged correctly
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Must normalize weights
Override the CATE inference options
Add blb inference to parent's options
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Need to redefine fit here for auto inference to work due to a quirk in how
wrap_fit is defined
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Check that all discrete treatments are represented
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
returns shape-conforming residuals
Copy and/or define models
Define parameter estimators
Define moment and mean gradient estimator
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Call `fit` from parent class
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Expand one-hot encoding to include the zero treatment
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Calculate confidence intervals for the parameter (marginal effect)
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
d_t=None here since we measure the effect across all Ts
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
def mlasso_model(): return MultiTaskLassoCV(
"cv=3, alphas=alpha_regs, max_iter=200)"
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
"alpha_regs = [5e-3, 1e-2, 5e-2]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
subset of features that are exogenous and create heterogeneity
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
subset of features wrt we estimate heterogeneity
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
introspect the constructor arguments to find the model parameters
to represent
Extract and sort argument names excluding 'self'
create dataframe
currently dowhy only support single outcome and single treatment
column names
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
cate estimator but not the effect.
don't proxy special methods
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
For when checking input values is disabled
Type to column extraction function
"Get number of arguments, some sklearn featurizer don't accept feature_names"
Featurizer doesn't have 'get_feature_names' or has atypical 'get_feature_names'
Get feature names using featurizer
All attempts at retrieving transformed feature names have failed
Delegate handling to downstream logic
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
assume that we should perform nested cross-validation if and only if
the model has a 'cv' attribute; this is a somewhat brittle assumption...
logic copied from check_cv
otherwise we will assume the user already set the cv attribute to something
compatible with splitting with a 'groups' argument
now we have to compute the folds explicitly because some classifiers (like LassoCV)
don't use the groups when calling split internally
Normalize weights
This class is mainly derived from statsmodels.iolib.summary.Summary
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
"want to enforce that each bad_arg was either in kwargs,"
or else it was in neither and is just taking its default value
Any access should throw
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
input feature name is already updated by cate_feature_names.
define the index of d_x to filter for each given T
filter X after broadcast with T for each given T
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
This flag is true when names are set in a child class instead
"If names are set in a child class, add an attribute reflecting that"
This works only if X is passed as a kwarg
We plan to enforce X as kwarg only in future releases
This checks if names have been set in a child class
"If names were set in a child class, don't do it again"
"Wraps-up fit by setting attributes, cleaning up, etc."
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"Treatment names is None, default to BaseCateEstimator"
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
Get input names
Summary
add statsmodels to parent's options
add debiasedlasso to parent's options
add blb to parent's options
TODO Share some logic with non-discrete version
Get input names
Summary
add statsmodels to parent's options
add statsmodels to parent's options
add blb to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
remove None arguments
"scores entries should be lists of scores, so make each entry a singleton list"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
generate an instance of the final model
generate an instance of the nuisance model
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
alteration even when we only want to fit_final.
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
"however, sklearn doesn't support both stratifying and grouping (see"
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
their own object that supports grouping if they want to use groups.
######################################################
These should be removed once `n_splits` is deprecated
######################################################
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"since we only allow single dimensional y, we could flatten the prediction"
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Handles the corner case when X=None but featurizer might be not None
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
TODO: support sample_var
Replacing to remove docstring
###################################################################
Everything below should be removed once parameters are deprecated
###################################################################
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE: important to use the rlearner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the rlearner's copy to retain the information from fitting
Handles the corner case when X=None but featurizer might be not None
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `StatsModelsInference`
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support sample_var
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add blb to parent's options
override only so that we can update the docstring to indicate
support for `GenericSingleTreatmentModelFinalInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
note that groups are not passed to score because they are only used for fitting
note that groups are not passed to score because they are only used for fitting
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
NOTE: important to get parent's wrapped copy so that
"after training wrapped featurizer is also trained, etc."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `blb`
######################################################
These should be removed once `n_splits` is deprecated
######################################################
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=None here since we measure the effect across all Ts
once the estimator has been fit
"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
an intercept even when bias is part of coef.
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
squeeze the first axis
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=None here since we measure the effect across all Ts
set the mean_pred_stderr
replace the mean_pred_stderr if T1 and T0 is a constant or a constant of vector
d_t=None here since we measure the effect across all Ts
d_t=None here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
"if stderr is zero, ppf will return nans and the loop below would never terminate"
so bail out early; note that it might be possible to correct the algorithm for
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
be clean
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
don't proxy special methods
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
second level bootstrap which would be prohibitive computationally?
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
can't import from econml.inference at top level without creating cyclical dependencies
Note that inference results are always methods even if the inference is for a property
(e.g. coef__inference() is a method but coef_ is a property)
Therefore we must insert a lambda if getting inference for a non-callable
"If inference is for a property, create a fresh lambda to avoid passing args through"
"try to get interval/std first if appropriate,"
since we don't prefer a wrapped method with this name
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code is a fork from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
Set parameters
Don't instantiate estimators now! Parameters of base_estimator might
"still change. Eg., when grid-searching with the nested object syntax."
self.estimators_ needs to be filled by the derived classes in fit.
Compute the number of jobs
Partition estimators between jobs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base GRF tree
=============================================================================
Determine output settings
"Important: This must be the first invocation of the random state at fit time, so that"
train/test splits are re-generatable from an external object simply by knowing the
random_state parameter of the tree. Can be useful in the future if one wants to create local
linear predictions. Currently is also useful for testing.
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
Set min_weight_leaf from min_weight_fraction_leaf
Build tree
We calculate the maximum number of samples from each half-split that any node in the tree can
hold. Used by criterion for memory space savings.
Initialize the criterion object and the criterion_val object if honest.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
=============================================================================
A MultOutputWrapper for GRF classes
=============================================================================
=============================================================================
Instantiations of Generalized Random Forest
=============================================================================
"Append a constant treatment if `fit_intercept=True`, the coefficient"
in front of the constant treatment is the intercept in the moment equation.
"Append a constant treatment and constant instrument if `fit_intercept=True`,"
the coefficient in front of the constant treatment is the intercept in the moment equation.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Base Generalized Random Forest
=============================================================================
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
We calculate the min eigenvalue proxy that each criterion is considering
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Generating indices a priori before parallelism ended up being orders of magnitude
faster than how sklearn does it. The reason is that random samplers do not release the
gil it seems.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
####################
Variance correction
####################
Subtract the average within bag variance. This ends up being equal to the
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
The negative part is just sq_between.
Objective bayes debiasing for the diagonals where we know a-prior they are positive
"The off diagonals we have no objective prior, so no correction is applied."
Finally correcting the pred_cov or pred_var
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
test that the subsampling scheme past to the trees is correct
test that the estimator calcualtes var correctly
test api
test accuracy
test the projection functionality of forests
test that the estimator calcualtes var correctly
test api
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
test that the subsampling scheme past to the trees is correct
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
can interpret without uncertainty
can't treat before interpreting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
Multi-dimensional output y
1-d y
multi-d y
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test inference results when `cate_feature_names` doesn not exist
Test inference results when `cate_feature_names` doesn not exist
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
pvalue for second column should be greater than zero since some points are on either side
of the tested value
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
only is not None when T1 is a constant or a list of constant
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Nuisance model has no score method, so nuisance_scores_ should be none"
Test non keyword based calls to fit
test non-array inputs
Test custom splitter
Test incomplete set of test folds
"y scores should be positive, since W predicts Y somewhat"
"t scores might not be, since W and T are uncorrelated"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
creating an instance should warn
using the instance should not warn
using the deprecated method should warn
don't warn if b and c are passed by keyword
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test at least one estimator from each category
test causal graph
test refutation estimate
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
ensure that we've got at least two of every row
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
need to make sure we get all *joint* combinations
IntentToTreat only supports binary treatments/instruments
IntentToTreat only supports binary treatments/instruments
IntentToTreat requires X
ensure we can serialize unfit estimator
these support only W but not X
"these support only binary, not general discrete T and Z"
ensure we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
TODO: add tests for extra properties like coef_ where they exist
TODO: add tests for extra properties like coef_ where they exist
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
TODO: ideally we could also test whether Z and X are jointly okay when both discrete
"however, with custom splits the checking happens in the first stage wrapper"
where we don't have all of the required information to do this;
we'd probably need to add it to _crossfit instead
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
The __warningregistry__'s need to be in a pristine state for tests
to work properly.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Create a wrapper around Lasso that doesn't support weights
since Lasso does natively support them starting in sklearn 0.23
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least 6 of every element
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
NOTE: this number may need to change if the default number of folds in
WeightedStratifiedKFold changes
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
predetermined splits ensure that all features are seen in each split
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
create a simple artificial setup where effect of moving from treatment
"a -> b is 2,"
"a -> c is 1, and"
"b -> c is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
should rule out some basic issues.
Note that explicitly specifying the dtype as object is necessary until
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
estimated effects should be identical when treatment is explicitly given
but const_marginal_effect should be reordered based on the explicit cagetories
1-> 2 in original ordering; combination of 3->1 and 3->2
test outer grouping
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure that we can serialize unfit estimator
ensure that we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef_ and intercept_ inference
verify we can generate the summary
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
test outer grouping
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
helper class
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
#########################
WeightedLassoCVWrapper #
#########################
perform 1D fit
perform 2D fit
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
_penalized_inds is only set during fitting
cv exists on penalized model
now we can access _penalized_inds
check that we can read the cv attribute back out from the underlying model
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Define data features
Added `_df`to names to be different from the default cate_estimator names
Generate data
################################
Single treatment and outcome #
################################
Test LinearDML
|--> Test featurizers
ColumnTransformer doesn't propagate column names
|--> Test re-fit
Test SparseLinearDML
Test ForestDML
###################################
Mutiple treatments and outcomes #
###################################
Test LinearDML
Test SparseLinearDML
"Single outcome only, ORF does not support multiple outcomes"
Test DMLOrthoForest
Test DROrthoForest
Test XLearner
Skipping population summary names test because bootstrap inference is too slow
Test SLearner
Test TLearner
Test LinearDRLearner
Test SparseLinearDRLearner
Test ForestDRLearner
Test LinearIntentToTreatDRIV
Test DeepIV
Test categorical treatments
Check refit
Check refit after setting categories
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test length of feature names equals to shap values shape
Treatment effect function
Outcome support
Treatment support
"Generate controls, covariates, treatments and outcomes"
Heterogeneous treatment effects
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
through shap package.
test shap could generate the plot from the shap_values
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
"Note: unlike other Metalearners, we need the controls' encoded column for training"
"Thus, we append the controls column before the one-hot-encoded T"
"We might want to revisit, though, since it's linearly determined by the others"
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
output is
"* a column of ones if X, W, and Z are all None"
* just X or W or Z if both of the others are None
* hstack([arrs]) for whatever subset are not None otherwise
ensure Z is 2D
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
"instruments, and outcomes"
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"TODO: check that Y, T, Z do not have multiple columns"
override only so that we can update the docstring to indicate support for `StatsModelsInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
TODO: allow the final model to actually use X? Then we'd need to rename the class
since we would actually be calculating a CATE rather than ATE.
TODO: allow the final model to actually use X?
TODO: allow the final model to actually use X?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
TODO: would it be useful to extend to handle controls ala vanilla DML?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
make T 2D if if was a vector
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: conisder working around relying on sklearn implementation details
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
"If classification methods produce multiple columns of output,"
we need to manually encode classes to ensure consistent column ordering.
We clone the estimator to make sure that all the folds are
"independent, and that it is pickle-able."
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
`predictions` is a list of method outputs from each fold.
"If each of those is also a list, then treat this as a"
multioutput-multiclass task. We need to separately concatenate
the method outputs for each label into an `n_labels` long list.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
TODO: consider working around relying on sklearn implementation details
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Call weighted lasso on reduced design matrix
Weighted tau
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set model to WeightedLassoCV by default so there's always a model to get and set attributes on
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
(e.g. former has 'positive' and 'precompute' while latter does not)
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
don't proxy special methods
"don't pass get_params through to model, because that will cause sklearn to clone this"
regressor incorrectly
"Note: for known attributes that have been set this method will not be called,"
so we should just throw here because this is an attribute belonging to this class
but which hasn't yet been set on this instance
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: generalize to multiple treatment case?
get index of best treatment
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make any access to matplotlib or plt throw an exception
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use first target
Write node mean CATE
Write node std of CATE
Fetch appropriate color for node
"red for negative, green for positive"
Write node mean CATE
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: consider working around relying on sklearn implementation details
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Convert rows to columns
Require group assignment t to be one-hot-encoded
Get predictions for the 2 splits
Must make sure indices are merged correctly
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Must normalize weights
Override the CATE inference options
Add blb inference to parent's options
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Need to redefine fit here for auto inference to work due to a quirk in how
wrap_fit is defined
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Check that all discrete treatments are represented
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
returns shape-conforming residuals
Copy and/or define models
Define parameter estimators
Define moment and mean gradient estimator
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Call `fit` from parent class
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Expand one-hot encoding to include the zero treatment
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Calculate confidence intervals for the parameter (marginal effect)
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
d_t=None here since we measure the effect across all Ts
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
"If both a .pyx and a .c file exist, we assume the .c file is up to date and don't force a recompile"
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
"alpha_regs = [5e-3, 1e-2, 5e-2, 1e-1]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs, max_iter=200)"
def mlasso_model(): return MultiTaskLassoCV(
"cv=3, alphas=alpha_regs, max_iter=200)"
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
The first s_x state variables are confounders. The final s_x variables are exogenous and can create
heterogeneity
"alpha_regs = [5e-3, 1e-2, 5e-2]"
"def lasso_model(): return LassoCV(cv=3, alphas=alpha_regs)"
"def mlasso_model(): return MultiTaskLassoCV(cv=3, alphas=alpha_regs)"
subset of features that are exogenous and create heterogeneity
strength of heterogeneity wrt the exogenous variables (assumed to be the last s_x features)
subset of features wrt we estimate heterogeneity
"Calculating the (kappa, kappa) block entry (of size n_treatments x n_treatments) of matrix Sigma"
"Calculating the (kappa, tau) block entry (of size n_treatments x n_treatments) of matrix M"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
introspect the constructor arguments to find the model parameters
to represent
Extract and sort argument names excluding 'self'
create dataframe
currently dowhy only support single outcome and single treatment
column names
"We don't allow user to call refit_final from this class, since internally dowhy effect estimate will only update"
cate estimator but not the effect.
don't proxy special methods
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
Type to column extraction function
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
assume that we should perform nested cross-validation if and only if
the model has a 'cv' attribute; this is a somewhat brittle assumption...
logic copied from check_cv
otherwise we will assume the user already set the cv attribute to something
compatible with splitting with a 'groups' argument
now we have to compute the folds explicitly because some classifiers (like LassoCV)
don't use the groups when calling split internally
Normalize weights
This class is mainly derived from statsmodels.iolib.summary.Summary
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
"want to enforce that each bad_arg was either in kwargs,"
or else it was in neither and is just taking its default value
Any access should throw
"As a convenience, also throw on calls to allow MissingModule to be used in lieu of specific imports"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
define the index of d_x to filter for each given T
filter X after broadcast with T for each given T
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
This flag is true when names are set in a child class instead
"If names are set in a child class, add an attribute reflecting that"
This works only if X is passed as a kwarg
We plan to enforce X as kwarg only in future releases
This checks if names have been set in a child class
"If names were set in a child class, don't do it again"
"Wraps-up fit by setting attributes, cleaning up, etc."
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"Treatment names is None, default to BaseCateEstimator"
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
Get input names
Summary
add statsmodels to parent's options
add debiasedlasso to parent's options
add blb to parent's options
TODO Share some logic with non-discrete version
Get input names
Summary
add statsmodels to parent's options
add statsmodels to parent's options
add blb to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
remove None arguments
"scores entries should be lists of scores, so make each entry a singleton list"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
generate an instance of the final model
generate an instance of the nuisance model
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
alteration even when we only want to fit_final.
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
"however, sklearn doesn't support both stratifying and grouping (see"
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
their own object that supports grouping if they want to use groups.
######################################################
These should be removed once `n_splits` is deprecated
######################################################
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Handles the corner case when X=None but featurizer might be not None
"This fails if X=None and featurizer is not None, but that case is handled above"
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
TODO: support sample_var
Replacing to remove docstring
###################################################################
Everything below should be removed once parameters are deprecated
###################################################################
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE: important to use the rlearner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the rlearner's copy to retain the information from fitting
Handles the corner case when X=None but featurizer might be not None
"This fails if X=None and featurizer is not None, but that case is handled above"
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `StatsModelsInference`
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support sample_var
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add blb to parent's options
override only so that we can update the docstring to indicate
support for `GenericSingleTreatmentModelFinalInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
note that groups are not passed to score because they are only used for fitting
note that groups are not passed to score because they are only used for fitting
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
NOTE: important to get parent's wrapped copy so that
"after training wrapped featurizer is also trained, etc."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `blb`
######################################################
These should be removed once `n_splits` is deprecated
######################################################
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=None here since we measure the effect across all Ts
once the estimator has been fit
"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
an intercept even when bias is part of coef.
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
d_t=None here since we measure the effect across all Ts
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=None here since we measure the effect across all Ts
d_t=None here since we measure the effect across all Ts
d_t=None here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
"get the length of X when it's effect, or length of coefficient/intercept when it's coefficient/intercpet"
to_include['point_estimate'] is a flatten vector with length d_t*d_y*nx
"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
"For effect summaries, d_t is None, but the result arrays behave as if d_t=1"
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
"if stderr is zero, ppf will return nans and the loop below would never terminate"
so bail out early; note that it might be possible to correct the algorithm for
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
be clean
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
don't proxy special methods
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
second level bootstrap which would be prohibitive computationally?
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
can't import from econml.inference at top level without creating cyclical dependencies
Note that inference results are always methods even if the inference is for a property
(e.g. coef__inference() is a method but coef_ is a property)
Therefore we must insert a lambda if getting inference for a non-callable
"If inference is for a property, create a fresh lambda to avoid passing args through"
"try to get interval/std first if appropriate,"
since we don't prefer a wrapped method with this name
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code is a fork from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
Set parameters
Don't instantiate estimators now! Parameters of base_estimator might
"still change. Eg., when grid-searching with the nested object syntax."
self.estimators_ needs to be filled by the derived classes in fit.
Compute the number of jobs
Partition estimators between jobs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base GRF tree
=============================================================================
Determine output settings
"Important: This must be the first invocation of the random state at fit time, so that"
train/test splits are re-generatable from an external object simply by knowing the
random_state parameter of the tree. Can be useful in the future if one wants to create local
linear predictions. Currently is also useful for testing.
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
Set min_weight_leaf from min_weight_fraction_leaf
Build tree
We calculate the maximum number of samples from each half-split that any node in the tree can
hold. Used by criterion for memory space savings.
Initialize the criterion object and the criterion_val object if honest.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
=============================================================================
A MultOutputWrapper for GRF classes
=============================================================================
=============================================================================
Instantiations of Generalized Random Forest
=============================================================================
"Append a constant treatment if `fit_intercept=True`, the coefficient"
in front of the constant treatment is the intercept in the moment equation.
"Append a constant treatment and constant instrument if `fit_intercept=True`,"
the coefficient in front of the constant treatment is the intercept in the moment equation.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Base Generalized Random Forest
=============================================================================
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
We calculate the min eigenvalue proxy that each criterion is considering
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Generating indices a priori before parallelism ended up being orders of magnitude
faster than how sklearn does it. The reason is that random samplers do not release the
gil it seems.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
####################
Variance correction
####################
Subtract the average within bag variance. This ends up being equal to the
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
The negative part is just sq_between.
Objective bayes debiasing for the diagonals where we know a-prior they are positive
"The off diagonals we have no objective prior, so no correction is applied."
Finally correcting the pred_cov or pred_var
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
test that the subsampling scheme past to the trees is correct
test that the estimator calcualtes var correctly
test api
test accuracy
test the projection functionality of forests
test that the estimator calcualtes var correctly
test api
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
test that the subsampling scheme past to the trees is correct
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
can interpret without uncertainty
can't treat before interpreting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
Multi-dimensional output y
1-d y
multi-d y
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test inference results when `cate_feature_names` doesn not exist
Test inference results when `cate_feature_names` doesn not exist
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
pvalue for second column should be greater than zero since some points are on either side
of the tested value
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Nuisance model has no score method, so nuisance_scores_ should be none"
Test non keyword based calls to fit
test non-array inputs
Test custom splitter
Test incomplete set of test folds
"y scores should be positive, since W predicts Y somewhat"
"t scores might not be, since W and T are uncorrelated"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
creating an instance should warn
using the instance should not warn
using the deprecated method should warn
don't warn if b and c are passed by keyword
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test at least one estimator from each category
test causal graph
test refutation estimate
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
ensure that we've got at least two of every row
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
need to make sure we get all *joint* combinations
IntentToTreat only supports binary treatments/instruments
IntentToTreat only supports binary treatments/instruments
IntentToTreat requires X
ensure we can serialize unfit estimator
these support only W but not X
"these support only binary, not general discrete T and Z"
ensure we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
TODO: add tests for extra properties like coef_ where they exist
TODO: add tests for extra properties like coef_ where they exist
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
TODO: ideally we could also test whether Z and X are jointly okay when both discrete
"however, with custom splits the checking happens in the first stage wrapper"
where we don't have all of the required information to do this;
we'd probably need to add it to _crossfit instead
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
The __warningregistry__'s need to be in a pristine state for tests
to work properly.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Create a wrapper around Lasso that doesn't support weights
since Lasso does natively support them starting in sklearn 0.23
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least 6 of every element
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
NOTE: this number may need to change if the default number of folds in
WeightedStratifiedKFold changes
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
predetermined splits ensure that all features are seen in each split
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
create a simple artificial setup where effect of moving from treatment
"a -> b is 2,"
"a -> c is 1, and"
"b -> c is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
should rule out some basic issues.
Note that explicitly specifying the dtype as object is necessary until
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
estimated effects should be identical when treatment is explicitly given
but const_marginal_effect should be reordered based on the explicit cagetories
1-> 2 in original ordering; combination of 3->1 and 3->2
test outer grouping
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure that we can serialize unfit estimator
ensure that we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef_ and intercept_ inference
verify we can generate the summary
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
test outer grouping
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
#########################
WeightedLassoCVWrapper #
#########################
perform 1D fit
perform 2D fit
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
_penalized_inds is only set during fitting
cv exists on penalized model
now we can access _penalized_inds
check that we can read the cv attribute back out from the underlying model
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Define data features
Added `_df`to names to be different from the default cate_estimator names
Generate data
################################
Single treatment and outcome #
################################
Test LinearDML
Test re-fit
Test SparseLinearDML
ForestDML
###################################
Mutiple treatments and outcomes #
###################################
Test LinearDML
Test SparseLinearDML
"Single outcome only, ORF does not support multiple outcomes"
Test DMLOrthoForest
Test DROrthoForest
Test XLearner
Skipping population summary names test because bootstrap inference is too slow
Test SLearner
Test TLearner
Test LinearDRLearner
Test SparseLinearDRLearner
Test ForestDRLearner
Test LinearIntentToTreatDRIV
Test DeepIV
Test categorical treatments
Check refit
Check refit after setting categories
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
Treatment effect function
Outcome support
Treatment support
"Generate controls, covariates, treatments and outcomes"
Heterogeneous treatment effects
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
through shap package.
test shap could generate the plot from the shap_values
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
"Note: unlike other Metalearners, we need the controls' encoded column for training"
"Thus, we append the controls column before the one-hot-encoded T"
"We might want to revisit, though, since it's linearly determined by the others"
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
output is
"* a column of ones if X, W, and Z are all None"
* just X or W or Z if both of the others are None
* hstack([arrs]) for whatever subset are not None otherwise
ensure Z is 2D
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
"instruments, and outcomes"
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"TODO: check that Y, T, Z do not have multiple columns"
override only so that we can update the docstring to indicate support for `StatsModelsInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
TODO: allow the final model to actually use X? Then we'd need to rename the class
since we would actually be calculating a CATE rather than ATE.
TODO: allow the final model to actually use X?
TODO: allow the final model to actually use X?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
TODO: would it be useful to extend to handle controls ala vanilla DML?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
make T 2D if if was a vector
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: conisder working around relying on sklearn implementation details
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
"If classification methods produce multiple columns of output,"
we need to manually encode classes to ensure consistent column ordering.
We clone the estimator to make sure that all the folds are
"independent, and that it is pickle-able."
"Prior to 0.24.0, this private scikit-learn method returned a tuple of two values"
`predictions` is a list of method outputs from each fold.
"If each of those is also a list, then treat this as a"
multioutput-multiclass task. We need to separately concatenate
the method outputs for each label into an `n_labels` long list.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
TODO: consider working around relying on sklearn implementation details
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Call weighted lasso on reduced design matrix
Weighted tau
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set model to WeightedLassoCV by default so there's always a model to get and set attributes on
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
(e.g. former has 'positive' and 'precompute' while latter does not)
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
don't proxy special methods
"don't pass get_params through to model, because that will cause sklearn to clone this"
regressor incorrectly
"Note: for known attributes that have been set this method will not be called,"
so we should just throw here because this is an attribute belonging to this class
but which hasn't yet been set on this instance
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: generalize to multiple treatment case?
get index of best treatment
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make any access to matplotlib or plt throw an exception
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use first target
Write node mean CATE
Write node std of CATE
Fetch appropriate color for node
"red for negative, green for positive"
Write node mean CATE
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: consider working around relying on sklearn implementation details
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Convert rows to columns
Require group assignment t to be one-hot-encoded
Get predictions for the 2 splits
Must make sure indices are merged correctly
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Must normalize weights
Override the CATE inference options
Add blb inference to parent's options
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Need to redefine fit here for auto inference to work due to a quirk in how
wrap_fit is defined
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Check that all discrete treatments are represented
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
returns shape-conforming residuals
Copy and/or define models
Define parameter estimators
Define moment and mean gradient estimator
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Call `fit` from parent class
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Expand one-hot encoding to include the zero treatment
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Calculate confidence intervals for the parameter (marginal effect)
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
d_t=None here since we measure the effect across all Ts
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
Type to column extraction function
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
assume that we should perform nested cross-validation if and only if
the model has a 'cv' attribute; this is a somewhat brittle assumption...
logic copied from check_cv
otherwise we will assume the user already set the cv attribute to something
compatible with splitting with a 'groups' argument
now we have to compute the folds explicitly because some classifiers (like LassoCV)
don't use the groups when calling split internally
Normalize weights
This class is mainly derived from statsmodels.iolib.summary.Summary
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
"want to enforce that each bad_arg was either in kwargs,"
or else it was in neither and is just taking its default value
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
define the index of d_x to filter for each given T
filter X after broadcast with T for each given T
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
"define masker by using entire dataset, otherwise Explainer will only sample 100 obs by default."
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
This flag is true when names are set in a child class instead
"If names are set in a child class, add an attribute reflecting that"
This works only if X is passed as a kwarg
We plan to enforce X as kwarg only in new releases
This checks if names have been set in a child class
"If names were set in a child class, don't do it again"
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
Get input names
Summary
add statsmodels to parent's options
add debiasedlasso to parent's options
add blb to parent's options
TODO Share some logic with non-discrete version
Get input names
Summary
add statsmodels to parent's options
add statsmodels to parent's options
add blb to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
remove None arguments
"scores entries should be lists of scores, so make each entry a singleton list"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
generate an instance of the final model
generate an instance of the nuisance model
_d_t is altered by fit nuisances to what prefit does. So we need to perform the same
alteration even when we only want to fit_final.
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
"however, sklearn doesn't support both stratifying and grouping (see"
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
their own object that supports grouping if they want to use groups.
######################################################
These should be removed once `n_splits` is deprecated
######################################################
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Handles the corner case when X=None but featurizer might be not None
"This fails if X=None and featurizer is not None, but that case is handled above"
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
TODO: support sample_var
Replacing to remove docstring
###################################################################
Everything below should be removed once parameters are deprecated
###################################################################
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE: important to use the rlearner_model_final_ attribute instead of the
attribute so that the trained featurizer will be passed through
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
We need to use the rlearner's copy to retain the information from fitting
Handles the corner case when X=None but featurizer might be not None
"This fails if X=None and featurizer is not None, but that case is handled above"
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `StatsModelsInference`
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support sample_var
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add blb to parent's options
override only so that we can update the docstring to indicate
support for `GenericSingleTreatmentModelFinalInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
note that groups are not passed to score because they are only used for fitting
note that groups are not passed to score because they are only used for fitting
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
NOTE: important to get parent's wrapped copy so that
"after training wrapped featurizer is also trained, etc."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `blb`
######################################################
These should be removed once `n_splits` is deprecated
######################################################
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=1 here since we measure the effect across all Ts
once the estimator has been fit
"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
an intercept even when bias is part of coef.
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
d_t=1 here since we measure the effect across all Ts
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=1 here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
"if stderr is zero, ppf will return nans and the loop below would never terminate"
so bail out early; note that it might be possible to correct the algorithm for
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
be clean
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
don't proxy special methods
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
second level bootstrap which would be prohibitive computationally?
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
can't import from econml.inference at top level without creating cyclical dependencies
Note that inference results are always methods even if the inference is for a property
(e.g. coef__inference() is a method but coef_ is a property)
Therefore we must insert a lambda if getting inference for a non-callable
"If inference is for a property, create a fresh lambda to avoid passing args through"
"try to get interval/std first if appropriate,"
since we don't prefer a wrapped method with this name
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code is a fork from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_base.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
Set parameters
Don't instantiate estimators now! Parameters of base_estimator might
"still change. Eg., when grid-searching with the nested object syntax."
self.estimators_ needs to be filled by the derived classes in fit.
Compute the number of jobs
Partition estimators between jobs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Types and constants
=============================================================================
=============================================================================
Base GRF tree
=============================================================================
Determine output settings
"Important: This must be the first invocation of the random state at fit time, so that"
train/test splits are re-generatable from an external object simply by knowing the
random_state parameter of the tree. Can be useful in the future if one wants to create local
linear predictions. Currently is also useful for testing.
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
Set min_weight_leaf from min_weight_fraction_leaf
Build tree
We calculate the maximum number of samples from each half-split that any node in the tree can
hold. Used by criterion for memory space savings.
Initialize the criterion object and the criterion_val object if honest.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
=============================================================================
A MultOutputWrapper for GRF classes
=============================================================================
=============================================================================
Instantiations of Generalized Random Forest
=============================================================================
"Append a constant treatment if `fit_intercept=True`, the coefficient"
in front of the constant treatment is the intercept in the moment equation.
"Append a constant treatment and constant instrument if `fit_intercept=True`,"
the coefficient in front of the constant treatment is the intercept in the moment equation.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
""
This code contains snippets of code from
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py
published under the following license and copyright:
BSD 3-Clause License
""
Copyright (c) 2007-2020 The scikit-learn developers.
All rights reserved.
=============================================================================
Base Generalized Random Forest
=============================================================================
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Get subsample sample size
Converting `min_var_fraction_leaf` to an absolute `min_var_leaf` that the GRFTree can handle
We calculate the min eigenvalue proxy that each criterion is considering
"on the overall mean jacobian, to determine the absolute level of `min_var_leaf`"
Check parameters
We re-initialize the subsample_random_seed_ only if we are not in warm_start mode or
if this is the first `fit` call of the warm start mode.
"Free allocated memory, if any"
the below are needed to replicate randomness of subsampling when warm_start=True
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Generating indices a priori before parallelism ended up being orders of magnitude
faster than how sklearn does it. The reason is that random samplers do not release the
gil it seems.
Advancing subsample_random_state. Assumes each prior fit call has the same number of
"samples at fit time. If not then this would not exactly replicate a single batch execution,"
but would still advance randomness enough so that tree subsamples will be different.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
Collect newly grown trees
Check data
Assign chunk of trees to jobs
avoid storing the output of every estimator by summing them here
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
Check data
Assign chunk of trees to jobs
Parallel loop
####################
Variance correction
####################
Subtract the average within bag variance. This ends up being equal to the
overall (E_{all trees}[moment^2] - E_bags[ E[mean_bag_moment]^2 ]) / sizeof(bag).
The negative part is just sq_between.
Objective bayes debiasing for the diagonals where we know a-prior they are positive
"The off diagonals we have no objective prior, so no correction is applied."
Finally correcting the pred_cov or pred_var
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
test that the subsampling scheme past to the trees is correct
test that the estimator calcualtes var correctly
test api
test accuracy
test the projection functionality of forests
test that the estimator calcualtes var correctly
test api
test that the estimator calcualtes var correctly
"test that the estimator accepts lists, tuples and pandas data frames"
test that we raise errors in mishandled situations.
test that the subsampling scheme past to the trees is correct
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
"can set final model for plain DML, but can't for LinearDML (hardcoded to StatsModelsRegression)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
can interpret without uncertainty
can't treat before interpreting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
testing importances
testing heterogeneity importances
Testing that all parameters do what they are supposed to
"testing predict, apply and decision path"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
Multi-dimensional output y
1-d y
multi-d y
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test inference results when `cate_feature_names` doesn not exist
Test inference results when `cate_feature_names` doesn not exist
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
pvalue for second column should be greater than zero since some points are on either side
of the tested value
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Nuisance model has no score method, so nuisance_scores_ should be none"
Test non keyword based calls to fit
test non-array inputs
Test custom splitter
Test incomplete set of test folds
"y scores should be positive, since W predicts Y somewhat"
"t scores might not be, since W and T are uncorrelated"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
creating an instance should warn
using the instance should not warn
using the deprecated method should warn
don't warn if b and c are passed by keyword
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
"Run ten experiments, recomputing the variance of 10 estimates of the effect in each experiment"
The average variance should be lower when using monte carlo iterations
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
ensure that we've got at least two of every row
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
need to make sure we get all *joint* combinations
IntentToTreat only supports binary treatments/instruments
IntentToTreat only supports binary treatments/instruments
IntentToTreat requires X
ensure we can serialize unfit estimator
these support only W but not X
"these support only binary, not general discrete T and Z"
ensure we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
TODO: add tests for extra properties like coef_ where they exist
TODO: add tests for extra properties like coef_ where they exist
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
TODO: ideally we could also test whether Z and X are jointly okay when both discrete
"however, with custom splits the checking happens in the first stage wrapper"
where we don't have all of the required information to do this;
we'd probably need to add it to _crossfit instead
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
The __warningregistry__'s need to be in a pristine state for tests
to work properly.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Create a wrapper around Lasso that doesn't support weights
since Lasso does natively support them starting in sklearn 0.23
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least 6 of every element
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
NOTE: this number may need to change if the default number of folds in
WeightedStratifiedKFold changes
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
predetermined splits ensure that all features are seen in each split
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
create a simple artificial setup where effect of moving from treatment
"a -> b is 2,"
"a -> c is 1, and"
"b -> c is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
should rule out some basic issues.
Note that explicitly specifying the dtype as object is necessary until
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
estimated effects should be identical when treatment is explicitly given
but const_marginal_effect should be reordered based on the explicit cagetories
1-> 2 in original ordering; combination of 3->1 and 3->2
test outer grouping
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure that we can serialize unfit estimator
ensure that we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef_ and intercept_ inference
verify we can generate the summary
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
test outer grouping
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
#########################
WeightedLassoCVWrapper #
#########################
perform 1D fit
perform 2D fit
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
_penalized_inds is only set during fitting
cv exists on penalized model
now we can access _penalized_inds
check that we can read the cv attribute back out from the underlying model
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
test base values equals to mean of constant marginal effect
test shape of shap values output is as expected
test shape of attribute of explanation object is as expected
Treatment effect function
Outcome support
Treatment support
"Generate controls, covariates, treatments and outcomes"
Heterogeneous treatment effects
"TODO There is a matrix dimension mismatch between multiple outcome and single outcome, should solve that"
through shap package.
test shap could generate the plot from the shap_values
Re-raise with more informative error message instead:
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
"Note: unlike other Metalearners, we don't drop the first column because"
we concatenate all treatments to the other features;
"We might want to revisit, though, since it's linearly determined by the others"
Check inputs
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
output is
"* a column of ones if X, W, and Z are all None"
* just X or W or Z if both of the others are None
* hstack([arrs]) for whatever subset are not None otherwise
ensure Z is 2D
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
"instruments, and outcomes"
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"TODO: check that Y, T, Z do not have multiple columns"
override only so that we can update the docstring to indicate support for `StatsModelsInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
TODO: allow the final model to actually use X? Then we'd need to rename the class
since we would actually be calculating a CATE rather than ATE.
TODO: allow the final model to actually use X?
TODO: allow the final model to actually use X?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
TODO: would it be useful to extend to handle controls ala vanilla DML?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
make T 2D if if was a vector
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: conisder working around relying on sklearn implementation details
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
"If classification methods produce multiple columns of output,"
we need to manually encode classes to ensure consistent column ordering.
We clone the estimator to make sure that all the folds are
"independent, and that it is pickle-able."
TODO. The API of the private scikit-learn `_fit_and_predict` has changed
"between 0.23.2 and 0.24. For this to work with <0.24, we need to add a"
case analysis based on sklearn version.
`predictions` is a list of method outputs from each fold.
"If each of those is also a list, then treat this as a"
multioutput-multiclass task. We need to separately concatenate
the method outputs for each label into an `n_labels` long list.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
TODO: consider working around relying on sklearn implementation details
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Call weighted lasso on reduced design matrix
Weighted tau
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set model to WeightedLassoCV by default so there's always a model to get and set attributes on
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
(e.g. former has 'positive' and 'precompute' while latter does not)
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
don't proxy special methods
"don't pass get_params through to model, because that will cause sklearn to clone this"
regressor incorrectly
"Note: for known attributes that have been set this method will not be called,"
so we should just throw here because this is an attribute belonging to this class
but which hasn't yet been set on this instance
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: generalize to multiple treatment case?
get index of best treatment
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use first target
Write node mean CATE
Write node std of CATE
Fetch appropriate color for node
"red for negative, green for positive"
Write node mean CATE
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: consider working around relying on sklearn implementation details
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Convert rows to columns
Require group assignment t to be one-hot-encoded
Get predictions for the 2 splits
Must make sure indices are merged correctly
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Must normalize weights
Override the CATE inference options
Add blb inference to parent's options
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Need to redefine fit here for auto inference to work due to a quirk in how
wrap_fit is defined
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Check that all discrete treatments are represented
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
returns shape-conforming residuals
Copy and/or define models
Define parameter estimators
Define moment and mean gradient estimator
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Call `fit` from parent class
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Expand one-hot encoding to include the zero treatment
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Calculate confidence intervals for the parameter (marginal effect)
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
"Note: unlike other Metalearners, we don't drop the first column because"
we concatenate all treatments to the other features;
"We might want to revisit, though, since it's linearly determined by the others"
Check inputs
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: consider working around relying on sklearn implementation details
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Convert rows to columns
Require group assignment t to be one-hot-encoded
Get predictions for the 2 splits
Must make sure indices are merged correctly
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Must normalize weights
Override the CATE inference options
Add blb inference to parent's options
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Need to redefine fit here for auto inference to work due to a quirk in how
wrap_fit is defined
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Check that all discrete treatments are represented
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
returns shape-conforming residuals
Copy and/or define models
Define parameter estimators
Define moment and mean gradient estimator
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Call `fit` from parent class
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Expand one-hot encoding to include the zero treatment
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Calculate confidence intervals for the parameter (marginal effect)
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `StatsModelsInference`
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support sample_var
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
#######################################
Core DML Tests
#######################################
How many samples
How many control features
How many treatment variables
Coefficients of how controls affect treatments
Coefficients of how controls affect outcome
Treatment effects that we want to estimate
Run dml estimation
How many samples
How many control features
How many treatment variables
Coefficients of how controls affect treatments
Coefficients of how controls affect outcome
Treatment effects that we want to estimate
Run dml estimation
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
note that groups are not passed to score because they are only used for fitting
note that groups are not passed to score because they are only used for fitting
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
make T 2D if if was a vector
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect interval as a function of const_marginal_effect_interval for a single treatment
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=1 here since we measure the effect across all Ts
once the estimator has been fit
"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
an intercept even when bias is part of coef.
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
d_t=1 here since we measure the effect across all Ts
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=1 here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
"if stderr is zero, ppf will return nans and the loop below would never terminate"
so bail out early; note that it might be possible to correct the algorithm for
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
be clean
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
output is
"* a column of ones if X, W, and Z are all None"
* just X or W or Z if both of the others are None
* hstack([arrs]) for whatever subset are not None otherwise
ensure Z is 2D
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
TODO: allow the final model to actually use X? Then we'd need to rename the class
since we would actually be calculating a CATE rather than ATE.
TODO: allow the final model to actually use X?
TODO: allow the final model to actually use X?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
TODO: would it be useful to extend to handle controls ala vanilla DML?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
"instruments, and outcomes"
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"TODO: check that Y, T, Z do not have multiple columns"
override only so that we can update the docstring to indicate support for `StatsModelsInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
TODO: support sample_var
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
Replacing to remove docstring
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: generalize to multiple treatment case?
get index of best treatment
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
#######################################################
Perfect Data DGPs for Testing Correctness of Code
#######################################################
Generate random control co-variates
Create epsilon residual treatments that deterministically sum up to
zero
Re-calibrate epsilon to make sure that empirical distribution of epsilon
conditional on each co-variate vector is equal to zero
We simply subtract the conditional mean from the epsilons
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Create epsilon residual treatments that deterministically sum up to
zero
Re-calibrate epsilon to make sure that empirical distribution of epsilon
conditional on each co-variate vector is equal to zero
We simply subtract the conditional mean from the epsilons
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Create epsilon residual treatments
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect + eta
Generate random control co-variates
Use the same treatment vector for each row
Construct outcomes as y = X*beta + T*effect
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
add statsmodels to parent's options
add debiasedlasso to parent's options
add blb to parent's options
TODO Share some logic with non-discrete version
add statsmodels to parent's options
add statsmodels to parent's options
add blb to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
assume that we should perform nested cross-validation if and only if
the model has a 'cv' attribute; this is a somewhat brittle assumption...
logic copied from check_cv
otherwise we will assume the user already set the cv attribute to something
compatible with splitting with a 'groups' argument
now we have to compute the folds explicitly because some classifiers (like LassoCV)
don't use the groups when calling split internally
Normalize weights
This class is mainly derived from statsmodels.iolib.summary.Summary
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
"want to enforce that each bad_arg was either in kwargs,"
or else it was in neither and is just taking its default value
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use first target
Write node mean CATE
Write node std of CATE
Write confidence interval information if at leaf node
Fetch appropriate color for node
"red for negative, green for positive"
Write node mean CATE
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
remove None arguments
"scores entries should be lists of scores, so make each entry a singleton list"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
"however, sklearn doesn't support both stratifying and grouping (see"
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
their own object that supports grouping if they want to use groups.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Estimators
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
don't proxy special methods
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
second level bootstrap which would be prohibitive computationally?
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
can't import from econml.inference at top level without creating cyclical dependencies
Note that inference results are always methods even if the inference is for a property
(e.g. coef__inference() is a method but coef_ is a property)
Therefore we must insert a lambda if getting inference for a non-callable
"If inference is for a property, create a fresh lambda to avoid passing args through"
"try to get interval/std first if appropriate,"
since we don't prefer a wrapped method with this name
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
can interpret without uncertainty
can't treat before interpreting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
Multi-dimensional output y
1-d y
multi-d y
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test inference results when `cate_feature_names` doesn not exist
Test inference results when `cate_feature_names` doesn not exist
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
pvalue for second column should be greater than zero since some points are on either side
of the tested value
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Nuisance model has no score method, so nuisance_scores_ should be none"
Test non keyword based calls to fit
test non-array inputs
Test custom splitter
Test incomplete set of test folds
"y scores should be positive, since W predicts Y somewhat"
"t scores might not be, since W and T are uncorrelated"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
creating an instance should warn
using the instance should not warn
using the deprecated method should warn
don't warn if b and c are passed by keyword
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
ensure that we've got at least two of every row
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
need to make sure we get all *joint* combinations
IntentToTreat only supports binary treatments/instruments
IntentToTreat only supports binary treatments/instruments
IntentToTreat requires X
ensure we can serialize unfit estimator
these support only W but not X
"these support only binary, not general discrete T and Z"
ensure we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
TODO: add tests for extra properties like coef_ where they exist
TODO: add tests for extra properties like coef_ where they exist
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
TODO: ideally we could also test whether Z and X are jointly okay when both discrete
"however, with custom splits the checking happens in the first stage wrapper"
where we don't have all of the required information to do this;
we'd probably need to add it to _crossfit instead
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
The __warningregistry__'s need to be in a pristine state for tests
to work properly.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Test Causal Forest API
Generate data with continuous treatments
Instantiate model with most of the default parameters.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Test CausalForest API
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
Test CausalForest API
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Test causal foret API
Test Causal Forest API
Create a wrapper around Lasso that doesn't support weights
since Lasso does natively support them starting in sklearn 0.23
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least 6 of every element
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
NOTE: this number may need to change if the default number of folds in
WeightedStratifiedKFold changes
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
predetermined splits ensure that all features are seen in each split
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
create a simple artificial setup where effect of moving from treatment
"a -> b is 2,"
"a -> c is 1, and"
"b -> c is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
should rule out some basic issues.
Note that explicitly specifying the dtype as object is necessary until
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
estimated effects should be identical when treatment is explicitly given
but const_marginal_effect should be reordered based on the explicit cagetories
1-> 2 in original ordering; combination of 3->1 and 3->2
test outer grouping
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
make sure we warn when using old aliases
make sure we can use the old alias as a type
make sure that we can still pickle the old aliases
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure that we can serialize unfit estimator
ensure that we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef_ and intercept_ inference
verify we can generate the summary
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
test outer grouping
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
#########################
WeightedLassoCVWrapper #
#########################
perform 1D fit
perform 2D fit
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
_penalized_inds is only set during fitting
cv exists on penalized model
now we can access _penalized_inds
check that we can read the cv attribute back out from the underlying model
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check that the point estimates are the same
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: conisder working around relying on sklearn implementation details
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
"If classification methods produce multiple columns of output,"
we need to manually encode classes to ensure consistent column ordering.
We clone the estimator to make sure that all the folds are
"independent, and that it is pickle-able."
Concatenate the predictions
`predictions` is a list of method outputs from each fold.
"If each of those is also a list, then treat this as a"
multioutput-multiclass task. We need to separately concatenate
the method outputs for each label into an `n_labels` long list.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
TODO: consider working around relying on sklearn implementation details
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Call weighted lasso on reduced design matrix
Inherit some parameters from the parent
Weighted tau
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set model to WeightedLassoCV by default so there's always a model to get and set attributes on
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
(e.g. former has 'positive' and 'precompute' while latter does not)
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
don't proxy special methods
"don't pass get_params through to model, because that will cause sklearn to clone this"
regressor incorrectly
"Note: for known attributes that have been set this method will not be called,"
so we should just throw here because this is an attribute belonging to this class
but which hasn't yet been set on this instance
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Construct the subsample of data
Split into estimation and splitting sample set
Fit the tree on the splitting sample
Set the estimation values based on the estimation split
Apply the trained tree on the estimation sample to get the path for every estimation sample
Calculate the total weight of estimation samples on each tree node:
\sum_i sample_weight[i] * 1{i \\in node}
Calculate the total number of estimation samples on each tree node:
|node| = \sum_{i} 1{i \\in node}
Calculate the weighted sum of responses on the estimation sample on each node:
\sum_{i} sample_weight[i] 1{i \\in node} Y_i
Calculate the predicted value on each node based on the estimation sample:
weighted sum of responses / total weight
"Calculate the criterion on each node based on the estimation sample and for each output dimension,"
summing the impurity across dimensions.
First we calculate the difference of observed label y of each node and predicted value for each
node that the sample falls in: y[i] - value_est[node]
If criterion is mse then calculate weighted sum of squared differences for each node
If criterion is mae then calculate weighted sum of absolute differences for each node
Normalize each weighted sum of criterion for each node by the total weight of each node
Prune tree to remove leafs that don't satisfy the leaf requirements on the estimation sample
and for each un-pruned tree set the value and the weight appropriately.
If minimum weight requirement or minimum leaf size requirement is not satisfied on estimation
"sample, then prune the whole sub-tree"
Set the numerator of the node to: \sum_{i} sample_weight[i] 1{i \\in node} Y_i / |node|
Set the value of the node to:
\sum_{i} sample_weight[i] 1{i \\in node} Y_i / \sum_{i} sample_weight[i] 1{i \\in node}
Set the denominator of the node to: \sum_{i} sample_weight[i] 1{i \\in node} / |node|
Set the weight of the node to: \sum_{i} sample_weight[i] 1{i \\in node}
Set the count to the estimation split count
Set the node impurity to the estimation split impurity
Validate or convert input data
Pre-sort indices to avoid that each individual tree of the
ensemble sorts the indices.
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
"Free allocated memory, if any"
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
TODO. This slicing should ultimately be done inside the parallel function
so that we don't need to create a matrix of size roughly n_samples * n_estimators
Collect newly grown trees
Helper class that accumulates an arbitrary function in parallel on the accumulator acc
and calls the function fn on each tree e and returns the mean output. The function fn
should take as input a tree e and associated numerator n and denominator d structures and
"return another function g_e, which takes as input X, check_input"
"If slice is not None, but rather a tuple (start, end), then a subset of the trees from"
index start to index end will be used. The returned result is essentially:
(mean over e in slice)(g_e(X)).
Check data
Assign chunk of trees to jobs
Check data
Check data
avoid storing the output of every estimator by summing them here
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) Y_i"
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x)"
"Calculate for each slice S: Q(S) = 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) (Y_i - \theta(X))"
where \theta(X) is the point estimate using the whole forest
Calculate the variance of the latter as E[Q(S)^2]
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
"Note: unlike other Metalearners, we don't drop the first column because"
we concatenate all treatments to the other features;
"We might want to revisit, though, since it's linearly determined by the others"
Check inputs
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Convert rows to columns
Require group assignment t to be one-hot-encoded
Get predictions for the 2 splits
Must make sure indices are merged correctly
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Must normalize weights
Override the CATE inference options
Add blb inference to parent's options
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Need to redefine fit here for auto inference to work due to a quirk in how
wrap_fit is defined
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Check that all discrete treatments are represented
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
returns shape-conforming residuals
Copy and/or define models
Define parameter estimators
Define moment and mean gradient estimator
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Call `fit` from parent class
weirdness of wrap_fit. We need to store d_t_in. But because wrap_fit decorates the parent
"fit, we need to set explicitly d_t_in here after super fit is called."
Override to flatten output if T is flat
Expand one-hot encoding to include the zero treatment
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Calculate confidence intervals for the parameter (marginal effect)
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `StatsModelsInference`
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support sample_var
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
#######################################
Core DML Tests
#######################################
How many samples
How many control features
How many treatment variables
Coefficients of how controls affect treatments
Coefficients of how controls affect outcome
Treatment effects that we want to estimate
Run dml estimation
How many samples
How many control features
How many treatment variables
Coefficients of how controls affect treatments
Coefficients of how controls affect outcome
Treatment effects that we want to estimate
Run dml estimation
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
note that groups are not passed to score because they are only used for fitting
note that groups are not passed to score because they are only used for fitting
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
make T 2D if if was a vector
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect interval as a function of const_marginal_effect_interval for a single treatment
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=1 here since we measure the effect across all Ts
once the estimator has been fit
"replacing _predict of super to fend against misuse, when the user has used a final linear model with"
an intercept even when bias is part of coef.
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
d_t=1 here since we measure the effect across all Ts
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=1 here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
"in the degenerate case where every point in the distribution is equal to the value tested, return nan"
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
"if stderr is zero, ppf will return nans and the loop below would never terminate"
so bail out early; note that it might be possible to correct the algorithm for
"this scenario, but since scipy's cdf returns nan whenever scale is zero it won't"
be clean
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
output is
"* a column of ones if X, W, and Z are all None"
* just X or W or Z if both of the others are None
* hstack([arrs]) for whatever subset are not None otherwise
ensure Z is 2D
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
TODO: allow the final model to actually use X? Then we'd need to rename the class
since we would actually be calculating a CATE rather than ATE.
TODO: allow the final model to actually use X?
TODO: allow the final model to actually use X?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
TODO: would it be useful to extend to handle controls ala vanilla DML?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
"instruments, and outcomes"
TODO: how do we incorporate the sample_weight and sample_var passed into this method
as arguments?
TODO: is there a good way to incorporate the other nuisance terms in the score?
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"TODO: check that Y, T, Z do not have multiple columns"
override only so that we can update the docstring to indicate support for `StatsModelsInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
TODO: support sample_var
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
Replacing to remove docstring
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: generalize to multiple treatment case?
get index of best treatment
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
#######################################################
Perfect Data DGPs for Testing Correctness of Code
#######################################################
Generate random control co-variates
Create epsilon residual treatments that deterministically sum up to
zero
Re-calibrate epsilon to make sure that empirical distribution of epsilon
conditional on each co-variate vector is equal to zero
We simply subtract the conditional mean from the epsilons
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Create epsilon residual treatments that deterministically sum up to
zero
Re-calibrate epsilon to make sure that empirical distribution of epsilon
conditional on each co-variate vector is equal to zero
We simply subtract the conditional mean from the epsilons
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Create epsilon residual treatments
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect + eta
Generate random control co-variates
Use the same treatment vector for each row
Construct outcomes as y = X*beta + T*effect
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
add statsmodels to parent's options
add debiasedlasso to parent's options
add blb to parent's options
TODO Share some logic with non-discrete version
add statsmodels to parent's options
add statsmodels to parent's options
add blb to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
assume that we should perform nested cross-validation if and only if
the model has a 'cv' attribute; this is a somewhat brittle assumption...
logic copied from check_cv
otherwise we will assume the user already set the cv attribute to something
compatible with splitting with a 'groups' argument
now we have to compute the folds explicitly because some classifiers (like LassoCV)
don't use the groups when calling split internally
Normalize weights
This class is mainly derived from statsmodels.iolib.summary.Summary
"if we're decorating a class, just update the __init__ method,"
so that the result is still a class instead of a wrapper method
"want to enforce that each bad_arg was either in kwargs,"
or else it was in neither and is just taking its default value
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use first target
Write node mean CATE
Write node std of CATE
Write confidence interval information if at leaf node
Fetch appropriate color for node
"red for negative, green for positive"
Write node mean CATE
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
remove None arguments
"scores entries should be lists of scores, so make each entry a singleton list"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
"TODO: ideally, we'd also infer whether we need a GroupKFold (if groups are passed)"
"however, sklearn doesn't support both stratifying and grouping (see"
"https://github.com/scikit-learn/scikit-learn/issues/13621), so for now the user needs to supply"
their own object that supports grouping if they want to use groups.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Estimators
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
don't proxy special methods
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
TODO: studentized bootstrap? this would be more accurate in most cases but can we avoid
second level bootstrap which would be prohibitive computationally?
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
can't import from econml.inference at top level without creating cyclical dependencies
Note that inference results are always methods even if the inference is for a property
(e.g. coef__inference() is a method but coef_ is a property)
Therefore we must insert a lambda if getting inference for a non-callable
"If inference is for a property, create a fresh lambda to avoid passing args through"
"try to get interval/std first if appropriate,"
since we don't prefer a wrapped method with this name
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
can interpret without uncertainty
can't treat before interpreting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
Multi-dimensional output y
1-d y
multi-d y
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test inference results when `cate_feature_names` doesn not exist
Test inference results when `cate_feature_names` doesn not exist
"test value 0 is less than estimate of 1 and variance is 0, so z score should be inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
"test value 1 is equal to the estimate of 1 and variance is 0, so z score should be nan"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
pvalue for second column should be greater than zero since some points are on either side
of the tested value
"test value 2 is greater than estimate of 1 and variance is 0, so z score should be -inf"
"predictions in column 1 have nonzero variance, so the zstat should always be some finite value"
pvalue is also nan when variance is 0 and the point tested is equal to the point tested
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Nuisance model has no score method, so nuisance_scores_ should be none"
Test non keyword based calls to fit
test non-array inputs
Test custom splitter
Test incomplete set of test folds
"y scores should be positive, since W predicts Y somewhat"
"t scores might not be, since W and T are uncorrelated"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
creating an instance should warn
using the instance should not warn
using the deprecated method should warn
don't warn if b and c are passed by keyword
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
ensure that we've got at least two of every row
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
need to make sure we get all *joint* combinations
IntentToTreat only supports binary treatments/instruments
IntentToTreat only supports binary treatments/instruments
IntentToTreat requires X
ensure we can serialize unfit estimator
these support only W but not X
"these support only binary, not general discrete T and Z"
ensure we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
TODO: add tests for extra properties like coef_ where they exist
TODO: add tests for extra properties like coef_ where they exist
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
TODO: ideally we could also test whether Z and X are jointly okay when both discrete
"however, with custom splits the checking happens in the first stage wrapper"
where we don't have all of the required information to do this;
we'd probably need to add it to _crossfit instead
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
The __warningregistry__'s need to be in a pristine state for tests
to work properly.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Test Causal Forest API
Generate data with continuous treatments
Instantiate model with most of the default parameters.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Test CausalForest API
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
Test CausalForest API
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Test causal foret API
Test Causal Forest API
Create a wrapper around Lasso that doesn't support weights
since Lasso does natively support them starting in sklearn 0.23
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least 6 of every element
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
NOTE: this number may need to change if the default number of folds in
WeightedStratifiedKFold changes
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
predetermined splits ensure that all features are seen in each split
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
create a simple artificial setup where effect of moving from treatment
"a -> b is 2,"
"a -> c is 1, and"
"b -> c is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
should rule out some basic issues.
Note that explicitly specifying the dtype as object is necessary until
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
estimated effects should be identical when treatment is explicitly given
but const_marginal_effect should be reordered based on the explicit cagetories
1-> 2 in original ordering; combination of 3->1 and 3->2
test outer grouping
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
make sure we warn when using old aliases
make sure we can use the old alias as a type
make sure that we can still pickle the old aliases
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure that we can serialize unfit estimator
ensure that we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef_ and intercept_ inference
verify we can generate the summary
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
test outer grouping
"NOTE: we should ideally use a stratified split with grouping, but sklearn doesn't have one yet"
test nested grouping
ensure that the grouping has worked correctly and we get all 10 copies of the items in
whichever groups we saw
test nested grouping
"by default, we use 5 split cross-validation for our T and Y models"
but we don't have enough groups here to split both the outer and inner samples with grouping
TODO: does this imply we should change some defaults to make this more likely to succeed?
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
#########################
WeightedLassoCVWrapper #
#########################
perform 1D fit
perform 2D fit
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
_penalized_inds is only set during fitting
cv exists on penalized model
now we can access _penalized_inds
check that we can read the cv attribute back out from the underlying model
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check that the point estimates are the same
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
"If classification methods produce multiple columns of output,"
we need to manually encode classes to ensure consistent column ordering.
We clone the estimator to make sure that all the folds are
"independent, and that it is pickle-able."
Concatenate the predictions
`predictions` is a list of method outputs from each fold.
"If each of those is also a list, then treat this as a"
multioutput-multiclass task. We need to separately concatenate
the method outputs for each label into an `n_labels` long list.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Call weighted lasso on reduced design matrix
Inherit some parameters from the parent
Weighted tau
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set model to WeightedLassoCV by default so there's always a model to get and set attributes on
whitelist known params because full set is not necessarily identical between LassoCV and MultiTaskLassoCV
(e.g. former has 'positive' and 'precompute' while latter does not)
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
don't proxy special methods
"don't pass get_params through to model, because that will cause sklearn to clone this"
regressor incorrectly
"Note: for known attributes that have been set this method will not be called,"
so we should just throw here because this is an attribute belonging to this class
but which hasn't yet been set on this instance
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Construct the subsample of data
Split into estimation and splitting sample set
Fit the tree on the splitting sample
Set the estimation values based on the estimation split
Apply the trained tree on the estimation sample to get the path for every estimation sample
Calculate the total weight of estimation samples on each tree node:
\sum_i sample_weight[i] * 1{i \\in node}
Calculate the total number of estimation samples on each tree node:
|node| = \sum_{i} 1{i \\in node}
Calculate the weighted sum of responses on the estimation sample on each node:
\sum_{i} sample_weight[i] 1{i \\in node} Y_i
Calculate the predicted value on each node based on the estimation sample:
weighted sum of responses / total weight
"Calculate the criterion on each node based on the estimation sample and for each output dimension,"
summing the impurity across dimensions.
First we calculate the difference of observed label y of each node and predicted value for each
node that the sample falls in: y[i] - value_est[node]
If criterion is mse then calculate weighted sum of squared differences for each node
If criterion is mae then calculate weighted sum of absolute differences for each node
Normalize each weighted sum of criterion for each node by the total weight of each node
Prune tree to remove leafs that don't satisfy the leaf requirements on the estimation sample
and for each un-pruned tree set the value and the weight appropriately.
If minimum weight requirement or minimum leaf size requirement is not satisfied on estimation
"sample, then prune the whole sub-tree"
Set the numerator of the node to: \sum_{i} sample_weight[i] 1{i \\in node} Y_i / |node|
Set the value of the node to:
\sum_{i} sample_weight[i] 1{i \\in node} Y_i / \sum_{i} sample_weight[i] 1{i \\in node}
Set the denominator of the node to: \sum_{i} sample_weight[i] 1{i \\in node} / |node|
Set the weight of the node to: \sum_{i} sample_weight[i] 1{i \\in node}
Set the count to the estimation split count
Set the node impurity to the estimation split impurity
Validate or convert input data
Pre-sort indices to avoid that each individual tree of the
ensemble sorts the indices.
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
"Free allocated memory, if any"
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
TODO. This slicing should ultimately be done inside the parallel function
so that we don't need to create a matrix of size roughly n_samples * n_estimators
Collect newly grown trees
Helper class that accumulates an arbitrary function in parallel on the accumulator acc
and calls the function fn on each tree e and returns the mean output. The function fn
should take as input a tree e and associated numerator n and denominator d structures and
"return another function g_e, which takes as input X, check_input"
"If slice is not None, but rather a tuple (start, end), then a subset of the trees from"
index start to index end will be used. The returned result is essentially:
(mean over e in slice)(g_e(X)).
Check data
Assign chunk of trees to jobs
Check data
Check data
avoid storing the output of every estimator by summing them here
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) Y_i"
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x)"
"Calculate for each slice S: Q(S) = 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) (Y_i - \theta(X))"
where \theta(X) is the point estimate using the whole forest
Calculate the variance of the latter as E[Q(S)^2]
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
"Note: unlike other Metalearners, we don't drop the first column because"
we concatenate all treatments to the other features;
"We might want to revisit, though, since it's linearly determined by the others"
Check inputs
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Require group assignment t to be one-hot-encoded
Define an inner function that iterates over group predictions
Convert rows to columns
Get predictions for the 2 splits
Must make sure indices are merged correctly
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Override the CATE inference options
Add blb inference to parent's options
Must normalize weights
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Override to flatten output if T is flat
T is flat
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
Copy and/or define models
Nuisance estimators shall be defined during fitting because they need to know the number of distinct
treatments
Define parameter estimators
Define moment and mean gradient estimator
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Define number of classes
Call `fit` from parent class
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Test expansion of treatment
"If expanded treatments are a vector, flatten const_marginal_effect_interval"
Calculate confidence intervals for the parameter (marginal effect)
"If T is a vector, preserve shape of the effect interval"
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
"Get a list of (parameter, covariance matrix) pairs"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support sample_var
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add statsmodels to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
#######################################
Core DML Tests
#######################################
How many samples
How many control features
How many treatment variables
Coefficients of how controls affect treatments
Coefficients of how controls affect outcome
Treatment effects that we want to estimate
Run dml estimation
How many samples
How many control features
How many treatment variables
Coefficients of how controls affect treatments
Coefficients of how controls affect outcome
Treatment effects that we want to estimate
Run dml estimation
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
store number of columns of T so that we can pass scalars to effect
TODO: support vector T and Y
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect interval as a function of const_marginal_effect_interval for a single treatment
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=1 here since we measure the effect across all Ts
once the estimator has been fit
We can write effect interval as a function of predict_interval of the final method for linear models
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
d_t=1 here since we measure the effect across all Ts
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=1 here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A cut-down version of the DML first stage wrapper, since we don't need to support linear first stages"
output is
"* a column of ones if X, W, and Z are all None"
* just X or W or Z if both of the others are None
* hstack([arrs]) for whatever subset are not None otherwise
ensure Z is 2D
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
TODO: allow the final model to actually use X? Then we'd need to rename the class
since we would actually be calculating a CATE rather than ATE.
TODO: allow the final model to actually use X?
TODO: allow the final model to actually use X?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
TODO: would it be useful to extend to handle controls ala vanilla DML?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
"instruments, and outcomes"
TODO: how do we incorporate the sample_weight and sample_var passed into this method
as arguments?
TODO: is there a good way to incorporate the other nuisance terms in the score?
"Replacing fit from _OrthoLearner, to reorder arguments and improve the docstring"
"Replacing score from _OrthoLearner, to reorder arguments and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"TODO: check that Y, T, Z do not have multiple columns"
override only so that we can update the docstring to indicate support for `StatsModelsInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
TODO: support sample_var
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
add statsmodels to parent's options
Replacing to remove docstring
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: generalize to multiple treatment case?
get index of best treatment
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
#######################################################
Perfect Data DGPs for Testing Correctness of Code
#######################################################
Generate random control co-variates
Create epsilon residual treatments that deterministically sum up to
zero
Re-calibrate epsilon to make sure that empirical distribution of epsilon
conditional on each co-variate vector is equal to zero
We simply subtract the conditional mean from the epsilons
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Create epsilon residual treatments that deterministically sum up to
zero
Re-calibrate epsilon to make sure that empirical distribution of epsilon
conditional on each co-variate vector is equal to zero
We simply subtract the conditional mean from the epsilons
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Create epsilon residual treatments
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect + eta
Generate random control co-variates
Use the same treatment vector for each row
Construct outcomes as y = X*beta + T*effect
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
add statsmodels to parent's options
add debiasedlasso to parent's options
TODO Share some logic with non-discrete version
add statsmodels to parent's options
add statsmodels to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
Normalize weights
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
This class is mainly derived from statsmodels.iolib.summary.Summary
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use first target
Write node mean CATE
Write node std of CATE
Write confidence interval information if at leaf node
Fetch appropriate color for node
"red for negative, green for positive"
Write node mean CATE
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"scores entries should be lists of scores, so make each entry a singleton list"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
use a binary array to get stratified split in case of discrete treatment
"stratify on T if discrete, and fine to pass T as second arg to KFold.split even when not"
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Estimators
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
"try to get interval first if appropriate, since we don't prefer a wrapped method with this name"
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
can interpret without uncertainty
can't treat before interpreting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
Multi-dimensional output y
1-d y
multi-d y
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test inference results when `cate_feature_names` doesn not exist
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Nuisance model has no score method, so nuisance_scores_ should be none"
Test non keyword based calls to fit
test non-array inputs
Test custom splitter
Test incomplete set of test folds
"y scores should be positive, since W predicts Y somewhat"
"t scores might not be, since W and T are uncorrelated"
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
ensure that we've got at least two of every row
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
need to make sure we get all *joint* combinations
IntentToTreat only supports binary treatments/instruments
IntentToTreat only supports binary treatments/instruments
TODO: add stratification to bootstrap so that we can use it
even with discrete treatments
IntentToTreat requires X
ensure we can serialize unfit estimator
these support only W but not X
"these support only binary, not general discrete T and Z"
ensure we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
TODO: add tests for extra properties like coef_ where they exist
TODO: add tests for extra properties like coef_ where they exist
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
TODO: ideally we could also test whether Z and X are jointly okay when both discrete
"however, with custom splits the checking happens in the first stage wrapper"
where we don't have all of the required information to do this;
we'd probably need to add it to _crossfit instead
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
The __warningregistry__'s need to be in a pristine state for tests
to work properly.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Create a wrapper around Lasso that doesn't support weights
since Lasso does natively support them starting in sklearn 0.23
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least 6 of every element
"2 outer splits, 3 inner splits when model_t is 'auto' and treatment is discrete"
NOTE: this number may need to change if the default number of folds in
WeightedStratifiedKFold changes
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
TODO: add stratification to bootstrap so that we can use it
even with discrete treatments
ensure we can serialize the unfit estimator
ensure we can pickle the fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
"TODO Add bootstrap inference, once discrete treatment issue is fixed"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
create a simple artificial setup where effect of moving from treatment
"a -> b is 2,"
"a -> c is 1, and"
"b -> c is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
should rule out some basic issues.
Note that explicitly specifying the dtype as object is necessary until
there's a fix for https://github.com/scikit-learn/scikit-learn/issues/15616
estimated effects should be identical when treatment is explicitly given
but const_marginal_effect should be reordered based on the explicit cagetories
1-> 2 in original ordering; combination of 3->1 and 3->2
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
ensure that we can serialize unfit estimator
TODO: add stratification to bootstrap so that we can use it even with discrete treatments
ensure that we can serialize fit estimator
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check that the point estimates are the same
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Call weighted lasso on reduced design matrix
Inherit some parameters from the parent
Weighted tau
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Construct the subsample of data
Split into estimation and splitting sample set
Fit the tree on the splitting sample
Set the estimation values based on the estimation split
Apply the trained tree on the estimation sample to get the path for every estimation sample
Calculate the total weight of estimation samples on each tree node:
\sum_i sample_weight[i] * 1{i \\in node}
Calculate the total number of estimation samples on each tree node:
|node| = \sum_{i} 1{i \\in node}
Calculate the weighted sum of responses on the estimation sample on each node:
\sum_{i} sample_weight[i] 1{i \\in node} Y_i
Prune tree to remove leafs that don't satisfy the leaf requirements on the estimation sample
and for each un-pruned tree set the value and the weight appropriately.
If minimum weight requirement or minimum leaf size requirement is not satisfied on estimation
"sample, then prune the whole sub-tree"
Set the value of the node to: \sum_{i} sample_weight[i] 1{i \\in node} Y_i / |node|
Set the weight of the node to: \sum_{i} sample_weight[i] 1{i \\in node} / |node|
Set the count to the estimation split count
Validate or convert input data
Pre-sort indices to avoid that each individual tree of the
ensemble sorts the indices.
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
"Free allocated memory, if any"
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
TODO. This slicing should ultimately be done inside the parallel function
so that we don't need to create a matrix of size roughly n_samples * n_estimators
Collect newly grown trees
Helper class that accumulates an arbitrary function in parallel on the accumulator acc
and calls the function fn on each tree e and returns the mean output. The function fn
"should take as input a tree e, and return another function g_e, which takes as input X, check_input"
"If slice is not None, but rather a tuple (start, end), then a subset of the trees from"
index start to index end will be used. The returned result is essentially:
(mean over e in slice)(g_e(X)).
Check data
Assign chunk of trees to jobs
Check data
Check data
avoid storing the output of every estimator by summing them here
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) Y_i"
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x)"
"Calculate for each slice S: Q(S) = 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) (Y_i - \theta(X))"
where \theta(X) is the point estimate using the whole forest
Calculate the variance of the latter as E[Q(S)^2]
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Require group assignment t to be one-hot-encoded
Define an inner function that iterates over group predictions
Convert rows to columns
Get predictions for the 2 splits
Must make sure indices are merged correctly
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Override the CATE inference options
Add blb inference to parent's options
Must normalize weights
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Override to flatten output if T is flat
T is flat
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
Copy and/or define models
Nuisance estimators shall be defined during fitting because they need to know the number of distinct
treatments
Define parameter estimators
Define moment and mean gradient estimator
Define autoencoder
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Define number of classes
Call `fit` from parent class
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Test expansion of treatment
"If expanded treatments are a vector, flatten const_marginal_effect_interval"
Calculate confidence intervals for the parameter (marginal effect)
"If T is a vector, preserve shape of the effect interval"
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
"Get a list of (parameter, covariance matrix) pairs"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support sample_var
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add statsmodels to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
#######################################
Core DML Tests
#######################################
How many samples
How many control features
How many treatment variables
Coefficients of how controls affect treatments
Coefficients of how controls affect outcome
Treatment effects that we want to estimate
Run dml estimation
How many samples
How many control features
How many treatment variables
Coefficients of how controls affect treatments
Coefficients of how controls affect outcome
Treatment effects that we want to estimate
Run dml estimation
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
store number of columns of T so that we can pass scalars to effect
TODO: support vector T and Y
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect interval as a function of const_marginal_effect_interval for a single treatment
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=1 here since we measure the effect across all Ts
once the estimator has been fit
We can write effect interval as a function of predict_interval of the final method for linear models
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
d_t=1 here since we measure the effect across all Ts
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=1 here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A cut-down version of the DML first stage wrapper, since we don't need to support W or linear first stages"
"if both X and Z are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
TODO: allow the final model to actually use X? Then we'd need to rename the class
since we would actually be calculating a CATE rather than ATE.
TODO: allow the final model to actually use X?
TODO: allow the final model to actually use X?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
TODO: would it be useful to extend to handle controls ala vanilla DML?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
"instruments, and outcomes"
TODO: how do we incorporate the sample_weight and sample_var passed into this method
as arguments?
TODO: is there a good way to incorporate the other nuisance terms in the score?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"TODO: check that Y, T, Z do not have multiple columns"
override only so that we can update the docstring to indicate support for `StatsModelsInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
TODO: support sample_var
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
add statsmodels to parent's options
Replacing to remove docstring
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: generalize to multiple treatment case?
get index of best treatment
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
#######################################################
Perfect Data DGPs for Testing Correctness of Code
#######################################################
Generate random control co-variates
Create epsilon residual treatments that deterministically sum up to
zero
Re-calibrate epsilon to make sure that empirical distribution of epsilon
conditional on each co-variate vector is equal to zero
We simply subtract the conditional mean from the epsilons
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Create epsilon residual treatments that deterministically sum up to
zero
Re-calibrate epsilon to make sure that empirical distribution of epsilon
conditional on each co-variate vector is equal to zero
We simply subtract the conditional mean from the epsilons
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Create epsilon residual treatments
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect + eta
Generate random control co-variates
Use the same treatment vector for each row
Construct outcomes as y = X*beta + T*effect
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
add statsmodels to parent's options
add debiasedlasso to parent's options
TODO Share some logic with non-discrete version
add statsmodels to parent's options
add statsmodels to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
Normalize weights
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
This class is mainly derived from statsmodels.iolib.summary.Summary
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use first target
Write node mean CATE
Write node std of CATE
Write confidence interval information if at leaf node
Fetch appropriate color for node
"red for negative, green for positive"
Write node mean CATE
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
drop first column since all columns sum to one
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Estimators
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
"try to get interval first if appropriate, since we don't prefer a wrapped method with this name"
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
can interpret without uncertainty
can't treat before interpreting
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
Multi-dimensional output y
1-d y
multi-d y
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test inference results when `cate_feature_names` doesn not exist
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Test non keyword based calls to fit
test non-array inputs
Test custom splitter
Test incomplete set of test folds
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
ensure that we've got at least two of every row
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
need to make sure we get all *joint* combinations
IntentToTreat only supports binary treatments/instruments
IntentToTreat only supports binary treatments/instruments
TODO: add stratification to bootstrap so that we can use it
even with discrete treatments
IntentToTreat requires X
these support only W but not X
"these support only binary, not general discrete T and Z"
make sure we can call the marginal_effect and effect methods
TODO: add tests for extra properties like coef_ where they exist
TODO: add tests for extra properties like coef_ where they exist
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
TODO: ideally we could also test whether Z and X are jointly okay when both discrete
"however, with custom splits the checking happens in the first stage wrapper"
where we don't have all of the required information to do this;
we'd probably need to add it to _crossfit instead
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
The __warningregistry__'s need to be in a pristine state for tests
to work properly.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
TODO: add stratification to bootstrap so that we can use it
even with discrete treatments
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
"TODO Add bootstrap inference, once discrete treatment issue is fixed"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"because there is one fewer unique element in the test set, fit_transform would return the wrong number of fts"
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
TODO: add stratification to bootstrap so that we can use it even with discrete treatments
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Call weighted lasso on reduced design matrix
Inherit some parameters from the parent
Weighted tau
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Construct the subsample of data
Split into estimation and splitting sample set
Fit the tree on the splitting sample
Set the estimation values based on the estimation split
Apply the trained tree on the estimation sample to get the path for every estimation sample
Calculate the total weight of estimation samples on each tree node:
\sum_i sample_weight[i] * 1{i \\in node}
Calculate the total number of estimation samples on each tree node:
|node| = \sum_{i} 1{i \\in node}
Calculate the weighted sum of responses on the estimation sample on each node:
\sum_{i} sample_weight[i] 1{i \\in node} Y_i
Prune tree to remove leafs that don't satisfy the leaf requirements on the estimation sample
and for each un-pruned tree set the value and the weight appropriately.
If minimum weight requirement or minimum leaf size requirement is not satisfied on estimation
"sample, then prune the whole sub-tree"
Set the value of the node to: \sum_{i} sample_weight[i] 1{i \\in node} Y_i / |node|
Set the weight of the node to: \sum_{i} sample_weight[i] 1{i \\in node} / |node|
Set the count to the estimation split count
Validate or convert input data
Pre-sort indices to avoid that each individual tree of the
ensemble sorts the indices.
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
"Free allocated memory, if any"
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
TODO. This slicing should ultimately be done inside the parallel function
so that we don't need to create a matrix of size roughly n_samples * n_estimators
Collect newly grown trees
Helper class that accumulates an arbitrary function in parallel on the accumulator acc
and calls the function fn on each tree e and returns the mean output. The function fn
"should take as input a tree e, and return another function g_e, which takes as input X, check_input"
"If slice is not None, but rather a tuple (start, end), then a subset of the trees from"
index start to index end will be used. The returned result is essentially:
(mean over e in slice)(g_e(X)).
Check data
Assign chunk of trees to jobs
Check data
Check data
avoid storing the output of every estimator by summing them here
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) Y_i"
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x)"
"Calculate for each slice S: Q(S) = 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) (Y_i - \theta(X))"
where \theta(X) is the point estimate using the whole forest
Calculate the variance of the latter as E[Q(S)^2]
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Require group assignment t to be one-hot-encoded
Define an inner function that iterates over group predictions
Convert rows to columns
Get predictions for the 2 splits
Must make sure indices are merged correctly
Estimators
OrthoForest parameters
Sub-forests
Auxiliary attributes
Fit check
TODO: Check performance
Override the CATE inference options
Add blb inference to parent's options
Must normalize weights
Crossfitting
Compute weighted nuisance estimates
-------------------------------------------------------------------------------
Calculate the covariance matrix corresponding to the BLB inference
""
1. Calculate the moments and gradient of the training data w.r.t the test point
2. Calculate the weighted moments for each tree slice to create a matrix
"U = (n_slices, n_T). The V = (U x grad^{-1}) matrix represents the deviation"
in that slice from the overall parameter estimate.
3. Calculate the covariance matrix (V.T x V) / n_slices
-------------------------------------------------------------------------------
Calclulate covariance matrix through BLB
Generate subsample indices
Build trees in parallel
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Define subsample size
Safety check
Draw points to create little bags
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Override to flatten output if T is flat
T is flat
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
Copy and/or define models
Nuisance estimators shall be defined during fitting because they need to know the number of distinct
treatments
Define parameter estimators
Define moment and mean gradient estimator
Define autoencoder
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Define number of classes
Call `fit` from parent class
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned is of shape (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Test whether the input estimator is supported
Test expansion of treatment
"If expanded treatments are a vector, flatten const_marginal_effect_interval"
Calculate confidence intervals for the parameter (marginal effect)
"If T is a vector, preserve shape of the effect interval"
Calculate confidence intervals for the effect
Calculate the effects
Calculate the standard deviations for the effects
"Get a list of (parameter, covariance matrix) pairs"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support sample_var
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add statsmodels to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
#######################################
Core DML Tests
#######################################
How many samples
How many control features
How many treatment variables
Coefficients of how controls affect treatments
Coefficients of how controls affect outcome
Treatment effects that we want to estimate
Run dml estimation
How many samples
How many control features
How many treatment variables
Coefficients of how controls affect treatments
Coefficients of how controls affect outcome
Treatment effects that we want to estimate
Run dml estimation
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
store number of columns of T so that we can pass scalars to effect
TODO: support vector T and Y
two stage approximation
"first, get basis expansions of T, X, and Z"
TODO: is it right that the effective number of intruments is the
"product of ft_X and ft_Z, not just ft_Z?"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect interval as a function of const_marginal_effect_interval for a single treatment
We can write effect inference as a function of const_marginal_effect_inference for a single treatment
d_t=1 here since we measure the effect across all Ts
once the estimator has been fit
We can write effect interval as a function of predict_interval of the final method for linear models
We can write effect inference as a function of prediction and prediction standard error of
the final method for linear models
d_t=1 here since we measure the effect across all Ts
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
"send treatment to the end, pull bounds to the front"
d_t=1 here since we measure the effect across all Ts
need to set the fit args before the estimator is fit
1. Uncertainty of Mean Point Estimate
2. Distribution of Point Estimate
3. Total Variance of Point Estimate
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A cut-down version of the DML first stage wrapper, since we don't need to support W or linear first stages"
"if both X and Z are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
"DMLATEIV is just like 2SLS; first regress T_res on Z_res, then regress Y_res on predicted T_res"
TODO: allow the final model to actually use X? Then we'd need to rename the class
since we would actually be calculating a CATE rather than ATE.
TODO: allow the final model to actually use X?
TODO: allow the final model to actually use X?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce X=None and improve the docstring"
TODO: would it be useful to extend to handle controls ala vanilla DML?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce W=None and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
"TODO: if opt_reweighted is False, we could change the logic to support multidimensional treatments,"
"instruments, and outcomes"
TODO: how do we incorporate the sample_weight and sample_var passed into this method
as arguments?
TODO: is there a good way to incorporate the other nuisance terms in the score?
"Replacing fit from _OrthoLearner, to enforce W=None and improve the docstring"
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
"we need to undo the one-hot encoding for calling effect,"
since it expects raw values
"TODO: check that Y, T, Z do not have multiple columns"
override only so that we can update the docstring to indicate support for `StatsModelsInference`
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
TODO: support sample_var
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
add statsmodels to parent's options
Replacing to remove docstring
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
#######################################################
Perfect Data DGPs for Testing Correctness of Code
#######################################################
Generate random control co-variates
Create epsilon residual treatments that deterministically sum up to
zero
Re-calibrate epsilon to make sure that empirical distribution of epsilon
conditional on each co-variate vector is equal to zero
We simply subtract the conditional mean from the epsilons
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Create epsilon residual treatments that deterministically sum up to
zero
Re-calibrate epsilon to make sure that empirical distribution of epsilon
conditional on each co-variate vector is equal to zero
We simply subtract the conditional mean from the epsilons
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Create epsilon residual treatments
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect + eta
Generate random control co-variates
Use the same treatment vector for each row
Construct outcomes as y = X*beta + T*effect
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
add statsmodels to parent's options
add debiasedlasso to parent's options
TODO Share some logic with non-discrete version
add statsmodels to parent's options
add statsmodels to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
Normalize weights
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
This class is mainly derived from statsmodels.iolib.summary.Summary
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use first target
Write node mean CATE
Write node std of CATE
Write confidence interval information if at leaf node
Fetch appropriate color for node
"red for negative, green for positive"
Write node mean CATE
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
drop first column since all columns sum to one
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Estimators
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
"try to get interval first if appropriate, since we don't prefer a wrapped method with this name"
AzureML
helper imports
write the details of the workspace to a configuration file to the notebook library
if y is a multioutput model
Make sure second dimension has 1 or more item
switch _inner Model to a MultiOutputRegressor
flatten array as automl only takes vectors for y
Inner single model to be passed that wrapper can use to pass into MultiOutputRegressor
Must be implemented for MultiOutputRegressor to view _InnerAutomatedMLModel
as an sklearn estimator
fit implementation for a single output model.
Create experiment for specified workspace
Configure automl_config with training set information.
"Wait for remote run to complete, the set the model"
"Loop through the kwargs and args if any of them is an AutoMLConfig file, pass them"
create model and pass model into final.
"If item is an automl config, get its corresponding"
AutomatedML Model and add it to new_Args
"If item is an automl config, get its corresponding"
AutomatedML Model and set it for this key in
kwargs
takes in either automated_ml config and instantiates
an AutomatedMLModel
The prefix can only be 18 characters long
"because prefixes come from kwarg_names, we must ensure they are"
short enough.
Get workspace from config file.
Take the intersect of the white for sample
weights and linear models
"show output is not stored in the config in AutomatedML, so we need to make it a field."
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
Multi-dimensional output y
1-d y
multi-d y
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Test non keyword based calls to fit
Test custom splitter
Test incomplete set of test folds
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: test something rather than just print...
"Note: no noise, just testing that we can exactly recover when we ought to be able to"
pick some arbitrary X
pick some arbitrary T
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
ensure that we've got at least two of every row
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
need to make sure we get all *joint* combinations
IntentToTreat only supports binary treatments/instruments
IntentToTreat only supports binary treatments/instruments
TODO: add stratification to bootstrap so that we can use it
even with discrete treatments
IntentToTreat requires X
these support only W but not X
"these support only binary, not general discrete T and Z"
make sure we can call the marginal_effect and effect methods
TODO: add tests for extra properties like coef_ where they exist
TODO: add tests for extra properties like coef_ where they exist
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
TODO: ideally we could also test whether Z and X are jointly okay when both discrete
"however, with custom splits the checking happens in the first stage wrapper"
where we don't have all of the required information to do this;
we'd probably need to add it to _crossfit instead
TODO: make IV related
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
The __warningregistry__'s need to be in a pristine state for tests
to work properly.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 20% test points to be outside of the confidence interval
Check that the intervals are not too wide
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
TODO: add stratification to bootstrap so that we can use it
even with discrete treatments
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
test coef__inference and intercept__inference
"ExitStack can be used as a ""do nothing"" ContextManager"
"ExitStack can be used as a ""do nothing"" ContextManager"
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
"TODO Add bootstrap inference, once discrete treatment issue is fixed"
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
TODO: add stratification to bootstrap so that we can use it even with discrete treatments
make sure we can call the marginal_effect and effect methods
test const marginal inference
test effect inference
test marginal effect inference
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
"for at least some of the examples, the CI should have nonzero width"
test coef__inference function works
test intercept__inference function works
test summary function works
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Linear models are required for parametric dml
sample weighting models are required for nonparametric dml
Test values
TLearner test
Instantiate TLearner
"Test constant and heterogeneous treatment effect, single and multi output y"
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate DomainAdaptationLearner
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Call weighted lasso on reduced design matrix
Inherit some parameters from the parent
Weighted tau
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_stderr_
intercept_stderr_
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Construct the subsample of data
Split into estimation and splitting sample set
Fit the tree on the splitting sample
Set the estimation values based on the estimation split
Apply the trained tree on the estimation sample to get the path for every estimation sample
Calculate the total weight of estimation samples on each tree node:
\sum_i sample_weight[i] * 1{i \\in node}
Calculate the total number of estimation samples on each tree node:
|node| = \sum_{i} 1{i \\in node}
Calculate the weighted sum of responses on the estimation sample on each node:
\sum_{i} sample_weight[i] 1{i \\in node} Y_i
Prune tree to remove leafs that don't satisfy the leaf requirements on the estimation sample
and for each un-pruned tree set the value and the weight appropriately.
If minimum weight requirement or minimum leaf size requirement is not satisfied on estimation
"sample, then prune the whole sub-tree"
Set the value of the node to: \sum_{i} sample_weight[i] 1{i \\in node} Y_i / |node|
Set the weight of the node to: \sum_{i} sample_weight[i] 1{i \\in node} / |node|
Set the count to the estimation split count
Validate or convert input data
Pre-sort indices to avoid that each individual tree of the
ensemble sorts the indices.
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
"Free allocated memory, if any"
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
TODO. This slicing should ultimately be done inside the parallel function
so that we don't need to create a matrix of size roughly n_samples * n_estimators
Collect newly grown trees
Helper class that accumulates an arbitrary function in parallel on the accumulator acc
and calls the function fn on each tree e and returns the mean output. The function fn
"should take as input a tree e, and return another function g_e, which takes as input X, check_input"
"If slice is not None, but rather a tuple (start, end), then a subset of the trees from"
index start to index end will be used. The returned result is essentially:
(mean over e in slice)(g_e(X)).
Check data
Assign chunk of trees to jobs
Check data
Check data
avoid storing the output of every estimator by summing them here
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) Y_i"
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x)"
"Calculate for each slice S: Q(S) = 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) (Y_i - \theta(X))"
where \theta(X) is the point estimate using the whole forest
Calculate the variance of the latter as E[Q(S)^2]
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Require group assignment t to be one-hot-encoded
Define an inner function that iterates over group predictions
Convert rows to columns
Get predictions for the 2 splits
Must make sure indices are merged correctly
Estimators
OrthoForest parameters
Sub-forests
Fit check
TODO: Check performance
Must normalize weights
Crossfitting
Compute weighted nuisance estimates
Generate subsample indices
Safety check
Build trees in parallel
Calculates weights
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned by LinearRegression is (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
Copy and/or define models
Nuisance estimators shall be defined during fitting because they need to know the number of distinct
treatments
Define parameter estimators
Define moment and mean gradient estimator
Define autoencoder
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Define number of classes
Call `fit` from parent class
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned by LinearRegression is (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support sample_var
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add statsmodels to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
#######################################
Core DML Tests
#######################################
How many samples
How many control features
How many treatment variables
Coefficients of how controls affect treatments
Coefficients of how controls affect outcome
Treatment effects that we want to estimate
Run dml estimation
How many samples
How many control features
How many treatment variables
Coefficients of how controls affect treatments
Coefficients of how controls affect outcome
Treatment effects that we want to estimate
Run dml estimation
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
store number of columns of T so that we can pass scalars to effect
two stage approximation
"first, get basis expansions of T, X, and Z"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect interval as a function of const_marginal_effect_interval for a single treatment
once the estimator has been fit
We can write effect interval as a function of predict_interval of the final method for linear models
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
need to set the fit args before the estimator is fit
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
TODO: support sample_var
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
add statsmodels to parent's options
Replacing to remove docstring
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
#######################################################
Perfect Data DGPs for Testing Correctness of Code
#######################################################
Generate random control co-variates
Create epsilon residual treatments that deterministically sum up to
zero
Re-calibrate epsilon to make sure that empirical distribution of epsilon
conditional on each co-variate vector is equal to zero
We simply subtract the conditional mean from the epsilons
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Create epsilon residual treatments that deterministically sum up to
zero
Re-calibrate epsilon to make sure that empirical distribution of epsilon
conditional on each co-variate vector is equal to zero
We simply subtract the conditional mean from the epsilons
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Create epsilon residual treatments
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect + eta
Generate random control co-variates
Use the same treatment vector for each row
Construct outcomes as y = X*beta + T*effect
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
add statsmodels to parent's options
add debiasedlasso to parent's options
TODO Share some logic with non-discrete version
add statsmodels to parent's options
add statsmodels to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
Normalize weights
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use first target
Write node mean CATE
Write node std of CATE
Write confidence interval information if at leaf node
Fetch appropriate color for node
"red for negative, green for positive"
Write node mean CATE
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
drop first column since all columns sum to one
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Estimators
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
"try to get interval first if appropriate, since we don't prefer a wrapped method with this name"
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
Multi-dimensional output y
1-d y
multi-d y
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Test non keyword based calls to fit
Test custom splitter
Test incomplete set of test folds
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
The __warningregistry__'s need to be in a pristine state for tests
to work properly.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
TODO: add stratification to bootstrap so that we can use it
even with discrete treatments
make sure we can call the marginal_effect and effect methods
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
"TODO Add bootstrap inference, once discrete treatment issue is fixed"
make sure we can call the marginal_effect and effect methods
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
TODO: add stratification to bootstrap so that we can use it even with discrete treatments
make sure we can call the marginal_effect and effect methods
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Call weighted lasso on reduced design matrix
Inherit some parameters from the parent
Weighted tau
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_std_err_
intercept_std_err_
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Construct the subsample of data
Split into estimation and splitting sample set
Fit the tree on the splitting sample
Set the estimation values based on the estimation split
Apply the trained tree on the estimation sample to get the path for every estimation sample
Calculate the total weight of estimation samples on each tree node:
\sum_i sample_weight[i] * 1{i \\in node}
Calculate the total number of estimation samples on each tree node:
|node| = \sum_{i} 1{i \\in node}
Calculate the weighted sum of responses on the estimation sample on each node:
\sum_{i} sample_weight[i] 1{i \\in node} Y_i
Prune tree to remove leafs that don't satisfy the leaf requirements on the estimation sample
and for each un-pruned tree set the value and the weight appropriately.
If minimum weight requirement or minimum leaf size requirement is not satisfied on estimation
"sample, then prune the whole sub-tree"
Set the value of the node to: \sum_{i} sample_weight[i] 1{i \\in node} Y_i / |node|
Set the weight of the node to: \sum_{i} sample_weight[i] 1{i \\in node} / |node|
Set the count to the estimation split count
Validate or convert input data
Pre-sort indices to avoid that each individual tree of the
ensemble sorts the indices.
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
"Free allocated memory, if any"
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
TODO. This slicing should ultimately be done inside the parallel function
so that we don't need to create a matrix of size roughly n_samples * n_estimators
Collect newly grown trees
Helper class that accumulates an arbitrary function in parallel on the accumulator acc
and calls the function fn on each tree e and returns the mean output. The function fn
"should take as input a tree e, and return another function g_e, which takes as input X, check_input"
"If slice is not None, but rather a tuple (start, end), then a subset of the trees from"
index start to index end will be used. The returned result is essentially:
(mean over e in slice)(g_e(X)).
Check data
Assign chunk of trees to jobs
Check data
Check data
avoid storing the output of every estimator by summing them here
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) Y_i"
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x)"
"Calculate for each slice S: Q(S) = 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) (Y_i - \theta(X))"
where \theta(X) is the point estimate using the whole forest
Calculate the variance of the latter as E[Q(S)^2]
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Require group assignment t to be one-hot-encoded
Define an inner function that iterates over group predictions
Convert rows to columns
Get predictions for the 2 splits
Must make sure indices are merged correctly
Estimators
OrthoForest parameters
Sub-forests
Fit check
TODO: Check performance
Must normalize weights
Crossfitting
Compute weighted nuisance estimates
Generate subsample indices
Safety check
Build trees in parallel
Calculates weights
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned by LinearRegression is (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
Copy and/or define models
Nuisance estimators shall be defined during fitting because they need to know the number of distinct
treatments
Define parameter estimators
Define moment and mean gradient estimator
Define autoencoder
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Define number of classes
Call `fit` from parent class
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned by LinearRegression is (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
This works both with our without the weighting trick as the treatments T are unit vector
treatments. And in the case of a weighting trick we also know that treatment is single-dimensional
A helper class that access all the internal fitted objects of a DML Cate Estimator. Used by
both Parametric and Non Parametric DML.
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support sample_var
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
add statsmodels to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
#######################################
Core DML Tests
#######################################
How many samples
How many control features
How many treatment variables
Coefficients of how controls affect treatments
Coefficients of how controls affect outcome
Treatment effects that we want to estimate
Run dml estimation
How many samples
How many control features
How many treatment variables
Coefficients of how controls affect treatments
Coefficients of how controls affect outcome
Treatment effects that we want to estimate
Run dml estimation
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
store number of columns of T so that we can pass scalars to effect
two stage approximation
"first, get basis expansions of T, X, and Z"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
We can write effect interval as a function of const_marginal_effect_interval for a single treatment
once the estimator has been fit
We can write effect interval as a function of predict_interval of the final method for linear models
"once the estimator has been fit, it's kosher to store d_t here"
(which needs to have been expanded if there's a discrete treatment)
need to set the fit args before the estimator is fit
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"TODO Allow for non-vector y, i.e. of shape (n, 1)"
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
TODO: support sample_var
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
add statsmodels to parent's options
Replacing to remove docstring
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
#######################################################
Perfect Data DGPs for Testing Correctness of Code
#######################################################
Generate random control co-variates
Create epsilon residual treatments that deterministically sum up to
zero
Re-calibrate epsilon to make sure that empirical distribution of epsilon
conditional on each co-variate vector is equal to zero
We simply subtract the conditional mean from the epsilons
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Create epsilon residual treatments that deterministically sum up to
zero
Re-calibrate epsilon to make sure that empirical distribution of epsilon
conditional on each co-variate vector is equal to zero
We simply subtract the conditional mean from the epsilons
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Create epsilon residual treatments
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect + eta
Generate random control co-variates
Use the same treatment vector for each row
Construct outcomes as y = X*beta + T*effect
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
add statsmodels to parent's options
add debiasedlasso to parent's options
TODO Share some logic with non-discrete version
add statsmodels to parent's options
add statsmodels to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
Normalize weights
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: We're relying on some of sklearn's non-public classes which are not completely stable.
"However, the alternative is reimplementing a bunch of intricate stuff by hand"
"HACK: it's not optimal to use a regex like this, but the base class's node_to_str doesn't expose any"
clean way of achieving this
make sure we don't accidentally escape anything in the substitution
Fetch appropriate color for node
"red for negative, green for positive"
in multi-target use first target
Write node mean CATE
Write node std of CATE
Write confidence interval information if at leaf node
Fetch appropriate color for node
"red for negative, green for positive"
Write node mean CATE
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
drop first column since all columns sum to one
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Estimators
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
"try to get interval first if appropriate, since we don't prefer a wrapped method with this name"
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"can't easily test output, but can at least test that we can all export_graphviz, render, and plot"
"prior to calling interpret, can't plot, render, etc."
can interpret without uncertainty
can't interpret with uncertainty if inference wasn't used during fit
can interpret with uncertainty if we refit
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
Multi-dimensional output y
1-d y
multi-d y
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Test non keyword based calls to fit
Test custom splitter
Test incomplete set of test folds
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
HACK: work around bug in assertWarns (https://bugs.python.org/issue29620)
this can be removed if the corresponding pull request (https://github.com/python/cpython/pull/4800) is ever merged
The __warningregistry__'s need to be in a pristine state for tests
to work properly.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params. Using n_jobs=1 since code coverage
does not work well with parallelism.
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
TODO: add stratification to bootstrap so that we can use it
even with discrete treatments
make sure we can call the marginal_effect and effect methods
"make sure we can call effect with implied scalar treatments,"
"no matter the dimensions of T, and also that we warn when there"
are multiple treatments
"ExitStack can be used as a ""do nothing"" ContextManager"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
"TODO Add bootstrap inference, once discrete treatment issue is fixed"
make sure we can call the marginal_effect and effect methods
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
We concatenate the two copies data
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
TODO: add stratification to bootstrap so that we can use it even with discrete treatments
make sure we can call the marginal_effect and effect methods
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
selective ridge has a simple implementation that we can test against
see https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution/164546#164546
"it should be the case that when we set fit_intercept to true,"
it doesn't matter whether the penalized model also fits an intercept or not
create an extra copy of rows with weight 2
"instead of a slice, explicitly return an array of indices"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Call weighted lasso on reduced design matrix
Inherit some parameters from the parent
Weighted tau
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_std_err_
intercept_std_err_
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
"The unpenalized model can't contain an intercept, because in the analysis above"
"we rely on the fact that M(X beta) = (M X) beta, but M(X beta + c) is not the same"
"as (M X) beta + c, so the learned coef and intercept will be wrong"
now regress X1 on y - X2 * beta2 to learn beta1
set coef_ and intercept_ attributes
Note that the penalized model should *not* have an intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Construct the subsample of data
Split into estimation and splitting sample set
Fit the tree on the splitting sample
Set the estimation values based on the estimation split
Apply the trained tree on the estimation sample to get the path for every estimation sample
Calculate the total weight of estimation samples on each tree node:
\sum_i sample_weight[i] * 1{i \\in node}
Calculate the total number of estimation samples on each tree node:
|node| = \sum_{i} 1{i \\in node}
Calculate the weighted sum of responses on the estimation sample on each node:
\sum_{i} sample_weight[i] 1{i \\in node} Y_i
Prune tree to remove leafs that don't satisfy the leaf requirements on the estimation sample
and for each un-pruned tree set the value and the weight appropriately.
If minimum weight requirement or minimum leaf size requirement is not satisfied on estimation
"sample, then prune the whole sub-tree"
Set the value of the node to: \sum_{i} sample_weight[i] 1{i \\in node} Y_i / |node|
Set the weight of the node to: \sum_{i} sample_weight[i] 1{i \\in node} / |node|
Set the count to the estimation split count
Validate or convert input data
Pre-sort indices to avoid that each individual tree of the
ensemble sorts the indices.
Remap output
reshape is necessary to preserve the data contiguity against vs
"[:, np.newaxis] that does not."
Check parameters
"Free allocated memory, if any"
We draw from the random state to get the random state we
would have got if we hadn't used a warm_start.
Parallel loop: we prefer the threading backend as the Cython code
for fitting the trees is internally releasing the Python GIL
making threading more efficient than multiprocessing in
"that case. However, for joblib 0.12+ we respect any"
"parallel_backend contexts set at a higher level,"
since correctness does not rely on using threads.
TODO. This slicing should ultimately be done inside the parallel function
so that we don't need to create a matrix of size roughly n_samples * n_estimators
Collect newly grown trees
Helper class that accumulates an arbitrary function in parallel on the accumulator acc
and calls the function fn on each tree e and returns the mean output. The function fn
"should take as input a tree e, and return another function g_e, which takes as input X, check_input"
"If slice is not None, but rather a tuple (start, end), then a subset of the trees from"
index start to index end will be used. The returned result is essentially:
(mean over e in slice)(g_e(X)).
Check data
Assign chunk of trees to jobs
Check data
Check data
avoid storing the output of every estimator by summing them here
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) Y_i"
"Calculate for each slice S: 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x)"
"Calculate for each slice S: Q(S) = 1/B_S \sum_{b\in S, i\in [n]} w_{b, i}(x) (Y_i - \theta(X))"
where \theta(X) is the point estimate using the whole forest
Calculate the variance of the latter as E[Q(S)^2]
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for doctest extension -------------------------------------------
we can document otherwise excluded entities here by returning False
or skip otherwise included entities by returning True
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
Check inputs
Check inputs
Estimate response function
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Create splits of causal tree
Make sure the correct exception is being rethrown
Must make sure indices are merged correctly
Require group assignment t to be one-hot-encoded
Define an inner function that iterates over group predictions
Convert rows to columns
Get predictions for the 2 splits
Must make sure indices are merged correctly
Estimators
OrthoForest parameters
Sub-forests
Fit check
TODO: Check performance
Must normalize weights
Crossfitting
Compute weighted nuisance estimates
Generate subsample indices
Safety check
Build trees in parallel
Calculates weights
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned by LinearRegression is (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
Copy and/or define models
Nuisance estimators shall be defined during fitting because they need to know the number of distinct
treatments
Define parameter estimators
Define moment and mean gradient estimator
Define autoencoder
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Define number of classes
Call `fit` from parent class
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned by LinearRegression is (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"TODO: consider whether we need more care around stateful featurizers,"
since we clone it and fit separate copies
"if both X and W are None, just return a column of ones"
"In this case, the Target is the one-hot-encoding of the treatment variable"
We need to go back to the label representation of the one-hot so as to call
the classifier.
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
NOTE This is used by the inference methods and has to be the overall featurizer. intended
for internal use by the library
NOTE This is used by the inference methods and is more for internal use to the library
override only so that we can update the docstring to indicate support for `StatsModelsInference`
TODO: support sample_var
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
#######################################
Core DML Tests
#######################################
How many samples
How many control features
How many treatment variables
Coefficients of how controls affect treatments
Coefficients of how controls affect outcome
Treatment effects that we want to estimate
Run dml estimation
How many samples
How many control features
How many treatment variables
Coefficients of how controls affect treatments
Coefficients of how controls affect outcome
Treatment effects that we want to estimate
Run dml estimation
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
store number of columns of T so that we can pass scalars to effect
two stage approximation
"first, get basis expansions of T, X, and Z"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"once the estimator has been fit, it's kosher to access its effect_op and store it here"
"(which needs to have seen the expanded d_t if there's a discrete treatment, etc.)"
"once the estimator has been fit, it's kosher to access its effect_op and store it here"
"(which needs to have seen the expanded d_t if there's a discrete treatment, etc.)"
need to set the fit args before the estimator is fit
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"TODO Allow for non-vector y, i.e. of shape (n, 1)"
Coding Remark: The reasoning around the multitask_model_final could have been simplified if
"we simply wrapped the model_final with a MultiOutputRegressor. However, because we also want"
"to allow even for model_final objects whose fit(X, y) can accept X=None"
"(e.g. the StatsModelsLinearRegression), we cannot take that route, because the MultiOutputRegressor"
checks that X is 2D array.
"Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing score from _OrthoLearner, to enforce Z=None and improve the docstring"
"Replacing fit from DRLearner, to add statsmodels inference in docstring"
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
"Replacing fit from DRLearner, to add debiasedlasso inference in docstring"
TODO: support sample_var
"Replacing this method which is invalid for this class, so that we make the"
dosctring empty and not appear in the docs.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
#######################################################
Perfect Data DGPs for Testing Correctness of Code
#######################################################
Generate random control co-variates
Create epsilon residual treatments that deterministically sum up to
zero
Re-calibrate epsilon to make sure that empirical distribution of epsilon
conditional on each co-variate vector is equal to zero
We simply subtract the conditional mean from the epsilons
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Create epsilon residual treatments that deterministically sum up to
zero
Re-calibrate epsilon to make sure that empirical distribution of epsilon
conditional on each co-variate vector is equal to zero
We simply subtract the conditional mean from the epsilons
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Create epsilon residual treatments
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect + eta
Generate random control co-variates
Use the same treatment vector for each row
Construct outcomes as y = X*beta + T*effect
Licensed under the MIT License.
"since inference objects can be stateful, we must copy it before fitting;"
otherwise this sequence wouldn't work:
"est1.fit(..., inference=inf)"
"est2.fit(..., inference=inf)"
est1.effect_interval(...)
because inf now stores state from fitting est2
call the wrapped fit method
NOTE: we call inference fit *after* calling the main fit method
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
if X is None then the shape of const_marginal_effect will be wrong because the number
of rows of T was not taken into account
need to store the *original* dimensions of T so that we can expand scalar inputs to match;
subclasses should overwrite self._d_t with post-transformed dimensions of T for generating treatments
"override effect to set defaults, which works with the new definition of _expand_treatments"
"NOTE: don't explicitly expand treatments here, because it's done in the super call"
add statsmodels to parent's options
add debiasedlasso to parent's options
TODO Share some logic with non-discrete version
add statsmodels to parent's options
add statsmodels to parent's options
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Our classes that derive from sklearn ones sometimes include
inherited docstrings that have embedded doctests; we need the following imports
so that they don't break.
Check if model is sparse enough for this model
"note that by default OneHotEncoder returns float64s, so need to convert to int"
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
tile T and repeat X along axis 0 (so that the duplicated rows of X remain consecutive)
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
Normalize weights
"TODO: Add other types of covariance estimation (e.g. Newey-West (HAC), HC2, HC3)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: allow different subsets for L1 and L2 regularization?
TODO: any better way to deal with sparsity?
TODO: any better way to deal with sparsity?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
use a binary array to get stratified split in case of discrete treatment
"if check_cv produced a new KFold or StratifiedKFold object, we need to set shuffle and random_state"
drop first column since all columns sum to one
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Estimators
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
group by product; sum and subtract original; divide by (n_p-1)
group by product; sum and subtract original; divide by (n_p-1)
"for now, require one feature per store/product combination"
TODO: would be nice to relax this somehow
"alphas vary by product, not by store"
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
"one cross-price term per product, which is based on the average price"
of all other goods sold at the same store in the same week
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
store-specific and product-specific gammas and betas (which are positively correlated)
"features: product dummies, store dummies"
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
"observe n_products * n_stores prices, same number of quantities"
"for direct regression comparisons, we need a pivoted version"
"""treatments"" for direct regression include treatments, plus treatments interacted with product dummies,"
"plus the same for ""group treatments"" (average treatment of other products in the same store/week)"
"for direct regression, we also need to append the features"
"(both the ""constant features"" as well as the normal ones)"
NOTE: need to set cv because default generic algorithm is super slow for sparse matrices
"alphas vary by product, not by store"
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
store-specific and product-specific gammas
store-specific and product-specific betas
"features: product dummies, store dummies"
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
we need only the prices for the compound model; all dummies are created internally
"observe n_products * n_stores prices, same number of quantities"
"simple results include treatments, plus treatments interacted with product dummies,"
for use with the direct methods
X should have 0 columns; we will instead pivot Y and fit against the features passed into the constructor
underspecified model
Y = alpha T + \sum_i alpha_i T_i + X beta + eta
T = X gamma + eps
how to score? distance from line of all solutions?
"given that 0, a, b, c, d is equivalent to x, a-x, b-x, c-x, d-x, compute the error"
"baselines: ridge, ridge-like (penalize alpha_i but not alpha_baseline or beta)"
"comparison: 2ml (OLS for T on X, OLS for Y on XxX^e, ridge or ridge-like for alphas)"
"features: one product dummy, one store dummy (each missing one level), constant"
"alphas vary by product, not by store"
store-specific and product-specific gammas
store-specific and product-specific betas
"features: product dummies, store dummies"
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
"columns: prices interacted with products (and constant), features"
"observe n_products * n_stores prices, same number of quantities"
use features starting at index 1+n_products to skip all prices
"pickleFile = open('pickledSparse_{0}_{1}_{2}_{3}.pickle'.format(n_exp, n_products, n_stores, n_weeks), 'rb')"
"alphass, ridges, lassos, doubleMls = pickle.load(pickleFile)"
pickleFile.close()
#############################################
Defining the parameters of Monte Carlo
#############################################
Estimation parameters
###################################################################
Estimating the parameters of the DGP with DML. Running multiple
Monte Carlo experiments.
###################################################################
Sparse coefficients of treatment as a function of co-variates
Coefficients of outcomes as a function of co-variates
"DGP. Create samples of data (y, T, X) from known truth"
DML Estimation.
Estimation with other methods for comparison
#########################################################
Plotting the results and saving
#########################################################
"plt.figure(figsize=(20, 10))"
"plt.subplot(1, 4, 1)"
"plt.title(""DML R^2: median {:.3f}, mean {:.3f}"".format(np.median(dml_r2score), np.mean(dml_r2score)))"
plt.hist(dml_r2score)
"plt.subplot(1, 4, 2)"
"plt.title(""Direct Lasso R^2: median {:.3f}, mean {:.3f}"".format(np.median(direct_r2score),"
np.mean(direct_r2score)))
plt.hist(direct_r2score)
"plt.subplot(1, 4, 3)"
"plt.title(""DML Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(dml_te),"
np.std(dml_te)))
plt.hist(np.array(dml_te).flatten())
"plt.subplot(1, 4, 4)"
"plt.title(""Direct Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(direct_te),"
np.std(direct_te)))
plt.hist(np.array(direct_te).flatten())
plt.tight_layout()
"plt.savefig(""r2_comparison.png"")"
Plotting the results and saving
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
"try to get interval first if appropriate, since we don't prefer a wrapped method with this name"
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make this test actually test something instead of generating images
Sparse coefficients of treatment as a function of co-variates
Coefficients of outcomes as a function of co-variates
"DGP. Create samples of data (y, T, X) from known truth"
"DGP. Create samples of data (y, T, X) from known truth"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
"NOTE: we use `obs = False` to get a confidence, rather than prediction, interval"
"statsmodels uses the last dimension instead of the first to store the confidence intervals,"
so we need to transpose the result
1-d output
2-d output
Single dimensional output y
Multi-dimensional output y
1-d y
multi-d y
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
test that we can do the same thing once we provide alpha explicitly
test that the lower and upper bounds differ
test that the estimated effect is usually within the bounds
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
test that we can do the same thing once we provide percentile bounds
test that the lower and upper bounds differ
TODO: test that the estimated effect is usually within the bounds
and that the true effect is also usually within the bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Test non keyword based calls to fit
Test custom splitter
Test incomplete set of test folds
"theta needs to be of dimension (1, d_t) if T is (n, d_t)"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
make sure cross product varies more slowly with first array
and that vectors are okay as inputs
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect
Constant treatment with multi output Y
Heterogeneous treatment
Heterogeneous treatment with multi output Y
TLearner test
Instantiate TLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate SLearner
Test inputs
Test constant treatment effect
Test constant treatment effect with multi output Y
Test heterogeneous treatment effect
Need interactions between T and features
Test heterogeneous treatment effect with multi output Y
Instantiate XLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Instantiate DomainAdaptationLearner
Test inputs
"Test constant and heterogeneous treatment effect, single and multi output y"
Get the true treatment effect
Get the true treatment effect
Fit learner and get the effect and marginal effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check whether the output shape is right
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
DGP coefficients
Generated outcomes
################
WeightedLasso #
################
Define weights
Define extended datasets
Range of alphas
Compare with Lasso
--> No intercept
--> With intercept
When DGP has no intercept
When DGP has intercept
--> Coerce coefficients to be positive
--> Toggle max_iter & tol
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
Mixed DGP scenario.
Define extended datasets
Define weights
Define multioutput
##################
WeightedLassoCV #
##################
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
Choose a smaller n to speed-up process
Compare fold weights
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
--> Force parameters to be positive
###########################
MultiTaskWeightedLassoCV #
###########################
Define alphas to test
Define splitter
Compare with MultiTaskLassoCV
--> No intercept
--> With intercept
Define weights
Define extended datasets
Define splitters
WeightedKFold splitter
Map weighted splitter to an extended splitter
Define alphas to test
Compare with LassoCV
--> No intercept
--> With intercept
################
DebiasedLasso #
################
Test DebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check 5-95 CI coverage for unit vectors
Test DebiasedLasso with weights for one DGP
Define weights
Define extended datasets
--> Check debiased coefficients
Define weights
Data from one DGP has weight 0. Check that we recover correct coefficients
--> Check debiased coeffcients
Test that attributes propagate correctly
Test MultiOutputDebiasedLasso without weights
--> Check debiased coeffcients without intercept
--> Check debiased coeffcients with intercept
--> Check CI coverage
Test MultiOutputDebiasedLasso with weights
Define weights
Define extended datasets
--> Check debiased coefficients
Unit vectors
Unit vectors
Check coeffcients and intercept are the same within tolerance
Check results are similar with tolerance 1e-6
Check if multitask
Check that same alpha is chosen
Check that the coefficients are similar
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
"Generating random segments aka binary features. We will use features 0,...,3 for heterogeneity."
The rest for controls. Just as an example.
Generating A/B test data
Generating an outcome with treatment effect heterogeneity. The first binary feature creates heterogeneity
We also have confounding on the first variable. We also have heteroskedastic errors.
Generate data with continuous treatments
Instantiate model with most of the default parameters
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
TODO: add stratification to bootstrap so that we can use it even with discrete treatments
make sure we can call the marginal_effect and effect methods
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
(incorrectly) use a final model with an intercept
"Because final model is fixed, actual values of T and Y don't matter"
Ensure reproducibility
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
ensure that we've got at least two of every element
"since T isn't passed to const_marginal_effect, defaults to one row if X is None"
TODO: add stratification to bootstrap so that we can use it even with discrete treatments
make sure we can call the marginal_effect and effect methods
"make sure we can call effect with implied scalar treatments, no matter the"
"dimensions of T, and also that we warn when there are multiple treatments"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
test that we can fit with a KFold instance
test that we can fit with a train/test iterable
Test inputs
self._test_inputs(DR_learner)
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Sparse DGP
Treatment effect coef
Other coefs
Features and controls
Test sparse estimator
"--> test coef_, intercept_"
--> test treatment effects
Restrict x_test to vectors of norm < 1
--> check inference
Check that a majority of true effects lie in the 5-95% CI
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Found a good split, return."
Record all splits in case the stratification by weight yeilds a worse partition
Reseed random generator and try again
"If KFold fails after n_trials, we try the next best thing: stratifying by weight groups"
"Found a good split, return."
Did not find a good split
Record the devaiation for the weight-stratified split to compare with KFold splits
Return most weight-balanced partition
Weight stratification algorithm
Sort weights for weight strata search
There are some leftover indices that have yet to be assigned
Append stratum splits to overall splits
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"Convert X, y into numpy arrays"
Define fit parameters
Some algorithms don't have a check_input option
Check weights array
Check that weights are size-compatible
Normalize inputs
Weight inputs
Fit base class without intercept
Fit Lasso
Reset intercept
The intercept is not calculated properly due the sqrt(weights) factor
so it must be recomputed
Fit lasso without weights
Make weighted splitter
Fit weighted model
Make weighted splitter
Fit weighted model
Select optimal penalty
Warn about consistency
"Convert X, y into numpy arrays"
Fit weighted lasso with user input
"Center X, y"
Calculate quantities that will be used later on. Account for centered data
Calculate coefficient and error variance
Add coefficient correction
Set coefficients and intercept standard errors
Set intercept
Return alpha to 'auto' state
"Note that in the case of no intercept, X_offset is 0"
Calculate the variance of the predictions
Calculate prediction confidence intervals
Assumes flattened y
Compute weighted residuals
To be done once per target. Assumes y can be flattened.
Assumes that X has already been offset
Special case: n_features=1
Compute Lasso coefficients for the columns of the design matrix
Call weighted lasso on reduced design matrix
Inherit some parameters from the parent
Weighted tau
Compute C_hat
Compute theta_hat
Allow for single output as well
"When only one output is passed in, the MultiOutputDebiasedLasso behaves like the DebiasedLasso"
Set coef_ attribute
Set intercept_ attribute
Set selected_alpha_ attribute
Set coef_std_err_
intercept_std_err_
set intercept_ attribute
set coef_ attribute
set alpha_ attribute
set alphas_ attribute
set n_iter_ attribute
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Calculate residuals
Estimate E[T_res | Z_res]
TODO. Deal with multi-class instrument
Calculate nuisances
Estimate E[T_res | Z_res]
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"We do a three way split, as typically a preliminary theta estimator would require"
many samples. So having 2/3 of the sample to train model_theta seems appropriate.
TODO. Deal with multi-class instrument
Estimate final model of theta(X) by minimizing the square loss:
"(prel_theta(X) + (Y_res - prel_theta(X) * T_res) * Z_res / cov[T,Z | X] - theta(X))^2"
"We clip the covariance so that it is bounded away from zero, so as to reduce variance"
at the expense of some small bias. For points with very small covariance we revert
to the model-based preliminary estimate and do not add the correction term.
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate r(Z) = E[Z | X] in cross fitting manner
Calculate residual T_res = T - p(X) and Z_res = Z - r(X)
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, Z | X] = E[(T-p(X))*(Z-r(X)) | X] = E[T*Z | X] - E[T |X]*E[Z | X]"
TODO. The solution below is not really a valid cross-fitting
as the test data are used to create the proj_t on the train
which in the second train-test loop is used to create the nuisance
cov on the test data. Hence the T variable of some sample
"is implicitly correlated with its cov nuisance, through this flow"
"of information. However, this seems a rather weak correlation."
The more kosher would be to do an internal nested cv loop for the T_XZ
model.
"Estimate h(X, Z) = E[T | X, Z] in cross fitting manner"
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
"Calculate residual T_res = T - p(X) and Z_res = h(Z, X) - p(X)"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"Estimate cov[T, E[T|X,Z] | X] = E[T * E[T|X,Z]] - E[T|X]^2"
#############################################################################
Classes for the DRIV implementation for the special case of intent-to-treat
A/B test
#############################################################################
Estimate preliminary theta in cross fitting manner
Estimate p(X) = E[T | X] in cross fitting manner
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross fitting manner
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"For DMLIV we also need a model for E[T | X, Z]. We use a classifier since T is binary"
"Because Z is also binary, we could have also done a more complex model_T_XZ, where we split"
the data based on Z=1 and Z=0 and fit a separate sub-model for each case.
model_T_XZ = lambda: model_clf()
#'days_visited': lambda:
"#X = np.random.uniform(-1, 1, size=(n, d))"
Turn strings into categories for numeric mapping
### Defining some generic regressors and classifiers
This a generic non-parametric regressor
"model = lambda: GradientBoostingRegressor(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001)"
"model = lambda: XGBWrapper(XGBRegressor(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='rmse', binary=False)"
model = lambda: RandomForestRegressor(n_estimators=100)
model = lambda: Lasso(alpha=0.0001) #CV(cv=5)
model = lambda: GradientBoostingRegressor(n_estimators=60)
model = lambda: LinearRegression(n_jobs=-1)
"This is a generic non-parametric classifier. We have to wrap it with the RegWrapper, because"
we want to use predict_proba and not predict. The RegWrapper calls predict_proba of the
underlying model whenever predict is called.
"model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=20, max_depth=3, min_samples_leaf=20,"
"n_iter_no_change=5, min_impurity_decrease=.001, tol=0.001))"
"model_clf = lambda: RegWrapper(XGBWrapper(XGBClassifier(gamma=0.001, n_estimators=50, min_child_weight=50, n_jobs=10),"
"early_stopping_rounds=5, eval_metric='logloss', binary=True))"
model_clf = lambda: RandomForestClassifier(n_estimators=100)
model_clf = lambda: RegWrapper(GradientBoostingClassifier(n_estimators=60))
"model_clf = lambda: RegWrapper(LogisticRegression(C=10, penalty='l1', solver='liblinear'))"
We need to specify models to be used for each of these residualizations
model_Z_X = lambda: model_clf() # model for E[Z | X]. We use a classifier since Z is binary
"E[T | X, Z]"
E[TZ | X]
We fit DMLATEIV with these models and then we call effect() to get the ATE.
n_splits determines the number of splits to be used for cross-fitting.
# Algorithm 2 - Current Method
In[121]:
# Algorithm 3 - DRIV ATE
dmliv_model_effect = lambda: model()
"prel_model_effect = GenericDMLIV(model_Y_X(), model_T_X(), model_T_XZ(),"
"dmliv_model_effect(),"
n_splits=1)
reshape in case we get fewer dimensions than expected from h (e.g. a scalar)
"HACK: DRIV doesn't expect a treatment dimension, so pretend we got a vector even if we really had a one-column array"
"Once multiple treatments are supported, we'll need to fix this"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
"A wrapper of statsmodel linear regression, wrapped in a sklearn interface."
We can use statsmodel for all hypothesis testing capabilities
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO. Deal with multi-class instrument/treatment
"Estimate h(Z, X) = E[T | Z, X] in cross-fitting manner"
Estimate residual Y_res = Y - q(X) = Y - E[Y | X] in cross-fitting manner
Estimate p(X) = E[T | X] in cross-fitting manner
"Estimate theta by minimizing square loss (Y_res - theta(X) * (h(Z, X) - p(X)))^2"
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
Check inputs
Check inputs
Check inputs
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Check inputs
Check inputs
Fit outcome model on X||W||T (concatenated)
Check inputs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Create splits of causal tree
Must make sure indices are merged correctly
Require group assignment t to be one-hot-encoded
Define an inner function that iterates over group predictions
Convert rows to columns
Get predictions for the 2 splits
Must make sure indices are merged correctly
Estimators
OrthoForest parameters
Sub-forests
Fit check
TODO: Check performance
Must normalize weights
Crossfitting
Compute weighted nuisance estimates
Generate subsample indices
Safety check
Build trees in parallel
Calculates weights
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned by LinearRegression is (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
Copy and/or define models
Nuisance estimators shall be defined during fitting because they need to know the number of distinct
treatments
Define parameter estimators
Define moment and mean gradient estimator
Define autoencoder
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Define number of classes
Call `fit` from parent class
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned by LinearRegression is (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"TODO: If T is a vector rather than a 2-D array, then the model's fit must accept a vector..."
"Do we want to reshape to an nx1, or just trust the user's choice of input?"
(Likewise for Y below)
need to override effect in case of discrete treatments
TODO: should this logic be moved up to the LinearCateEstimator class and
removed from here and from the OrthoForest implementation?
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
create an identity matrix of size d_t (or just a 1-element array if T was a vector)
the nth row will allow us to compute the marginal effect of the nth component of treatment
TODO: Doing this kronecker/reshaping/transposing stuff so that predict can be called
"rather than just using coef_ seems silly, but one benefit is that we can use linear models"
that don't expose a coef_ (e.g. a GridSearchCV over underlying linear models)
TODO: handle case where final model doesn't directly expose coef_?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
#######################################
Core DML Tests
#######################################
How many samples
How many control features
How many treatment variables
Coefficients of how controls affect treatments
Coefficients of how controls affect outcome
Treatment effects that we want to estimate
Run dml estimation
How many samples
How many control features
How many treatment variables
Coefficients of how controls affect treatments
Coefficients of how controls affect outcome
Treatment effects that we want to estimate
Run dml estimation
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
store number of columns of W so that we can create correctly shaped zero array in effect and marginal effect
store number of columns of T so that we can pass scalars to effect
two stage approximation
"first, get basis expansions of T, X, and Z"
"regress T expansion on X,Z expansions concatenated with W"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
"in case vectors were passed for Y or T, keep track of trailing dims for reshaping effect output"
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
#######################################################
Perfect Data DGPs for Testing Correctness of Code
#######################################################
Generate random control co-variates
Create epsilon residual treatments that deterministically sum up to
zero
Re-calibrate epsilon to make sure that empirical distribution of epsilon
conditional on each co-variate vector is equal to zero
We simply subtract the conditional mean from the epsilons
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Create epsilon residual treatments that deterministically sum up to
zero
Re-calibrate epsilon to make sure that empirical distribution of epsilon
conditional on each co-variate vector is equal to zero
We simply subtract the conditional mean from the epsilons
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Create epsilon residual treatments
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect + eta
Generate random control co-variates
Use the same treatment vector for each row
Construct outcomes as y = X*beta + T*effect
Licensed under the MIT License.
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
"TODO: if T0 or T1 are scalars, we'll promote them to vectors;"
should it be possible to promote them to 2D arrays if that's what we saw during training?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
TODO: wouldn't making X1 vary more slowly than X2 be more intuitive?
(but note that changing this would necessitate changes to callers
to switch the order to preserve behavior where order is important)
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
Normalize weights
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: allow different subsets for L1 and L2 regularization?
TODO: any better way to deal with sparsity?
TODO: any better way to deal with sparsity?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Estimators
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
group by product; sum and subtract original; divide by (n_p-1)
group by product; sum and subtract original; divide by (n_p-1)
"for now, require one feature per store/product combination"
TODO: would be nice to relax this somehow
"alphas vary by product, not by store"
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
"one cross-price term per product, which is based on the average price"
of all other goods sold at the same store in the same week
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
store-specific and product-specific gammas and betas (which are positively correlated)
"features: product dummies, store dummies"
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
"observe n_products * n_stores prices, same number of quantities"
"for direct regression comparisons, we need a pivoted version"
"""treatments"" for direct regression include treatments, plus treatments interacted with product dummies,"
"plus the same for ""group treatments"" (average treatment of other products in the same store/week)"
"for direct regression, we also need to append the features"
"(both the ""constant features"" as well as the normal ones)"
NOTE: need to set cv because default generic algorithm is super slow for sparse matrices
"alphas vary by product, not by store"
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
store-specific and product-specific gammas
store-specific and product-specific betas
"features: product dummies, store dummies"
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
we need only the prices for the compound model; all dummies are created internally
"observe n_products * n_stores prices, same number of quantities"
"simple results include treatments, plus treatments interacted with product dummies,"
for use with the direct methods
X should have 0 columns; we will instead pivot Y and fit against the features passed into the constructor
underspecified model
Y = alpha T + \sum_i alpha_i T_i + X beta + eta
T = X gamma + eps
how to score? distance from line of all solutions?
"given that 0, a, b, c, d is equivalent to x, a-x, b-x, c-x, d-x, compute the error"
"baselines: ridge, ridge-like (penalize alpha_i but not alpha_baseline or beta)"
"comparison: 2ml (OLS for T on X, OLS for Y on XxX^e, ridge or ridge-like for alphas)"
"features: one product dummy, one store dummy (each missing one level), constant"
"alphas vary by product, not by store"
store-specific and product-specific gammas
store-specific and product-specific betas
"features: product dummies, store dummies"
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
"columns: prices interacted with products (and constant), features"
"observe n_products * n_stores prices, same number of quantities"
use features starting at index 1+n_products to skip all prices
"pickleFile = open('pickledSparse_{0}_{1}_{2}_{3}.pickle'.format(n_exp, n_products, n_stores, n_weeks), 'rb')"
"alphass, ridges, lassos, doubleMls = pickle.load(pickleFile)"
pickleFile.close()
#############################################
Defining the parameters of Monte Carlo
#############################################
Estimation parameters
###################################################################
Estimating the parameters of the DGP with DML. Running multiple
Monte Carlo experiments.
###################################################################
Sparse coefficients of treatment as a function of co-variates
Coefficients of outcomes as a function of co-variates
"DGP. Create samples of data (y, T, X) from known truth"
DML Estimation.
Estimation with other methods for comparison
#########################################################
Plotting the results and saving
#########################################################
"plt.figure(figsize=(20, 10))"
"plt.subplot(1, 4, 1)"
"plt.title(""DML R^2: median {:.3f}, mean {:.3f}"".format(np.median(dml_r2score), np.mean(dml_r2score)))"
plt.hist(dml_r2score)
"plt.subplot(1, 4, 2)"
"plt.title(""Direct Lasso R^2: median {:.3f}, mean {:.3f}"".format(np.median(direct_r2score),"
np.mean(direct_r2score)))
plt.hist(direct_r2score)
"plt.subplot(1, 4, 3)"
"plt.title(""DML Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(dml_te),"
np.std(dml_te)))
plt.hist(np.array(dml_te).flatten())
"plt.subplot(1, 4, 4)"
"plt.title(""Direct Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(direct_te),"
np.std(direct_te)))
plt.hist(np.array(direct_te).flatten())
plt.tight_layout()
"plt.savefig(""r2_comparison.png"")"
Plotting the results and saving
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
TODO: what if some args can be None?
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make this test actually test something instead of generating images
Sparse coefficients of treatment as a function of co-variates
Coefficients of outcomes as a function of co-variates
"DGP. Create samples of data (y, T, X) from known truth"
"DGP. Create samples of data (y, T, X) from known truth"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
also test vector t and y
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that we can do the same thing once we provide percentile bounds
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that we can do the same thing once we provide percentile bounds
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that the lower and upper bounds differ
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that we can do the same thing once we provide percentile bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivatives are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
TLearner test
Instantiate TLearner
Test inputs
Test constant treatment effect
Test heterogeneous treatment effect
Instantiate SLearner
Test inputs
Test constant treatment effect
Test heterogeneous treatment effect
Need interactions between T and features
Instantiate XLearner
Test inputs
Test constant treatment effect
Test heterogeneous treatment effect
Instantiate DomainAdaptationLearner
Test inputs
Test constant treatment effect
Test heterogeneous treatment effect
Instantiate DomainAdaptationLearner
Test inputs
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
just make sure we can call the marginal_effect and effect methods
"for vector-valued T, verify that default scalar T0 and T1 work"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
TODO: it seems like roughly 20% of the calls to _test_sparse are failing - find out what's going wrong
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
note that this would fail for the non-sparse DMLCateEstimator
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
Check inputs
Check inputs
Check inputs
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Check inputs
Check inputs
Fit outcome model on X||W||T (concatenated)
Check inputs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Create splits of causal tree
Must make sure indices are merged correctly
Require group assignment t to be one-hot-encoded
Define an inner function that iterates over group predictions
Convert rows to columns
Get predictions for the 2 splits
Must make sure indices are merged correctly
Estimators
OrthoForest parameters
Sub-forests
Fit check
TODO: Check performance
Must normalize weights
Crossfitting
Compute weighted nuisance estimates
Generate subsample indices
Safety check
Build trees in parallel
Calculates weights
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned by LinearRegression is (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
Copy and/or define models
Nuisance estimators shall be defined during fitting because they need to know the number of distinct
treatments
Define parameter estimators
Define moment and mean gradient estimator
Define autoencoder
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Define number of classes
Call `fit` from parent class
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned by LinearRegression is (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"TODO: If T is a vector rather than a 2-D array, then the model's fit must accept a vector..."
"Do we want to reshape to an nx1, or just trust the user's choice of input?"
(Likewise for Y below)
need to override effect in case of discrete treatments
TODO: should this logic be moved up to the LinearCateEstimator class and
removed from here and from the OrthoForest implementation?
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
create an identity matrix of size d_t (or just a 1-element array if T was a vector)
the nth row will allow us to compute the marginal effect of the nth component of treatment
TODO: Doing this kronecker/reshaping/transposing stuff so that predict can be called
"rather than just using coef_ seems silly, but one benefit is that we can use linear models"
that don't expose a coef_ (e.g. a GridSearchCV over underlying linear models)
TODO: handle case where final model doesn't directly expose coef_?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
#######################################
Core DML Tests
#######################################
How many samples
How many control features
How many treatment variables
Coefficients of how controls affect treatments
Coefficients of how controls affect outcome
Treatment effects that we want to estimate
Run dml estimation
How many samples
How many control features
How many treatment variables
Coefficients of how controls affect treatments
Coefficients of how controls affect outcome
Treatment effects that we want to estimate
Run dml estimation
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
two stage approximation
"first, get basis expansions of T, X, and Z"
"regress T expansion on X,Z expansions"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
TODO: allow 1D arguments for Y and T
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
#######################################################
Perfect Data DGPs for Testing Correctness of Code
#######################################################
Generate random control co-variates
Create epsilon residual treatments that deterministically sum up to
zero
Re-calibrate epsilon to make sure that empirical distribution of epsilon
conditional on each co-variate vector is equal to zero
We simply subtract the conditional mean from the epsilons
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Create epsilon residual treatments that deterministically sum up to
zero
Re-calibrate epsilon to make sure that empirical distribution of epsilon
conditional on each co-variate vector is equal to zero
We simply subtract the conditional mean from the epsilons
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Create epsilon residual treatments
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect + eta
Generate random control co-variates
Use the same treatment vector for each row
Construct outcomes as y = X*beta + T*effect
Licensed under the MIT License.
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
"TODO: if T0 or T1 are scalars, we'll promote them to vectors;"
should it be possible to promote them to 2D arrays if that's what we saw during training?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
TODO: wouldn't making X1 vary more slowly than X2 be more intuitive?
(but note that changing this would necessitate changes to callers
to switch the order to preserve behavior where order is important)
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
Normalize weights
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: allow different subsets for L1 and L2 regularization?
TODO: any better way to deal with sparsity?
TODO: any better way to deal with sparsity?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Estimators
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
group by product; sum and subtract original; divide by (n_p-1)
group by product; sum and subtract original; divide by (n_p-1)
"for now, require one feature per store/product combination"
TODO: would be nice to relax this somehow
"alphas vary by product, not by store"
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
"one cross-price term per product, which is based on the average price"
of all other goods sold at the same store in the same week
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
store-specific and product-specific gammas and betas (which are positively correlated)
"features: product dummies, store dummies"
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
"observe n_products * n_stores prices, same number of quantities"
"for direct regression comparisons, we need a pivoted version"
"""treatments"" for direct regression include treatments, plus treatments interacted with product dummies,"
"plus the same for ""group treatments"" (average treatment of other products in the same store/week)"
"for direct regression, we also need to append the features"
"(both the ""constant features"" as well as the normal ones)"
NOTE: need to set cv because default generic algorithm is super slow for sparse matrices
"alphas vary by product, not by store"
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
store-specific and product-specific gammas
store-specific and product-specific betas
"features: product dummies, store dummies"
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
we need only the prices for the compound model; all dummies are created internally
"observe n_products * n_stores prices, same number of quantities"
"simple results include treatments, plus treatments interacted with product dummies,"
for use with the direct methods
X should have 0 columns; we will instead pivot Y and fit against the features passed into the constructor
underspecified model
Y = alpha T + \sum_i alpha_i T_i + X beta + eta
T = X gamma + eps
how to score? distance from line of all solutions?
"given that 0, a, b, c, d is equivalent to x, a-x, b-x, c-x, d-x, compute the error"
"baselines: ridge, ridge-like (penalize alpha_i but not alpha_baseline or beta)"
"comparison: 2ml (OLS for T on X, OLS for Y on XxX^e, ridge or ridge-like for alphas)"
"features: one product dummy, one store dummy (each missing one level), constant"
"alphas vary by product, not by store"
store-specific and product-specific gammas
store-specific and product-specific betas
"features: product dummies, store dummies"
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
"columns: prices interacted with products (and constant), features"
"observe n_products * n_stores prices, same number of quantities"
use features starting at index 1+n_products to skip all prices
"pickleFile = open('pickledSparse_{0}_{1}_{2}_{3}.pickle'.format(n_exp, n_products, n_stores, n_weeks), 'rb')"
"alphass, ridges, lassos, doubleMls = pickle.load(pickleFile)"
pickleFile.close()
#############################################
Defining the parameters of Monte Carlo
#############################################
Estimation parameters
###################################################################
Estimating the parameters of the DGP with DML. Running multiple
Monte Carlo experiments.
###################################################################
Sparse coefficients of treatment as a function of co-variates
Coefficients of outcomes as a function of co-variates
"DGP. Create samples of data (y, T, X) from known truth"
DML Estimation.
Estimation with other methods for comparison
#########################################################
Plotting the results and saving
#########################################################
"plt.figure(figsize=(20, 10))"
"plt.subplot(1, 4, 1)"
"plt.title(""DML R^2: median {:.3f}, mean {:.3f}"".format(np.median(dml_r2score), np.mean(dml_r2score)))"
plt.hist(dml_r2score)
"plt.subplot(1, 4, 2)"
"plt.title(""Direct Lasso R^2: median {:.3f}, mean {:.3f}"".format(np.median(direct_r2score),"
np.mean(direct_r2score)))
plt.hist(direct_r2score)
"plt.subplot(1, 4, 3)"
"plt.title(""DML Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(dml_te),"
np.std(dml_te)))
plt.hist(np.array(dml_te).flatten())
"plt.subplot(1, 4, 4)"
"plt.title(""Direct Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(direct_te),"
np.std(direct_te)))
plt.hist(np.array(direct_te).flatten())
plt.tight_layout()
"plt.savefig(""r2_comparison.png"")"
Plotting the results and saving
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
TODO: what if some args can be None?
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make this test actually test something instead of generating images
Sparse coefficients of treatment as a function of co-variates
Coefficients of outcomes as a function of co-variates
"DGP. Create samples of data (y, T, X) from known truth"
"DGP. Create samples of data (y, T, X) from known truth"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that we can do the same thing once we provide percentile bounds
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that we can do the same thing once we provide percentile bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivitaves are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
TLearner test
Instantiate TLearner
Test inputs
Test constant treatment effect
Test heterogeneous treatment effect
Instantiate SLearner
Test inputs
Test constant treatment effect
Test heterogeneous treatment effect
Need interactions between T and features
Instantiate XLearner
Test inputs
Test constant treatment effect
Test heterogeneous treatment effect
Instantiate DomainAdaptationLearner
Test inputs
Test constant treatment effect
Test heterogeneous treatment effect
Instantiate DomainAdaptationLearner
Test inputs
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
just make sure we can call the marginal_effect and effect methods
"for vector-valued T, verify that default scalar T0 and T1 work"
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
TODO: it seems like roughly 20% of the calls to _test_sparse are failing - find out what's going wrong
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
note that this would fail for the non-sparse DMLCateEstimator
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
Check inputs
Check inputs
Check inputs
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Check inputs
Check inputs
Fit outcome model on X||W||T (concatenated)
Check inputs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Create splits of causal tree
Must make sure indices are merged correctly
Require group assignment t to be one-hot-encoded
Define an inner function that iterates over group predictions
Convert rows to columns
Get predictions for the 2 splits
Must make sure indices are merged correctly
Estimators
OrthoForest parameters
Sub-forests
Fit check
TODO: Check performance
Must normalize weights
Crossfitting
Compute weighted nuisance estimates
Generate subsample indices
Safety check
Build trees in parallel
Calculates weights
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
need safe=False when cloning for WeightedModelWrapper
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Compute residuals
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned by LinearRegression is (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
Copy and/or define models
Nuisance estimators shall be defined during fitting because they need to know the number of distinct
treatments
Define parameter estimators
Define moment and mean gradient estimator
Define autoencoder
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Define number of classes
Call `fit` from parent class
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Compute partial moments
Compute coefficient by OLS on residuals
ell_2 regularization
Ridge regression estimate
"Parameter returned by LinearRegression is (d_T, )"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"TODO: If T is a vector rather than a 2-D array, then the model's fit must accept a vector..."
"Do we want to reshape to an nx1, or just trust the user's choice of input?"
(Likewise for Y below)
need to override effect in case of discrete treatments
TODO: should this logic be moved up to the LinearCateEstimator class and
removed from here and from the OrthoForest implementation?
Track training dimensions to see if Y or T is a vector instead of a 2-dimensional array
create an identity matrix of size d_t (or just a 1-element array if T was a vector)
the nth row will allow us to compute the marginal effect of the nth component of treatment
TODO: Doing this kronecker/reshaping/transposing stuff so that predict can be called
"rather than just using coef_ seems silly, but one benefit is that we can use linear models"
that don't expose a coef_ (e.g. a GridSearchCV over underlying linear models)
TODO: handle case where final model doesn't directly expose coef_?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
#######################################
Core DML Tests
#######################################
How many samples
How many control features
How many treatment variables
Coefficients of how controls affect treatments
Coefficients of how controls affect outcome
Treatment effects that we want to estimate
Run dml estimation
How many samples
How many control features
How many treatment variables
Coefficients of how controls affect treatments
Coefficients of how controls affect outcome
Treatment effects that we want to estimate
Run dml estimation
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
two stage approximation
"first, get basis expansions of T, X, and Z"
"regress T expansion on X,Z expansions"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
TODO: allow 1D arguments for Y and T
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
#######################################################
Perfect Data DGPs for Testing Correctness of Code
#######################################################
Generate random control co-variates
Create epsilon residual treatments that deterministically sum up to
zero
Re-calibrate epsilon to make sure that empirical distribution of epsilon
conditional on each co-variate vector is equal to zero
We simply subtract the conditional mean from the epsilons
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Create epsilon residual treatments that deterministically sum up to
zero
Re-calibrate epsilon to make sure that empirical distribution of epsilon
conditional on each co-variate vector is equal to zero
We simply subtract the conditional mean from the epsilons
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Create epsilon residual treatments
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect + eta
Generate random control co-variates
Use the same treatment vector for each row
Construct outcomes as y = X*beta + T*effect
Licensed under the MIT License.
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
TODO: wouldn't making X1 vary more slowly than X2 be more intuitive?
(but note that changing this would necessitate changes to callers
to switch the order to preserve behavior where order is important)
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
Normalize weights
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: allow different subsets for L1 and L2 regularization?
TODO: any better way to deal with sparsity?
TODO: any better way to deal with sparsity?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Estimators
Causal tree parameters
Tree structure
No need for a random split since the data is already
a random subsample from the original input
node list stores the nodes that are yet to be splitted
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
a split is determined by a feature and a sample pair
the number of possible splits is at most (number of features) * (number of node samples)
"we draw random such pairs by drawing a random number in {0, n_feats * n_node_samples}"
parse row and column of random pair
the sample of the pair is the integer division of the random number with n_feats
calculate the binary indicator of whether sample i is on the left or the right
side of proposed split j. So this is an n_samples x n_proposals matrix
calculate the number of samples on the left child for each proposed split
calculate the analogous binary indicator for the samples in the estimation set
calculate the number of estimation samples on the left child of each proposed split
find the upper and lower bound on the size of the left split for the split
to be valid so as for the split to be balanced and leave at least min_leaf_size
on each side.
similarly for the estimation sample set
if there is no valid split then don't create any children
filter only the valid splits
calculate the average influence vector of the samples in the left child
calculate the average influence vector of the samples in the right child
take the square of each of the entries of the influence vectors and normalize
by size of each child
calculate the vector score of each candidate split as the average of left and right
influence vectors
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
calculate the scalar score of each split by aggregating across the vector of scores
Find split that minimizes criterion
Create child nodes with corresponding subsamples
add the created children to the list of not yet split nodes
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
group by product; sum and subtract original; divide by (n_p-1)
group by product; sum and subtract original; divide by (n_p-1)
"for now, require one feature per store/product combination"
TODO: would be nice to relax this somehow
"alphas vary by product, not by store"
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
"one cross-price term per product, which is based on the average price"
of all other goods sold at the same store in the same week
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
store-specific and product-specific gammas and betas (which are positively correlated)
"features: product dummies, store dummies"
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
"observe n_products * n_stores prices, same number of quantities"
"for direct regression comparisons, we need a pivoted version"
"""treatments"" for direct regression include treatments, plus treatments interacted with product dummies,"
"plus the same for ""group treatments"" (average treatment of other products in the same store/week)"
"for direct regression, we also need to append the features"
"(both the ""constant features"" as well as the normal ones)"
NOTE: need to set cv because default generic algorithm is super slow for sparse matrices
"alphas vary by product, not by store"
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
store-specific and product-specific gammas
store-specific and product-specific betas
"features: product dummies, store dummies"
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
we need only the prices for the compound model; all dummies are created internally
"observe n_products * n_stores prices, same number of quantities"
"simple results include treatments, plus treatments interacted with product dummies,"
for use with the direct methods
X should have 0 columns; we will instead pivot Y and fit against the features passed into the constructor
underspecified model
Y = alpha T + \sum_i alpha_i T_i + X beta + eta
T = X gamma + eps
how to score? distance from line of all solutions?
"given that 0, a, b, c, d is equivalent to x, a-x, b-x, c-x, d-x, compute the error"
"baselines: ridge, ridge-like (penalize alpha_i but not alpha_baseline or beta)"
"comparison: 2ml (OLS for T on X, OLS for Y on XxX^e, ridge or ridge-like for alphas)"
"features: one product dummy, one store dummy (each missing one level), constant"
"alphas vary by product, not by store"
store-specific and product-specific gammas
store-specific and product-specific betas
"features: product dummies, store dummies"
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
"columns: prices interacted with products (and constant), features"
"observe n_products * n_stores prices, same number of quantities"
use features starting at index 1+n_products to skip all prices
"pickleFile = open('pickledSparse_{0}_{1}_{2}_{3}.pickle'.format(n_exp, n_products, n_stores, n_weeks), 'rb')"
"alphass, ridges, lassos, doubleMls = pickle.load(pickleFile)"
pickleFile.close()
#############################################
Defining the parameters of Monte Carlo
#############################################
Estimation parameters
###################################################################
Estimating the parameters of the DGP with DML. Running multiple
Monte Carlo experiments.
###################################################################
Sparse coefficients of treatment as a function of co-variates
Coefficients of outcomes as a function of co-variates
"DGP. Create samples of data (y, T, X) from known truth"
DML Estimation.
Estimation with other methods for comparison
#########################################################
Plotting the results and saving
#########################################################
"plt.figure(figsize=(20, 10))"
"plt.subplot(1, 4, 1)"
"plt.title(""DML R^2: median {:.3f}, mean {:.3f}"".format(np.median(dml_r2score), np.mean(dml_r2score)))"
plt.hist(dml_r2score)
"plt.subplot(1, 4, 2)"
"plt.title(""Direct Lasso R^2: median {:.3f}, mean {:.3f}"".format(np.median(direct_r2score),"
np.mean(direct_r2score)))
plt.hist(direct_r2score)
"plt.subplot(1, 4, 3)"
"plt.title(""DML Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(dml_te),"
np.std(dml_te)))
plt.hist(np.array(dml_te).flatten())
"plt.subplot(1, 4, 4)"
"plt.title(""Direct Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(direct_te),"
np.std(direct_te)))
plt.hist(np.array(direct_te).flatten())
plt.tight_layout()
"plt.savefig(""r2_comparison.png"")"
Plotting the results and saving
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
TODO: what if some args can be None?
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"require all cells to complete within 15 minutes, which will help prevent us from"
creating notebooks that are annoying for our users to actually run themselves
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make this test actually test something instead of generating images
Sparse coefficients of treatment as a function of co-variates
Coefficients of outcomes as a function of co-variates
"DGP. Create samples of data (y, T, X) from known truth"
"DGP. Create samples of data (y, T, X) from known truth"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that we can do the same thing once we provide percentile bounds
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that we can do the same thing once we provide percentile bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivitaves are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Set random seed
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
TLearner test
Instantiate TLearner
Test inputs
Test constant treatment effect
Test heterogeneous treatment effect
Instantiate SLearner
Test inputs
Test constant treatment effect
Test heterogeneous treatment effect
Need interactions between T and features
Instantiate XLearner
Test inputs
Test constant treatment effect
Test heterogeneous treatment effect
Instantiate DomainAdaptationLearner
Test inputs
Test constant treatment effect
Test heterogeneous treatment effect
Instantiate DomainAdaptationLearner
Test inputs
Test constant treatment effect
Test heterogeneous treatment effect
Test heterogenous treatment effect for W =/= None
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Only for heterogeneous TE
Fit learner on X and W and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
just make sure we can call the marginal_effect and effect methods
create a simple artificial setup where effect of moving from treatment
"1 -> 2 is 2,"
"1 -> 3 is 1, and"
"2 -> 3 is -1 (necessarily, by composing the previous two effects)"
"Using an uneven number of examples from different classes,"
"and having the treatments in non-lexicographic order,"
Should rule out some basic issues.
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
TODO: it seems like roughly 20% of the calls to _test_sparse are failing - find out what's going wrong
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
note that this would fail for the non-sparse DMLCateEstimator
configuration is all pulled from setup.cfg
-*- coding: utf-8 -*-
""
Configuration file for the Sphinx documentation builder.
""
This file does only contain a selection of the most common options. For a
full list see the documentation:
http://www.sphinx-doc.org/en/master/config
-- Path setup --------------------------------------------------------------
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
""
-- Project information -----------------------------------------------------
-- General configuration ---------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
""
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
""
"source_suffix = ['.rst', '.md']"
The master toctree document.
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
The name of the Pygments (syntax highlighting) style to use.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
html_static_path = ['_static']
"Custom sidebar templates, must be a dictionary that maps document names"
to template names.
""
The default sidebars (for documents that don't match any pattern) are
defined by theme itself.  Builtin themes are using these templates by
"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',"
'searchbox.html']``.
""
html_sidebars = {}
-- Options for HTMLHelp output ---------------------------------------------
Output file base name for HTML help builder.
-- Options for LaTeX output ------------------------------------------------
The paper size ('letterpaper' or 'a4paper').
""
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
""
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
""
"'preamble': '',"
Latex figure (float) alignment
""
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
-- Options for manual page output ------------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
-- Options for Texinfo output ----------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
-- Options for Epub output -------------------------------------------------
Bibliographic Dublin Core info.
The unique identifier of the text. This can be a ISBN number
or the project homepage.
""
epub_identifier = ''
A unique identification for the text.
""
epub_uid = ''
A list of files that should not be packed into the epub file.
-- Extension configuration -------------------------------------------------
-- Options for intersphinx extension ---------------------------------------
Example configuration for intersphinx: refer to the Python standard library.
-- Options for todo extension ----------------------------------------------
"If true, `todo` and `todoList` produce output, else they produce nothing."
##################
Global settings #
##################
Global plotting controls
"Control for support size, can control for more"
#################
File utilities #
#################
#################
Plotting utils #
#################
bias
var
rmse
r2
Infer feature dimension
Metrics by support plots
Authors: Miruna Oprescu <moprescu@microsoft.com>
Vasilis Syrgkanis <vasy@microsoft.com>
Steven Wu <zhiww@microsoft.com>
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Find the leaf node that this x belongs too and parse the corresponding estimate
Safety check
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
Doesn't have sample weights
Is a linear model
Weighted linear regression
Calculates weights
Bootstraping has repetitions in tree sample so we need to iterate
over all indices
Similar for `a` weights
normalize weights
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of W and the outcome as
"a function of W, using only the train fold"
Then compute residuals T-g(W) and Y-f(W) on test fold
We create fake treatment points from the same distribution as the residuals created during the fit process
"For each target x, we evaluate the model_final.predict for each such treatment point, and then we average"
"over the predictions to get the prediction at x, i.e. \tau(x) = E_{res_T\sim D_train}[predict(x, res_T)]"
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Compute coefficient by OLS on residuals
"Split the data in half, train and test"
Fit with LassoCV the treatment as a function of x and the outcome as
"a function of x, using only the train fold"
Then compute residuals p-g(x) and q-q(x) on test fold
Estimate multipliers for second order orthogonal method
"split the data into two parts: one for splitting, the other for estimation at the leafs"
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
compute the base estimate for the current node using double ml or second order double ml
compute the influence functions here that are used for the criterion
generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
compute criterion for each proposal
if splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
estimate the local parameter at the leaf using the estimate data
###################
Argument parsing #
###################
#########################################
Parameters constant across experiments #
#########################################
Outcome support
Treatment support
Evaluation grid
Treatment effects array
Other variables
##########################
Data Generating Process #
##########################
Log iteration
"Generate controls, features, treatment and outcome"
T and Y residuals to be used in later scripts
Save generated dataset
#################
ORF parameters #
#################
######################################
Train and evaluate treatment effect #
######################################
########
Plots #
########
###############
Save results #
###############
##############
Run Rscript #
##############
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Check inputs
Check inputs
Check inputs
Check inputs
Check inputs
Check inputs
Check inputs
Train model on controls. Assign higher weight to units resembling
treated units.
Train model on the treated. Assign higher weight to units resembling
control units.
Check inputs
Check inputs
Check inputs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Must make sure indices are merged correctly
Require group assignment t to be one-hot-encoded
Define an inner function that iterates over group predictions
Convert rows to columns
Get predictions for the 2 splits
Must make sure indices are merged correctly
Estimators
OrthoTree parameters
Tree structure
Initialize causal tree parameters
Create splits of causal tree
Estimate treatment effects at the leafs
Compute heterogeneous treatement effect for x's in x_list by finding
the corresponding split and associating the effect computed on that leaf
Estimators
OrthoForest parameters
Sub-forests
Fit check
TODO: Check performance
Must normalize weights
Crossfitting
Compute weighted nuisance estimates
Generate subsample indices
Safety check
Build trees in parallel
Calculates weights
Bootstraping has repetitions in tree sample
Similar for `a` weights
Bootstraping has repetitions in tree sample
Copy and/or define models
Define nuisance estimators
Define parameter estimators
Define
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Compute residuals
Compute coefficient by OLS on residuals
"Parameter returned by LinearRegression is (d_T, )"
Return moments and gradients
Compute residuals
Compute moments
"Moments shape is (n, d_T)"
Compute moment gradients
Copy and/or define models
Nuisance estimators shall be defined during fitting because they need to know the number of distinct
treatments
Define parameter estimators
Define moment and mean gradient estimator
Define autoencoder
"Check that T is shape (n, )"
Check T is numeric
Train label encoder
Define number of classes
Call `fit` from parent class
"Test that T contains all treatments. If not, return None"
Nuissance estimates evaluated with cross-fitting
Define 2-fold iterator
Check if there is only one example of some class
No need to crossfit for internal nodes
Compute partial moments
"If any of the values in the parameter estimate is nan, return None"
Return moments and gradients
Compute partial moments
Compute moments
"Moments shape is (n, d_T-1)"
Compute moment gradients
Need to calculate this in an elegant way for when propensity is 0
This will flatten T
Check that T is numeric
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Handle case where Y or T is a vector instead of a 2-dimensional array
"TODO: If T is a vector rather than a 2-D array, then the model's fit must accept a vector..."
"Do we want to reshape to an nx1, or just trust the user's choice of input?"
(Likewise for Y below)
NOTE: the fact that we stack X first then W is relied upon
by the SparseLinearDMLCateEstimator implementation;
"if it's changed here then it needs to be changed there, too"
TODO: Doing this kronecker/reshaping/transposing stuff so that predict can be called
"rather than just using coef_ seems silly, but one benefit is that we can use linear models"
that don't expose a coef_ (e.g. a GridSearchCV over underlying linear models)
TODO: handle case where final model doesn't directly expose coef_?
"TODO: yuck, is there any way to avoid having to know the structure of XW"
and the size of X to apply the features here?
flatten the matrix features so that we can properly perform the cross product
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
#######################################
Core DML Tests
#######################################
How many samples
How many control features
How many treatment variables
Coefficients of how controls affect treatments
Coefficients of how controls affect outcome
Treatment effects that we want to estimate
Run dml estimation
How many samples
How many control features
How many treatment variables
Coefficients of how controls affect treatments
Coefficients of how controls affect outcome
Treatment effects that we want to estimate
Run dml estimation
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"this will have dimension (d,) + shape(X)"
send the first dimension to the end
columns are featurized independently; partial derivatives are only non-zero
when taken with respect to the same column each time
don't fit intercept; manually add column of ones to the data instead;
this allows us to ignore the intercept when computing marginal effects
two stage approximation
"first, get basis expansions of T, X, and Z"
"regress T expansion on X,Z expansions"
"predict ft_T from interacted ft_X, ft_Z"
"dT should be an n×dₜ×fₜ array (but if T was a vector, or if there is only one feature,"
dT may be only 2-dimensional)
promote dT to 3D if necessary (e.g. if T was a vector)
reshape ft_X and dT to allow cross product (result has shape n×dₜ×fₜ×f_x)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make sure to use random seeds wherever necessary
"TODO: make sure that the public API consistently uses ""T"" instead of ""P"" for the treatment"
"unfortunately with the Theano and Tensorflow backends,"
the straightforward use of K.stop_gradient can cause an error
because the parameters of the intermediate layers are now disconnected from the loss;
therefore we add a pointless multiplication by 0 to the values in each of the variables in vs
so that those layers remain connected but with 0 gradient
|| t - mu_i || ^2
LL = C - log(sum(pi_i/sig^d * exp(-d2/(2*sig^2))))
Use logsumexp for numeric stability:
LL = C - log(sum(exp(-d2/(2*sig^2) + log(pi_i/sig^d))))
TODO: does the numeric stability actually make any difference?
"CNTK backend can't randomize across batches and doesn't implement cumsum (at least as of June 2018,"
see Known Issues on https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-Keras)
generate cumulative sum via matrix multiplication
"Generate standard uniform values in shape (batch_size,1)"
"(since we can't use the dynamic batch_size with random.uniform in CNTK,"
we use uniform_like instead with an input of an appropriate shape)
convert to floats and multiply to perform equivalent of logical AND
"Generate standard normal values in shape (batch_size,1,d_t)"
"(since we can't use the dynamic batch_size with random.normal in CNTK,"
we use normal_like instead with an input of an appropriate shape)
"exactly one entry should be nonzero for each b,d combination; use sum to select it"
prevent gradient from passing through sampling
three options: biased or upper-bound loss require a single number of samples;
unbiased can take different numbers for the network and its gradient
"sample: (() -> Layer, int) -> Layer"
we want to separately sample the gradient; we use stop_gradient to treat the sampled model as constant
"the overall computation ensures that we have an interpretable loss (y-h̅(p,x))²,"
"but also that the gradient is -2(y-h̅(p,x))∇h̅(p,x) with *different* samples used for each average"
TODO: allow 1D arguments for Y and T
the dimensionality of the output of the network
TODO: is there a more robust way to do this?
TODO: do we need to give the user more control over other arguments to fit?
"subtle point: we need to build a new model each time,"
because each model encapsulates its randomness
TODO: do we need to give the user more control over other arguments to fit?
"TODO: it seems like we need to sum over the batch because we can only apply gradient to a scalar,"
not a general tensor (because of how backprop works in every framework)
"(alternatively, we could iterate through the batch in addition to iterating through the output,"
but this seems annoying...)
"Therefore, it's important that we use a batch size of 1 when we call predict with this model"
TODO: any way to get this to work on batches of arbitrary size?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
#######################################################
Perfect Data DGPs for Testing Correctness of Code
#######################################################
Generate random control co-variates
Create epsilon residual treatments that deterministically sum up to
zero
Re-calibrate epsilon to make sure that empirical distribution of epsilon
conditional on each co-variate vector is equal to zero
We simply subtract the conditional mean from the epsilons
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Create epsilon residual treatments that deterministically sum up to
zero
Re-calibrate epsilon to make sure that empirical distribution of epsilon
conditional on each co-variate vector is equal to zero
We simply subtract the conditional mean from the epsilons
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect
Generate random control co-variates
Create epsilon residual treatments
Construct treatments as T = X*A + epsilon
Construct outcomes as y = X*beta + T*effect + eta
Generate random control co-variates
Use the same treatment vector for each row
Construct outcomes as y = X*beta + T*effect
Licensed under the MIT License.
"TODO: what if input is sparse? - there's no equivalent to einsum,"
but tensordot can't be applied to this problem because we don't sum over m
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: any way to avoid creating a copy if the array was already dense?
"the call is necessary if the input was something like a list, though"
"scipy sparse arrays don't support reshaping (even for 2D they throw not implemented errors),"
so convert to pydata sparse first
"in the 2D case, we can convert back to scipy sparse; in other cases we can't"
both inputs were scipy and we can safely convert back to scipy because it's 2D
TODO: wouldn't making X1 vary more slowly than X2 be more intuitive?
(but note that changing this would necessitate changes to callers
to switch the order to preserve behavior where order is important)
note: in contrast to np.hstack this only works with arrays of dimension at least 2
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
"Confusingly, this needs to concatenate, not stack (stack returns an array with an extra dimension)"
same number of input definitions as arrays
input definitions have same number of dimensions as each array
all result indices are unique
all result indices must match at least one input index
"map indices to all array, axis pairs for that index"
each index has the same cardinality wherever it appears
"State: list of (set of letters, list of (corresponding indices, value))"
Algo: while list contains more than one entry
take two entries
sort both lists by intersection of their indices
"merge compatible entries (where intersection of indices is equal - in the resulting list,"
"take the union of indices and the product of values), stepping through each list linearly"
TODO: might be faster to break into connected components first
"e.g. for ""ab,d,bc->ad"", the two components ""ab,bc"" and ""d"" are independent,"
"so compute their content separately, then take cartesian product"
this would save a few pointless sorts by empty tuples
TODO: Consider investigating other performance ideas for these cases
where the dense method beat the sparse method (usually sparse is faster)
"e,facd,c->cfed"
sparse: 0.0335489
dense:  0.011465999999999997
"gbd,da,egb->da"
sparse: 0.0791625
dense:  0.007319099999999995
"dcc,d,faedb,c->abe"
sparse: 1.2868097
dense:  0.44605229999999985
"when indices are repeated within an array, pre-filter the coordinates and data"
TODO: would using einsum's paths to optimize the order of merging help?
Normalize weights
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: allow different subsets for L1 and L2 regularization?
TODO: any better way to deal with sparsity?
TODO: any better way to deal with sparsity?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Input datasets
Estimators
Causal tree parameters
Tree structure
If by splitting we have too small leaves or if we reached the maximum number of splits we stop
Create local sample set
Compute nuisance estimates for the current node
Nuisance estimate cannot be calculated
Estimate parameter for current node
Node estimate cannot be calculated
Calculate moments and gradient of moments for current data
Calculate inverse gradient
The gradient matrix is not invertible.
No good split can be found
Calculate point-wise pseudo-outcomes rho
Generate random proposals of dimensions to split
"Append to the proposals a tuple (dimension, threshold) where the threshold is randomly chosen"
Compute criterion for each proposal
eta specifies how much weight to put on individual heterogeneity vs common heterogeneity
across parameters. we give some benefit to individual heterogeneity factors for cases
where there might be large discontinuities in some parameter as the conditioning set varies
If splitting creates valid leafs in terms of mean leaf size
Calculate criterion for split
Else set criterion to infinity so that this split is not chosen
If no good split was found
Find split that minimizes criterion
Set the split attributes at the node
Create child nodes with corresponding subsamples
Recursively split children
Return parent node
No need for a random split since the data is already
a random subsample from the original input
Estimate the local parameter at the leaf using the estimate data
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
group by product; sum and subtract original; divide by (n_p-1)
group by product; sum and subtract original; divide by (n_p-1)
"for now, require one feature per store/product combination"
TODO: would be nice to relax this somehow
"alphas vary by product, not by store"
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
"one cross-price term per product, which is based on the average price"
of all other goods sold at the same store in the same week
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
store-specific and product-specific gammas and betas (which are positively correlated)
"features: product dummies, store dummies"
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
"observe n_products * n_stores prices, same number of quantities"
"for direct regression comparisons, we need a pivoted version"
"""treatments"" for direct regression include treatments, plus treatments interacted with product dummies,"
"plus the same for ""group treatments"" (average treatment of other products in the same store/week)"
"for direct regression, we also need to append the features"
"(both the ""constant features"" as well as the normal ones)"
NOTE: need to set cv because default generic algorithm is super slow for sparse matrices
"alphas vary by product, not by store"
"note: product varies faster than store in the flattened list - this must also be true of gammas, betas, etc."
store-specific and product-specific gammas
store-specific and product-specific betas
"features: product dummies, store dummies"
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
we need only the prices for the compound model; all dummies are created internally
"observe n_products * n_stores prices, same number of quantities"
"simple results include treatments, plus treatments interacted with product dummies,"
for use with the direct methods
X should have 0 columns; we will instead pivot Y and fit against the features passed into the constructor
underspecified model
Y = alpha T + \sum_i alpha_i T_i + X beta + eta
T = X gamma + eps
how to score? distance from line of all solutions?
"given that 0, a, b, c, d is equivalent to x, a-x, b-x, c-x, d-x, compute the error"
"baselines: ridge, ridge-like (penalize alpha_i but not alpha_baseline or beta)"
"comparison: 2ml (OLS for T on X, OLS for Y on XxX^e, ridge or ridge-like for alphas)"
"features: one product dummy, one store dummy (each missing one level), constant"
"alphas vary by product, not by store"
store-specific and product-specific gammas
store-specific and product-specific betas
"features: product dummies, store dummies"
"with one missing to preserve rank: store_n = sum(product_j) - sum(store_j, j!=n)"
"columns: prices interacted with products (and constant), features"
"observe n_products * n_stores prices, same number of quantities"
use features starting at index 1+n_products to skip all prices
"pickleFile = open('pickledSparse_{0}_{1}_{2}_{3}.pickle'.format(n_exp, n_products, n_stores, n_weeks), 'rb')"
"alphass, ridges, lassos, doubleMls = pickle.load(pickleFile)"
pickleFile.close()
#############################################
Defining the parameters of Monte Carlo
#############################################
Estimation parameters
###################################################################
Estimating the parameters of the DGP with DML. Running multiple
Monte Carlo experiments.
###################################################################
Sparse coefficients of treatment as a function of co-variates
Coefficients of outcomes as a function of co-variates
"DGP. Create samples of data (y, T, X) from known truth"
DML Estimation.
Estimation with other methods for comparison
#########################################################
Plotting the results and saving
#########################################################
"plt.figure(figsize=(20, 10))"
"plt.subplot(1, 4, 1)"
"plt.title(""DML R^2: median {:.3f}, mean {:.3f}"".format(np.median(dml_r2score), np.mean(dml_r2score)))"
plt.hist(dml_r2score)
"plt.subplot(1, 4, 2)"
"plt.title(""Direct Lasso R^2: median {:.3f}, mean {:.3f}"".format(np.median(direct_r2score),"
np.mean(direct_r2score)))
plt.hist(direct_r2score)
"plt.subplot(1, 4, 3)"
"plt.title(""DML Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(dml_te),"
np.std(dml_te)))
plt.hist(np.array(dml_te).flatten())
"plt.subplot(1, 4, 4)"
"plt.title(""Direct Treatment Effect Distribution: mean {:.3f}, std {:.3f}"".format(np.mean(direct_te),"
np.std(direct_te)))
plt.hist(np.array(direct_te).flatten())
plt.tight_layout()
"plt.savefig(""r2_comparison.png"")"
Plotting the results and saving
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: Add a __dir__ implementation?
TODO: what if some args can be None?
"for attributes that exist on the wrapped object, just compute the mean of the wrapped calls"
"if the attribute exists on the wrapped object once we remove the suffix,"
then we should be computing a confidence interval for the wrapped calls
"collect extra arguments and pass them through, if the wrapped attribute was callable"
don't pass extra arguments if the wrapped attribute wasn't callable to begin with
Remove children with nonwhite mothers from the treatment group
Remove children with nonwhite mothers from the treatment group
Select columns
Scale the numeric variables
"Change the binary variable 'first' takes values in {1,2}"
Append a column of ones as intercept
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
TODO: make this test actually test something instead of generating images
Sparse coefficients of treatment as a function of co-variates
Coefficients of outcomes as a function of co-variates
"DGP. Create samples of data (y, T, X) from known truth"
"DGP. Create samples of data (y, T, X) from known truth"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
simple DGP only for illustration
Define the treatment model neural network architecture
"This will take the concatenation of one-dimensional values z and x as input,"
"so the input shape is (d_z + d_x,)"
The exact shape of the final layer is not critical because the Deep IV framework will
add extra layers on top for the mixture density network
Define the response model neural network architecture
"This will take the concatenation of one-dimensional values t and x as input,"
"so the input shape is (d_t + d_x,)"
"The output should match the shape of y, so it must have shape (d_y,) in this case"
"NOTE: For the response model, it is important to define the model *outside*"
"of the lambda passed to the DeepIvEstimator, as we do here,"
so that the same weights will be reused in each instantiation
number of samples to use in second estimate of the response
(to make loss estimate unbiased)
Keras optimizer to use for training - see https://keras.io/optimizers/
do something with predictions...
Doesn't work with CNTK backend as of 2018-07-17 - see https://github.com/keras-team/keras/issues/10715
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
test = True ensures we draw test set images
test = True ensures we draw test set images
re-draw to get new independent treatment and implied response
we need to make sure z _never_ does anything in these g functions (fitted and true)
above is necesary so that reduced form doesn't win
covariates: time and emotion
random instrument
z -> price
true observable demand function
errors
response
"pi,mu,sig = MixtureOfGaussians(n, d)(x_network)"
"ll = MixtureOfGaussiansLogLoss(n, d)([pi,mu,sig,t_input])"
For some reason this doesn't work at all when run against the CNTK backend...
"model.compile('nadam', loss=lambda _,l:l)"
"model.fit([x,t], [np.zeros((5000,1))], epochs=500)"
generate a valiation set
"to generate a random symmetric positive semidefinite covariance matrix, we can use A*A^T"
convex combinations of semidefinite covariance matrices are themselves semidefinite
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that we can do the same thing once we provide percentile bounds
test that we can fit with the same arguments as the base estimator
"test that we can get the same attribute for the bootstrap as the original, with the same shape"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that we can do the same thing once we provide percentile bounds
"test that we can do the same thing with the results of a method, rather than an attribute"
"test that we can get an interval for the same attribute for the bootstrap as the original,"
with the same shape for the lower and upper bounds
test that we can do the same thing once we provide percentile bounds
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
number of inputs in specification must match number of inputs
must have an output
output indices must be unique
output indices must be present in an input
number of indices must match number of dimensions for each input
repeated indices must always have consistent sizes
transpose
tensordot
trace
TODO: set up proper flag for this
pick indices at random with replacement from the first 7 letters of the alphabet
"of all of the distinct indices that appear in any input,"
pick a random subset of them (of size at most 5) to appear in the output
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Preprocess data
Convert 'week' to a date
"week_zero = datetime.datetime.strptime(""09/07/89"", ""%m/%d/%y"")"
"oj_data[""week""] = pd.to_timedelta(oj_data[""week""], unit='w') + week_zero"
Take log of price
Make brand numeric
"remove meaningless features (e.g. cross-price effects of products on themselves),"
which have all zero coeffs
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"first polynomials are 1, x, x*x-1, x*x*x-3*x"
"first derivitaves are -x, -x^2+1 (since there's just one column, joint-ness doesn't matter)"
TODO: this tests that we can run the method; how do we test that the results are reasonable?
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Generate data
DGP constants
Test data
Constant treatment effect and propensity
Heterogeneous treatment and propensity
TLearner test
Instantiate TLearner
Test inputs
Test constant treatment effect
Test heterogeneous treatment effect
Instantiate SLearner
Test inputs
Test constant treatment effect
Test heterogeneous treatment effect
Need interactions between T and features
Instantiate XLearner
Test inputs
Test constant treatment effect
Test heterogeneous treatment effect
Instantiate DomainAdaptationLearner
Test inputs
Test constant treatment effect
Test heterogeneous treatment effect
Instantiate DomainAdaptationLearner
Test inputs
Test constant treatment effect
Test heterogeneous treatment effect
Fit learner and get the effect
Get the true treatment effect
Compute treatment effect residuals (absolute)
Check that at least 90% of predictions are within tolerance interval
Check that one can pass in regular lists
Check that it fails correctly if lists of different shape are passed in
Check that it fails when T contains values other than 0 and 1
"Check that it works when T, Y have shape (n, 1)"
learner_instance.effect(TestMetalearners.X_test)
Generate covariates
Generate treatment
Calculate outcome
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
DGP constants
Generate data
Test data
Remove warnings that might be raised by the models passed into the ORF
Generate data with continuous treatments
Instantiate model with most of the default parameters
Test inputs for continuous treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
Check that outputs have the correct shape
Test continuous treatments with controls
Test continuous treatments without controls
Generate data with binary treatments
Instantiate model with default params
Test inputs for binary treatments
--> Check that one can pass in regular lists
--> Check that it fails correctly if lists of different shape are passed in
"--> Check that it works when T, Y have shape (n, 1)"
"--> Check that it fails correctly when T has shape (n, 2)"
--> Check that it fails correctly when the treatments are not numeric
Check that outputs have the correct shape
Test binary treatments with controls
Test binary treatments without controls
Only applicable to continuous treatments
Generate data for 2 treatments
Test multiple treatments with controls
Compute the treatment effect on test points
Compute treatment effect residuals
Multiple treatments
Allow at most 10% test points to be outside of the tolerance interval
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
add column of ones to X
"for each row, create the d_y*d_t*(d_x+1) features (which are matrices of size d_y by d_t)"
all solutions to underdetermined (or exactly determined) Ax=b are given by A⁺b+(I-A⁺A)w for some arbitrary w
"note that if Ax=b is overdetermined, this will raise an assertion error"
just make sure we can call the marginal_effect and effect methods
just make sure we can call the marginal_effect and effect methods
"to correctly recover coefficients for T via OLS, we need e_t to be orthogonal to [W;X]"
"to correctly recover coefficients for Y via OLS, we need ([X; W]⊗[1; ϕ(X); W])⁺ e_y ="
-([X; W]⊗[1; ϕ(X); W])⁺ ((ϕ(X)⊗e_t)a_X+(W⊗e_t)a_W)
"then, to correctly recover a in the third stage, we additionally need (ϕ(X)⊗e_t)ᵀ e_y = 0"
TODO: it seems like roughly 20% of the calls to _test_sparse are failing - find out what's going wrong
sparse test case: heterogeneous effect by product
need at least as many rows in e_y as there are distinct columns
in [X;X⊗W;W⊗W;X⊗e_t] to find a solution for e_t
note that this would fail for the non-sparse DMLCateEstimator
recover simple features by initializing complex features appropriately
using full set of matrix features should be equivalent to using non-matrix featurizer
