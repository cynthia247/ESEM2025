Commit Message
from Cython.Distutils import build_ext
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
The dependencies are the same as the contents of requirements.txt
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Check for connected component on X
Compute sparse mutual reachability graph
"if max_dist > 0, max distance to use when the reachability is infinite"
Check connected component on mutual reachability
"If more than one component, it means that even if the distance matrix X"
"has one component, there exists with less than `min_samples` neighbors"
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Non-precomputed matrices may contain non-finite values.
Rows with these values
Pass only the purely finite indices into hdbscan
We will later assign all non-finite points to the background -1 cluster
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
remap indices to align with original data in the case of non-finite entries.
"Unltimately, for each Ci, we only require the"
"minimum of DSPC(Ci, Cj) over all Cj != Ci."
"So let's call this value DSPC_wrt(Ci), i.e."
density separation 'with respect to' Ci.
If exactly one of the points is noise
Set the density sparseness of the cluster
to the sparsest value seen so far.
Check whether density separations with
respect to each of these clusters can
be reduced.
"In case min_outlier_sep is still np.inf, we assign a new value to it."
This only makes sense if num_clusters = 1 since it has turned out
that the MR-MST has no edges between a noise point and a core point.
DSPC_wrt[Ci] might be infinite if the connected component for Ci is
"an ""island"" in the MR-MST. Whereas for other clusters Cj and Ck, the"
MR-MST might contain an edge with one point in Cj and ther other one
"in Ck. Here, we replace the infinite density separation of Ci by"
another large enough value.
""
TODO: Think of a better yet efficient way to handle this.
Extract flat clustering from HDBSCAN's hierarchy for 7 clusters
Use a previously initialized/trained HDBSCAN
Handle the trivial case first.
Always generate prediction_data to avoid later woes
This will later be chosen according to n_clusters
Initialize and train clusterer if one was not previously supplied.
Always generate prediction data
We do not pass cluster_selection_epsilon here.
"While this adds unnecessary computation, it makes the code"
easier to read and debug.
"Train on 'X'. Do this even if the supplied clusterer was trained,"
because we want to make sure it fits 'X'.
"Pick an epsilon value right after a split produces n_clusters,"
and the don't split further for smaller epsilon (larger lambda)
Or use the specified cluster_selection_epsilon
"Extract tree related stuff, in order to re-assign labels"
Get labels according to the required cluster_selection_epsilon
Reflect the related changes in HDBSCAN.
PredictionData attached to HDBSCAN should also change.
A function re_init is defined in this module to handle this.
"From a fitted HDBSCAN model, predict for n_clusters=5"
Store prediciton data for later use.
and use this prediction data to predict on new points
Get number of fitted clusters for later use.
We'll need the condensed tree later...
"If none of the three arguments: prediction_data, n_clusters,"
"and cluster_selection_epsilon are supplied,"
then use clusterer's prediciton data directly
"If either of n_clusters or cluster_selection_epsilon were supplied,"
then build prediction data from these by modifying clusterer's
Get prediction data from clusterer
Modify prediction_data to reflect new n_clusters
"First, make a copy of prediction data to avoid modifying source"
Cluster selection method is hold by condensed_tree.
Change from 'eom' to 'leaf' if n_clusters is too large.
This change does not affect the tree associated with 'clusterer'
Re-initialize prediction_data for the specified n_clusters or epsilon
============================================================
Now we're ready to use prediction_data
"The rest of the code is copied from HDBSCAN's approximate_predict,"
but modified to use prediction_data instead of clusterer's attribute
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering from prediction data
Initialize probabilities
k-NN for prediciton points to training set
Loop over prediction points to compute probabilities
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"Find row in tree where nearest neighbor drops out,"
so we can get a lambda value for the nearest neighbor
"Assign lambda as min(lambda-to-neighbor, neighbor's-lambda-to-tree)"
"Equivalently, this assigns core distance for prediction point as"
"max(dist-to-neighbor, neighbor's-dist-to-tree)"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the nearest exemplar persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Merge the two probabilities to produce a single set of probabilities
Include probability that the nearest neighbor belongs to a cluster
Rename variable so it's easy to understand what's being returned
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering at the chosen epsilon from prediction_data
"When no clusters found, return array of 0's"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the point persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Include probability that the point belongs to a cluster
Aggregate the three probabilities to produce membership vectors
Re-name variable to clarify what's being returned.
"With method 'eom', max clusters are produced for epsilon=0,"
as computed by
Increasing epsilon can only reduce the number of ouput clusters.
"To select epsilon, consider all values where clusters are split"
Subtract the extra e-12 to avoid numerical errors in comparison
"Then, we avoid splitting for all epsilon below this."
Use an epsilon value that produces the right number of clusters.
The condensed tree of HDBSCAN has this information.
Extract the lambda levels (=1/distance) from the condensed tree
We don't want values that produce a large cluster and
just one or two individual points.
Keep only those lambda values corresponding to cluster separation;
"i.e., with child_sizes > 1"
"Get the unique values, because when two clusters fall out of one,"
the entry with lambda is repeated.
lambda values are sorted by np.unique.
"Now, get epsilon (distance threshold) as 1/lambda"
"At this epsilon, n_clusters have been split."
Stop splits at epsilons smaller than this.
"To allow for numerical errors,"
predData must be a pre-trained PredictionData instance from hdbscan
"If n_clusters is specified, compute cluster_selection_epsilon;"
This is the key modification:
Select clusters according to selection method and epsilon.
_new_select_clusters is a modification of get_clusters
from hdbscan._hdbscan_tree
"raw tree, used later to get exemplars and lambda values"
"Re-do the cluster map: Map cluster numbers in tree (N, N+1, ..)"
to the cluster labels produced as output
Re-compute lambdas and exemplars for selected clusters;
max_lambda <=> smallest distance <=> most persistent point(s)
Map all sub-clusters of selected cluster to the selected cluster's
label in output.
Map lambdas too...
Create set of exemplar points for later use.
Novel points are assigned based on cluster of closest exemplar.
"For each selected cluster, get all of its leaves,"
"and leaves of leaves, and so on..."
Largest lambda => Most persistent points
Get the most persistent points
Add most persistent points as exemplars
Add exemplars for each leaf of each selected cluster.
(exclude root)
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Extract the chosen cluster bounds. If enough duplicate data points exist in the
data the lambda value might be infinite. This breaks labeling and highlighting
the chosen clusters.
Extract the plot range of the y-axis and set default center and height values for ellipses.
Extremly dense clusters might result in near infinite lambda values. Setting max_height
based on the percentile should alleviate the impact on plotting.
Set center and height to default values if necessary
Ensure the ellipse is visible
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
"node_desat=None,"
Extract node color data
Extract edge color data
Compute or extract layout
Add edges
Add FLASC features
Add raw data features
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
"the point is in the dataset, fix lambda for rounding errors"
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
Support branch detection within clusters.
Select finite data points
Construct tree
Allocate to maintain data point indices
Find neighbours for non-noise points
Check clusterer state
Validate parameters
Extract state
Configure parallelization
Detect branches
Maintain data indices for non-finite data
Combined result
Branching result
Clusters to branches
List points within cluster
Extract MST edges within cluster
Compute in cluster centrality
Construct cluster approximation graph
Extract centrality MST and compute single linkage
Re-label edges with data ids
Return values
Allocate output (won't be filled completely)
Fill (undirected) MST edges with within-cluster-ids
Fill neighbors with within-cluster-ids
Fill mutual reachabilities
Extract unique edges that stay within the cluster
Query KDTree/BallTree for neighours within the distance
Count number of returned edges per point
Create full edge list
Create output
Reset noise labels to k-cluster
Allocate output
Compute the labels and probabilities
Reorder other parts
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
Disable for now -- need to refactor to meet newer standards
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
this fails if no $DISPLAY specified
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
no prediction data error
wrong dimensions error
no clusters warning
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
without epsilon we should see many noise points as children of root.
for this random seed an epsilon of 0.2 will produce exactly 2 noise
points at that cut in single linkage.
"If the following line does not raise an error, the test passes"
"If the following line does not raise an error, the test passes"
Disable for now -- need to refactor to meet newer standards
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
"Given negative, zero and positive denominator and positive numerator"
Make sure safe division is always positive and doesn't raise ZeroDivision error
--- Branch Detection Data
There are no fast metrics that are not supported by KDTree or BallTree!
"Cosine and arccoss both crash HDBSCAN. They go down the BallTree path, but"
the implementation does not support them.
Distance matrix
Sparse matrix
--- Detecting Branches
Generate single-cluster data
"Without persistence, find 6 branches"
"Mac & Windows give 71, Linux gives 72. Probably different random values."
Adding presistence removes some branches
--- Branch Detector Functionality
A point on a branch (not noise) exact labels change per run
A point in a cluster
A noise point
--- Attribute Output Formats
--- Attribute plots
Ignore future warnings thrown by sklearn
Create a nice dataset with 6 circular clusters and 2 moons
"Given, the base HDBSCAN with method 'eom'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
"Given, the base HDBSCAN with method 'leaf'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
Method 'eom'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
Method 'leaf'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
"Given the max number of clusters that can be produced by 'eom',"
(these are produced for epsilon=0) (??? Needs verification)
"When we try flat clustering with 'eom' method for more n_clusters,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
"assert ""Cannot predict"" in str(w[-1].message)"
"the resulting clusterer switches to using method 'leaf',"
and the resulting probabilities and labels must match
"Given the base HDBSCAN trained on some data,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the clustering should match that due to approximate_predict,"
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the number of clusters produced must match the original n_clusters"
and all probabilities are <= 1.
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat with specified n_clusters,"
"Then, the requested number of clusters must be produced"
and all probabilities are <= 1.
When using approximate_predict_flat with more clusters
"than 'eom' can handle,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
But the requested number of clusters must still be produced using 'leaf'
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
from Cython.Distutils import build_ext
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
The dependencies are the same as the contents of requirements.txt
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Check for connected component on X
Compute sparse mutual reachability graph
"if max_dist > 0, max distance to use when the reachability is infinite"
Check connected component on mutual reachability
"If more than one component, it means that even if the distance matrix X"
"has one component, there exists with less than `min_samples` neighbors"
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Non-precomputed matrices may contain non-finite values.
Rows with these values
Pass only the purely finite indices into hdbscan
We will later assign all non-finite points to the background -1 cluster
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
remap indices to align with original data in the case of non-finite entries.
"Unltimately, for each Ci, we only require the"
"minimum of DSPC(Ci, Cj) over all Cj != Ci."
"So let's call this value DSPC_wrt(Ci), i.e."
density separation 'with respect to' Ci.
If exactly one of the points is noise
Set the density sparseness of the cluster
to the sparsest value seen so far.
Check whether density separations with
respect to each of these clusters can
be reduced.
"In case min_outlier_sep is still np.inf, we assign a new value to it."
This only makes sense if num_clusters = 1 since it has turned out
that the MR-MST has no edges between a noise point and a core point.
DSPC_wrt[Ci] might be infinite if the connected component for Ci is
"an ""island"" in the MR-MST. Whereas for other clusters Cj and Ck, the"
MR-MST might contain an edge with one point in Cj and ther other one
"in Ck. Here, we replace the infinite density separation of Ci by"
another large enough value.
""
TODO: Think of a better yet efficient way to handle this.
Extract flat clustering from HDBSCAN's hierarchy for 7 clusters
Use a previously initialized/trained HDBSCAN
Handle the trivial case first.
Always generate prediction_data to avoid later woes
This will later be chosen according to n_clusters
Initialize and train clusterer if one was not previously supplied.
Always generate prediction data
We do not pass cluster_selection_epsilon here.
"While this adds unnecessary computation, it makes the code"
easier to read and debug.
"Train on 'X'. Do this even if the supplied clusterer was trained,"
because we want to make sure it fits 'X'.
"Pick an epsilon value right after a split produces n_clusters,"
and the don't split further for smaller epsilon (larger lambda)
Or use the specified cluster_selection_epsilon
"Extract tree related stuff, in order to re-assign labels"
Get labels according to the required cluster_selection_epsilon
Reflect the related changes in HDBSCAN.
PredictionData attached to HDBSCAN should also change.
A function re_init is defined in this module to handle this.
"From a fitted HDBSCAN model, predict for n_clusters=5"
Store prediciton data for later use.
and use this prediction data to predict on new points
Get number of fitted clusters for later use.
We'll need the condensed tree later...
"If none of the three arguments: prediction_data, n_clusters,"
"and cluster_selection_epsilon are supplied,"
then use clusterer's prediciton data directly
"If either of n_clusters or cluster_selection_epsilon were supplied,"
then build prediction data from these by modifying clusterer's
Get prediction data from clusterer
Modify prediction_data to reflect new n_clusters
"First, make a copy of prediction data to avoid modifying source"
Cluster selection method is hold by condensed_tree.
Change from 'eom' to 'leaf' if n_clusters is too large.
This change does not affect the tree associated with 'clusterer'
Re-initialize prediction_data for the specified n_clusters or epsilon
============================================================
Now we're ready to use prediction_data
"The rest of the code is copied from HDBSCAN's approximate_predict,"
but modified to use prediction_data instead of clusterer's attribute
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering from prediction data
Initialize probabilities
k-NN for prediciton points to training set
Loop over prediction points to compute probabilities
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"Find row in tree where nearest neighbor drops out,"
so we can get a lambda value for the nearest neighbor
"Assign lambda as min(lambda-to-neighbor, neighbor's-lambda-to-tree)"
"Equivalently, this assigns core distance for prediction point as"
"max(dist-to-neighbor, neighbor's-dist-to-tree)"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the nearest exemplar persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Merge the two probabilities to produce a single set of probabilities
Include probability that the nearest neighbor belongs to a cluster
Rename variable so it's easy to understand what's being returned
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering at the chosen epsilon from prediction_data
"When no clusters found, return array of 0's"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the point persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Include probability that the point belongs to a cluster
Aggregate the three probabilities to produce membership vectors
Re-name variable to clarify what's being returned.
"With method 'eom', max clusters are produced for epsilon=0,"
as computed by
Increasing epsilon can only reduce the number of ouput clusters.
"To select epsilon, consider all values where clusters are split"
Subtract the extra e-12 to avoid numerical errors in comparison
"Then, we avoid splitting for all epsilon below this."
Use an epsilon value that produces the right number of clusters.
The condensed tree of HDBSCAN has this information.
Extract the lambda levels (=1/distance) from the condensed tree
We don't want values that produce a large cluster and
just one or two individual points.
Keep only those lambda values corresponding to cluster separation;
"i.e., with child_sizes > 1"
"Get the unique values, because when two clusters fall out of one,"
the entry with lambda is repeated.
lambda values are sorted by np.unique.
"Now, get epsilon (distance threshold) as 1/lambda"
"At this epsilon, n_clusters have been split."
Stop splits at epsilons smaller than this.
"To allow for numerical errors,"
predData must be a pre-trained PredictionData instance from hdbscan
"If n_clusters is specified, compute cluster_selection_epsilon;"
This is the key modification:
Select clusters according to selection method and epsilon.
_new_select_clusters is a modification of get_clusters
from hdbscan._hdbscan_tree
"raw tree, used later to get exemplars and lambda values"
"Re-do the cluster map: Map cluster numbers in tree (N, N+1, ..)"
to the cluster labels produced as output
Re-compute lambdas and exemplars for selected clusters;
max_lambda <=> smallest distance <=> most persistent point(s)
Map all sub-clusters of selected cluster to the selected cluster's
label in output.
Map lambdas too...
Create set of exemplar points for later use.
Novel points are assigned based on cluster of closest exemplar.
"For each selected cluster, get all of its leaves,"
"and leaves of leaves, and so on..."
Largest lambda => Most persistent points
Get the most persistent points
Add most persistent points as exemplars
Add exemplars for each leaf of each selected cluster.
(exclude root)
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Extract the chosen cluster bounds. If enough duplicate data points exist in the
data the lambda value might be infinite. This breaks labeling and highlighting
the chosen clusters.
Extract the plot range of the y-axis and set default center and height values for ellipses.
Extremly dense clusters might result in near infinite lambda values. Setting max_height
based on the percentile should alleviate the impact on plotting.
Set center and height to default values if necessary
Ensure the ellipse is visible
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
"node_desat=None,"
Extract node color data
Extract edge color data
Compute or extract layout
Add edges
Add FLASC features
Add raw data features
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
"the point is in the dataset, fix lambda for rounding errors"
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
Support branch detection within clusters.
Select finite data points
Construct tree
Allocate to maintain data point indices
Find neighbours for non-noise points
Check clusterer state
Validate parameters
Extract state
Configure parallelization
Detect branches
Maintain data indices for non-finite data
Combined result
Branching result
Clusters to branches
List points within cluster
Extract MST edges within cluster
Compute in cluster centrality
Construct cluster approximation graph
Extract centrality MST and compute single linkage
Re-label edges with data ids
Return values
Allocate output (won't be filled completely)
Fill (undirected) MST edges with within-cluster-ids
Fill neighbors with within-cluster-ids
Fill mutual reachabilities
Extract unique edges that stay within the cluster
Query KDTree/BallTree for neighours within the distance
Count number of returned edges per point
Create full edge list
Create output
Reset noise labels to k-cluster
Allocate output
Compute the labels and probabilities
Reorder other parts
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
Disable for now -- need to refactor to meet newer standards
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
this fails if no $DISPLAY specified
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
no prediction data error
wrong dimensions error
no clusters warning
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
without epsilon we should see many noise points as children of root.
for this random seed an epsilon of 0.2 will produce exactly 2 noise
points at that cut in single linkage.
Disable for now -- need to refactor to meet newer standards
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
"Given negative, zero and positive denominator and positive numerator"
Make sure safe division is always positive and doesn't raise ZeroDivision error
--- Branch Detection Data
There are no fast metrics that are not supported by KDTree or BallTree!
"Cosine and arccoss both crash HDBSCAN. They go down the BallTree path, but"
the implementation does not support them.
Distance matrix
Sparse matrix
--- Detecting Branches
Generate single-cluster data
"Without persistence, find 6 branches"
"Mac & Windows give 71, Linux gives 72. Probably different random values."
Adding presistence removes some branches
--- Branch Detector Functionality
A point on a branch (not noise) exact labels change per run
A point in a cluster
A noise point
--- Attribute Output Formats
--- Attribute plots
Ignore future warnings thrown by sklearn
Create a nice dataset with 6 circular clusters and 2 moons
"Given, the base HDBSCAN with method 'eom'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
"Given, the base HDBSCAN with method 'leaf'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
Method 'eom'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
Method 'leaf'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
"Given the max number of clusters that can be produced by 'eom',"
(these are produced for epsilon=0) (??? Needs verification)
"When we try flat clustering with 'eom' method for more n_clusters,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
"assert ""Cannot predict"" in str(w[-1].message)"
"the resulting clusterer switches to using method 'leaf',"
and the resulting probabilities and labels must match
"Given the base HDBSCAN trained on some data,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the clustering should match that due to approximate_predict,"
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the number of clusters produced must match the original n_clusters"
and all probabilities are <= 1.
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat with specified n_clusters,"
"Then, the requested number of clusters must be produced"
and all probabilities are <= 1.
When using approximate_predict_flat with more clusters
"than 'eom' can handle,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
But the requested number of clusters must still be produced using 'leaf'
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
The dependencies are the same as the contents of requirements.txt
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Check for connected component on X
Compute sparse mutual reachability graph
"if max_dist > 0, max distance to use when the reachability is infinite"
Check connected component on mutual reachability
"If more than one component, it means that even if the distance matrix X"
"has one component, there exists with less than `min_samples` neighbors"
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Non-precomputed matrices may contain non-finite values.
Rows with these values
Pass only the purely finite indices into hdbscan
We will later assign all non-finite points to the background -1 cluster
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
remap indices to align with original data in the case of non-finite entries.
"Unltimately, for each Ci, we only require the"
"minimum of DSPC(Ci, Cj) over all Cj != Ci."
"So let's call this value DSPC_wrt(Ci), i.e."
density separation 'with respect to' Ci.
If exactly one of the points is noise
Set the density sparseness of the cluster
to the sparsest value seen so far.
Check whether density separations with
respect to each of these clusters can
be reduced.
"In case min_outlier_sep is still np.inf, we assign a new value to it."
This only makes sense if num_clusters = 1 since it has turned out
that the MR-MST has no edges between a noise point and a core point.
DSPC_wrt[Ci] might be infinite if the connected component for Ci is
"an ""island"" in the MR-MST. Whereas for other clusters Cj and Ck, the"
MR-MST might contain an edge with one point in Cj and ther other one
"in Ck. Here, we replace the infinite density separation of Ci by"
another large enough value.
""
TODO: Think of a better yet efficient way to handle this.
Extract flat clustering from HDBSCAN's hierarchy for 7 clusters
Use a previously initialized/trained HDBSCAN
Handle the trivial case first.
Always generate prediction_data to avoid later woes
This will later be chosen according to n_clusters
Initialize and train clusterer if one was not previously supplied.
Always generate prediction data
We do not pass cluster_selection_epsilon here.
"While this adds unnecessary computation, it makes the code"
easier to read and debug.
"Train on 'X'. Do this even if the supplied clusterer was trained,"
because we want to make sure it fits 'X'.
"Pick an epsilon value right after a split produces n_clusters,"
and the don't split further for smaller epsilon (larger lambda)
Or use the specified cluster_selection_epsilon
"Extract tree related stuff, in order to re-assign labels"
Get labels according to the required cluster_selection_epsilon
Reflect the related changes in HDBSCAN.
PredictionData attached to HDBSCAN should also change.
A function re_init is defined in this module to handle this.
"From a fitted HDBSCAN model, predict for n_clusters=5"
Store prediciton data for later use.
and use this prediction data to predict on new points
Get number of fitted clusters for later use.
We'll need the condensed tree later...
"If none of the three arguments: prediction_data, n_clusters,"
"and cluster_selection_epsilon are supplied,"
then use clusterer's prediciton data directly
"If either of n_clusters or cluster_selection_epsilon were supplied,"
then build prediction data from these by modifying clusterer's
Get prediction data from clusterer
Modify prediction_data to reflect new n_clusters
"First, make a copy of prediction data to avoid modifying source"
Cluster selection method is hold by condensed_tree.
Change from 'eom' to 'leaf' if n_clusters is too large.
This change does not affect the tree associated with 'clusterer'
Re-initialize prediction_data for the specified n_clusters or epsilon
============================================================
Now we're ready to use prediction_data
"The rest of the code is copied from HDBSCAN's approximate_predict,"
but modified to use prediction_data instead of clusterer's attribute
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering from prediction data
Initialize probabilities
k-NN for prediciton points to training set
Loop over prediction points to compute probabilities
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"Find row in tree where nearest neighbor drops out,"
so we can get a lambda value for the nearest neighbor
"Assign lambda as min(lambda-to-neighbor, neighbor's-lambda-to-tree)"
"Equivalently, this assigns core distance for prediction point as"
"max(dist-to-neighbor, neighbor's-dist-to-tree)"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the nearest exemplar persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Merge the two probabilities to produce a single set of probabilities
Include probability that the nearest neighbor belongs to a cluster
Rename variable so it's easy to understand what's being returned
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering at the chosen epsilon from prediction_data
"When no clusters found, return array of 0's"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the point persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Include probability that the point belongs to a cluster
Aggregate the three probabilities to produce membership vectors
Re-name variable to clarify what's being returned.
"With method 'eom', max clusters are produced for epsilon=0,"
as computed by
Increasing epsilon can only reduce the number of ouput clusters.
"To select epsilon, consider all values where clusters are split"
Subtract the extra e-12 to avoid numerical errors in comparison
"Then, we avoid splitting for all epsilon below this."
Use an epsilon value that produces the right number of clusters.
The condensed tree of HDBSCAN has this information.
Extract the lambda levels (=1/distance) from the condensed tree
We don't want values that produce a large cluster and
just one or two individual points.
Keep only those lambda values corresponding to cluster separation;
"i.e., with child_sizes > 1"
"Get the unique values, because when two clusters fall out of one,"
the entry with lambda is repeated.
lambda values are sorted by np.unique.
"Now, get epsilon (distance threshold) as 1/lambda"
"At this epsilon, n_clusters have been split."
Stop splits at epsilons smaller than this.
"To allow for numerical errors,"
predData must be a pre-trained PredictionData instance from hdbscan
"If n_clusters is specified, compute cluster_selection_epsilon;"
This is the key modification:
Select clusters according to selection method and epsilon.
_new_select_clusters is a modification of get_clusters
from hdbscan._hdbscan_tree
"raw tree, used later to get exemplars and lambda values"
"Re-do the cluster map: Map cluster numbers in tree (N, N+1, ..)"
to the cluster labels produced as output
Re-compute lambdas and exemplars for selected clusters;
max_lambda <=> smallest distance <=> most persistent point(s)
Map all sub-clusters of selected cluster to the selected cluster's
label in output.
Map lambdas too...
Create set of exemplar points for later use.
Novel points are assigned based on cluster of closest exemplar.
"For each selected cluster, get all of its leaves,"
"and leaves of leaves, and so on..."
Largest lambda => Most persistent points
Get the most persistent points
Add most persistent points as exemplars
Add exemplars for each leaf of each selected cluster.
(exclude root)
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Extract the chosen cluster bounds. If enough duplicate data points exist in the
data the lambda value might be infinite. This breaks labeling and highlighting
the chosen clusters.
Extract the plot range of the y-axis and set default center and height values for ellipses.
Extremly dense clusters might result in near infinite lambda values. Setting max_height
based on the percentile should alleviate the impact on plotting.
Set center and height to default values if necessary
Ensure the ellipse is visible
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
"node_desat=None,"
Extract node color data
Extract edge color data
Compute or extract layout
Add edges
Add FLASC features
Add raw data features
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
"the point is in the dataset, fix lambda for rounding errors"
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
Support branch detection within clusters.
Select finite data points
Construct tree
Allocate to maintain data point indices
Find neighbours for non-noise points
Check clusterer state
Validate parameters
Extract state
Configure parallelization
Detect branches
Maintain data indices for non-finite data
Combined result
Branching result
Clusters to branches
List points within cluster
Extract MST edges within cluster
Compute in cluster centrality
Construct cluster approximation graph
Extract centrality MST and compute single linkage
Re-label edges with data ids
Return values
Allocate output (won't be filled completely)
Fill (undirected) MST edges with within-cluster-ids
Fill neighbors with within-cluster-ids
Fill mutual reachabilities
Extract unique edges that stay within the cluster
Query KDTree/BallTree for neighours within the distance
Count number of returned edges per point
Create full edge list
Create output
Reset noise labels to k-cluster
Allocate output
Compute the labels and probabilities
Reorder other parts
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
Disable for now -- need to refactor to meet newer standards
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
this fails if no $DISPLAY specified
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
no prediction data error
wrong dimensions error
no clusters warning
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
without epsilon we should see many noise points as children of root.
for this random seed an epsilon of 0.2 will produce exactly 2 noise
points at that cut in single linkage.
Disable for now -- need to refactor to meet newer standards
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
"Given negative, zero and positive denominator and positive numerator"
Make sure safe division is always positive and doesn't raise ZeroDivision error
--- Branch Detection Data
There are no fast metrics that are not supported by KDTree or BallTree!
"Cosine and arccoss both crash HDBSCAN. They go down the BallTree path, but"
the implementation does not support them.
Distance matrix
Sparse matrix
--- Detecting Branches
Generate single-cluster data
"Without persistence, find 6 branches"
"Mac & Windows give 71, Linux gives 72. Probably different random values."
Adding presistence removes some branches
--- Branch Detector Functionality
A point on a branch (not noise) exact labels change per run
A point in a cluster
A noise point
--- Attribute Output Formats
--- Attribute plots
Ignore future warnings thrown by sklearn
Create a nice dataset with 6 circular clusters and 2 moons
"Given, the base HDBSCAN with method 'eom'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
"Given, the base HDBSCAN with method 'leaf'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
Method 'eom'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
Method 'leaf'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
"Given the max number of clusters that can be produced by 'eom',"
(these are produced for epsilon=0) (??? Needs verification)
"When we try flat clustering with 'eom' method for more n_clusters,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
"assert ""Cannot predict"" in str(w[-1].message)"
"the resulting clusterer switches to using method 'leaf',"
and the resulting probabilities and labels must match
"Given the base HDBSCAN trained on some data,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the clustering should match that due to approximate_predict,"
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the number of clusters produced must match the original n_clusters"
and all probabilities are <= 1.
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat with specified n_clusters,"
"Then, the requested number of clusters must be produced"
and all probabilities are <= 1.
When using approximate_predict_flat with more clusters
"than 'eom' can handle,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
But the requested number of clusters must still be produced using 'leaf'
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
The dependencies are the same as the contents of requirements.txt
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Check for connected component on X
Compute sparse mutual reachability graph
"if max_dist > 0, max distance to use when the reachability is infinite"
Check connected component on mutual reachability
"If more than one component, it means that even if the distance matrix X"
"has one component, there exists with less than `min_samples` neighbors"
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Non-precomputed matrices may contain non-finite values.
Rows with these values
Pass only the purely finite indices into hdbscan
We will later assign all non-finite points to the background -1 cluster
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
remap indices to align with original data in the case of non-finite entries.
"Unltimately, for each Ci, we only require the"
"minimum of DSPC(Ci, Cj) over all Cj != Ci."
"So let's call this value DSPC_wrt(Ci), i.e."
density separation 'with respect to' Ci.
If exactly one of the points is noise
Set the density sparseness of the cluster
to the sparsest value seen so far.
Check whether density separations with
respect to each of these clusters can
be reduced.
"In case min_outlier_sep is still np.inf, we assign a new value to it."
This only makes sense if num_clusters = 1 since it has turned out
that the MR-MST has no edges between a noise point and a core point.
DSPC_wrt[Ci] might be infinite if the connected component for Ci is
"an ""island"" in the MR-MST. Whereas for other clusters Cj and Ck, the"
MR-MST might contain an edge with one point in Cj and ther other one
"in Ck. Here, we replace the infinite density separation of Ci by"
another large enough value.
""
TODO: Think of a better yet efficient way to handle this.
Extract flat clustering from HDBSCAN's hierarchy for 7 clusters
Use a previously initialized/trained HDBSCAN
Handle the trivial case first.
Always generate prediction_data to avoid later woes
This will later be chosen according to n_clusters
Initialize and train clusterer if one was not previously supplied.
Always generate prediction data
We do not pass cluster_selection_epsilon here.
"While this adds unnecessary computation, it makes the code"
easier to read and debug.
"Train on 'X'. Do this even if the supplied clusterer was trained,"
because we want to make sure it fits 'X'.
"Pick an epsilon value right after a split produces n_clusters,"
and the don't split further for smaller epsilon (larger lambda)
Or use the specified cluster_selection_epsilon
"Extract tree related stuff, in order to re-assign labels"
Get labels according to the required cluster_selection_epsilon
Reflect the related changes in HDBSCAN.
PredictionData attached to HDBSCAN should also change.
A function re_init is defined in this module to handle this.
"From a fitted HDBSCAN model, predict for n_clusters=5"
Store prediciton data for later use.
and use this prediction data to predict on new points
Get number of fitted clusters for later use.
We'll need the condensed tree later...
"If none of the three arguments: prediction_data, n_clusters,"
"and cluster_selection_epsilon are supplied,"
then use clusterer's prediciton data directly
"If either of n_clusters or cluster_selection_epsilon were supplied,"
then build prediction data from these by modifying clusterer's
Get prediction data from clusterer
Modify prediction_data to reflect new n_clusters
"First, make a copy of prediction data to avoid modifying source"
Cluster selection method is hold by condensed_tree.
Change from 'eom' to 'leaf' if n_clusters is too large.
This change does not affect the tree associated with 'clusterer'
Re-initialize prediction_data for the specified n_clusters or epsilon
============================================================
Now we're ready to use prediction_data
"The rest of the code is copied from HDBSCAN's approximate_predict,"
but modified to use prediction_data instead of clusterer's attribute
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering from prediction data
Initialize probabilities
k-NN for prediciton points to training set
Loop over prediction points to compute probabilities
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"Find row in tree where nearest neighbor drops out,"
so we can get a lambda value for the nearest neighbor
"Assign lambda as min(lambda-to-neighbor, neighbor's-lambda-to-tree)"
"Equivalently, this assigns core distance for prediction point as"
"max(dist-to-neighbor, neighbor's-dist-to-tree)"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the nearest exemplar persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Merge the two probabilities to produce a single set of probabilities
Include probability that the nearest neighbor belongs to a cluster
Rename variable so it's easy to understand what's being returned
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering at the chosen epsilon from prediction_data
"When no clusters found, return array of 0's"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the point persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Include probability that the point belongs to a cluster
Aggregate the three probabilities to produce membership vectors
Re-name variable to clarify what's being returned.
"With method 'eom', max clusters are produced for epsilon=0,"
as computed by
Increasing epsilon can only reduce the number of ouput clusters.
"To select epsilon, consider all values where clusters are split"
Subtract the extra e-12 to avoid numerical errors in comparison
"Then, we avoid splitting for all epsilon below this."
Use an epsilon value that produces the right number of clusters.
The condensed tree of HDBSCAN has this information.
Extract the lambda levels (=1/distance) from the condensed tree
We don't want values that produce a large cluster and
just one or two individual points.
Keep only those lambda values corresponding to cluster separation;
"i.e., with child_sizes > 1"
"Get the unique values, because when two clusters fall out of one,"
the entry with lambda is repeated.
lambda values are sorted by np.unique.
"Now, get epsilon (distance threshold) as 1/lambda"
"At this epsilon, n_clusters have been split."
Stop splits at epsilons smaller than this.
"To allow for numerical errors,"
predData must be a pre-trained PredictionData instance from hdbscan
"If n_clusters is specified, compute cluster_selection_epsilon;"
This is the key modification:
Select clusters according to selection method and epsilon.
_new_select_clusters is a modification of get_clusters
from hdbscan._hdbscan_tree
"raw tree, used later to get exemplars and lambda values"
"Re-do the cluster map: Map cluster numbers in tree (N, N+1, ..)"
to the cluster labels produced as output
Re-compute lambdas and exemplars for selected clusters;
max_lambda <=> smallest distance <=> most persistent point(s)
Map all sub-clusters of selected cluster to the selected cluster's
label in output.
Map lambdas too...
Create set of exemplar points for later use.
Novel points are assigned based on cluster of closest exemplar.
"For each selected cluster, get all of its leaves,"
"and leaves of leaves, and so on..."
Largest lambda => Most persistent points
Get the most persistent points
Add most persistent points as exemplars
Add exemplars for each leaf of each selected cluster.
(exclude root)
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Extract the chosen cluster bounds. If enough duplicate data points exist in the
data the lambda value might be infinite. This breaks labeling and highlighting
the chosen clusters.
Extract the plot range of the y-axis and set default center and height values for ellipses.
Extremly dense clusters might result in near infinite lambda values. Setting max_height
based on the percentile should alleviate the impact on plotting.
Set center and height to default values if necessary
Ensure the ellipse is visible
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
"node_desat=None,"
Extract node color data
Extract edge color data
Compute or extract layout
Add edges
Add FLASC features
Add raw data features
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
"the point is in the dataset, fix lambda for rounding errors"
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
Support branch detection within clusters.
Select finite data points
Construct tree
Allocate to maintain data point indices
Find neighbours for non-noise points
Check clusterer state
Validate parameters
Extract state
Configure parallelization
Detect branches
Maintain data indices for non-finite data
Combined result
Branching result
Clusters to branches
List points within cluster
Extract MST edges within cluster
Compute in cluster centrality
Construct cluster approximation graph
Extract centrality MST and compute single linkage
Re-label edges with data ids
Return values
Allocate output (won't be filled completely)
Fill (undirected) MST edges with within-cluster-ids
Fill neighbors with within-cluster-ids
Fill mutual reachabilities
Extract unique edges that stay within the cluster
Query KDTree/BallTree for neighours within the distance
Count number of returned edges per point
Create full edge list
Create output
Reset noise labels to k-cluster
Allocate output
Compute the labels and probabilities
Reorder other parts
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
Disable for now -- need to refactor to meet newer standards
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
this fails if no $DISPLAY specified
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
no prediction data error
wrong dimensions error
no clusters warning
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
without epsilon we should see many noise points as children of root.
for this random seed an epsilon of 0.2 will produce exactly 2 noise
points at that cut in single linkage.
Disable for now -- need to refactor to meet newer standards
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
"Given negative, zero and positive denominator and positive numerator"
Make sure safe division is always positive and doesn't raise ZeroDivision error
--- Branch Detection Data
There are no fast metrics that are not supported by KDTree or BallTree!
"Cosine and arccoss both crash HDBSCAN. They go down the BallTree path, but"
the implementation does not support them.
Distance matrix
Sparse matrix
--- Detecting Branches
Generate single-cluster data
"Without persistence, find 6 branches"
"Mac & Windows give 71, Linux gives 72. Probably different random values."
Adding presistence removes some branches
--- Branch Detector Functionality
A point on a branch (not noise) exact labels change per run
A point in a cluster
A noise point
--- Attribute Output Formats
--- Attribute plots
Ignore future warnings thrown by sklearn
Create a nice dataset with 6 circular clusters and 2 moons
"Given, the base HDBSCAN with method 'eom'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
"Given, the base HDBSCAN with method 'leaf'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
Method 'eom'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
Method 'leaf'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
"Given the max number of clusters that can be produced by 'eom',"
(these are produced for epsilon=0) (??? Needs verification)
"When we try flat clustering with 'eom' method for more n_clusters,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
"assert ""Cannot predict"" in str(w[-1].message)"
"the resulting clusterer switches to using method 'leaf',"
and the resulting probabilities and labels must match
"Given the base HDBSCAN trained on some data,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the clustering should match that due to approximate_predict,"
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the number of clusters produced must match the original n_clusters"
and all probabilities are <= 1.
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat with specified n_clusters,"
"Then, the requested number of clusters must be produced"
and all probabilities are <= 1.
When using approximate_predict_flat with more clusters
"than 'eom' can handle,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
But the requested number of clusters must still be produced using 'leaf'
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
The dependencies are the same as the contents of requirements.txt
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Check for connected component on X
Compute sparse mutual reachability graph
"if max_dist > 0, max distance to use when the reachability is infinite"
Check connected component on mutual reachability
"If more than one component, it means that even if the distance matrix X"
"has one component, there exists with less than `min_samples` neighbors"
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Non-precomputed matrices may contain non-finite values.
Rows with these values
Pass only the purely finite indices into hdbscan
We will later assign all non-finite points to the background -1 cluster
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
remap indices to align with original data in the case of non-finite entries.
"Unltimately, for each Ci, we only require the"
"minimum of DSPC(Ci, Cj) over all Cj != Ci."
"So let's call this value DSPC_wrt(Ci), i.e."
density separation 'with respect to' Ci.
If exactly one of the points is noise
Set the density sparseness of the cluster
to the sparsest value seen so far.
Check whether density separations with
respect to each of these clusters can
be reduced.
"In case min_outlier_sep is still np.inf, we assign a new value to it."
This only makes sense if num_clusters = 1 since it has turned out
that the MR-MST has no edges between a noise point and a core point.
DSPC_wrt[Ci] might be infinite if the connected component for Ci is
"an ""island"" in the MR-MST. Whereas for other clusters Cj and Ck, the"
MR-MST might contain an edge with one point in Cj and ther other one
"in Ck. Here, we replace the infinite density separation of Ci by"
another large enough value.
""
TODO: Think of a better yet efficient way to handle this.
Extract flat clustering from HDBSCAN's hierarchy for 7 clusters
Use a previously initialized/trained HDBSCAN
Handle the trivial case first.
Always generate prediction_data to avoid later woes
This will later be chosen according to n_clusters
Initialize and train clusterer if one was not previously supplied.
Always generate prediction data
We do not pass cluster_selection_epsilon here.
"While this adds unnecessary computation, it makes the code"
easier to read and debug.
"Train on 'X'. Do this even if the supplied clusterer was trained,"
because we want to make sure it fits 'X'.
"Pick an epsilon value right after a split produces n_clusters,"
and the don't split further for smaller epsilon (larger lambda)
Or use the specified cluster_selection_epsilon
"Extract tree related stuff, in order to re-assign labels"
Get labels according to the required cluster_selection_epsilon
Reflect the related changes in HDBSCAN.
PredictionData attached to HDBSCAN should also change.
A function re_init is defined in this module to handle this.
"From a fitted HDBSCAN model, predict for n_clusters=5"
Store prediciton data for later use.
and use this prediction data to predict on new points
Get number of fitted clusters for later use.
We'll need the condensed tree later...
"If none of the three arguments: prediction_data, n_clusters,"
"and cluster_selection_epsilon are supplied,"
then use clusterer's prediciton data directly
"If either of n_clusters or cluster_selection_epsilon were supplied,"
then build prediction data from these by modifying clusterer's
Get prediction data from clusterer
Modify prediction_data to reflect new n_clusters
"First, make a copy of prediction data to avoid modifying source"
Cluster selection method is hold by condensed_tree.
Change from 'eom' to 'leaf' if n_clusters is too large.
This change does not affect the tree associated with 'clusterer'
Re-initialize prediction_data for the specified n_clusters or epsilon
============================================================
Now we're ready to use prediction_data
"The rest of the code is copied from HDBSCAN's approximate_predict,"
but modified to use prediction_data instead of clusterer's attribute
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering from prediction data
Initialize probabilities
k-NN for prediciton points to training set
Loop over prediction points to compute probabilities
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"Find row in tree where nearest neighbor drops out,"
so we can get a lambda value for the nearest neighbor
"Assign lambda as min(lambda-to-neighbor, neighbor's-lambda-to-tree)"
"Equivalently, this assigns core distance for prediction point as"
"max(dist-to-neighbor, neighbor's-dist-to-tree)"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the nearest exemplar persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Merge the two probabilities to produce a single set of probabilities
Include probability that the nearest neighbor belongs to a cluster
Rename variable so it's easy to understand what's being returned
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering at the chosen epsilon from prediction_data
"When no clusters found, return array of 0's"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the point persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Include probability that the point belongs to a cluster
Aggregate the three probabilities to produce membership vectors
Re-name variable to clarify what's being returned.
"With method 'eom', max clusters are produced for epsilon=0,"
as computed by
Increasing epsilon can only reduce the number of ouput clusters.
"To select epsilon, consider all values where clusters are split"
Subtract the extra e-12 to avoid numerical errors in comparison
"Then, we avoid splitting for all epsilon below this."
Use an epsilon value that produces the right number of clusters.
The condensed tree of HDBSCAN has this information.
Extract the lambda levels (=1/distance) from the condensed tree
We don't want values that produce a large cluster and
just one or two individual points.
Keep only those lambda values corresponding to cluster separation;
"i.e., with child_sizes > 1"
"Get the unique values, because when two clusters fall out of one,"
the entry with lambda is repeated.
lambda values are sorted by np.unique.
"Now, get epsilon (distance threshold) as 1/lambda"
"At this epsilon, n_clusters have been split."
Stop splits at epsilons smaller than this.
"To allow for numerical errors,"
predData must be a pre-trained PredictionData instance from hdbscan
"If n_clusters is specified, compute cluster_selection_epsilon;"
This is the key modification:
Select clusters according to selection method and epsilon.
_new_select_clusters is a modification of get_clusters
from hdbscan._hdbscan_tree
"raw tree, used later to get exemplars and lambda values"
"Re-do the cluster map: Map cluster numbers in tree (N, N+1, ..)"
to the cluster labels produced as output
Re-compute lambdas and exemplars for selected clusters;
max_lambda <=> smallest distance <=> most persistent point(s)
Map all sub-clusters of selected cluster to the selected cluster's
label in output.
Map lambdas too...
Create set of exemplar points for later use.
Novel points are assigned based on cluster of closest exemplar.
"For each selected cluster, get all of its leaves,"
"and leaves of leaves, and so on..."
Largest lambda => Most persistent points
Get the most persistent points
Add most persistent points as exemplars
Add exemplars for each leaf of each selected cluster.
(exclude root)
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Extract the chosen cluster bounds. If enough duplicate data points exist in the
data the lambda value might be infinite. This breaks labeling and highlighting
the chosen clusters.
Extract the plot range of the y-axis and set default center and height values for ellipses.
Extremly dense clusters might result in near infinite lambda values. Setting max_height
based on the percentile should alleviate the impact on plotting.
Set center and height to default values if necessary
Ensure the ellipse is visible
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
"node_desat=None,"
Extract node color data
Extract edge color data
Compute or extract layout
Add edges
Add FLASC features
Add raw data features
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
"the point is in the dataset, fix lambda for rounding errors"
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
Support branch detection within clusters.
Select finite data points
Construct tree
Allocate to maintain data point indices
Find neighbours for non-noise points
Check clusterer state
Validate parameters
Extract state
Configure parallelization
Detect branches
Maintain data indices for non-finite data
Combined result
Branching result
Clusters to branches
List points within cluster
Extract MST edges within cluster
Compute in cluster centrality
Construct cluster approximation graph
Extract centrality MST and compute single linkage
Re-label edges with data ids
Return values
Allocate output (won't be filled completely)
Fill (undirected) MST edges with within-cluster-ids
Fill neighbors with within-cluster-ids
Fill mutual reachabilities
Extract unique edges that stay within the cluster
Query KDTree/BallTree for neighours within the distance
Count number of returned edges per point
Create full edge list
Create output
Reset noise labels to k-cluster
Allocate output
Compute the labels and probabilities
Reorder other parts
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
Disable for now -- need to refactor to meet newer standards
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
this fails if no $DISPLAY specified
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
no prediction data error
wrong dimensions error
no clusters warning
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
without epsilon we should see many noise points as children of root.
for this random seed an epsilon of 0.2 will produce exactly 2 noise
points at that cut in single linkage.
Disable for now -- need to refactor to meet newer standards
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
"Given negative, zero and positive denominator and positive numerator"
Make sure safe division is always positive and doesn't raise ZeroDivision error
--- Branch Detection Data
There are no fast metrics that are not supported by KDTree or BallTree!
"Cosine and arccoss both crash HDBSCAN. They go down the BallTree path, but"
the implementation does not support them.
Distance matrix
Sparse matrix
--- Detecting Branches
Generate single-cluster data
"Without persistence, find 6 branches"
"Mac & Windows give 71, Linux gives 72. Probably different random values."
Adding presistence removes some branches
--- Branch Detector Functionality
A point on a branch (not noise) exact labels change per run
A point in a cluster
A noise point
--- Attribute Output Formats
--- Attribute plots
Ignore future warnings thrown by sklearn
Create a nice dataset with 6 circular clusters and 2 moons
"Given, the base HDBSCAN with method 'eom'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
"Given, the base HDBSCAN with method 'leaf'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
Method 'eom'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
Method 'leaf'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
"Given the max number of clusters that can be produced by 'eom',"
(these are produced for epsilon=0) (??? Needs verification)
"When we try flat clustering with 'eom' method for more n_clusters,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
"assert ""Cannot predict"" in str(w[-1].message)"
"the resulting clusterer switches to using method 'leaf',"
and the resulting probabilities and labels must match
"Given the base HDBSCAN trained on some data,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the clustering should match that due to approximate_predict,"
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the number of clusters produced must match the original n_clusters"
and all probabilities are <= 1.
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat with specified n_clusters,"
"Then, the requested number of clusters must be produced"
and all probabilities are <= 1.
When using approximate_predict_flat with more clusters
"than 'eom' can handle,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
But the requested number of clusters must still be produced using 'leaf'
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
The dependencies are the same as the contents of requirements.txt
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Check for connected component on X
Compute sparse mutual reachability graph
"if max_dist > 0, max distance to use when the reachability is infinite"
Check connected component on mutual reachability
"If more than one component, it means that even if the distance matrix X"
"has one component, there exists with less than `min_samples` neighbors"
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Non-precomputed matrices may contain non-finite values.
Rows with these values
Pass only the purely finite indices into hdbscan
We will later assign all non-finite points to the background -1 cluster
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
remap indices to align with original data in the case of non-finite entries.
"Unltimately, for each Ci, we only require the"
"minimum of DSPC(Ci, Cj) over all Cj != Ci."
"So let's call this value DSPC_wrt(Ci), i.e."
density separation 'with respect to' Ci.
If exactly one of the points is noise
Set the density sparseness of the cluster
to the sparsest value seen so far.
Check whether density separations with
respect to each of these clusters can
be reduced.
"In case min_outlier_sep is still np.inf, we assign a new value to it."
This only makes sense if num_clusters = 1 since it has turned out
that the MR-MST has no edges between a noise point and a core point.
DSPC_wrt[Ci] might be infinite if the connected component for Ci is
"an ""island"" in the MR-MST. Whereas for other clusters Cj and Ck, the"
MR-MST might contain an edge with one point in Cj and ther other one
"in Ck. Here, we replace the infinite density separation of Ci by"
another large enough value.
""
TODO: Think of a better yet efficient way to handle this.
Extract flat clustering from HDBSCAN's hierarchy for 7 clusters
Use a previously initialized/trained HDBSCAN
Handle the trivial case first.
Always generate prediction_data to avoid later woes
This will later be chosen according to n_clusters
Initialize and train clusterer if one was not previously supplied.
Always generate prediction data
We do not pass cluster_selection_epsilon here.
"While this adds unnecessary computation, it makes the code"
easier to read and debug.
"Train on 'X'. Do this even if the supplied clusterer was trained,"
because we want to make sure it fits 'X'.
"Pick an epsilon value right after a split produces n_clusters,"
and the don't split further for smaller epsilon (larger lambda)
Or use the specified cluster_selection_epsilon
"Extract tree related stuff, in order to re-assign labels"
Get labels according to the required cluster_selection_epsilon
Reflect the related changes in HDBSCAN.
PredictionData attached to HDBSCAN should also change.
A function re_init is defined in this module to handle this.
"From a fitted HDBSCAN model, predict for n_clusters=5"
Store prediciton data for later use.
and use this prediction data to predict on new points
Get number of fitted clusters for later use.
We'll need the condensed tree later...
"If none of the three arguments: prediction_data, n_clusters,"
"and cluster_selection_epsilon are supplied,"
then use clusterer's prediciton data directly
"If either of n_clusters or cluster_selection_epsilon were supplied,"
then build prediction data from these by modifying clusterer's
Get prediction data from clusterer
Modify prediction_data to reflect new n_clusters
"First, make a copy of prediction data to avoid modifying source"
Cluster selection method is hold by condensed_tree.
Change from 'eom' to 'leaf' if n_clusters is too large.
This change does not affect the tree associated with 'clusterer'
Re-initialize prediction_data for the specified n_clusters or epsilon
============================================================
Now we're ready to use prediction_data
"The rest of the code is copied from HDBSCAN's approximate_predict,"
but modified to use prediction_data instead of clusterer's attribute
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering from prediction data
Initialize probabilities
k-NN for prediciton points to training set
Loop over prediction points to compute probabilities
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"Find row in tree where nearest neighbor drops out,"
so we can get a lambda value for the nearest neighbor
"Assign lambda as min(lambda-to-neighbor, neighbor's-lambda-to-tree)"
"Equivalently, this assigns core distance for prediction point as"
"max(dist-to-neighbor, neighbor's-dist-to-tree)"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the nearest exemplar persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Merge the two probabilities to produce a single set of probabilities
Include probability that the nearest neighbor belongs to a cluster
Rename variable so it's easy to understand what's being returned
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering at the chosen epsilon from prediction_data
"When no clusters found, return array of 0's"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the point persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Include probability that the point belongs to a cluster
Aggregate the three probabilities to produce membership vectors
Re-name variable to clarify what's being returned.
"With method 'eom', max clusters are produced for epsilon=0,"
as computed by
Increasing epsilon can only reduce the number of ouput clusters.
"To select epsilon, consider all values where clusters are split"
Subtract the extra e-12 to avoid numerical errors in comparison
"Then, we avoid splitting for all epsilon below this."
Use an epsilon value that produces the right number of clusters.
The condensed tree of HDBSCAN has this information.
Extract the lambda levels (=1/distance) from the condensed tree
We don't want values that produce a large cluster and
just one or two individual points.
Keep only those lambda values corresponding to cluster separation;
"i.e., with child_sizes > 1"
"Get the unique values, because when two clusters fall out of one,"
the entry with lambda is repeated.
lambda values are sorted by np.unique.
"Now, get epsilon (distance threshold) as 1/lambda"
"At this epsilon, n_clusters have been split."
Stop splits at epsilons smaller than this.
"To allow for numerical errors,"
predData must be a pre-trained PredictionData instance from hdbscan
"If n_clusters is specified, compute cluster_selection_epsilon;"
This is the key modification:
Select clusters according to selection method and epsilon.
_new_select_clusters is a modification of get_clusters
from hdbscan._hdbscan_tree
"raw tree, used later to get exemplars and lambda values"
"Re-do the cluster map: Map cluster numbers in tree (N, N+1, ..)"
to the cluster labels produced as output
Re-compute lambdas and exemplars for selected clusters;
max_lambda <=> smallest distance <=> most persistent point(s)
Map all sub-clusters of selected cluster to the selected cluster's
label in output.
Map lambdas too...
Create set of exemplar points for later use.
Novel points are assigned based on cluster of closest exemplar.
"For each selected cluster, get all of its leaves,"
"and leaves of leaves, and so on..."
Largest lambda => Most persistent points
Get the most persistent points
Add most persistent points as exemplars
Add exemplars for each leaf of each selected cluster.
(exclude root)
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Extract the chosen cluster bounds. If enough duplicate data points exist in the
data the lambda value might be infinite. This breaks labeling and highlighting
the chosen clusters.
Extract the plot range of the y-axis and set default center and height values for ellipses.
Extremly dense clusters might result in near infinite lambda values. Setting max_height
based on the percentile should alleviate the impact on plotting.
Set center and height to default values if necessary
Ensure the ellipse is visible
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
"node_desat=None,"
Extract node color data
Extract edge color data
Compute or extract layout
Add edges
Add FLASC features
Add raw data features
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
"the point is in the dataset, fix lambda for rounding errors"
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
Support branch detection within clusters.
Select finite data points
Construct tree
Allocate to maintain data point indices
Find neighbours for non-noise points
Check clusterer state
Validate parameters
Extract state
Configure parallelization
Detect branches
Maintain data indices for non-finite data
Combined result
Branching result
Clusters to branches
List points within cluster
Extract MST edges within cluster
Compute in cluster centrality
Construct cluster approximation graph
Extract centrality MST and compute single linkage
Re-label edges with data ids
Return values
Allocate output (won't be filled completely)
Fill (undirected) MST edges with within-cluster-ids
Fill neighbors with within-cluster-ids
Fill mutual reachabilities
Extract unique edges that stay within the cluster
Query KDTree/BallTree for neighours within the distance
Count number of returned edges per point
Create full edge list
Create output
Reset noise labels to k-cluster
Allocate output
Compute the labels and probabilities
Reorder other parts
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
Disable for now -- need to refactor to meet newer standards
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
this fails if no $DISPLAY specified
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
no prediction data error
wrong dimensions error
no clusters warning
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
without epsilon we should see many noise points as children of root.
for this random seed an epsilon of 0.2 will produce exactly 2 noise
points at that cut in single linkage.
Disable for now -- need to refactor to meet newer standards
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
"Given negative, zero and positive denominator and positive numerator"
Make sure safe division is always positive and doesn't raise ZeroDivision error
--- Branch Detection Data
There are no fast metrics that are not supported by KDTree or BallTree!
"Cosine and arccoss both crash HDBSCAN. They go down the BallTree path, but"
the implementation does not support them.
Distance matrix
Sparse matrix
--- Detecting Branches
Generate single-cluster data
"Without persistence, find 6 branches"
"Mac & Windows give 71, Linux gives 72. Probably different random values."
Adding presistence removes some branches
--- Branch Detector Functionality
A point on a branch (not noise) exact labels change per run
A point in a cluster
A noise point
--- Attribute Output Formats
--- Attribute plots
Ignore future warnings thrown by sklearn
Create a nice dataset with 6 circular clusters and 2 moons
"Given, the base HDBSCAN with method 'eom'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
"Given, the base HDBSCAN with method 'leaf'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
Method 'eom'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
Method 'leaf'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
"Given the max number of clusters that can be produced by 'eom',"
(these are produced for epsilon=0) (??? Needs verification)
"When we try flat clustering with 'eom' method for more n_clusters,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
"assert ""Cannot predict"" in str(w[-1].message)"
"the resulting clusterer switches to using method 'leaf',"
and the resulting probabilities and labels must match
"Given the base HDBSCAN trained on some data,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the clustering should match that due to approximate_predict,"
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the number of clusters produced must match the original n_clusters"
and all probabilities are <= 1.
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat with specified n_clusters,"
"Then, the requested number of clusters must be produced"
and all probabilities are <= 1.
When using approximate_predict_flat with more clusters
"than 'eom' can handle,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
But the requested number of clusters must still be produced using 'leaf'
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
The dependencies are the same as the contents of requirements.txt
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Check for connected component on X
Compute sparse mutual reachability graph
"if max_dist > 0, max distance to use when the reachability is infinite"
Check connected component on mutual reachability
"If more than one component, it means that even if the distance matrix X"
"has one component, there exists with less than `min_samples` neighbors"
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Non-precomputed matrices may contain non-finite values.
Rows with these values
Pass only the purely finite indices into hdbscan
We will later assign all non-finite points to the background -1 cluster
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
remap indices to align with original data in the case of non-finite entries.
"Unltimately, for each Ci, we only require the"
"minimum of DSPC(Ci, Cj) over all Cj != Ci."
"So let's call this value DSPC_wrt(Ci), i.e."
density separation 'with respect to' Ci.
If exactly one of the points is noise
Set the density sparseness of the cluster
to the sparsest value seen so far.
Check whether density separations with
respect to each of these clusters can
be reduced.
"In case min_outlier_sep is still np.inf, we assign a new value to it."
This only makes sense if num_clusters = 1 since it has turned out
that the MR-MST has no edges between a noise point and a core point.
DSPC_wrt[Ci] might be infinite if the connected component for Ci is
"an ""island"" in the MR-MST. Whereas for other clusters Cj and Ck, the"
MR-MST might contain an edge with one point in Cj and ther other one
"in Ck. Here, we replace the infinite density separation of Ci by"
another large enough value.
""
TODO: Think of a better yet efficient way to handle this.
Extract flat clustering from HDBSCAN's hierarchy for 7 clusters
Use a previously initialized/trained HDBSCAN
Handle the trivial case first.
Always generate prediction_data to avoid later woes
This will later be chosen according to n_clusters
Initialize and train clusterer if one was not previously supplied.
Always generate prediction data
We do not pass cluster_selection_epsilon here.
"While this adds unnecessary computation, it makes the code"
easier to read and debug.
"Train on 'X'. Do this even if the supplied clusterer was trained,"
because we want to make sure it fits 'X'.
"Pick an epsilon value right after a split produces n_clusters,"
and the don't split further for smaller epsilon (larger lambda)
Or use the specified cluster_selection_epsilon
"Extract tree related stuff, in order to re-assign labels"
Get labels according to the required cluster_selection_epsilon
Reflect the related changes in HDBSCAN.
PredictionData attached to HDBSCAN should also change.
A function re_init is defined in this module to handle this.
"From a fitted HDBSCAN model, predict for n_clusters=5"
Store prediciton data for later use.
and use this prediction data to predict on new points
Get number of fitted clusters for later use.
We'll need the condensed tree later...
"If none of the three arguments: prediction_data, n_clusters,"
"and cluster_selection_epsilon are supplied,"
then use clusterer's prediciton data directly
"If either of n_clusters or cluster_selection_epsilon were supplied,"
then build prediction data from these by modifying clusterer's
Get prediction data from clusterer
Modify prediction_data to reflect new n_clusters
"First, make a copy of prediction data to avoid modifying source"
Cluster selection method is hold by condensed_tree.
Change from 'eom' to 'leaf' if n_clusters is too large.
This change does not affect the tree associated with 'clusterer'
Re-initialize prediction_data for the specified n_clusters or epsilon
============================================================
Now we're ready to use prediction_data
"The rest of the code is copied from HDBSCAN's approximate_predict,"
but modified to use prediction_data instead of clusterer's attribute
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering from prediction data
Initialize probabilities
k-NN for prediciton points to training set
Loop over prediction points to compute probabilities
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"Find row in tree where nearest neighbor drops out,"
so we can get a lambda value for the nearest neighbor
"Assign lambda as min(lambda-to-neighbor, neighbor's-lambda-to-tree)"
"Equivalently, this assigns core distance for prediction point as"
"max(dist-to-neighbor, neighbor's-dist-to-tree)"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the nearest exemplar persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Merge the two probabilities to produce a single set of probabilities
Include probability that the nearest neighbor belongs to a cluster
Rename variable so it's easy to understand what's being returned
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering at the chosen epsilon from prediction_data
"When no clusters found, return array of 0's"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the point persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Include probability that the point belongs to a cluster
Aggregate the three probabilities to produce membership vectors
Re-name variable to clarify what's being returned.
"With method 'eom', max clusters are produced for epsilon=0,"
as computed by
Increasing epsilon can only reduce the number of ouput clusters.
"To select epsilon, consider all values where clusters are split"
Subtract the extra e-12 to avoid numerical errors in comparison
"Then, we avoid splitting for all epsilon below this."
Use an epsilon value that produces the right number of clusters.
The condensed tree of HDBSCAN has this information.
Extract the lambda levels (=1/distance) from the condensed tree
We don't want values that produce a large cluster and
just one or two individual points.
Keep only those lambda values corresponding to cluster separation;
"i.e., with child_sizes > 1"
"Get the unique values, because when two clusters fall out of one,"
the entry with lambda is repeated.
lambda values are sorted by np.unique.
"Now, get epsilon (distance threshold) as 1/lambda"
"At this epsilon, n_clusters have been split."
Stop splits at epsilons smaller than this.
"To allow for numerical errors,"
predData must be a pre-trained PredictionData instance from hdbscan
"If n_clusters is specified, compute cluster_selection_epsilon;"
This is the key modification:
Select clusters according to selection method and epsilon.
_new_select_clusters is a modification of get_clusters
from hdbscan._hdbscan_tree
"raw tree, used later to get exemplars and lambda values"
"Re-do the cluster map: Map cluster numbers in tree (N, N+1, ..)"
to the cluster labels produced as output
Re-compute lambdas and exemplars for selected clusters;
max_lambda <=> smallest distance <=> most persistent point(s)
Map all sub-clusters of selected cluster to the selected cluster's
label in output.
Map lambdas too...
Create set of exemplar points for later use.
Novel points are assigned based on cluster of closest exemplar.
"For each selected cluster, get all of its leaves,"
"and leaves of leaves, and so on..."
Largest lambda => Most persistent points
Get the most persistent points
Add most persistent points as exemplars
Add exemplars for each leaf of each selected cluster.
(exclude root)
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Extract the chosen cluster bounds. If enough duplicate data points exist in the
data the lambda value might be infinite. This breaks labeling and highlighting
the chosen clusters.
Extract the plot range of the y-axis and set default center and height values for ellipses.
Extremly dense clusters might result in near infinite lambda values. Setting max_height
based on the percentile should alleviate the impact on plotting.
Set center and height to default values if necessary
Ensure the ellipse is visible
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
"the point is in the dataset, fix lambda for rounding errors"
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
Disable for now -- need to refactor to meet newer standards
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
this fails if no $DISPLAY specified
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
no prediction data error
wrong dimensions error
no clusters warning
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
without epsilon we should see many noise points as children of root.
for this random seed an epsilon of 0.2 will produce exactly 2 noise
points at that cut in single linkage.
Disable for now -- need to refactor to meet newer standards
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
"Given negative, zero and positive denominator and positive numerator"
Make sure safe division is always positive and doesn't raise ZeroDivision error
Ignore future warnings thrown by sklearn
Create a nice dataset with 6 circular clusters and 2 moons
"Given, the base HDBSCAN with method 'eom'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
"Given, the base HDBSCAN with method 'leaf'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
Method 'eom'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
Method 'leaf'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
"Given the max number of clusters that can be produced by 'eom',"
(these are produced for epsilon=0) (??? Needs verification)
"When we try flat clustering with 'eom' method for more n_clusters,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
"assert ""Cannot predict"" in str(w[-1].message)"
"the resulting clusterer switches to using method 'leaf',"
and the resulting probabilities and labels must match
"Given the base HDBSCAN trained on some data,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the clustering should match that due to approximate_predict,"
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the number of clusters produced must match the original n_clusters"
and all probabilities are <= 1.
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat with specified n_clusters,"
"Then, the requested number of clusters must be produced"
and all probabilities are <= 1.
When using approximate_predict_flat with more clusters
"than 'eom' can handle,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
But the requested number of clusters must still be produced using 'leaf'
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
The dependencies are the same as the contents of requirements.txt
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Check for connected component on X
Compute sparse mutual reachability graph
"if max_dist > 0, max distance to use when the reachability is infinite"
Check connected component on mutual reachability
"If more than one component, it means that even if the distance matrix X"
"has one component, there exists with less than `min_samples` neighbors"
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Non-precomputed matrices may contain non-finite values.
Rows with these values
Pass only the purely finite indices into hdbscan
We will later assign all non-finite points to the background -1 cluster
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
remap indices to align with original data in the case of non-finite entries.
"Unltimately, for each Ci, we only require the"
"minimum of DSPC(Ci, Cj) over all Cj != Ci."
"So let's call this value DSPC_wrt(Ci), i.e."
density separation 'with respect to' Ci.
If exactly one of the points is noise
Set the density sparseness of the cluster
to the sparsest value seen so far.
Check whether density separations with
respect to each of these clusters can
be reduced.
"In case min_outlier_sep is still np.inf, we assign a new value to it."
This only makes sense if num_clusters = 1 since it has turned out
that the MR-MST has no edges between a noise point and a core point.
DSPC_wrt[Ci] might be infinite if the connected component for Ci is
"an ""island"" in the MR-MST. Whereas for other clusters Cj and Ck, the"
MR-MST might contain an edge with one point in Cj and ther other one
"in Ck. Here, we replace the infinite density separation of Ci by"
another large enough value.
""
TODO: Think of a better yet efficient way to handle this.
Extract flat clustering from HDBSCAN's hierarchy for 7 clusters
Use a previously initialized/trained HDBSCAN
Handle the trivial case first.
Always generate prediction_data to avoid later woes
This will later be chosen according to n_clusters
Initialize and train clusterer if one was not previously supplied.
Always generate prediction data
We do not pass cluster_selection_epsilon here.
"While this adds unnecessary computation, it makes the code"
easier to read and debug.
"Train on 'X'. Do this even if the supplied clusterer was trained,"
because we want to make sure it fits 'X'.
"Pick an epsilon value right after a split produces n_clusters,"
and the don't split further for smaller epsilon (larger lambda)
Or use the specified cluster_selection_epsilon
"Extract tree related stuff, in order to re-assign labels"
Get labels according to the required cluster_selection_epsilon
Reflect the related changes in HDBSCAN.
PredictionData attached to HDBSCAN should also change.
A function re_init is defined in this module to handle this.
"From a fitted HDBSCAN model, predict for n_clusters=5"
Store prediciton data for later use.
and use this prediction data to predict on new points
Get number of fitted clusters for later use.
We'll need the condensed tree later...
"If none of the three arguments: prediction_data, n_clusters,"
"and cluster_selection_epsilon are supplied,"
then use clusterer's prediciton data directly
"If either of n_clusters or cluster_selection_epsilon were supplied,"
then build prediction data from these by modifying clusterer's
Get prediction data from clusterer
Modify prediction_data to reflect new n_clusters
"First, make a copy of prediction data to avoid modifying source"
Cluster selection method is hold by condensed_tree.
Change from 'eom' to 'leaf' if n_clusters is too large.
This change does not affect the tree associated with 'clusterer'
Re-initialize prediction_data for the specified n_clusters or epsilon
============================================================
Now we're ready to use prediction_data
"The rest of the code is copied from HDBSCAN's approximate_predict,"
but modified to use prediction_data instead of clusterer's attribute
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering from prediction data
Initialize probabilities
k-NN for prediciton points to training set
Loop over prediction points to compute probabilities
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"Find row in tree where nearest neighbor drops out,"
so we can get a lambda value for the nearest neighbor
"Assign lambda as min(lambda-to-neighbor, neighbor's-lambda-to-tree)"
"Equivalently, this assigns core distance for prediction point as"
"max(dist-to-neighbor, neighbor's-dist-to-tree)"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the nearest exemplar persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Merge the two probabilities to produce a single set of probabilities
Include probability that the nearest neighbor belongs to a cluster
Rename variable so it's easy to understand what's being returned
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering at the chosen epsilon from prediction_data
"When no clusters found, return array of 0's"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the point persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Include probability that the point belongs to a cluster
Aggregate the three probabilities to produce membership vectors
Re-name variable to clarify what's being returned.
"With method 'eom', max clusters are produced for epsilon=0,"
as computed by
Increasing epsilon can only reduce the number of ouput clusters.
"To select epsilon, consider all values where clusters are split"
Subtract the extra e-12 to avoid numerical errors in comparison
"Then, we avoid splitting for all epsilon below this."
Use an epsilon value that produces the right number of clusters.
The condensed tree of HDBSCAN has this information.
Extract the lambda levels (=1/distance) from the condensed tree
We don't want values that produce a large cluster and
just one or two individual points.
Keep only those lambda values corresponding to cluster separation;
"i.e., with child_sizes > 1"
"Get the unique values, because when two clusters fall out of one,"
the entry with lambda is repeated.
lambda values are sorted by np.unique.
"Now, get epsilon (distance threshold) as 1/lambda"
"At this epsilon, n_clusters have been split."
Stop splits at epsilons smaller than this.
"To allow for numerical errors,"
predData must be a pre-trained PredictionData instance from hdbscan
"If n_clusters is specified, compute cluster_selection_epsilon;"
This is the key modification:
Select clusters according to selection method and epsilon.
_new_select_clusters is a modification of get_clusters
from hdbscan._hdbscan_tree
"raw tree, used later to get exemplars and lambda values"
"Re-do the cluster map: Map cluster numbers in tree (N, N+1, ..)"
to the cluster labels produced as output
Re-compute lambdas and exemplars for selected clusters;
max_lambda <=> smallest distance <=> most persistent point(s)
Map all sub-clusters of selected cluster to the selected cluster's
label in output.
Map lambdas too...
Create set of exemplar points for later use.
Novel points are assigned based on cluster of closest exemplar.
"For each selected cluster, get all of its leaves,"
"and leaves of leaves, and so on..."
Largest lambda => Most persistent points
Get the most persistent points
Add most persistent points as exemplars
Add exemplars for each leaf of each selected cluster.
(exclude root)
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Extract the chosen cluster bounds. If enough duplicate data points exist in the
data the lambda value might be infinite. This breaks labeling and highlighting
the chosen clusters.
Extract the plot range of the y-axis and set default center and height values for ellipses.
Extremly dense clusters might result in near infinite lambda values. Setting max_height
based on the percentile should alleviate the impact on plotting.
Set center and height to default values if necessary
Ensure the ellipse is visible
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
"the point is in the dataset, fix lambda for rounding errors"
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
Disable for now -- need to refactor to meet newer standards
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
this fails if no $DISPLAY specified
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
no prediction data error
wrong dimensions error
no clusters warning
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
without epsilon we should see many noise points as children of root.
for this random seed an epsilon of 0.2 will produce exactly 2 noise
points at that cut in single linkage.
Disable for now -- need to refactor to meet newer standards
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
"Given negative, zero and positive denominator and positive numerator"
Make sure safe division is always positive and doesn't raise ZeroDivision error
Ignore future warnings thrown by sklearn
Create a nice dataset with 6 circular clusters and 2 moons
"Given, the base HDBSCAN with method 'eom'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
"Given, the base HDBSCAN with method 'leaf'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
Method 'eom'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
Method 'leaf'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
"Given the max number of clusters that can be produced by 'eom',"
(these are produced for epsilon=0) (??? Needs verification)
"When we try flat clustering with 'eom' method for more n_clusters,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
"assert ""Cannot predict"" in str(w[-1].message)"
"the resulting clusterer switches to using method 'leaf',"
and the resulting probabilities and labels must match
"Given the base HDBSCAN trained on some data,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the clustering should match that due to approximate_predict,"
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the number of clusters produced must match the original n_clusters"
and all probabilities are <= 1.
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat with specified n_clusters,"
"Then, the requested number of clusters must be produced"
and all probabilities are <= 1.
When using approximate_predict_flat with more clusters
"than 'eom' can handle,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
But the requested number of clusters must still be produced using 'leaf'
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
The dependencies are the same as the contents of requirements.txt
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Check for connected component on X
Compute sparse mutual reachability graph
"if max_dist > 0, max distance to use when the reachability is infinite"
Check connected component on mutual reachability
"If more than one component, it means that even if the distance matrix X"
"has one component, there exists with less than `min_samples` neighbors"
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Non-precomputed matrices may contain non-finite values.
Rows with these values
Pass only the purely finite indices into hdbscan
We will later assign all non-finite points to the background -1 cluster
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
remap indices to align with original data in the case of non-finite entries.
"Unltimately, for each Ci, we only require the"
"minimum of DSPC(Ci, Cj) over all Cj != Ci."
"So let's call this value DSPC_wrt(Ci), i.e."
density separation 'with respect to' Ci.
If exactly one of the points is noise
Set the density sparseness of the cluster
to the sparsest value seen so far.
Check whether density separations with
respect to each of these clusters can
be reduced.
"In case min_outlier_sep is still np.inf, we assign a new value to it."
This only makes sense if num_clusters = 1 since it has turned out
that the MR-MST has no edges between a noise point and a core point.
DSPC_wrt[Ci] might be infinite if the connected component for Ci is
"an ""island"" in the MR-MST. Whereas for other clusters Cj and Ck, the"
MR-MST might contain an edge with one point in Cj and ther other one
"in Ck. Here, we replace the infinite density separation of Ci by"
another large enough value.
""
TODO: Think of a better yet efficient way to handle this.
Extract flat clustering from HDBSCAN's hierarchy for 7 clusters
Use a previously initialized/trained HDBSCAN
Handle the trivial case first.
Always generate prediction_data to avoid later woes
This will later be chosen according to n_clusters
Initialize and train clusterer if one was not previously supplied.
Always generate prediction data
We do not pass cluster_selection_epsilon here.
"While this adds unnecessary computation, it makes the code"
easier to read and debug.
"Train on 'X'. Do this even if the supplied clusterer was trained,"
because we want to make sure it fits 'X'.
"Pick an epsilon value right after a split produces n_clusters,"
and the don't split further for smaller epsilon (larger lambda)
Or use the specified cluster_selection_epsilon
"Extract tree related stuff, in order to re-assign labels"
Get labels according to the required cluster_selection_epsilon
Reflect the related changes in HDBSCAN.
PredictionData attached to HDBSCAN should also change.
A function re_init is defined in this module to handle this.
"From a fitted HDBSCAN model, predict for n_clusters=5"
Store prediciton data for later use.
and use this prediction data to predict on new points
Get number of fitted clusters for later use.
We'll need the condensed tree later...
"If none of the three arguments: prediction_data, n_clusters,"
"and cluster_selection_epsilon are supplied,"
then use clusterer's prediciton data directly
"If either of n_clusters or cluster_selection_epsilon were supplied,"
then build prediction data from these by modifying clusterer's
Get prediction data from clusterer
Modify prediction_data to reflect new n_clusters
"First, make a copy of prediction data to avoid modifying source"
Cluster selection method is hold by condensed_tree.
Change from 'eom' to 'leaf' if n_clusters is too large.
This change does not affect the tree associated with 'clusterer'
Re-initialize prediction_data for the specified n_clusters or epsilon
============================================================
Now we're ready to use prediction_data
"The rest of the code is copied from HDBSCAN's approximate_predict,"
but modified to use prediction_data instead of clusterer's attribute
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering from prediction data
Initialize probabilities
k-NN for prediciton points to training set
Loop over prediction points to compute probabilities
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"Find row in tree where nearest neighbor drops out,"
so we can get a lambda value for the nearest neighbor
"Assign lambda as min(lambda-to-neighbor, neighbor's-lambda-to-tree)"
"Equivalently, this assigns core distance for prediction point as"
"max(dist-to-neighbor, neighbor's-dist-to-tree)"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the nearest exemplar persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Merge the two probabilities to produce a single set of probabilities
Include probability that the nearest neighbor belongs to a cluster
Rename variable so it's easy to understand what's being returned
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering at the chosen epsilon from prediction_data
"When no clusters found, return array of 0's"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the point persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Include probability that the point belongs to a cluster
Aggregate the three probabilities to produce membership vectors
Re-name variable to clarify what's being returned.
"With method 'eom', max clusters are produced for epsilon=0,"
as computed by
Increasing epsilon can only reduce the number of ouput clusters.
"To select epsilon, consider all values where clusters are split"
Subtract the extra e-12 to avoid numerical errors in comparison
"Then, we avoid splitting for all epsilon below this."
Use an epsilon value that produces the right number of clusters.
The condensed tree of HDBSCAN has this information.
Extract the lambda levels (=1/distance) from the condensed tree
We don't want values that produce a large cluster and
just one or two individual points.
Keep only those lambda values corresponding to cluster separation;
"i.e., with child_sizes > 1"
"Get the unique values, because when two clusters fall out of one,"
the entry with lambda is repeated.
lambda values are sorted by np.unique.
"Now, get epsilon (distance threshold) as 1/lambda"
"At this epsilon, n_clusters have been split."
Stop splits at epsilons smaller than this.
"To allow for numerical errors,"
predData must be a pre-trained PredictionData instance from hdbscan
"If n_clusters is specified, compute cluster_selection_epsilon;"
This is the key modification:
Select clusters according to selection method and epsilon.
_new_select_clusters is a modification of get_clusters
from hdbscan._hdbscan_tree
"raw tree, used later to get exemplars and lambda values"
"Re-do the cluster map: Map cluster numbers in tree (N, N+1, ..)"
to the cluster labels produced as output
Re-compute lambdas and exemplars for selected clusters;
max_lambda <=> smallest distance <=> most persistent point(s)
Map all sub-clusters of selected cluster to the selected cluster's
label in output.
Map lambdas too...
Create set of exemplar points for later use.
Novel points are assigned based on cluster of closest exemplar.
"For each selected cluster, get all of its leaves,"
"and leaves of leaves, and so on..."
Largest lambda => Most persistent points
Get the most persistent points
Add most persistent points as exemplars
Add exemplars for each leaf of each selected cluster.
(exclude root)
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Extract the chosen cluster bounds. If enough duplicate data points exist in the
data the lambda value might be infinite. This breaks labeling and highlighting
the chosen clusters.
Extract the plot range of the y-axis and set default center and height values for ellipses.
Extremly dense clusters might result in near infinite lambda values. Setting max_height
based on the percentile should alleviate the impact on plotting.
Set center and height to default values if necessary
Ensure the ellipse is visible
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
"the point is in the dataset, fix lambda for rounding errors"
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
Disable for now -- need to refactor to meet newer standards
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
this fails if no $DISPLAY specified
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
no prediction data error
wrong dimensions error
no clusters warning
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
without epsilon we should see many noise points as children of root.
for this random seed an epsilon of 0.2 will produce exactly 2 noise
points at that cut in single linkage.
Disable for now -- need to refactor to meet newer standards
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
"Given negative, zero and positive denominator and positive numerator"
Make sure safe division is always positive and doesn't raise ZeroDivision error
Ignore future warnings thrown by sklearn
Create a nice dataset with 6 circular clusters and 2 moons
"Given, the base HDBSCAN with method 'eom'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
"Given, the base HDBSCAN with method 'leaf'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
Method 'eom'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
Method 'leaf'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
"Given the max number of clusters that can be produced by 'eom',"
(these are produced for epsilon=0) (??? Needs verification)
"When we try flat clustering with 'eom' method for more n_clusters,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
"assert ""Cannot predict"" in str(w[-1].message)"
"the resulting clusterer switches to using method 'leaf',"
and the resulting probabilities and labels must match
"Given the base HDBSCAN trained on some data,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the clustering should match that due to approximate_predict,"
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the number of clusters produced must match the original n_clusters"
and all probabilities are <= 1.
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat with specified n_clusters,"
"Then, the requested number of clusters must be produced"
and all probabilities are <= 1.
When using approximate_predict_flat with more clusters
"than 'eom' can handle,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
But the requested number of clusters must still be produced using 'leaf'
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
The dependencies are the same as the contents of requirements.txt
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Check for connected component on X
Compute sparse mutual reachability graph
"if max_dist > 0, max distance to use when the reachability is infinite"
Check connected component on mutual reachability
"If more than one component, it means that even if the distance matrix X"
"has one component, there exists with less than `min_samples` neighbors"
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Non-precomputed matrices may contain non-finite values.
Rows with these values
Pass only the purely finite indices into hdbscan
We will later assign all non-finite points to the background -1 cluster
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
remap indices to align with original data in the case of non-finite entries.
"Unltimately, for each Ci, we only require the"
"minimum of DSPC(Ci, Cj) over all Cj != Ci."
"So let's call this value DSPC_wrt(Ci), i.e."
density separation 'with respect to' Ci.
If exactly one of the points is noise
Set the density sparseness of the cluster
to the sparsest value seen so far.
Check whether density separations with
respect to each of these clusters can
be reduced.
"In case min_outlier_sep is still np.inf, we assign a new value to it."
This only makes sense if num_clusters = 1 since it has turned out
that the MR-MST has no edges between a noise point and a core point.
DSPC_wrt[Ci] might be infinite if the connected component for Ci is
"an ""island"" in the MR-MST. Whereas for other clusters Cj and Ck, the"
MR-MST might contain an edge with one point in Cj and ther other one
"in Ck. Here, we replace the infinite density separation of Ci by"
another large enough value.
""
TODO: Think of a better yet efficient way to handle this.
Extract flat clustering from HDBSCAN's hierarchy for 7 clusters
Use a previously initialized/trained HDBSCAN
Handle the trivial case first.
Always generate prediction_data to avoid later woes
This will later be chosen according to n_clusters
Initialize and train clusterer if one was not previously supplied.
Always generate prediction data
We do not pass cluster_selection_epsilon here.
"While this adds unnecessary computation, it makes the code"
easier to read and debug.
"Train on 'X'. Do this even if the supplied clusterer was trained,"
because we want to make sure it fits 'X'.
"Pick an epsilon value right after a split produces n_clusters,"
and the don't split further for smaller epsilon (larger lambda)
Or use the specified cluster_selection_epsilon
"Extract tree related stuff, in order to re-assign labels"
Get labels according to the required cluster_selection_epsilon
Reflect the related changes in HDBSCAN.
PredictionData attached to HDBSCAN should also change.
A function re_init is defined in this module to handle this.
"From a fitted HDBSCAN model, predict for n_clusters=5"
Store prediciton data for later use.
and use this prediction data to predict on new points
Get number of fitted clusters for later use.
We'll need the condensed tree later...
"If none of the three arguments: prediction_data, n_clusters,"
"and cluster_selection_epsilon are supplied,"
then use clusterer's prediciton data directly
"If either of n_clusters or cluster_selection_epsilon were supplied,"
then build prediction data from these by modifying clusterer's
Get prediction data from clusterer
Modify prediction_data to reflect new n_clusters
"First, make a copy of prediction data to avoid modifying source"
Cluster selection method is hold by condensed_tree.
Change from 'eom' to 'leaf' if n_clusters is too large.
This change does not affect the tree associated with 'clusterer'
Re-initialize prediction_data for the specified n_clusters or epsilon
============================================================
Now we're ready to use prediction_data
"The rest of the code is copied from HDBSCAN's approximate_predict,"
but modified to use prediction_data instead of clusterer's attribute
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering from prediction data
Initialize probabilities
k-NN for prediciton points to training set
Loop over prediction points to compute probabilities
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"Find row in tree where nearest neighbor drops out,"
so we can get a lambda value for the nearest neighbor
"Assign lambda as min(lambda-to-neighbor, neighbor's-lambda-to-tree)"
"Equivalently, this assigns core distance for prediction point as"
"max(dist-to-neighbor, neighbor's-dist-to-tree)"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the nearest exemplar persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Merge the two probabilities to produce a single set of probabilities
Include probability that the nearest neighbor belongs to a cluster
Rename variable so it's easy to understand what's being returned
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering at the chosen epsilon from prediction_data
"When no clusters found, return array of 0's"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the point persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Include probability that the point belongs to a cluster
Aggregate the three probabilities to produce membership vectors
Re-name variable to clarify what's being returned.
"With method 'eom', max clusters are produced for epsilon=0,"
as computed by
Increasing epsilon can only reduce the number of ouput clusters.
"To select epsilon, consider all values where clusters are split"
Subtract the extra e-12 to avoid numerical errors in comparison
"Then, we avoid splitting for all epsilon below this."
Use an epsilon value that produces the right number of clusters.
The condensed tree of HDBSCAN has this information.
Extract the lambda levels (=1/distance) from the condensed tree
We don't want values that produce a large cluster and
just one or two individual points.
Keep only those lambda values corresponding to cluster separation;
"i.e., with child_sizes > 1"
"Get the unique values, because when two clusters fall out of one,"
the entry with lambda is repeated.
lambda values are sorted by np.unique.
"Now, get epsilon (distance threshold) as 1/lambda"
"At this epsilon, n_clusters have been split."
Stop splits at epsilons smaller than this.
"To allow for numerical errors,"
predData must be a pre-trained PredictionData instance from hdbscan
"If n_clusters is specified, compute cluster_selection_epsilon;"
This is the key modification:
Select clusters according to selection method and epsilon.
_new_select_clusters is a modification of get_clusters
from hdbscan._hdbscan_tree
"raw tree, used later to get exemplars and lambda values"
"Re-do the cluster map: Map cluster numbers in tree (N, N+1, ..)"
to the cluster labels produced as output
Re-compute lambdas and exemplars for selected clusters;
max_lambda <=> smallest distance <=> most persistent point(s)
Map all sub-clusters of selected cluster to the selected cluster's
label in output.
Map lambdas too...
Create set of exemplar points for later use.
Novel points are assigned based on cluster of closest exemplar.
"For each selected cluster, get all of its leaves,"
"and leaves of leaves, and so on..."
Largest lambda => Most persistent points
Get the most persistent points
Add most persistent points as exemplars
Add exemplars for each leaf of each selected cluster.
(exclude root)
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Extract the chosen cluster bounds. If enough duplicate data points exist in the
data the lambda value might be infinite. This breaks labeling and highlighting
the chosen clusters.
Extract the plot range of the y-axis and set default center and height values for ellipses.
Extremly dense clusters might result in near infinite lambda values. Setting max_height
based on the percentile should alleviate the impact on plotting.
Set center and height to default values if necessary
Ensure the ellipse is visible
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
"the point is in the dataset, fix lambda for rounding errors"
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
Disable for now -- need to refactor to meet newer standards
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
this fails if no $DISPLAY specified
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
no prediction data error
wrong dimensions error
no clusters warning
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
without epsilon we should see many noise points as children of root.
for this random seed an epsilon of 0.2 will produce exactly 2 noise
points at that cut in single linkage.
Disable for now -- need to refactor to meet newer standards
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
"Given negative, zero and positive denominator and positive numerator"
Make sure safe division is always positive and doesn't raise ZeroDivision error
Ignore future warnings thrown by sklearn
Create a nice dataset with 6 circular clusters and 2 moons
"Given, the base HDBSCAN with method 'eom'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
"Given, the base HDBSCAN with method 'leaf'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
Method 'eom'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
Method 'leaf'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
"Given the max number of clusters that can be produced by 'eom',"
(these are produced for epsilon=0) (??? Needs verification)
"When we try flat clustering with 'eom' method for more n_clusters,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
"assert ""Cannot predict"" in str(w[-1].message)"
"the resulting clusterer switches to using method 'leaf',"
and the resulting probabilities and labels must match
"Given the base HDBSCAN trained on some data,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the clustering should match that due to approximate_predict,"
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the number of clusters produced must match the original n_clusters"
and all probabilities are <= 1.
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat with specified n_clusters,"
"Then, the requested number of clusters must be produced"
and all probabilities are <= 1.
When using approximate_predict_flat with more clusters
"than 'eom' can handle,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
But the requested number of clusters must still be produced using 'leaf'
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
The dependencies are the same as the contents of requirements.txt
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Check for connected component on X
Compute sparse mutual reachability graph
"if max_dist > 0, max distance to use when the reachability is infinite"
Check connected component on mutual reachability
"If more than one component, it means that even if the distance matrix X"
"has one component, there exists with less than `min_samples` neighbors"
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Non-precomputed matrices may contain non-finite values.
Rows with these values
Pass only the purely finite indices into hdbscan
We will later assign all non-finite points to the background -1 cluster
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
remap indices to align with original data in the case of non-finite entries.
"Unltimately, for each Ci, we only require the"
"minimum of DSPC(Ci, Cj) over all Cj != Ci."
"So let's call this value DSPC_wrt(Ci), i.e."
density separation 'with respect to' Ci.
If exactly one of the points is noise
Set the density sparseness of the cluster
to the sparsest value seen so far.
Check whether density separations with
respect to each of these clusters can
be reduced.
"In case min_outlier_sep is still np.inf, we assign a new value to it."
This only makes sense if num_clusters = 1 since it has turned out
that the MR-MST has no edges between a noise point and a core point.
DSPC_wrt[Ci] might be infinite if the connected component for Ci is
"an ""island"" in the MR-MST. Whereas for other clusters Cj and Ck, the"
MR-MST might contain an edge with one point in Cj and ther other one
"in Ck. Here, we replace the infinite density separation of Ci by"
another large enough value.
""
TODO: Think of a better yet efficient way to handle this.
Extract flat clustering from HDBSCAN's hierarchy for 7 clusters
Use a previously initialized/trained HDBSCAN
Handle the trivial case first.
Always generate prediction_data to avoid later woes
This will later be chosen according to n_clusters
Initialize and train clusterer if one was not previously supplied.
Always generate prediction data
We do not pass cluster_selection_epsilon here.
"While this adds unnecessary computation, it makes the code"
easier to read and debug.
"Train on 'X'. Do this even if the supplied clusterer was trained,"
because we want to make sure it fits 'X'.
"Pick an epsilon value right after a split produces n_clusters,"
and the don't split further for smaller epsilon (larger lambda)
Or use the specified cluster_selection_epsilon
"Extract tree related stuff, in order to re-assign labels"
Get labels according to the required cluster_selection_epsilon
Reflect the related changes in HDBSCAN.
PredictionData attached to HDBSCAN should also change.
A function re_init is defined in this module to handle this.
"From a fitted HDBSCAN model, predict for n_clusters=5"
Store prediciton data for later use.
and use this prediction data to predict on new points
Get number of fitted clusters for later use.
We'll need the condensed tree later...
"If none of the three arguments: prediction_data, n_clusters,"
"and cluster_selection_epsilon are supplied,"
then use clusterer's prediciton data directly
"If either of n_clusters or cluster_selection_epsilon were supplied,"
then build prediction data from these by modifying clusterer's
Get prediction data from clusterer
Modify prediction_data to reflect new n_clusters
"First, make a copy of prediction data to avoid modifying source"
Cluster selection method is hold by condensed_tree.
Change from 'eom' to 'leaf' if n_clusters is too large.
This change does not affect the tree associated with 'clusterer'
Re-initialize prediction_data for the specified n_clusters or epsilon
============================================================
Now we're ready to use prediction_data
"The rest of the code is copied from HDBSCAN's approximate_predict,"
but modified to use prediction_data instead of clusterer's attribute
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering from prediction data
Initialize probabilities
k-NN for prediciton points to training set
Loop over prediction points to compute probabilities
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"Find row in tree where nearest neighbor drops out,"
so we can get a lambda value for the nearest neighbor
"Assign lambda as min(lambda-to-neighbor, neighbor's-lambda-to-tree)"
"Equivalently, this assigns core distance for prediction point as"
"max(dist-to-neighbor, neighbor's-dist-to-tree)"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the nearest exemplar persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Merge the two probabilities to produce a single set of probabilities
Include probability that the nearest neighbor belongs to a cluster
Rename variable so it's easy to understand what's being returned
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering at the chosen epsilon from prediction_data
"When no clusters found, return array of 0's"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the point persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Include probability that the point belongs to a cluster
Aggregate the three probabilities to produce membership vectors
Re-name variable to clarify what's being returned.
"With method 'eom', max clusters are produced for epsilon=0,"
as computed by
Increasing epsilon can only reduce the number of ouput clusters.
"To select epsilon, consider all values where clusters are split"
Subtract the extra e-12 to avoid numerical errors in comparison
"Then, we avoid splitting for all epsilon below this."
Use an epsilon value that produces the right number of clusters.
The condensed tree of HDBSCAN has this information.
Extract the lambda levels (=1/distance) from the condensed tree
We don't want values that produce a large cluster and
just one or two individual points.
Keep only those lambda values corresponding to cluster separation;
"i.e., with child_sizes > 1"
"Get the unique values, because when two clusters fall out of one,"
the entry with lambda is repeated.
lambda values are sorted by np.unique.
"Now, get epsilon (distance threshold) as 1/lambda"
"At this epsilon, n_clusters have been split."
Stop splits at epsilons smaller than this.
"To allow for numerical errors,"
predData must be a pre-trained PredictionData instance from hdbscan
"If n_clusters is specified, compute cluster_selection_epsilon;"
This is the key modification:
Select clusters according to selection method and epsilon.
_new_select_clusters is a modification of get_clusters
from hdbscan._hdbscan_tree
"raw tree, used later to get exemplars and lambda values"
"Re-do the cluster map: Map cluster numbers in tree (N, N+1, ..)"
to the cluster labels produced as output
Re-compute lambdas and exemplars for selected clusters;
max_lambda <=> smallest distance <=> most persistent point(s)
Map all sub-clusters of selected cluster to the selected cluster's
label in output.
Map lambdas too...
Create set of exemplar points for later use.
Novel points are assigned based on cluster of closest exemplar.
"For each selected cluster, get all of its leaves,"
"and leaves of leaves, and so on..."
Largest lambda => Most persistent points
Get the most persistent points
Add most persistent points as exemplars
Add exemplars for each leaf of each selected cluster.
(exclude root)
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Extract the chosen cluster bounds. If enough duplicate data points exist in the
data the lambda value might be infinite. This breaks labeling and highlighting
the chosen clusters.
Extract the plot range of the y-axis and set default center and height values for ellipses.
Extremly dense clusters might result in near infinite lambda values. Setting max_height
based on the percentile should alleviate the impact on plotting.
Set center and height to default values if necessary
Ensure the ellipse is visible
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
"the point is in the dataset, fix lambda for rounding errors"
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
Disable for now -- need to refactor to meet newer standards
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
this fails if no $DISPLAY specified
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
no prediction data error
wrong dimensions error
no clusters warning
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
without epsilon we should see many noise points as children of root.
for this random seed an epsilon of 0.2 will produce exactly 2 noise
points at that cut in single linkage.
Disable for now -- need to refactor to meet newer standards
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
"Given negative, zero and positive denominator and positive numerator"
Make sure safe division is always positive and doesn't raise ZeroDivision error
Ignore future warnings thrown by sklearn
Create a nice dataset with 6 circular clusters and 2 moons
"Given, the base HDBSCAN with method 'eom'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
"Given, the base HDBSCAN with method 'leaf'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
Method 'eom'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
Method 'leaf'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
"Given the max number of clusters that can be produced by 'eom',"
(these are produced for epsilon=0) (??? Needs verification)
"When we try flat clustering with 'eom' method for more n_clusters,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
"assert ""Cannot predict"" in str(w[-1].message)"
"the resulting clusterer switches to using method 'leaf',"
and the resulting probabilities and labels must match
"Given the base HDBSCAN trained on some data,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the clustering should match that due to approximate_predict,"
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the number of clusters produced must match the original n_clusters"
and all probabilities are <= 1.
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat with specified n_clusters,"
"Then, the requested number of clusters must be produced"
and all probabilities are <= 1.
When using approximate_predict_flat with more clusters
"than 'eom' can handle,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
But the requested number of clusters must still be produced using 'leaf'
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
The dependencies are the same as the contents of requirements.txt
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Check for connected component on X
Compute sparse mutual reachability graph
"if max_dist > 0, max distance to use when the reachability is infinite"
Check connected component on mutual reachability
"If more than one component, it means that even if the distance matrix X"
"has one component, there exists with less than `min_samples` neighbors"
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Non-precomputed matrices may contain non-finite values.
Rows with these values
Pass only the purely finite indices into hdbscan
We will later assign all non-finite points to the background -1 cluster
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
remap indices to align with original data in the case of non-finite entries.
"Unltimately, for each Ci, we only require the"
"minimum of DSPC(Ci, Cj) over all Cj != Ci."
"So let's call this value DSPC_wrt(Ci), i.e."
density separation 'with respect to' Ci.
If exactly one of the points is noise
Set the density sparseness of the cluster
to the sparsest value seen so far.
Check whether density separations with
respect to each of these clusters can
be reduced.
"In case min_outlier_sep is still np.inf, we assign a new value to it."
This only makes sense if num_clusters = 1 since it has turned out
that the MR-MST has no edges between a noise point and a core point.
DSPC_wrt[Ci] might be infinite if the connected component for Ci is
"an ""island"" in the MR-MST. Whereas for other clusters Cj and Ck, the"
MR-MST might contain an edge with one point in Cj and ther other one
"in Ck. Here, we replace the infinite density separation of Ci by"
another large enough value.
""
TODO: Think of a better yet efficient way to handle this.
Extract flat clustering from HDBSCAN's hierarchy for 7 clusters
Use a previously initialized/trained HDBSCAN
Handle the trivial case first.
Always generate prediction_data to avoid later woes
This will later be chosen according to n_clusters
Initialize and train clusterer if one was not previously supplied.
Always generate prediction data
We do not pass cluster_selection_epsilon here.
"While this adds unnecessary computation, it makes the code"
easier to read and debug.
"Train on 'X'. Do this even if the supplied clusterer was trained,"
because we want to make sure it fits 'X'.
"Pick an epsilon value right after a split produces n_clusters,"
and the don't split further for smaller epsilon (larger lambda)
Or use the specified cluster_selection_epsilon
"Extract tree related stuff, in order to re-assign labels"
Get labels according to the required cluster_selection_epsilon
Reflect the related changes in HDBSCAN.
PredictionData attached to HDBSCAN should also change.
A function re_init is defined in this module to handle this.
"From a fitted HDBSCAN model, predict for n_clusters=5"
Store prediciton data for later use.
and use this prediction data to predict on new points
Get number of fitted clusters for later use.
We'll need the condensed tree later...
"If none of the three arguments: prediction_data, n_clusters,"
"and cluster_selection_epsilon are supplied,"
then use clusterer's prediciton data directly
"If either of n_clusters or cluster_selection_epsilon were supplied,"
then build prediction data from these by modifying clusterer's
Get prediction data from clusterer
Modify prediction_data to reflect new n_clusters
"First, make a copy of prediction data to avoid modifying source"
Cluster selection method is hold by condensed_tree.
Change from 'eom' to 'leaf' if n_clusters is too large.
This change does not affect the tree associated with 'clusterer'
Re-initialize prediction_data for the specified n_clusters or epsilon
============================================================
Now we're ready to use prediction_data
"The rest of the code is copied from HDBSCAN's approximate_predict,"
but modified to use prediction_data instead of clusterer's attribute
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering from prediction data
Initialize probabilities
k-NN for prediciton points to training set
Loop over prediction points to compute probabilities
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"Find row in tree where nearest neighbor drops out,"
so we can get a lambda value for the nearest neighbor
"Assign lambda as min(lambda-to-neighbor, neighbor's-lambda-to-tree)"
"Equivalently, this assigns core distance for prediction point as"
"max(dist-to-neighbor, neighbor's-dist-to-tree)"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the nearest exemplar persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Merge the two probabilities to produce a single set of probabilities
Include probability that the nearest neighbor belongs to a cluster
Rename variable so it's easy to understand what's being returned
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering at the chosen epsilon from prediction_data
"When no clusters found, return array of 0's"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the point persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Include probability that the point belongs to a cluster
Aggregate the three probabilities to produce membership vectors
Re-name variable to clarify what's being returned.
"With method 'eom', max clusters are produced for epsilon=0,"
as computed by
Increasing epsilon can only reduce the number of ouput clusters.
"To select epsilon, consider all values where clusters are split"
Subtract the extra e-12 to avoid numerical errors in comparison
"Then, we avoid splitting for all epsilon below this."
Use an epsilon value that produces the right number of clusters.
The condensed tree of HDBSCAN has this information.
Extract the lambda levels (=1/distance) from the condensed tree
We don't want values that produce a large cluster and
just one or two individual points.
Keep only those lambda values corresponding to cluster separation;
"i.e., with child_sizes > 1"
"Get the unique values, because when two clusters fall out of one,"
the entry with lambda is repeated.
lambda values are sorted by np.unique.
"Now, get epsilon (distance threshold) as 1/lambda"
"At this epsilon, n_clusters have been split."
Stop splits at epsilons smaller than this.
"To allow for numerical errors,"
predData must be a pre-trained PredictionData instance from hdbscan
"If n_clusters is specified, compute cluster_selection_epsilon;"
This is the key modification:
Select clusters according to selection method and epsilon.
_new_select_clusters is a modification of get_clusters
from hdbscan._hdbscan_tree
"raw tree, used later to get exemplars and lambda values"
"Re-do the cluster map: Map cluster numbers in tree (N, N+1, ..)"
to the cluster labels produced as output
Re-compute lambdas and exemplars for selected clusters;
max_lambda <=> smallest distance <=> most persistent point(s)
Map all sub-clusters of selected cluster to the selected cluster's
label in output.
Map lambdas too...
Create set of exemplar points for later use.
Novel points are assigned based on cluster of closest exemplar.
"For each selected cluster, get all of its leaves,"
"and leaves of leaves, and so on..."
Largest lambda => Most persistent points
Get the most persistent points
Add most persistent points as exemplars
Add exemplars for each leaf of each selected cluster.
(exclude root)
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Extract the chosen cluster bounds. If enough duplicate data points exist in the
data the lambda value might be infinite. This breaks labeling and highlighting
the chosen clusters.
Extract the plot range of the y-axis and set default center and height values for ellipses.
Extremly dense clusters might result in near infinite lambda values. Setting max_height
based on the percentile should alleviate the impact on plotting.
Set center and height to default values if necessary
Ensure the ellipse is visible
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
"the point is in the dataset, fix lambda for rounding errors"
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
Disable for now -- need to refactor to meet newer standards
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
this fails if no $DISPLAY specified
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
no prediction data error
wrong dimensions error
no clusters warning
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
without epsilon we should see many noise points as children of root.
for this random seed an epsilon of 0.2 will produce exactly 2 noise
points at that cut in single linkage.
Disable for now -- need to refactor to meet newer standards
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
"Given negative, zero and positive denominator and positive numerator"
Make sure safe division is always positive and doesn't raise ZeroDivision error
Ignore future warnings thrown by sklearn
Create a nice dataset with 6 circular clusters and 2 moons
"Given, the base HDBSCAN with method 'eom'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
"Given, the base HDBSCAN with method 'leaf'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
Method 'eom'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
Method 'leaf'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
"Given the max number of clusters that can be produced by 'eom',"
(these are produced for epsilon=0) (??? Needs verification)
"When we try flat clustering with 'eom' method for more n_clusters,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
"assert ""Cannot predict"" in str(w[-1].message)"
"the resulting clusterer switches to using method 'leaf',"
and the resulting probabilities and labels must match
"Given the base HDBSCAN trained on some data,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the clustering should match that due to approximate_predict,"
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the number of clusters produced must match the original n_clusters"
and all probabilities are <= 1.
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat with specified n_clusters,"
"Then, the requested number of clusters must be produced"
and all probabilities are <= 1.
When using approximate_predict_flat with more clusters
"than 'eom' can handle,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
But the requested number of clusters must still be produced using 'leaf'
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
The dependencies are the same as the contents of requirements.txt
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Check for connected component on X
Compute sparse mutual reachability graph
"if max_dist > 0, max distance to use when the reachability is infinite"
Check connected component on mutual reachability
"If more than one component, it means that even if the distance matrix X"
"has one component, there exists with less than `min_samples` neighbors"
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Non-precomputed matrices may contain non-finite values.
Rows with these values
Pass only the purely finite indices into hdbscan
We will later assign all non-finite points to the background -1 cluster
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
remap indices to align with original data in the case of non-finite entries.
"Unltimately, for each Ci, we only require the"
"minimum of DSPC(Ci, Cj) over all Cj != Ci."
"So let's call this value DSPC_wrt(Ci), i.e."
density separation 'with respect to' Ci.
If exactly one of the points is noise
Set the density sparseness of the cluster
to the sparsest value seen so far.
Check whether density separations with
respect to each of these clusters can
be reduced.
"In case min_outlier_sep is still np.inf, we assign a new value to it."
This only makes sense if num_clusters = 1 since it has turned out
that the MR-MST has no edges between a noise point and a core point.
DSPC_wrt[Ci] might be infinite if the connected component for Ci is
"an ""island"" in the MR-MST. Whereas for other clusters Cj and Ck, the"
MR-MST might contain an edge with one point in Cj and ther other one
"in Ck. Here, we replace the infinite density separation of Ci by"
another large enough value.
""
TODO: Think of a better yet efficient way to handle this.
Extract flat clustering from HDBSCAN's hierarchy for 7 clusters
Use a previously initialized/trained HDBSCAN
Handle the trivial case first.
Always generate prediction_data to avoid later woes
This will later be chosen according to n_clusters
Initialize and train clusterer if one was not previously supplied.
Always generate prediction data
We do not pass cluster_selection_epsilon here.
"While this adds unnecessary computation, it makes the code"
easier to read and debug.
"Train on 'X'. Do this even if the supplied clusterer was trained,"
because we want to make sure it fits 'X'.
"Pick an epsilon value right after a split produces n_clusters,"
and the don't split further for smaller epsilon (larger lambda)
Or use the specified cluster_selection_epsilon
"Extract tree related stuff, in order to re-assign labels"
Get labels according to the required cluster_selection_epsilon
Reflect the related changes in HDBSCAN.
PredictionData attached to HDBSCAN should also change.
A function re_init is defined in this module to handle this.
"From a fitted HDBSCAN model, predict for n_clusters=5"
Store prediciton data for later use.
and use this prediction data to predict on new points
Get number of fitted clusters for later use.
We'll need the condensed tree later...
"If none of the three arguments: prediction_data, n_clusters,"
"and cluster_selection_epsilon are supplied,"
then use clusterer's prediciton data directly
"If either of n_clusters or cluster_selection_epsilon were supplied,"
then build prediction data from these by modifying clusterer's
Get prediction data from clusterer
Modify prediction_data to reflect new n_clusters
"First, make a copy of prediction data to avoid modifying source"
Cluster selection method is hold by condensed_tree.
Change from 'eom' to 'leaf' if n_clusters is too large.
This change does not affect the tree associated with 'clusterer'
Re-initialize prediction_data for the specified n_clusters or epsilon
============================================================
Now we're ready to use prediction_data
"The rest of the code is copied from HDBSCAN's approximate_predict,"
but modified to use prediction_data instead of clusterer's attribute
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering from prediction data
Initialize probabilities
k-NN for prediciton points to training set
Loop over prediction points to compute probabilities
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"Find row in tree where nearest neighbor drops out,"
so we can get a lambda value for the nearest neighbor
"Assign lambda as min(lambda-to-neighbor, neighbor's-lambda-to-tree)"
"Equivalently, this assigns core distance for prediction point as"
"max(dist-to-neighbor, neighbor's-dist-to-tree)"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the nearest exemplar persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Merge the two probabilities to produce a single set of probabilities
Include probability that the nearest neighbor belongs to a cluster
Rename variable so it's easy to understand what's being returned
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering at the chosen epsilon from prediction_data
"When no clusters found, return array of 0's"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the point persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Include probability that the point belongs to a cluster
Aggregate the three probabilities to produce membership vectors
Re-name variable to clarify what's being returned.
"With method 'eom', max clusters are produced for epsilon=0,"
as computed by
Increasing epsilon can only reduce the number of ouput clusters.
"To select epsilon, consider all values where clusters are split"
Subtract the extra e-12 to avoid numerical errors in comparison
"Then, we avoid splitting for all epsilon below this."
Use an epsilon value that produces the right number of clusters.
The condensed tree of HDBSCAN has this information.
Extract the lambda levels (=1/distance) from the condensed tree
We don't want values that produce a large cluster and
just one or two individual points.
Keep only those lambda values corresponding to cluster separation;
"i.e., with child_sizes > 1"
"Get the unique values, because when two clusters fall out of one,"
the entry with lambda is repeated.
lambda values are sorted by np.unique.
"Now, get epsilon (distance threshold) as 1/lambda"
"At this epsilon, n_clusters have been split."
Stop splits at epsilons smaller than this.
"To allow for numerical errors,"
predData must be a pre-trained PredictionData instance from hdbscan
"If n_clusters is specified, compute cluster_selection_epsilon;"
This is the key modification:
Select clusters according to selection method and epsilon.
_new_select_clusters is a modification of get_clusters
from hdbscan._hdbscan_tree
"raw tree, used later to get exemplars and lambda values"
"Re-do the cluster map: Map cluster numbers in tree (N, N+1, ..)"
to the cluster labels produced as output
Re-compute lambdas and exemplars for selected clusters;
max_lambda <=> smallest distance <=> most persistent point(s)
Map all sub-clusters of selected cluster to the selected cluster's
label in output.
Map lambdas too...
Create set of exemplar points for later use.
Novel points are assigned based on cluster of closest exemplar.
"For each selected cluster, get all of its leaves,"
"and leaves of leaves, and so on..."
Largest lambda => Most persistent points
Get the most persistent points
Add most persistent points as exemplars
Add exemplars for each leaf of each selected cluster.
(exclude root)
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Extract the chosen cluster bounds. If enough duplicate data points exist in the
data the lambda value might be infinite. This breaks labeling and highlighting
the chosen clusters.
Extract the plot range of the y-axis and set default center and height values for ellipses.
Extremly dense clusters might result in near infinite lambda values. Setting max_height
based on the percentile should alleviate the impact on plotting.
Set center and height to default values if necessary
Ensure the ellipse is visible
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
"the point is in the dataset, fix lambda for rounding errors"
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
Disable for now -- need to refactor to meet newer standards
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
this fails if no $DISPLAY specified
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
no prediction data error
wrong dimensions error
no clusters warning
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
without epsilon we should see many noise points as children of root.
for this random seed an epsilon of 0.2 will produce exactly 2 noise
points at that cut in single linkage.
Disable for now -- need to refactor to meet newer standards
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
"Given negative, zero and positive denominator and positive numerator"
Make sure safe division is always positive and doesn't raise ZeroDivision error
Ignore future warnings thrown by sklearn
Create a nice dataset with 6 circular clusters and 2 moons
"Given, the base HDBSCAN with method 'eom'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
"Given, the base HDBSCAN with method 'leaf'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
Method 'eom'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
Method 'leaf'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
"Given the max number of clusters that can be produced by 'eom',"
(these are produced for epsilon=0) (??? Needs verification)
"When we try flat clustering with 'eom' method for more n_clusters,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
"assert ""Cannot predict"" in str(w[-1].message)"
"the resulting clusterer switches to using method 'leaf',"
and the resulting probabilities and labels must match
"Given the base HDBSCAN trained on some data,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the clustering should match that due to approximate_predict,"
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the number of clusters produced must match the original n_clusters"
and all probabilities are <= 1.
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat with specified n_clusters,"
"Then, the requested number of clusters must be produced"
and all probabilities are <= 1.
When using approximate_predict_flat with more clusters
"than 'eom' can handle,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
But the requested number of clusters must still be produced using 'leaf'
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
The dependencies are the same as the contents of requirements.txt
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Check for connected component on X
Compute sparse mutual reachability graph
"if max_dist > 0, max distance to use when the reachability is infinite"
Check connected component on mutual reachability
"If more than one component, it means that even if the distance matrix X"
"has one component, there exists with less than `min_samples` neighbors"
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Non-precomputed matrices may contain non-finite values.
Rows with these values
Pass only the purely finite indices into hdbscan
We will later assign all non-finite points to the background -1 cluster
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
remap indices to align with original data in the case of non-finite entries.
"Unltimately, for each Ci, we only require the"
"minimum of DSPC(Ci, Cj) over all Cj != Ci."
"So let's call this value DSPC_wrt(Ci), i.e."
density separation 'with respect to' Ci.
If exactly one of the points is noise
Set the density sparseness of the cluster
to the sparsest value seen so far.
Check whether density separations with
respect to each of these clusters can
be reduced.
"In case min_outlier_sep is still np.inf, we assign a new value to it."
This only makes sense if num_clusters = 1 since it has turned out
that the MR-MST has no edges between a noise point and a core point.
DSPC_wrt[Ci] might be infinite if the connected component for Ci is
"an ""island"" in the MR-MST. Whereas for other clusters Cj and Ck, the"
MR-MST might contain an edge with one point in Cj and ther other one
"in Ck. Here, we replace the infinite density separation of Ci by"
another large enough value.
""
TODO: Think of a better yet efficient way to handle this.
Extract flat clustering from HDBSCAN's hierarchy for 7 clusters
Use a previously initialized/trained HDBSCAN
Handle the trivial case first.
Always generate prediction_data to avoid later woes
This will later be chosen according to n_clusters
Initialize and train clusterer if one was not previously supplied.
Always generate prediction data
We do not pass cluster_selection_epsilon here.
"While this adds unnecessary computation, it makes the code"
easier to read and debug.
"Train on 'X'. Do this even if the supplied clusterer was trained,"
because we want to make sure it fits 'X'.
"Pick an epsilon value right after a split produces n_clusters,"
and the don't split further for smaller epsilon (larger lambda)
Or use the specified cluster_selection_epsilon
"Extract tree related stuff, in order to re-assign labels"
Get labels according to the required cluster_selection_epsilon
Reflect the related changes in HDBSCAN.
PredictionData attached to HDBSCAN should also change.
A function re_init is defined in this module to handle this.
"From a fitted HDBSCAN model, predict for n_clusters=5"
Store prediciton data for later use.
and use this prediction data to predict on new points
Get number of fitted clusters for later use.
We'll need the condensed tree later...
"If none of the three arguments: prediction_data, n_clusters,"
"and cluster_selection_epsilon are supplied,"
then use clusterer's prediciton data directly
"If either of n_clusters or cluster_selection_epsilon were supplied,"
then build prediction data from these by modifying clusterer's
Get prediction data from clusterer
Modify prediction_data to reflect new n_clusters
"First, make a copy of prediction data to avoid modifying source"
Cluster selection method is hold by condensed_tree.
Change from 'eom' to 'leaf' if n_clusters is too large.
This change does not affect the tree associated with 'clusterer'
Re-initialize prediction_data for the specified n_clusters or epsilon
============================================================
Now we're ready to use prediction_data
"The rest of the code is copied from HDBSCAN's approximate_predict,"
but modified to use prediction_data instead of clusterer's attribute
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering from prediction data
Initialize probabilities
k-NN for prediciton points to training set
Loop over prediction points to compute probabilities
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"Find row in tree where nearest neighbor drops out,"
so we can get a lambda value for the nearest neighbor
"Assign lambda as min(lambda-to-neighbor, neighbor's-lambda-to-tree)"
"Equivalently, this assigns core distance for prediction point as"
"max(dist-to-neighbor, neighbor's-dist-to-tree)"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the nearest exemplar persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Merge the two probabilities to produce a single set of probabilities
Include probability that the nearest neighbor belongs to a cluster
Rename variable so it's easy to understand what's being returned
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering at the chosen epsilon from prediction_data
"When no clusters found, return array of 0's"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the point persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Include probability that the point belongs to a cluster
Aggregate the three probabilities to produce membership vectors
Re-name variable to clarify what's being returned.
"With method 'eom', max clusters are produced for epsilon=0,"
as computed by
Increasing epsilon can only reduce the number of ouput clusters.
"To select epsilon, consider all values where clusters are split"
Subtract the extra e-12 to avoid numerical errors in comparison
"Then, we avoid splitting for all epsilon below this."
Use an epsilon value that produces the right number of clusters.
The condensed tree of HDBSCAN has this information.
Extract the lambda levels (=1/distance) from the condensed tree
We don't want values that produce a large cluster and
just one or two individual points.
Keep only those lambda values corresponding to cluster separation;
"i.e., with child_sizes > 1"
"Get the unique values, because when two clusters fall out of one,"
the entry with lambda is repeated.
lambda values are sorted by np.unique.
"Now, get epsilon (distance threshold) as 1/lambda"
"At this epsilon, n_clusters have been split."
Stop splits at epsilons smaller than this.
"To allow for numerical errors,"
predData must be a pre-trained PredictionData instance from hdbscan
"If n_clusters is specified, compute cluster_selection_epsilon;"
This is the key modification:
Select clusters according to selection method and epsilon.
_new_select_clusters is a modification of get_clusters
from hdbscan._hdbscan_tree
"raw tree, used later to get exemplars and lambda values"
"Re-do the cluster map: Map cluster numbers in tree (N, N+1, ..)"
to the cluster labels produced as output
Re-compute lambdas and exemplars for selected clusters;
max_lambda <=> smallest distance <=> most persistent point(s)
Map all sub-clusters of selected cluster to the selected cluster's
label in output.
Map lambdas too...
Create set of exemplar points for later use.
Novel points are assigned based on cluster of closest exemplar.
"For each selected cluster, get all of its leaves,"
"and leaves of leaves, and so on..."
Largest lambda => Most persistent points
Get the most persistent points
Add most persistent points as exemplars
Add exemplars for each leaf of each selected cluster.
(exclude root)
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Extract the chosen cluster bounds. If enough duplicate data points exist in the
data the lambda value might be infinite. This breaks labeling and highlighting
the chosen clusters.
Extract the plot range of the y-axis and set default center and height values for ellipses.
Extremly dense clusters might result in near infinite lambda values. Setting max_height
based on the percentile should alleviate the impact on plotting.
Set center and height to default values if necessary
Ensure the ellipse is visible
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
"the point is in the dataset, fix lambda for rounding errors"
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
Disable for now -- need to refactor to meet newer standards
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
this fails if no $DISPLAY specified
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
no prediction data error
wrong dimensions error
no clusters warning
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
without epsilon we should see many noise points as children of root.
for this random seed an epsilon of 0.2 will produce exactly 2 noise
points at that cut in single linkage.
Disable for now -- need to refactor to meet newer standards
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
"Given negative, zero and positive denominator and positive numerator"
Make sure safe division is always positive and doesn't raise ZeroDivision error
Ignore future warnings thrown by sklearn
Create a nice dataset with 6 circular clusters and 2 moons
"Given, the base HDBSCAN with method 'eom'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
"Given, the base HDBSCAN with method 'leaf'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
Method 'eom'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
Method 'leaf'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
"Given the max number of clusters that can be produced by 'eom',"
(these are produced for epsilon=0) (??? Needs verification)
"When we try flat clustering with 'eom' method for more n_clusters,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
"assert ""Cannot predict"" in str(w[-1].message)"
"the resulting clusterer switches to using method 'leaf',"
and the resulting probabilities and labels must match
"Given the base HDBSCAN trained on some data,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the clustering should match that due to approximate_predict,"
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the number of clusters produced must match the original n_clusters"
and all probabilities are <= 1.
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat with specified n_clusters,"
"Then, the requested number of clusters must be produced"
and all probabilities are <= 1.
When using approximate_predict_flat with more clusters
"than 'eom' can handle,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
But the requested number of clusters must still be produced using 'leaf'
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
The dependencies are the same as the contents of requirements.txt
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Check for connected component on X
Compute sparse mutual reachability graph
"if max_dist > 0, max distance to use when the reachability is infinite"
Check connected component on mutual reachability
"If more than one component, it means that even if the distance matrix X"
"has one component, there exists with less than `min_samples` neighbors"
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Non-precomputed matrices may contain non-finite values.
Rows with these values
Pass only the purely finite indices into hdbscan
We will later assign all non-finite points to the background -1 cluster
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
remap indices to align with original data in the case of non-finite entries.
"Unltimately, for each Ci, we only require the"
"minimum of DSPC(Ci, Cj) over all Cj != Ci."
"So let's call this value DSPC_wrt(Ci), i.e."
density separation 'with respect to' Ci.
If exactly one of the points is noise
Set the density sparseness of the cluster
to the sparsest value seen so far.
Check whether density separations with
respect to each of these clusters can
be reduced.
"In case min_outlier_sep is still np.inf, we assign a new value to it."
This only makes sense if num_clusters = 1 since it has turned out
that the MR-MST has no edges between a noise point and a core point.
DSPC_wrt[Ci] might be infinite if the connected component for Ci is
"an ""island"" in the MR-MST. Whereas for other clusters Cj and Ck, the"
MR-MST might contain an edge with one point in Cj and ther other one
"in Ck. Here, we replace the infinite density separation of Ci by"
another large enough value.
""
TODO: Think of a better yet efficient way to handle this.
Extract flat clustering from HDBSCAN's hierarchy for 7 clusters
Use a previously initialized/trained HDBSCAN
Handle the trivial case first.
Always generate prediction_data to avoid later woes
This will later be chosen according to n_clusters
Initialize and train clusterer if one was not previously supplied.
Always generate prediction data
We do not pass cluster_selection_epsilon here.
"While this adds unnecessary computation, it makes the code"
easier to read and debug.
"Train on 'X'. Do this even if the supplied clusterer was trained,"
because we want to make sure it fits 'X'.
"Pick an epsilon value right after a split produces n_clusters,"
and the don't split further for smaller epsilon (larger lambda)
Or use the specified cluster_selection_epsilon
"Extract tree related stuff, in order to re-assign labels"
Get labels according to the required cluster_selection_epsilon
Reflect the related changes in HDBSCAN.
PredictionData attached to HDBSCAN should also change.
A function re_init is defined in this module to handle this.
"From a fitted HDBSCAN model, predict for n_clusters=5"
Store prediciton data for later use.
and use this prediction data to predict on new points
Get number of fitted clusters for later use.
We'll need the condensed tree later...
"If none of the three arguments: prediction_data, n_clusters,"
"and cluster_selection_epsilon are supplied,"
then use clusterer's prediciton data directly
"If either of n_clusters or cluster_selection_epsilon were supplied,"
then build prediction data from these by modifying clusterer's
Get prediction data from clusterer
Modify prediction_data to reflect new n_clusters
"First, make a copy of prediction data to avoid modifying source"
Cluster selection method is hold by condensed_tree.
Change from 'eom' to 'leaf' if n_clusters is too large.
This change does not affect the tree associated with 'clusterer'
Re-initialize prediction_data for the specified n_clusters or epsilon
============================================================
Now we're ready to use prediction_data
"The rest of the code is copied from HDBSCAN's approximate_predict,"
but modified to use prediction_data instead of clusterer's attribute
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering from prediction data
Initialize probabilities
k-NN for prediciton points to training set
Loop over prediction points to compute probabilities
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"Find row in tree where nearest neighbor drops out,"
so we can get a lambda value for the nearest neighbor
"Assign lambda as min(lambda-to-neighbor, neighbor's-lambda-to-tree)"
"Equivalently, this assigns core distance for prediction point as"
"max(dist-to-neighbor, neighbor's-dist-to-tree)"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the nearest exemplar persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Merge the two probabilities to produce a single set of probabilities
Include probability that the nearest neighbor belongs to a cluster
Rename variable so it's easy to understand what's being returned
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering at the chosen epsilon from prediction_data
"When no clusters found, return array of 0's"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the point persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Include probability that the point belongs to a cluster
Aggregate the three probabilities to produce membership vectors
Re-name variable to clarify what's being returned.
"With method 'eom', max clusters are produced for epsilon=0,"
as computed by
Increasing epsilon can only reduce the number of ouput clusters.
"To select epsilon, consider all values where clusters are split"
Subtract the extra e-12 to avoid numerical errors in comparison
"Then, we avoid splitting for all epsilon below this."
Use an epsilon value that produces the right number of clusters.
The condensed tree of HDBSCAN has this information.
Extract the lambda levels (=1/distance) from the condensed tree
We don't want values that produce a large cluster and
just one or two individual points.
Keep only those lambda values corresponding to cluster separation;
"i.e., with child_sizes > 1"
"Get the unique values, because when two clusters fall out of one,"
the entry with lambda is repeated.
lambda values are sorted by np.unique.
"Now, get epsilon (distance threshold) as 1/lambda"
"At this epsilon, n_clusters have been split."
Stop splits at epsilons smaller than this.
"To allow for numerical errors,"
predData must be a pre-trained PredictionData instance from hdbscan
"If n_clusters is specified, compute cluster_selection_epsilon;"
This is the key modification:
Select clusters according to selection method and epsilon.
_new_select_clusters is a modification of get_clusters
from hdbscan._hdbscan_tree
"raw tree, used later to get exemplars and lambda values"
"Re-do the cluster map: Map cluster numbers in tree (N, N+1, ..)"
to the cluster labels produced as output
Re-compute lambdas and exemplars for selected clusters;
max_lambda <=> smallest distance <=> most persistent point(s)
Map all sub-clusters of selected cluster to the selected cluster's
label in output.
Map lambdas too...
Create set of exemplar points for later use.
Novel points are assigned based on cluster of closest exemplar.
"For each selected cluster, get all of its leaves,"
"and leaves of leaves, and so on..."
Largest lambda => Most persistent points
Get the most persistent points
Add most persistent points as exemplars
Add exemplars for each leaf of each selected cluster.
(exclude root)
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Extract the chosen cluster bounds. If enough duplicate data points exist in the
data the lambda value might be infinite. This breaks labeling and highlighting
the chosen clusters.
Extract the plot range of the y-axis and set default center and height values for ellipses.
Extremly dense clusters might result in near infinite lambda values. Setting max_height
based on the percentile should alleviate the impact on plotting.
Set center and height to default values if necessary
Ensure the ellipse is visible
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
"the point is in the dataset, fix lambda for rounding errors"
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
Disable for now -- need to refactor to meet newer standards
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
this fails if no $DISPLAY specified
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
no prediction data error
wrong dimensions error
no clusters warning
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
without epsilon we should see many noise points as children of root.
for this random seed an epsilon of 0.2 will produce exactly 2 noise
points at that cut in single linkage.
Disable for now -- need to refactor to meet newer standards
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
"Given negative, zero and positive denominator and positive numerator"
Make sure safe division is always positive and doesn't raise ZeroDivision error
Ignore future warnings thrown by sklearn
Create a nice dataset with 6 circular clusters and 2 moons
"Given, the base HDBSCAN with method 'eom'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
"Given, the base HDBSCAN with method 'leaf'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
Method 'eom'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
Method 'leaf'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
"Given the max number of clusters that can be produced by 'eom',"
(these are produced for epsilon=0) (??? Needs verification)
"When we try flat clustering with 'eom' method for more n_clusters,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
"the resulting clusterer switches to using method 'leaf',"
and the resulting probabilities and labels must match
"Given the base HDBSCAN trained on some data,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the clustering should match that due to approximate_predict,"
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the number of clusters produced must match the original n_clusters"
and all probabilities are <= 1.
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat with specified n_clusters,"
"Then, the requested number of clusters must be produced"
and all probabilities are <= 1.
When using approximate_predict_flat with more clusters
"than 'eom' can handle,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
But the requested number of clusters must still be produced using 'leaf'
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
The dependencies are the same as the contents of requirements.txt
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Check for connected component on X
Compute sparse mutual reachability graph
"if max_dist > 0, max distance to use when the reachability is infinite"
Check connected component on mutual reachability
"If more than one component, it means that even if the distance matrix X"
"has one component, there exists with less than `min_samples` neighbors"
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Non-precomputed matrices may contain non-finite values.
Rows with these values
Pass only the purely finite indices into hdbscan
We will later assign all non-finite points to the background -1 cluster
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
remap indices to align with original data in the case of non-finite entries.
"Unltimately, for each Ci, we only require the"
"minimum of DSPC(Ci, Cj) over all Cj != Ci."
"So let's call this value DSPC_wrt(Ci), i.e."
density separation 'with respect to' Ci.
If exactly one of the points is noise
Set the density sparseness of the cluster
to the sparsest value seen so far.
Check whether density separations with
respect to each of these clusters can
be reduced.
"In case min_outlier_sep is still np.inf, we assign a new value to it."
This only makes sense if num_clusters = 1 since it has turned out
that the MR-MST has no edges between a noise point and a core point.
DSPC_wrt[Ci] might be infinite if the connected component for Ci is
"an ""island"" in the MR-MST. Whereas for other clusters Cj and Ck, the"
MR-MST might contain an edge with one point in Cj and ther other one
"in Ck. Here, we replace the infinite density separation of Ci by"
another large enough value.
""
TODO: Think of a better yet efficient way to handle this.
Extract flat clustering from HDBSCAN's hierarchy for 7 clusters
Use a previously initialized/trained HDBSCAN
Handle the trivial case first.
Always generate prediction_data to avoid later woes
This will later be chosen according to n_clusters
Initialize and train clusterer if one was not previously supplied.
Always generate prediction data
We do not pass cluster_selection_epsilon here.
"While this adds unnecessary computation, it makes the code"
easier to read and debug.
"Train on 'X'. Do this even if the supplied clusterer was trained,"
because we want to make sure it fits 'X'.
"Pick an epsilon value right after a split produces n_clusters,"
and the don't split further for smaller epsilon (larger lambda)
Or use the specified cluster_selection_epsilon
"Extract tree related stuff, in order to re-assign labels"
Get labels according to the required cluster_selection_epsilon
Reflect the related changes in HDBSCAN.
PredictionData attached to HDBSCAN should also change.
A function re_init is defined in this module to handle this.
"From a fitted HDBSCAN model, predict for n_clusters=5"
Store prediciton data for later use.
and use this prediction data to predict on new points
Get number of fitted clusters for later use.
We'll need the condensed tree later...
"If none of the three arguments: prediction_data, n_clusters,"
"and cluster_selection_epsilon are supplied,"
then use clusterer's prediciton data directly
"If either of n_clusters or cluster_selection_epsilon were supplied,"
then build prediction data from these by modifying clusterer's
Get prediction data from clusterer
Modify prediction_data to reflect new n_clusters
"First, make a copy of prediction data to avoid modifying source"
Cluster selection method is hold by condensed_tree.
Change from 'eom' to 'leaf' if n_clusters is too large.
This change does not affect the tree associated with 'clusterer'
Re-initialize prediction_data for the specified n_clusters or epsilon
============================================================
Now we're ready to use prediction_data
"The rest of the code is copied from HDBSCAN's approximate_predict,"
but modified to use prediction_data instead of clusterer's attribute
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering from prediction data
Initialize probabilities
k-NN for prediciton points to training set
Loop over prediction points to compute probabilities
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"Find row in tree where nearest neighbor drops out,"
so we can get a lambda value for the nearest neighbor
"Assign lambda as min(lambda-to-neighbor, neighbor's-lambda-to-tree)"
"Equivalently, this assigns core distance for prediction point as"
"max(dist-to-neighbor, neighbor's-dist-to-tree)"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the nearest exemplar persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Merge the two probabilities to produce a single set of probabilities
Include probability that the nearest neighbor belongs to a cluster
Rename variable so it's easy to understand what's being returned
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering at the chosen epsilon from prediction_data
"When no clusters found, return array of 0's"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the point persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Include probability that the point belongs to a cluster
Aggregate the three probabilities to produce membership vectors
Re-name variable to clarify what's being returned.
"With method 'eom', max clusters are produced for epsilon=0,"
as computed by
Increasing epsilon can only reduce the number of ouput clusters.
"To select epsilon, consider all values where clusters are split"
Subtract the extra e-12 to avoid numerical errors in comparison
"Then, we avoid splitting for all epsilon below this."
Use an epsilon value that produces the right number of clusters.
The condensed tree of HDBSCAN has this information.
Extract the lambda levels (=1/distance) from the condensed tree
We don't want values that produce a large cluster and
just one or two individual points.
Keep only those lambda values corresponding to cluster separation;
"i.e., with child_sizes > 1"
"Get the unique values, because when two clusters fall out of one,"
the entry with lambda is repeated.
lambda values are sorted by np.unique.
"Now, get epsilon (distance threshold) as 1/lambda"
"At this epsilon, n_clusters have been split."
Stop splits at epsilons smaller than this.
"To allow for numerical errors,"
predData must be a pre-trained PredictionData instance from hdbscan
"If n_clusters is specified, compute cluster_selection_epsilon;"
This is the key modification:
Select clusters according to selection method and epsilon.
_new_select_clusters is a modification of get_clusters
from hdbscan._hdbscan_tree
"raw tree, used later to get exemplars and lambda values"
"Re-do the cluster map: Map cluster numbers in tree (N, N+1, ..)"
to the cluster labels produced as output
Re-compute lambdas and exemplars for selected clusters;
max_lambda <=> smallest distance <=> most persistent point(s)
Map all sub-clusters of selected cluster to the selected cluster's
label in output.
Map lambdas too...
Create set of exemplar points for later use.
Novel points are assigned based on cluster of closest exemplar.
"For each selected cluster, get all of its leaves,"
"and leaves of leaves, and so on..."
Largest lambda => Most persistent points
Get the most persistent points
Add most persistent points as exemplars
Add exemplars for each leaf of each selected cluster.
(exclude root)
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Extract the chosen cluster bounds. If enough duplicate data points exist in the
data the lambda value might be infinite. This breaks labeling and highlighting
the chosen clusters.
Extract the plot range of the y-axis and set default center and height values for ellipses.
Extremly dense clusters might result in near infinite lambda values. Setting max_height
based on the percentile should alleviate the impact on plotting.
Set center and height to default values if necessary
Ensure the ellipse is visible
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
"the point is in the dataset, fix lambda for rounding errors"
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
Disable for now -- need to refactor to meet newer standards
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
this fails if no $DISPLAY specified
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
no prediction data error
wrong dimensions error
no clusters warning
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
without epsilon we should see many noise points as children of root.
for this random seed an epsilon of 0.2 will produce exactly 2 noise
points at that cut in single linkage.
Disable for now -- need to refactor to meet newer standards
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
"Given negative, zero and positive denominator and positive numerator"
Make sure safe division is always positive and doesn't raise ZeroDivision error
Ignore future warnings thrown by sklearn
Create a nice dataset with 6 circular clusters and 2 moons
"Given, the base HDBSCAN with method 'eom'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
"Given, the base HDBSCAN with method 'leaf'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
Method 'eom'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
Method 'leaf'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
"Given the max number of clusters that can be produced by 'eom',"
(these are produced for epsilon=0) (??? Needs verification)
"When we try flat clustering with 'eom' method for more n_clusters,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
"the resulting clusterer switches to using method 'leaf',"
and the resulting probabilities and labels must match
"Given the base HDBSCAN trained on some data,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the clustering should match that due to approximate_predict,"
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the number of clusters produced must match the original n_clusters"
and all probabilities are <= 1.
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat with specified n_clusters,"
"Then, the requested number of clusters must be produced"
and all probabilities are <= 1.
When using approximate_predict_flat with more clusters
"than 'eom' can handle,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
But the requested number of clusters must still be produced using 'leaf'
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
The dependencies are the same as the contents of requirements.txt
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Check for connected component on X
Compute sparse mutual reachability graph
"if max_dist > 0, max distance to use when the reachability is infinite"
Check connected component on mutual reachability
"If more than one component, it means that even if the distance matrix X"
"has one component, there exists with less than `min_samples` neighbors"
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Non-precomputed matrices may contain non-finite values.
Rows with these values
Pass only the purely finite indices into hdbscan
We will later assign all non-finite points to the background -1 cluster
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
remap indices to align with original data in the case of non-finite entries.
"Unltimately, for each Ci, we only require the"
"minimum of DSPC(Ci, Cj) over all Cj != Ci."
"So let's call this value DSPC_wrt(Ci), i.e."
density separation 'with respect to' Ci.
If exactly one of the points is noise
Set the density sparseness of the cluster
to the sparsest value seen so far.
Check whether density separations with
respect to each of these clusters can
be reduced.
"In case min_outlier_sep is still np.inf, we assign a new value to it."
This only makes sense if num_clusters = 1 since it has turned out
that the MR-MST has no edges between a noise point and a core point.
DSPC_wrt[Ci] might be infinite if the connected component for Ci is
"an ""island"" in the MR-MST. Whereas for other clusters Cj and Ck, the"
MR-MST might contain an edge with one point in Cj and ther other one
"in Ck. Here, we replace the infinite density separation of Ci by"
another large enough value.
""
TODO: Think of a better yet efficient way to handle this.
Extract flat clustering from HDBSCAN's hierarchy for 7 clusters
Use a previously initialized/trained HDBSCAN
Handle the trivial case first.
Always generate prediction_data to avoid later woes
This will later be chosen according to n_clusters
Initialize and train clusterer if one was not previously supplied.
Always generate prediction data
We do not pass cluster_selection_epsilon here.
"While this adds unnecessary computation, it makes the code"
easier to read and debug.
"Train on 'X'. Do this even if the supplied clusterer was trained,"
because we want to make sure it fits 'X'.
"Pick an epsilon value right after a split produces n_clusters,"
and the don't split further for smaller epsilon (larger lambda)
Or use the specified cluster_selection_epsilon
"Extract tree related stuff, in order to re-assign labels"
Get labels according to the required cluster_selection_epsilon
Reflect the related changes in HDBSCAN.
PredictionData attached to HDBSCAN should also change.
A function re_init is defined in this module to handle this.
"From a fitted HDBSCAN model, predict for n_clusters=5"
Store prediciton data for later use.
and use this prediction data to predict on new points
Get number of fitted clusters for later use.
We'll need the condensed tree later...
"If none of the three arguments: prediction_data, n_clusters,"
"and cluster_selection_epsilon are supplied,"
then use clusterer's prediciton data directly
"If either of n_clusters or cluster_selection_epsilon were supplied,"
then build prediction data from these by modifying clusterer's
Get prediction data from clusterer
Modify prediction_data to reflect new n_clusters
"First, make a copy of prediction data to avoid modifying source"
Cluster selection method is hold by condensed_tree.
Change from 'eom' to 'leaf' if n_clusters is too large.
This change does not affect the tree associated with 'clusterer'
Re-initialize prediction_data for the specified n_clusters or epsilon
============================================================
Now we're ready to use prediction_data
"The rest of the code is copied from HDBSCAN's approximate_predict,"
but modified to use prediction_data instead of clusterer's attribute
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering from prediction data
Initialize probabilities
k-NN for prediciton points to training set
Loop over prediction points to compute probabilities
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"Find row in tree where nearest neighbor drops out,"
so we can get a lambda value for the nearest neighbor
"Assign lambda as min(lambda-to-neighbor, neighbor's-lambda-to-tree)"
"Equivalently, this assigns core distance for prediction point as"
"max(dist-to-neighbor, neighbor's-dist-to-tree)"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the nearest exemplar persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Merge the two probabilities to produce a single set of probabilities
Include probability that the nearest neighbor belongs to a cluster
Rename variable so it's easy to understand what's being returned
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering at the chosen epsilon from prediction_data
"When no clusters found, return array of 0's"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the point persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Include probability that the point belongs to a cluster
Aggregate the three probabilities to produce membership vectors
Re-name variable to clarify what's being returned.
"With method 'eom', max clusters are produced for epsilon=0,"
as computed by
Increasing epsilon can only reduce the number of ouput clusters.
"To select epsilon, consider all values where clusters are split"
Subtract the extra e-12 to avoid numerical errors in comparison
"Then, we avoid splitting for all epsilon below this."
Use an epsilon value that produces the right number of clusters.
The condensed tree of HDBSCAN has this information.
Extract the lambda levels (=1/distance) from the condensed tree
We don't want values that produce a large cluster and
just one or two individual points.
Keep only those lambda values corresponding to cluster separation;
"i.e., with child_sizes > 1"
"Get the unique values, because when two clusters fall out of one,"
the entry with lambda is repeated.
lambda values are sorted by np.unique.
"Now, get epsilon (distance threshold) as 1/lambda"
"At this epsilon, n_clusters have been split."
Stop splits at epsilons smaller than this.
"To allow for numerical errors,"
predData must be a pre-trained PredictionData instance from hdbscan
"If n_clusters is specified, compute cluster_selection_epsilon;"
This is the key modification:
Select clusters according to selection method and epsilon.
_new_select_clusters is a modification of get_clusters
from hdbscan._hdbscan_tree
"raw tree, used later to get exemplars and lambda values"
"Re-do the cluster map: Map cluster numbers in tree (N, N+1, ..)"
to the cluster labels produced as output
Re-compute lambdas and exemplars for selected clusters;
max_lambda <=> smallest distance <=> most persistent point(s)
Map all sub-clusters of selected cluster to the selected cluster's
label in output.
Map lambdas too...
Create set of exemplar points for later use.
Novel points are assigned based on cluster of closest exemplar.
"For each selected cluster, get all of its leaves,"
"and leaves of leaves, and so on..."
Largest lambda => Most persistent points
Get the most persistent points
Add most persistent points as exemplars
Add exemplars for each leaf of each selected cluster.
(exclude root)
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Extract the chosen cluster bounds. If enough duplicate data points exist in the
data the lambda value might be infinite. This breaks labeling and highlighting
the chosen clusters.
Extract the plot range of the y-axis and set default center and height values for ellipses.
Extremly dense clusters might result in near infinite lambda values. Setting max_height
based on the percentile should alleviate the impact on plotting.
Set center and height to default values if necessary
Ensure the ellipse is visible
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
"the point is in the dataset, fix lambda for rounding errors"
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
Disable for now -- need to refactor to meet newer standards
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
this fails if no $DISPLAY specified
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
no prediction data error
wrong dimensions error
no clusters warning
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
without epsilon we should see many noise points as children of root.
for this random seed an epsilon of 0.2 will produce exactly 2 noise
points at that cut in single linkage.
Disable for now -- need to refactor to meet newer standards
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
"Given negative, zero and positive denominator and positive numerator"
Make sure safe division is always positive and doesn't raise ZeroDivision error
Ignore future warnings thrown by sklearn
Create a nice dataset with 6 circular clusters and 2 moons
"Given, the base HDBSCAN with method 'eom'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
"Given, the base HDBSCAN with method 'leaf'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
Method 'eom'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
Method 'leaf'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
"Given the max number of clusters that can be produced by 'eom',"
(these are produced for epsilon=0) (??? Needs verification)
"When we try flat clustering with 'eom' method for more n_clusters,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
"the resulting clusterer switches to using method 'leaf',"
and the resulting probabilities and labels must match
"Given the base HDBSCAN trained on some data,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the clustering should match that due to approximate_predict,"
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the number of clusters produced must match the original n_clusters"
and all probabilities are <= 1.
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat with specified n_clusters,"
"Then, the requested number of clusters must be produced"
and all probabilities are <= 1.
When using approximate_predict_flat with more clusters
"than 'eom' can handle,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
But the requested number of clusters must still be produced using 'leaf'
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
The dependencies are the same as the contents of requirements.txt
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Check for connected component on X
Compute sparse mutual reachability graph
"if max_dist > 0, max distance to use when the reachability is infinite"
Check connected component on mutual reachability
"If more than one component, it means that even if the distance matrix X"
"has one component, there exists with less than `min_samples` neighbors"
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Non-precomputed matrices may contain non-finite values.
Rows with these values
Pass only the purely finite indices into hdbscan
We will later assign all non-finite points to the background -1 cluster
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
remap indices to align with original data in the case of non-finite entries.
"Unltimately, for each Ci, we only require the"
"minimum of DSPC(Ci, Cj) over all Cj != Ci."
"So let's call this value DSPC_wrt(Ci), i.e."
density separation 'with respect to' Ci.
If exactly one of the points is noise
Set the density sparseness of the cluster
to the sparsest value seen so far.
Check whether density separations with
respect to each of these clusters can
be reduced.
"In case min_outlier_sep is still np.inf, we assign a new value to it."
This only makes sense if num_clusters = 1 since it has turned out
that the MR-MST has no edges between a noise point and a core point.
DSPC_wrt[Ci] might be infinite if the connected component for Ci is
"an ""island"" in the MR-MST. Whereas for other clusters Cj and Ck, the"
MR-MST might contain an edge with one point in Cj and ther other one
"in Ck. Here, we replace the infinite density separation of Ci by"
another large enough value.
""
TODO: Think of a better yet efficient way to handle this.
Extract flat clustering from HDBSCAN's hierarchy for 7 clusters
Use a previously initialized/trained HDBSCAN
Handle the trivial case first.
Always generate prediction_data to avoid later woes
This will later be chosen according to n_clusters
Initialize and train clusterer if one was not previously supplied.
Always generate prediction data
We do not pass cluster_selection_epsilon here.
"While this adds unnecessary computation, it makes the code"
easier to read and debug.
"Train on 'X'. Do this even if the supplied clusterer was trained,"
because we want to make sure it fits 'X'.
"Pick an epsilon value right after a split produces n_clusters,"
and the don't split further for smaller epsilon (larger lambda)
Or use the specified cluster_selection_epsilon
"Extract tree related stuff, in order to re-assign labels"
Get labels according to the required cluster_selection_epsilon
Reflect the related changes in HDBSCAN.
PredictionData attached to HDBSCAN should also change.
A function re_init is defined in this module to handle this.
"From a fitted HDBSCAN model, predict for n_clusters=5"
Store prediciton data for later use.
and use this prediction data to predict on new points
Get number of fitted clusters for later use.
We'll need the condensed tree later...
"If none of the three arguments: prediction_data, n_clusters,"
"and cluster_selection_epsilon are supplied,"
then use clusterer's prediciton data directly
"If either of n_clusters or cluster_selection_epsilon were supplied,"
then build prediction data from these by modifying clusterer's
Get prediction data from clusterer
Modify prediction_data to reflect new n_clusters
"First, make a copy of prediction data to avoid modifying source"
Cluster selection method is hold by condensed_tree.
Change from 'eom' to 'leaf' if n_clusters is too large.
This change does not affect the tree associated with 'clusterer'
Re-initialize prediction_data for the specified n_clusters or epsilon
============================================================
Now we're ready to use prediction_data
"The rest of the code is copied from HDBSCAN's approximate_predict,"
but modified to use prediction_data instead of clusterer's attribute
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering from prediction data
Initialize probabilities
k-NN for prediciton points to training set
Loop over prediction points to compute probabilities
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"Find row in tree where nearest neighbor drops out,"
so we can get a lambda value for the nearest neighbor
"Assign lambda as min(lambda-to-neighbor, neighbor's-lambda-to-tree)"
"Equivalently, this assigns core distance for prediction point as"
"max(dist-to-neighbor, neighbor's-dist-to-tree)"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the nearest exemplar persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Merge the two probabilities to produce a single set of probabilities
Include probability that the nearest neighbor belongs to a cluster
Rename variable so it's easy to understand what's being returned
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering at the chosen epsilon from prediction_data
"When no clusters found, return array of 0's"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the point persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Include probability that the point belongs to a cluster
Aggregate the three probabilities to produce membership vectors
Re-name variable to clarify what's being returned.
"With method 'eom', max clusters are produced for epsilon=0,"
as computed by
Increasing epsilon can only reduce the number of ouput clusters.
"To select epsilon, consider all values where clusters are split"
Subtract the extra e-12 to avoid numerical errors in comparison
"Then, we avoid splitting for all epsilon below this."
Use an epsilon value that produces the right number of clusters.
The condensed tree of HDBSCAN has this information.
Extract the lambda levels (=1/distance) from the condensed tree
We don't want values that produce a large cluster and
just one or two individual points.
Keep only those lambda values corresponding to cluster separation;
"i.e., with child_sizes > 1"
"Get the unique values, because when two clusters fall out of one,"
the entry with lambda is repeated.
lambda values are sorted by np.unique.
"Now, get epsilon (distance threshold) as 1/lambda"
"At this epsilon, n_clusters have been split."
Stop splits at epsilons smaller than this.
"To allow for numerical errors,"
predData must be a pre-trained PredictionData instance from hdbscan
"If n_clusters is specified, compute cluster_selection_epsilon;"
This is the key modification:
Select clusters according to selection method and epsilon.
_new_select_clusters is a modification of get_clusters
from hdbscan._hdbscan_tree
"raw tree, used later to get exemplars and lambda values"
"Re-do the cluster map: Map cluster numbers in tree (N, N+1, ..)"
to the cluster labels produced as output
Re-compute lambdas and exemplars for selected clusters;
max_lambda <=> smallest distance <=> most persistent point(s)
Map all sub-clusters of selected cluster to the selected cluster's
label in output.
Map lambdas too...
Create set of exemplar points for later use.
Novel points are assigned based on cluster of closest exemplar.
"For each selected cluster, get all of its leaves,"
"and leaves of leaves, and so on..."
Largest lambda => Most persistent points
Get the most persistent points
Add most persistent points as exemplars
Add exemplars for each leaf of each selected cluster.
(exclude root)
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Extract the chosen cluster bounds. If enough duplicate data points exist in the
data the lambda value might be infinite. This breaks labeling and highlighting
the chosen clusters.
Extract the plot range of the y-axis and set default center and height values for ellipses.
Extremly dense clusters might result in near infinite lambda values. Setting max_height
based on the percentile should alleviate the impact on plotting.
Set center and height to default values if necessary
Ensure the ellipse is visible
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
"the point is in the dataset, fix lambda for rounding errors"
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
Disable for now -- need to refactor to meet newer standards
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
this fails if no $DISPLAY specified
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
no prediction data error
wrong dimensions error
no clusters warning
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
without epsilon we should see many noise points as children of root.
for this random seed an epsilon of 0.2 will produce exactly 2 noise
points at that cut in single linkage.
Disable for now -- need to refactor to meet newer standards
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
"Given negative, zero and positive denominator and positive numerator"
Make sure safe division is always positive and doesn't raise ZeroDivision error
Ignore future warnings thrown by sklearn
Create a nice dataset with 6 circular clusters and 2 moons
"Given, the base HDBSCAN with method 'eom'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
"Given, the base HDBSCAN with method 'leaf'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
Method 'eom'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
Method 'leaf'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
"Given the max number of clusters that can be produced by 'eom',"
(these are produced for epsilon=0) (??? Needs verification)
"When we try flat clustering with 'eom' method for more n_clusters,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
"the resulting clusterer switches to using method 'leaf',"
and the resulting probabilities and labels must match
"Given the base HDBSCAN trained on some data,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the clustering should match that due to approximate_predict,"
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the number of clusters produced must match the original n_clusters"
and all probabilities are <= 1.
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat with specified n_clusters,"
"Then, the requested number of clusters must be produced"
and all probabilities are <= 1.
When using approximate_predict_flat with more clusters
"than 'eom' can handle,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
But the requested number of clusters must still be produced using 'leaf'
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
The dependencies are the same as the contents of requirements.txt
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Check for connected component on X
Compute sparse mutual reachability graph
"if max_dist > 0, max distance to use when the reachability is infinite"
Check connected component on mutual reachability
"If more than one component, it means that even if the distance matrix X"
"has one component, there exists with less than `min_samples` neighbors"
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Non-precomputed matrices may contain non-finite values.
Rows with these values
Pass only the purely finite indices into hdbscan
We will later assign all non-finite points to the background -1 cluster
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
remap indices to align with original data in the case of non-finite entries.
"Unltimately, for each Ci, we only require the"
"minimum of DSPC(Ci, Cj) over all Cj != Ci."
"So let's call this value DSPC_wrt(Ci), i.e."
density separation 'with respect to' Ci.
If exactly one of the points is noise
Set the density sparseness of the cluster
to the sparsest value seen so far.
Check whether density separations with
respect to each of these clusters can
be reduced.
"In case min_outlier_sep is still np.inf, we assign a new value to it."
This only makes sense if num_clusters = 1 since it has turned out
that the MR-MST has no edges between a noise point and a core point.
DSPC_wrt[Ci] might be infinite if the connected component for Ci is
"an ""island"" in the MR-MST. Whereas for other clusters Cj and Ck, the"
MR-MST might contain an edge with one point in Cj and ther other one
"in Ck. Here, we replace the infinite density separation of Ci by"
another large enough value.
""
TODO: Think of a better yet efficient way to handle this.
Extract flat clustering from HDBSCAN's hierarchy for 7 clusters
Use a previously initialized/trained HDBSCAN
Handle the trivial case first.
Always generate prediction_data to avoid later woes
This will later be chosen according to n_clusters
Initialize and train clusterer if one was not previously supplied.
Always generate prediction data
We do not pass cluster_selection_epsilon here.
"While this adds unnecessary computation, it makes the code"
easier to read and debug.
"Train on 'X'. Do this even if the supplied clusterer was trained,"
because we want to make sure it fits 'X'.
"Pick an epsilon value right after a split produces n_clusters,"
and the don't split further for smaller epsilon (larger lambda)
Or use the specified cluster_selection_epsilon
"Extract tree related stuff, in order to re-assign labels"
Get labels according to the required cluster_selection_epsilon
Reflect the related changes in HDBSCAN.
PredictionData attached to HDBSCAN should also change.
A function re_init is defined in this module to handle this.
"From a fitted HDBSCAN model, predict for n_clusters=5"
Store prediciton data for later use.
and use this prediction data to predict on new points
Get number of fitted clusters for later use.
We'll need the condensed tree later...
"If none of the three arguments: prediction_data, n_clusters,"
"and cluster_selection_epsilon are supplied,"
then use clusterer's prediciton data directly
"If either of n_clusters or cluster_selection_epsilon were supplied,"
then build prediction data from these by modifying clusterer's
Get prediction data from clusterer
Modify prediction_data to reflect new n_clusters
"First, make a copy of prediction data to avoid modifying source"
Cluster selection method is hold by condensed_tree.
Change from 'eom' to 'leaf' if n_clusters is too large.
This change does not affect the tree associated with 'clusterer'
Re-initialize prediction_data for the specified n_clusters or epsilon
============================================================
Now we're ready to use prediction_data
"The rest of the code is copied from HDBSCAN's approximate_predict,"
but modified to use prediction_data instead of clusterer's attribute
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering from prediction data
Initialize probabilities
k-NN for prediciton points to training set
Loop over prediction points to compute probabilities
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"Find row in tree where nearest neighbor drops out,"
so we can get a lambda value for the nearest neighbor
"Assign lambda as min(lambda-to-neighbor, neighbor's-lambda-to-tree)"
"Equivalently, this assigns core distance for prediction point as"
"max(dist-to-neighbor, neighbor's-dist-to-tree)"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the nearest exemplar persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Merge the two probabilities to produce a single set of probabilities
Include probability that the nearest neighbor belongs to a cluster
Rename variable so it's easy to understand what's being returned
Extract condensed tree for later use
Choose flat clustering based on cluster_selection_epsilon or n_clusters.
"If neither is specified, use clusterer's cluster_selection_epsilon"
Use the same prediction_data as clusterer's
Compute cluster_selection_epsilon so that a flat clustering
produces a specified number of n_clusters
"With method 'eom', we may fail to get 'n_clusters' clusters. So,"
Create another instance of prediction_data that is consistent
with the selected value of epsilon.
Flat clustering at the chosen epsilon from prediction_data
"When no clusters found, return array of 0's"
Probabilities based on distance to closest exemplar in each cluster:
Use new prediction_data that points to exemplars that are specific
to the choice of n_clusters
Probabilities based on how long the point persists in
each cluster (with respect to most persistent exemplar)
Use new clusters that are defined by the choice of n_clusters.
Include probability that the point belongs to a cluster
Aggregate the three probabilities to produce membership vectors
Re-name variable to clarify what's being returned.
"With method 'eom', max clusters are produced for epsilon=0,"
as computed by
Increasing epsilon can only reduce the number of ouput clusters.
"To select epsilon, consider all values where clusters are split"
Subtract the extra e-12 to avoid numerical errors in comparison
"Then, we avoid splitting for all epsilon below this."
Use an epsilon value that produces the right number of clusters.
The condensed tree of HDBSCAN has this information.
Extract the lambda levels (=1/distance) from the condensed tree
We don't want values that produce a large cluster and
just one or two individual points.
Keep only those lambda values corresponding to cluster separation;
"i.e., with child_sizes > 1"
"Get the unique values, because when two clusters fall out of one,"
the entry with lambda is repeated.
lambda values are sorted by np.unique.
"Now, get epsilon (distance threshold) as 1/lambda"
"At this epsilon, n_clusters have been split."
Stop splits at epsilons smaller than this.
"To allow for numerical errors,"
predData must be a pre-trained PredictionData instance from hdbscan
"If n_clusters is specified, compute cluster_selection_epsilon;"
This is the key modification:
Select clusters according to selection method and epsilon.
_new_select_clusters is a modification of get_clusters
from hdbscan._hdbscan_tree
"raw tree, used later to get exemplars and lambda values"
"Re-do the cluster map: Map cluster numbers in tree (N, N+1, ..)"
to the cluster labels produced as output
Re-compute lambdas and exemplars for selected clusters;
max_lambda <=> smallest distance <=> most persistent point(s)
Map all sub-clusters of selected cluster to the selected cluster's
label in output.
Map lambdas too...
Create set of exemplar points for later use.
Novel points are assigned based on cluster of closest exemplar.
"For each selected cluster, get all of its leaves,"
"and leaves of leaves, and so on..."
Largest lambda => Most persistent points
Get the most persistent points
Add most persistent points as exemplars
Add exemplars for each leaf of each selected cluster.
(exclude root)
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Extract the chosen cluster bounds. If enough duplicate data points exist in the
data the lambda value might be infinite. This breaks labeling and highlighting
the chosen clusters.
Extract the plot range of the y-axis and set default center and height values for ellipses.
Extremly dense clusters might result in near infinite lambda values. Setting max_height
based on the percentile should alleviate the impact on plotting.
Set center and height to default values if necessary
Ensure the ellipse is visible
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
"the point is in the dataset, fix lambda for rounding errors"
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
Disable for now -- need to refactor to meet newer standards
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
this fails if no $DISPLAY specified
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
no prediction data error
wrong dimensions error
no clusters warning
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
without epsilon we should see many noise points as children of root.
for this random seed an epsilon of 0.2 will produce exactly 2 noise
points at that cut in single linkage.
Disable for now -- need to refactor to meet newer standards
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
"Given negative, zero and positive denominator and positive numerator"
Make sure safe division is always positive and doesn't raise ZeroDivision error
Ignore future warnings thrown by sklearn
Create a nice dataset with 6 circular clusters and 2 moons
"Given, the base HDBSCAN with method 'eom'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
"Given, the base HDBSCAN with method 'leaf'"
"When we ask for flat clustering with same n_clusters,"
"Then, the labels and probabilities should match"
Method 'eom'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
Method 'leaf'...
"Given, a flat clustering for required n_clusters,"
"When we run the base HDBSCAN using it's epsilon,"
"Then, the labels and probabilities should match"
"Given the max number of clusters that can be produced by 'eom',"
(these are produced for epsilon=0) (??? Needs verification)
"When we try flat clustering with 'eom' method for more n_clusters,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
"the resulting clusterer switches to using method 'leaf',"
and the resulting probabilities and labels must match
"Given the base HDBSCAN trained on some data,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the clustering should match that due to approximate_predict,"
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat without specifying n_clusters,"
"Then, the number of clusters produced must match the original n_clusters"
and all probabilities are <= 1.
"Given a flat clustering trained for some n_clusters,"
"When using approximate_predict_flat with specified n_clusters,"
"Then, the requested number of clusters must be produced"
and all probabilities are <= 1.
When using approximate_predict_flat with more clusters
"than 'eom' can handle,"
"Then, a warning is raised saying 'eom' can't get this clustering,"
But the requested number of clusters must still be produced using 'leaf'
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called with new data for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the test set
and all probabilities are <= 1.
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When all_points_membership_vectors_flat is called,"
"Then the number of clusters in memberships matches those of clusterer,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Ignore user warnings in this function
"Given a flat clustering trained for n_clusters picked by HDBSCAN,"
"When all_points_membership_vectors_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
========================================
"Given a flat clustering for a specified n_clusters,"
"When membership_vector_flat is called for some n_clusters,"
"Then the number of clusters in memberships should be as requested,"
and the number of points should equal those in the training set
and all probabilities are <= 1.
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
The dependencies are the same as the contents of requirements.txt
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Check for connected component on X
Compute sparse mutual reachability graph
"if max_dist > 0, max distance to use when the reachability is infinite"
Check connected component on mutual reachability
"If more than one component, it means that even if the distance matrix X"
"has one component, there exists with less than `min_samples` neighbors"
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
"Unltimately, for each Ci, we only require the"
"minimum of DSPC(Ci, Cj) over all Cj != Ci."
"So let's call this value DSPC_wrt(Ci), i.e."
density separation 'with respect to' Ci.
If exactly one of the points is noise
Set the density sparseness of the cluster
to the sparsest value seen so far.
Check whether density separations with
respect to each of these clusters can
be reduced.
"In case min_outlier_sep is still np.inf, we assign a new value to it."
This only makes sense if num_clusters = 1 since it has turned out
that the MR-MST has no edges between a noise point and a core point.
DSPC_wrt[Ci] might be infinite if the connected component for Ci is
"an ""island"" in the MR-MST. Whereas for other clusters Cj and Ck, the"
MR-MST might contain an edge with one point in Cj and ther other one
"in Ck. Here, we replace the infinite density separation of Ci by"
another large enough value.
""
TODO: Think of a better yet efficient way to handle this.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Extract the chosen cluster bounds. If enough duplicate data points exist in the
data the lambda value might be infinite. This breaks labeling and highlighting
the chosen clusters.
Extract the plot range of the y-axis and set default center and height values for ellipses.
Extremly dense clusters might result in near infinite lambda values. Setting max_height
based on the percentile should alleviate the impact on plotting.
Set center and height to default values if necessary
Ensure the ellipse is visible
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
Disable for now -- need to refactor to meet newer standards
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
this fails if no $DISPLAY specified
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
Disable for now -- need to refactor to meet newer standards
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
The dependencies are the same as the contents of requirements.txt
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Check for connected component on X
Compute sparse mutual reachability graph
"if max_dist > 0, max distance to use when the reachability is infinite"
Check connected component on mutual reachability
"If more than one component, it means that even if the distance matrix X"
"has one component, there exists with less than `min_samples` neighbors"
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
"Unltimately, for each Ci, we only require the"
"minimum of DSPC(Ci, Cj) over all Cj != Ci."
"So let's call this value DSPC_wrt(Ci), i.e."
density separation 'with respect to' Ci.
If exactly one of the points is noise
Set the density sparseness of the cluster
to the sparsest value seen so far.
Check whether density separations with
respect to each of these clusters can
be reduced.
"In case min_outlier_sep is still np.inf, we assign a new value to it."
This only makes sense if num_clusters = 1 since it has turned out
that the MR-MST has no edges between a noise point and a core point.
DSPC_wrt[Ci] might be infinite if the connected component for Ci is
"an ""island"" in the MR-MST. Whereas for other clusters Cj and Ck, the"
MR-MST might contain an edge with one point in Cj and ther other one
"in Ck. Here, we replace the infinite density separation of Ci by"
another large enough value.
""
TODO: Think of a better yet efficient way to handle this.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Extract the chosen cluster bounds. If enough duplicate data points exist in the
data the lambda value might be infinite. This breaks labeling and highlighting
the chosen clusters.
Extract the plot range of the y-axis and set default center and height values for ellipses.
Extremly dense clusters might result in near infinite lambda values. Setting max_height
based on the percentile should alleviate the impact on plotting.
Set center and height to default values if necessary
Ensure the ellipse is visible
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
Disable for now -- need to refactor to meet newer standards
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
this fails if no $DISPLAY specified
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
Disable for now -- need to refactor to meet newer standards
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
The dependencies are the same as the contents of requirements.txt
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Check for connected component on X
Compute sparse mutual reachability graph
"if max_dist > 0, max distance to use when the reachability is infinite"
Check connected component on mutual reachability
"If more than one component, it means that even if the distance matrix X"
"has one component, there exists with less than `min_samples` neighbors"
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
"Unltimately, for each Ci, we only require the"
"minimum of DSPC(Ci, Cj) over all Cj != Ci."
"So let's call this value DSPC_wrt(Ci), i.e."
density separation 'with respect to' Ci.
If exactly one of the points is noise
Set the density sparseness of the cluster
to the sparsest value seen so far.
Check whether density separations with
respect to each of these clusters can
be reduced.
"In case min_outlier_sep is still np.inf, we assign a new value to it."
This only makes sense if num_clusters = 1 since it has turned out
that the MR-MST has no edges between a noise point and a core point.
DSPC_wrt[Ci] might be infinite if the connected component for Ci is
"an ""island"" in the MR-MST. Whereas for other clusters Cj and Ck, the"
MR-MST might contain an edge with one point in Cj and ther other one
"in Ck. Here, we replace the infinite density separation of Ci by"
another large enough value.
""
TODO: Think of a better yet efficient way to handle this.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Extract the chosen cluster bounds. If enough duplicate data points exist in the
data the lambda value might be infinite. This breaks labeling and highlighting
the chosen clusters.
Extract the plot range of the y-axis and set default center and height values for ellipses.
Extremly dense clusters might result in near infinite lambda values. Setting max_height
based on the percentile should alleviate the impact on plotting.
Set center and height to default values if necessary
Ensure the ellipse is visible
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
Disable for now -- need to refactor to meet newer standards
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
this fails if no $DISPLAY specified
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
Disable for now -- need to refactor to meet newer standards
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
The dependencies are the same as the contents of requirements.txt
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Check for connected component on X
Compute sparse mutual reachability graph
"if max_dist > 0, max distance to use when the reachability is infinite"
Check connected component on mutual reachability
"If more than one component, it means that even if the distance matrix X"
"has one component, there exists with less than `min_samples` neighbors"
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
"Unltimately, for each Ci, we only require the"
"minimum of DSPC(Ci, Cj) over all Cj != Ci."
"So let's call this value DSPC_wrt(Ci), i.e."
density separation 'with respect to' Ci.
If exactly one of the points is noise
Set the density sparseness of the cluster
to the sparsest value seen so far.
Check whether density separations with
respect to each of these clusters can
be reduced.
"In case min_outlier_sep is still np.inf, we assign a new value to it."
This only makes sense if num_clusters = 1 since it has turned out
that the MR-MST has no edges between a noise point and a core point.
DSPC_wrt[Ci] might be infinite if the connected component for Ci is
"an ""island"" in the MR-MST. Whereas for other clusters Cj and Ck, the"
MR-MST might contain an edge with one point in Cj and ther other one
"in Ck. Here, we replace the infinite density separation of Ci by"
another large enough value.
""
TODO: Think of a better yet efficient way to handle this.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Extract the chosen cluster bounds. If enough duplicate data points exist in the
data the lambda value might be infinite. This breaks labeling and highlighting
the chosen clusters.
Extract the plot range of the y-axis and set default center and height values for ellipses.
Extremly dense clusters might result in near infinite lambda values. Setting max_height
based on the percentile should alleviate the impact on plotting.
Set center and height to default values if necessary
Ensure the ellipse is visible
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
Disable for now -- need to refactor to meet newer standards
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
Disable for now -- need to refactor to meet newer standards
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
The dependencies are the same as the contents of requirements.txt
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Check for connected component on X
Compute sparse mutual reachability graph
"if max_dist > 0, max distance to use when the reachability is infinite"
Check connected component on mutual reachability
"If more than one component, it means that even if the distance matrix X"
"has one component, there exists with less than `min_samples` neighbors"
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
"Unltimately, for each Ci, we only require the"
"minimum of DSPC(Ci, Cj) over all Cj != Ci."
"So let's call this value DSPC_wrt(Ci), i.e."
density separation 'with respect to' Ci.
If exactly one of the points is noise
Set the density sparseness of the cluster
to the sparsest value seen so far.
Check whether density separations with
respect to each of these clusters can
be reduced.
"In case min_outlier_sep is still np.inf, we assign a new value to it."
This only makes sense if num_clusters = 1 since it has turned out
that the MR-MST has no edges between a noise point and a core point.
DSPC_wrt[Ci] might be infinite if the connected component for Ci is
"an ""island"" in the MR-MST. Whereas for other clusters Cj and Ck, the"
MR-MST might contain an edge with one point in Cj and ther other one
"in Ck. Here, we replace the infinite density separation of Ci by"
another large enough value.
""
TODO: Think of a better yet efficient way to handle this.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
The dependencies are the same as the contents of requirements.txt
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Compute sparse mutual reachability graph
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Compute sparse mutual reachability graph
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Compute sparse mutual reachability graph
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Compute sparse mutual reachability graph
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Compute sparse mutual reachability graph
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"Treating this case explicitly, instead of letting"
"sklearn.metrics.pairwise_distances handle it,"
enables the usage of numpy.inf in the distance
matrix to indicate missing distance information.
TODO: Check if copying is necessary
raise TypeError('Sparse distance matrices not yet supported')
Warn if the MST couldn't be constructed around the missing distances
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Compute sparse mutual reachability graph
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
"Only non-sparse, precomputed distance matrices are handled here"
and thereby allowed to contain numpy.inf for missing distances
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
Handle sparse precomputed distance matrices separately
"Only non-sparse, precomputed distance matrices are allowed"
to have numpy.inf values indicating missing distances
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
"for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):"
"axis.plot(xs, ys, color='black', linewidth=1)"
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"Import numpy here, only when headers are needed"
Add numpy headers to include_dirs
Call original build_ext command
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
raise TypeError('Sparse distance matrices not yet supported')
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Compute sparse mutual reachability graph
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
"Return the only cluster, the root"
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
"When no clusters found, return array of 0's"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
assert_array_almost_equal(
"vector,"
"np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
raise TypeError('Sparse distance matrices not yet supported')
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Compute sparse mutual reachability graph
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
def test_rsl_unavailable_hierarchy():
clusterer = RobustSingleLinkage()
with warnings.catch_warnings(record=True) as w:
tree = clusterer.cluster_hierarchy_
assert len(w) > 0
assert tree is None
"H, y = shuffle(X, y, random_state=7)"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_unavailable_attributes():
clusterer = HDBSCAN(gen_min_span_tree=False)
with warnings.catch_warnings(record=True) as w:
tree = clusterer.condensed_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.single_linkage_tree_
assert len(w) > 0
assert tree is None
with warnings.catch_warnings(record=True) as w:
scores = clusterer.outlier_scores_
assert len(w) > 0
assert scores is None
with warnings.catch_warnings(record=True) as w:
tree = clusterer.minimum_spanning_tree_
assert len(w) > 0
assert tree is None
def test_hdbscan_min_span_tree_availability():
clusterer = HDBSCAN().fit(X)
tree = clusterer.minimum_spanning_tree_
assert tree is None
D = distance.squareform(distance.pdist(X))
D /= np.max(D)
HDBSCAN(metric='precomputed').fit(D)
tree = clusterer.minimum_spanning_tree_
assert tree is None
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
"assert_array_almost_equal(vector, np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
"assert_array_almost_equal(vector, np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
"assert_array_almost_equal(vector, np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
Supporting numpy prior to version 1.7 is a little painful ...
Make sure y is an inexact type to avoid bad behavior on abs(MIN_INT).
"This will cause casting of x later. Also, make sure to allow"
"subclasses (e.g., for numpy.ma)."
"Because we're using boolean indexing, x & y must be the same"
"shape. Ideally, we'd just do x, y = broadcast_arrays(x, y)."
"It's in lib.stride_tricks, though, so we can't import it here."
Avoid subtraction with infinite/nan values...
Check for equality of infinite values...
Make NaN == NaN
raise TypeError('Sparse distance matrices not yet supported')
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Compute sparse mutual reachability graph
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
"H, y = shuffle(X, y, random_state=7)"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
"assert_array_almost_equal(vector, np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
"assert_array_almost_equal(vector, np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
"assert_array_almost_equal(vector, np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
Supporting numpy prior to version 1.7 is a little painful ...
Make sure y is an inexact type to avoid bad behavior on abs(MIN_INT).
"This will cause casting of x later. Also, make sure to allow"
"subclasses (e.g., for numpy.ma)."
"Because we're using boolean indexing, x & y must be the same"
"shape. Ideally, we'd just do x, y = broadcast_arrays(x, y)."
"It's in lib.stride_tricks, though, so we can't import it here."
Avoid subtraction with infinite/nan values...
Check for equality of infinite values...
Make NaN == NaN
raise TypeError('Sparse distance matrices not yet supported')
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Compute sparse mutual reachability graph
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
"H, y = shuffle(X, y, random_state=7)"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
def test_hdbscan_membership_vector():
clusterer = HDBSCAN(prediction_data=True).fit(X)
"vector = membership_vector(clusterer, np.array([[-1.5, -1.0]]))"
"assert_array_almost_equal(vector, np.array([[ 0.05705305,  0.05974177,  0.12228153]]))"
"vector = membership_vector(clusterer, np.array([[1.5, -1.0]]))"
"assert_array_almost_equal(vector, np.array([[ 0.09462176,  0.32061556,  0.10112905]]))"
"vector = membership_vector(clusterer, np.array([[0.0, 0.0]]))"
"assert_array_almost_equal(vector, np.array([[ 0.03545607,  0.03363318,  0.04643177]]))"
""
def test_hdbscan_all_points_membership_vectors():
clusterer = HDBSCAN(prediction_data=True).fit(X)
vects = all_points_membership_vectors(clusterer)
"assert_array_almost_equal(vects[0], np.array([7.86400992e-002,"
"2.52734246e-001,"
8.38299608e-002]))
"assert_array_almost_equal(vects[-1], np.array([8.09055344e-001,"
"8.35882503e-002,"
1.07356406e-001]))
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
Supporting numpy prior to version 1.7 is a little painful ...
Make sure y is an inexact type to avoid bad behavior on abs(MIN_INT).
"This will cause casting of x later. Also, make sure to allow"
"subclasses (e.g., for numpy.ma)."
"Because we're using boolean indexing, x & y must be the same"
"shape. Ideally, we'd just do x, y = broadcast_arrays(x, y)."
"It's in lib.stride_tricks, though, so we can't import it here."
Avoid subtraction with infinite/nan values...
Check for equality of infinite values...
Make NaN == NaN
raise TypeError('Sparse distance matrices not yet supported')
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Compute sparse mutual reachability graph
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
"H, y = shuffle(X, y, random_state=7)"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
Supporting numpy prior to version 1.7 is a little painful ...
Make sure y is an inexact type to avoid bad behavior on abs(MIN_INT).
"This will cause casting of x later. Also, make sure to allow"
"subclasses (e.g., for numpy.ma)."
"Because we're using boolean indexing, x & y must be the same"
"shape. Ideally, we'd just do x, y = broadcast_arrays(x, y)."
"It's in lib.stride_tricks, though, so we can't import it here."
Avoid subtraction with infinite/nan values...
Check for equality of infinite values...
Make NaN == NaN
raise TypeError('Sparse distance matrices not yet supported')
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Compute sparse mutual reachability graph
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
"prediction data only applies to the persistent model, so remove"
it from the keyword args we pass on the the function
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
Support various prediction methods for predicting cluster membership
of new or unseen points. There are several ways to interpret how
"to do this correctly, so we provide several methods for"
the different use cases that may arise.
raw_condensed_tree = condensed_tree.to_numpy()
New point departs with the old
Find appropriate cluster based on lambda of new point
Find appropriate cluster based on lambda of new point
We need to find where in the tree the new point would go
for the purposes of outlier membership approximation
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
"H, y = shuffle(X, y, random_state=7)"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
Supporting numpy prior to version 1.7 is a little painful ...
Make sure y is an inexact type to avoid bad behavior on abs(MIN_INT).
"This will cause casting of x later. Also, make sure to allow"
"subclasses (e.g., for numpy.ma)."
"Because we're using boolean indexing, x & y must be the same"
"shape. Ideally, we'd just do x, y = broadcast_arrays(x, y)."
"It's in lib.stride_tricks, though, so we can't import it here."
Avoid subtraction with infinite/nan values...
Check for equality of infinite values...
Make NaN == NaN
raise TypeError('Sparse distance matrices not yet supported')
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Compute sparse mutual reachability graph
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
"H, y = shuffle(X, y, random_state=7)"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
Supporting numpy prior to version 1.7 is a little painful ...
Make sure y is an inexact type to avoid bad behavior on abs(MIN_INT).
"This will cause casting of x later. Also, make sure to allow"
"subclasses (e.g., for numpy.ma)."
"Because we're using boolean indexing, x & y must be the same"
"shape. Ideally, we'd just do x, y = broadcast_arrays(x, y)."
"It's in lib.stride_tricks, though, so we can't import it here."
Avoid subtraction with infinite/nan values...
Check for equality of infinite values...
Make NaN == NaN
raise TypeError('Sparse distance matrices not yet supported')
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Compute sparse mutual reachability graph
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
"H, y = shuffle(X, y, random_state=7)"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
"A little ""fancy"" we select from the flattened array reshape back"
(Fortran format to get indexing right) and take the product to do an and
then convert back to boolean type.
Density sparseness is not well defined if there are no
internal edges (as per the referenced paper). However
MATLAB code from the original authors simply selects the
largest of *all* the edges in the case that there are
"no internal edges, so we do the same here"
"If there are any internal edges, then subselect them out"
If there are no internal edges then we want to take the
"max over all the edges that exist in the MST, so we simply"
do nothing and return all the edges in the MST.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
Supporting numpy prior to version 1.7 is a little painful ...
Make sure y is an inexact type to avoid bad behavior on abs(MIN_INT).
"This will cause casting of x later. Also, make sure to allow"
"subclasses (e.g., for numpy.ma)."
"Because we're using boolean indexing, x & y must be the same"
"shape. Ideally, we'd just do x, y = broadcast_arrays(x, y)."
"It's in lib.stride_tricks, though, so we can't import it here."
Avoid subtraction with infinite/nan values...
Check for equality of infinite values...
Make NaN == NaN
raise TypeError('Sparse distance matrices not yet supported')
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Compute sparse mutual reachability graph
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
"H, y = shuffle(X, y, random_state=7)"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
Supporting numpy prior to version 1.7 is a little painful ...
Make sure y is an inexact type to avoid bad behavior on abs(MIN_INT).
"This will cause casting of x later. Also, make sure to allow"
"subclasses (e.g., for numpy.ma)."
"Because we're using boolean indexing, x & y must be the same"
"shape. Ideally, we'd just do x, y = broadcast_arrays(x, y)."
"It's in lib.stride_tricks, though, so we can't import it here."
Avoid subtraction with infinite/nan values...
Check for equality of infinite values...
Make NaN == NaN
raise TypeError('Sparse distance matrices not yet supported')
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Compute sparse mutual reachability graph
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
Python 2 and 3 compliant string_type checking
We can't do much with sparse matrices ...
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
TO DO: Need heuristic to decide when to go to boruvka;
still debugging for now
Inherits from sklearn
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
We can't do much with sparse matrices ...
Need heuristic to decide when to go to boruvka;
still debugging for now
Need heuristic to decide when to go to boruvka;
still debugging for now
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
"H, y = shuffle(X, y, random_state=7)"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
Probably not applicable now #
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
Supporting numpy prior to version 1.7 is a little painful ...
Make sure y is an inexact type to avoid bad behavior on abs(MIN_INT).
"This will cause casting of x later. Also, make sure to allow subclasses"
"(e.g., for numpy.ma)."
"Because we're using boolean indexing, x & y must be the same shape."
"Ideally, we'd just do x, y = broadcast_arrays(x, y). It's in"
"lib.stride_tricks, though, so we can't import it here."
Avoid subtraction with infinite/nan values...
Check for equality of infinite values...
Make NaN == NaN
raise TypeError('Sparse distance matrices not yet supported')
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Compute sparse mutual reachability graph
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
Python 2 and 3 compliant string_type checking
TO DO: Need heuristic to decide when to go to boruvka; still debugging for now
TO DO: Need heuristic to decide when to go to boruvka; still debugging for now
Inherits from sklearn
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Ensure we don't try to take log of zero
Finally we need the horizontal lines that occur at cluster splits.
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
The Cython routines used require contiguous arrays
The Cython routines used require contiguous arrays
Need heuristic to decide when to go to boruvka; still debugging for now
Need heuristic to decide when to go to boruvka; still debugging for now
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
"H, y = shuffle(X, y, random_state=7)"
import pickle
from sklearn.cluster.tests.common import generate_clustered_data
"X = generate_clustered_data(n_clusters=n_clusters, n_samples_per_cluster=50)"
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"H, y = shuffle(X, y, random_state=7)"
"metric is the function reference, not the string key."
## Probably not applicable now #########################
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
"## We now install the package in a virtualenv to build docs, so this is not needed"
"sys.path.insert(0, os.path.abspath('../'))"
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"'sphinx.ext.napoleon',"
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
Try and work around older sklearn api
raise TypeError('Sparse distance matrices not yet supported')
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Compute sparse mutual reachability graph
Compute the minimum spanning tree for the sparse graph
Convert the graph to scipy cluster array format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicit in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
Python 2 and 3 compliant string_type checking
TO DO: Need heuristic to decide when to go to boruvka; still debugging for now
TO DO: Need heuristic to decide when to go to boruvka; still debugging for now
Inherits from sklearn
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Finally we need the horizontal lines that occur at cluster splits.
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
Need heuristic to decide when to go to boruvka; still debugging for now
Need heuristic to decide when to go to boruvka; still debugging for now
import pickle
"number of clusters, ignoring noise if present"
"assert_equal(n_clusters_1, n_clusters)"
"assert_equal(n_clusters_2, n_clusters)"
"assert_equal(n_clusters_1, n_clusters)"
"assert_equal(n_clusters_2, n_clusters)"
"metric is the function reference, not the string key."
"assert_equal(n_clusters_1, n_clusters)"
"assert_equal(n_clusters_2, n_clusters)"
import pickle
"number of clusters, ignoring noise if present"
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
## Probably not applicable now #########################
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
""
"hdbscan documentation build configuration file, created by"
sphinx-quickstart on Sat May 28 10:34:44 2016.
""
This file is execfile()d with the current directory set to its
containing dir.
""
Note that not all possible configuration values are present in this
autogenerated file.
""
All configuration values have a default; values that are commented out
serve to show the default.
"If extensions (or modules to document with autodoc) are in another directory,"
add these directories to sys.path here. If the directory is relative to the
"documentation root, use os.path.abspath to make it absolute, like shown here."
-- General configuration ------------------------------------------------
"If your documentation needs a minimal Sphinx version, state it here."
needs_sphinx = '1.0'
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
'sphinx.ext.napoleon'
'numpy_ext.numpydoc'
napoleon_google_docstring = False
napoleon_numpy_docstring = True
"Add any paths that contain templates here, relative to this directory."
The suffix(es) of source filenames.
You can specify multiple suffix as a list of string:
"source_suffix = ['.rst', '.md']"
The encoding of source files.
source_encoding = 'utf-8-sig'
The master toctree document.
General information about the project.
"The version info for the project you're documenting, acts as replacement for"
"|version| and |release|, also used in various other places throughout the"
built documents.
""
The short X.Y version.
"The full version, including alpha/beta/rc tags."
The language for content autogenerated by Sphinx. Refer to documentation
for a list of supported languages.
""
This is also used if you do content translation via gettext catalogs.
"Usually you set ""language"" from the command line for these cases."
"There are two options for replacing |today|: either, you set today to some"
"non-false value, then it is used:"
today = ''
"Else, today_fmt is used as the format for a strftime call."
"today_fmt = '%B %d, %Y'"
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
The reST default role (used for this markup: `text`) to use for all
documents.
default_role = None
"If true, '()' will be appended to :func: etc. cross-reference text."
add_function_parentheses = True
"If true, the current module name will be prepended to all description"
unit titles (such as .. function::).
add_module_names = True
"If true, sectionauthor and moduleauthor directives will be shown in the"
output. They are ignored by default.
show_authors = False
The name of the Pygments (syntax highlighting) style to use.
A list of ignored prefixes for module index sorting.
modindex_common_prefix = []
"If true, keep warnings as ""system message"" paragraphs in the built documents."
keep_warnings = False
"If true, `todo` and `todoList` produce output, else they produce nothing."
-- Options for HTML output ----------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
html_theme = 'alabaster'
Theme options are theme-specific and customize the look and feel of a theme
"further.  For a list of options available for each theme, see the"
documentation.
html_theme_options = {}
"Add any paths that contain custom themes here, relative to this directory."
html_theme_path = []
"The name for this set of Sphinx documents.  If None, it defaults to"
"""<project> v<release> documentation""."
html_title = None
A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = None
The name of an image file (relative to this directory) to place at the top
of the sidebar.
html_logo = None
The name of an image file (within the static path) to use as favicon of the
docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
pixels large.
html_favicon = None
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Add any extra paths that contain custom files (such as robots.txt or
".htaccess) here, relative to this directory. These files are copied"
directly to the root of the documentation.
html_extra_path = []
"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
using the given strftime format.
"html_last_updated_fmt = '%b %d, %Y'"
"If true, SmartyPants will be used to convert quotes and dashes to"
typographically correct entities.
html_use_smartypants = True
"Custom sidebar templates, maps document names to template names."
html_sidebars = {}
"Additional templates that should be rendered to pages, maps page names to"
template names.
html_additional_pages = {}
"If false, no module index is generated."
html_domain_indices = True
"If false, no index is generated."
html_use_index = True
"If true, the index is split into individual pages for each letter."
html_split_index = False
"If true, links to the reST sources are added to the pages."
html_show_sourcelink = True
"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
html_show_sphinx = True
"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
html_show_copyright = True
"If true, an OpenSearch description file will be output, and all pages will"
contain a <link> tag referring to it.  The value of this option must be the
base URL from which the finished HTML is served.
html_use_opensearch = ''
"This is the file name suffix for HTML files (e.g. "".xhtml"")."
html_file_suffix = None
Language to be used for generating the HTML full-text search index.
Sphinx supports the following languages:
"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
html_search_language = 'en'
"A dictionary with options for the search language support, empty by default."
Now only 'ja' uses this config value
html_search_options = {'type': 'default'}
The name of a javascript file (relative to the configuration directory) that
"implements a search results scorer. If empty, the default will be used."
html_search_scorer = 'scorer.js'
Output file base name for HTML help builder.
-- Options for LaTeX output ---------------------------------------------
The paper size ('letterpaper' or 'a4paper').
"'papersize': 'letterpaper',"
"The font size ('10pt', '11pt' or '12pt')."
"'pointsize': '10pt',"
Additional stuff for the LaTeX preamble.
"'preamble': '',"
Latex figure (float) alignment
"'figure_align': 'htbp',"
Grouping the document tree into LaTeX files. List of tuples
"(source start file, target name, title,"
"author, documentclass [howto, manual, or own class])."
The name of an image file (relative to this directory) to place at the top of
the title page.
latex_logo = None
"For ""manual"" documents, if this is true, then toplevel headings are parts,"
not chapters.
latex_use_parts = False
"If true, show page references after internal links."
latex_show_pagerefs = False
"If true, show URL addresses after external links."
latex_show_urls = False
Documents to append as an appendix to all manuals.
latex_appendices = []
"If false, no module index is generated."
latex_domain_indices = True
-- Options for manual page output ---------------------------------------
One entry per manual page. List of tuples
"(source start file, name, description, authors, manual section)."
"If true, show URL addresses after external links."
man_show_urls = False
-- Options for Texinfo output -------------------------------------------
Grouping the document tree into Texinfo files. List of tuples
"(source start file, target name, title, author,"
"dir menu entry, description, category)"
Documents to append as an appendix to all manuals.
texinfo_appendices = []
"If false, no module index is generated."
texinfo_domain_indices = True
"How to display URL addresses: 'footnote', 'no', or 'inline'."
texinfo_show_urls = 'footnote'
"If true, do not generate a @detailmenu in the ""Top"" node's menu."
texinfo_no_detailmenu = False
-- Options for Epub output ----------------------------------------------
Bibliographic Dublin Core info.
The basename for the epub file. It defaults to the project name.
epub_basename = project
The HTML theme for the epub output. Since the default themes are not
"optimized for small screen space, using the same theme for HTML and epub"
"output is usually not wise. This defaults to 'epub', a theme designed to save"
visual space.
epub_theme = 'epub'
The language of the text. It defaults to the language option
or 'en' if the language is not set.
epub_language = ''
The scheme of the identifier. Typical schemes are ISBN or URL.
epub_scheme = ''
The unique identifier of the text. This can be a ISBN number
or the project homepage.
epub_identifier = ''
A unique identification for the text.
epub_uid = ''
A tuple containing the cover image and cover page html template filenames.
epub_cover = ()
"A sequence of (type, uri, title) tuples for the guide element of content.opf."
epub_guide = ()
HTML files that should be inserted before the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_pre_files = []
HTML files that should be inserted after the pages created by sphinx.
The format is a list of tuples containing the path and title.
epub_post_files = []
A list of files that should not be packed into the epub file.
The depth of the table of contents in toc.ncx.
epub_tocdepth = 3
Allow duplicate toc entries.
epub_tocdup = True
Choose between 'default' and 'includehidden'.
epub_tocscope = 'default'
Fix unsupported image types using the Pillow.
epub_fix_images = False
Scale large images.
epub_max_image_width = 0
"How to display URL addresses: 'footnote', 'no', or 'inline'."
epub_show_urls = 'inline'
"If false, no index is generated."
epub_use_index = True
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
Try and work around older sklearn api
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicite in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicite in mst_linkage_core_vector
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
Python 2 and 3 compliant string_type checking
TO DO: Need heuristic to decide when to go to boruvka; still debugging for now
TO DO: Need heuristic to decide when to go to boruvka; still debugging for now
Inherits from sklearn
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Finally we need the horizontal lines that occur at cluster splits.
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
Need heuristic to decide when to go to boruvka; still debugging for now
Need heuristic to decide when to go to boruvka; still debugging for now
import pickle
"number of clusters, ignoring noise if present"
"assert_equal(n_clusters_1, n_clusters)"
"assert_equal(n_clusters_2, n_clusters)"
"assert_equal(n_clusters_1, n_clusters)"
"assert_equal(n_clusters_2, n_clusters)"
"metric is the function reference, not the string key."
"assert_equal(n_clusters_1, n_clusters)"
"assert_equal(n_clusters_2, n_clusters)"
import pickle
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
## Probably not applicable now #########################
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
Try and work around older sklearn api
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicite in mst_linkage_core_cdist
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
The Cython routines used require contiguous arrays
Get distance to kth nearest neighbour
Mutual reachability distance is implicite in mst_linkage_core_cdist
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
Python 2 and 3 compliant string_type checking
TO DO: Need heuristic to decide when to go to boruvka; still debugging for now
TO DO: Need heuristic to decide when to go to boruvka; still debugging for now
Inherits from sklearn
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Finally we need the horizontal lines that occur at cluster splits.
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
Need heuristic to decide when to go to boruvka; still debugging for now
Need heuristic to decide when to go to boruvka; still debugging for now
import pickle
"number of clusters, ignoring noise if present"
"assert_equal(n_clusters_1, n_clusters)"
"assert_equal(n_clusters_2, n_clusters)"
"assert_equal(n_clusters_1, n_clusters)"
"assert_equal(n_clusters_2, n_clusters)"
"metric is the function reference, not the string key."
"assert_equal(n_clusters_1, n_clusters)"
"assert_equal(n_clusters_2, n_clusters)"
import pickle
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
## Probably not applicable now #########################
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
Try and work around older sklearn api
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicite in mst_linkage_core_cdist
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Get distance to kth nearest neighbour
Mutual reachability distance is implicite in mst_linkage_core_cdist
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
Python 2 and 3 compliant string_type checking
TO DO: Need heuristic to decide when to go to boruvka; still debugging for now
TO DO: Need heuristic to decide when to go to boruvka; still debugging for now
Inherits from sklearn
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Finally we need the horizontal lines that occur at cluster splits.
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
Need heuristic to decide when to go to boruvka; still debugging for now
Need heuristic to decide when to go to boruvka; still debugging for now
import pickle
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
## Probably not applicable now #########################
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
Try and work around older sklearn api
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicite in mst_linkage_core_cdist
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Get distance to kth nearest neighbour
Mutual reachability distance is implicite in mst_linkage_core_cdist
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
Python 2 and 3 compliant string_type checking
TO DO: Need heuristic to decide when to go to boruvka; still debugging for now
TO DO: Need heuristic to decide when to go to boruvka; still debugging for now
Inherits from sklearn
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Finally we need the horizontal lines that occur at cluster splits.
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
Need heuristic to decide when to go to boruvka; still debugging for now
Need heuristic to decide when to go to boruvka; still debugging for now
import pickle
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
## Probably not applicable now #########################
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
Try and work around older sklearn api
mst_linkage_core does not generate a full minimal spanning tree
If a tree is required then we must build the edges from the information
returned by mst_linkage_core (i.e. just the order of points to be merged)
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
TO DO: Deal with p for minkowski appropriately
Get distance to kth nearest neighbour
Mutual reachability distance is implicite in mst_linkage_core_cdist
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Get distance to kth nearest neighbour
Mutual reachability distance is implicite in mst_linkage_core_cdist
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Sort edges of the min_spanning_tree by weight
Convert edge list into standard hierarchical clustering format
Checks input and converts to an nd-array where possible
Python 2 and 3 compliant string_type checking
TO DO: Need heuristic to decide when to go to boruvka; still debugging for now
TO DO: Need heuristic to decide when to go to boruvka; still debugging for now
Inherits from sklearn
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Finally we need the horizontal lines that occur at cluster splits.
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
Need heuristic to decide when to go to boruvka; still debugging for now
Need heuristic to decide when to go to boruvka; still debugging for now
import pickle
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
## Probably not applicable now #########################
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
Need heuristic to decide when to go to boruvka; still debugging for now
Need heuristic to decide when to go to boruvka; still debugging for now
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Finally we need the horizontal lines that occur at cluster splits.
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
Need heuristic to decide when to go to boruvka; still debugging for now
Need heuristic to decide when to go to boruvka; still debugging for now
import pickle
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
## Probably not applicable now #########################
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
Need heuristic to decide when to go to boruvka; still debugging for now
Need heuristic to decide when to go to boruvka; still debugging for now
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Finally we need the horizontal lines that occur at cluster splits.
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
Need heuristic to decide when to go to boruvka; still debugging for now
Need heuristic to decide when to go to boruvka; still debugging for now
import pickle
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
## Probably not applicable now #########################
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
Need heuristic to decide when to go to boruvka; still debugging for now
Need heuristic to decide when to go to boruvka; still debugging for now
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Finally we need the horizontal lines that occur at cluster splits.
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
Need heuristic to decide when to go to boruvka; still debugging for now
Need heuristic to decide when to go to boruvka; still debugging for now
import pickle
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
## Probably not applicable now #########################
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
Need heuristic to decide when to go to boruvka; still debugging for now
Need heuristic to decide when to go to boruvka; still debugging for now
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Finally we need the horizontal lines that occur at cluster splits.
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
Need heuristic to decide when to go to boruvka; still debugging for now
Need heuristic to decide when to go to boruvka; still debugging for now
import pickle
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
## Probably not applicable now #########################
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"kdtree_pdist_mutual_reachability,"
"balltree_pdist_mutual_reachability,"
"kdtree_mutual_reachability,"
balltree_mutual_reachability
Need heuristic to decide when to go to boruvka; still debugging for now
Need heuristic to decide when to go to boruvka; still debugging for now
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Finally we need the horizontal lines that occur at cluster splits.
If the cluster was formed prior to the cut and is large enough
If the cluster had not been merged before the cut
Generate labels for each data point
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
Need heuristic to decide when to go to boruvka; still debugging for now
Need heuristic to decide when to go to boruvka; still debugging for now
import pickle
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
## Probably not applicable now #########################
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"kdtree_pdist_mutual_reachability,"
"balltree_pdist_mutual_reachability,"
"kdtree_mutual_reachability,"
balltree_mutual_reachability
Need heuristic to decide when to go to boruvka; still debugging for now
Need heuristic to decide when to go to boruvka; still debugging for now
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Finally we need the horizontal lines that occur at cluster splits.
If the cluster was formed prior to the cut and is large enough
If the cluster had not been merged before the cut
Generate labels for each data point
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
Need heuristic to decide when to go to boruvka; still debugging for now
Need heuristic to decide when to go to boruvka; still debugging for now
import pickle
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
## Probably not applicable now #########################
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
"kdtree_pdist_mutual_reachability,"
"balltree_pdist_mutual_reachability,"
"kdtree_mutual_reachability,"
balltree_mutual_reachability
Need heuristic to decide when to go to boruvka; still debugging for now
Need heuristic to decide when to go to boruvka; still debugging for now
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Finally we need the horizontal lines that occur at cluster splits.
If the cluster was formed prior to the cut and is large enough
If the cluster had not been merged before the cut
Generate labels for each data point
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
Need heuristic to decide when to go to boruvka; still debugging for now
Need heuristic to decide when to go to boruvka; still debugging for now
import pickle
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
## Probably not applicable now #########################
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Finally we need the horizontal lines that occur at cluster splits.
If the cluster was formed prior to the cut and is large enough
If the cluster had not been merged before the cut
Generate labels for each data point
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
import pickle
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
## Probably not applicable now #########################
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
Steve Astels <sastels@gmail.com>
John Healy <jchealy@gmail.com>
""
License: BSD 3 clause
-*- coding: utf-8 -*-
Author: Leland McInnes <leland.mcinnes@gmail.com>
""
License: BSD 3 clause
We want to get the x and y coordinates for the start of each cluster
"Initialize the leaves, since we know where they go, the iterate"
"through everything from the leaves back, setting coords as we go"
"We use bars to plot the 'icicles', so we need to generate centers, tops,"
bottoms and widths for each rectangle. We can go through each cluster
and do this for each in turn.
Finally we need the horizontal lines that occur at cluster splits.
Get a 2D projection; if we have a lot of dimensions use PCA first
Use PCA to get down to 32 dimension
import pickle
"number of clusters, ignoring noise if present"
"metric is the function reference, not the string key."
## Probably not applicable now #########################
def test_dbscan_sparse():
def test_dbscan_balltree():
def test_pickle():
def test_dbscan_core_samples_toy():
def test_boundaries():
Generate datasets. We choose the size big enough to see the scalability
"of the algorithms, but not too big to avoid too long running times"
normalize dataset for easier parameter selection
estimate bandwidth for mean shift
connectivity matrix for structured Ward
make connectivity symmetric
create clustering estimators
predict cluster memberships
plot
-*- coding: utf-8 -*-
#############################################################################
Generate sample data
#############################################################################
Compute DBSCAN
"Number of clusters in labels, ignoring noise if present."
#############################################################################
Plot result
Black removed and is used for noise instead.
Black used for noise.
Black used for noise.
