Commit Message
This will read version from pyproject.toml
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Making the DetectionImageFolder class available for import from this module
Define the allowed image extensions
Get image filename and path
Load and convert image to RGB
Apply transformation if specified
TODO: Under development for efficiency improvement
Only run recognition on animal detections
Get image path and corresponding bbox xyxy for cropping
Load and crop image with supervision
Apply transformation if specified
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Making the provided classes available for import from this module
Convert PIL Image to Torch Tensor
Original shape
New shape
Scale ratio (new / old) and compute padding
Resize image
Pad image
Convert the image to a PyTorch tensor and normalize it
Resize and pad the image using a customized letterbox function.
Normalization constants
Define the sequence of transformations
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
!!! Output paths need to be optimized !!!
Category filtering
if not all([x in exclude_category_ids for x in category]):
Category filtering
if not all([x in exclude_category_ids for x in category]):
if not all([x in exclude_category_ids for x in category_id_list]):
Find classifications for this detection
Load JSON data from the file
Ensure the destination directories exist
Process each image detection
Check if there is any category '0' with confidence above the threshold
Construct the source and destination file paths
Copy the file to the appropriate directory
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Importing basic libraries
Placeholder class-level attributes to be defined in derived classes
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Importing basic libraries
self.predictor.args.device = device # Will uncomment later
Creating a DataLoader for batching and parallel processing of the images
Normalize the coordinates for timelapse compatibility
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Importing basic libraries
Creating a DataLoader for batching and parallel processing of the images
Normalize the coordinates for timelapse compatibility
backbone
"self.cls_dla_up = dla_modules.DLAUp(channels[-3:], scales=scales[:3])"
bottleneck conv
localization head
classification head
decode_cls = self.cls_dla_up(encode[-3:])
clsmap = self.cls_head(decode_cls)
Assert that the dataset is either 'general' or 'ennedi'
"checkpoint = load_state_dict_from_url(url, map_location=torch.device(self.device)) # NOTE: This function is not used in the current implementation"
Load the class names and other metadata from the checkpoint
Load the model architecture
Load checkpoint into model
Remove 'model.' prefix from the state_dict keys if the key starts with 'model.'
Load the new state_dict
Creating a Dataloader for batching and parallel processing of the images
Flatten the lists since we know its a single image
Calculate the total number of detections
Pre-allocate based on total possible detections
Loop through each species
Get the detections for this species
Apply the confidence threshold
Fill the preds_array with the valid detections
Call the forward method of the model in evaluation mode
x = self.fc(x)
y = self.softmax(self.up(x))
patches' height & width
unfold on height
if non-perfect division on height
get the residual patch and add it to the fold
unfold on width
"if non-perfect division on width, the same"
reshaping
patches' height & width
lists of pixels numbers
cut into patches to get limits
if non-perfect division on height
if non-perfect division on width
@property
def area(self) -> int:
''' To get area '''
return 1 # always 1 pixel
local maxima
adaptive threshold for counting
negative sample
count
locations and scores
upsample class map
softmax
cat to heatmap
LMDS
step 1 - get patches and limits
step 2 - inference to get maps
step 3 - patch the maps into initial coordinates system
(step 4 - upsample)
outputs = self.model(patch)[0]
cat
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Image size for the Opossum classifier
Class names for prediction
"If pretrained, use the provided URL to fetch the weights"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Making the PlainResNetInference class available for import from this module
Following the ResNet structure to extract features
Initialize the network and weights
... [Missing weight URL definition for ResNet18]
... [Missing weight URL definition for ResNet50]
Print missing and unused keys for debugging purposes
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Image size for the classifier
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Image size for the Opossum classifier
Class names for prediction
"If pretrained, use the provided URL to fetch the weights"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Image size for the Opossum classifier
Class names for prediction
"If pretrained, use the provided URL to fetch the weights"
Load and set configurations from the YAML file
Configuration file for the Sphinx documentation builder.
""
"For the full list of built-in configuration values, see the documentation:"
https://www.sphinx-doc.org/en/master/usage/configuration.html
-- Project information -----------------------------------------------------
https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information
-- General configuration ---------------------------------------------------
https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration
-- Options for HTML output -------------------------------------------------
https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output
-- Options for todo extension ----------------------------------------------
https://www.sphinx-doc.org/en/master/usage/extensions/todo.html#configuration
"Determine the factor, which shifts the decimal point of x"
just behind the last significant digit
"Shift decimal point by multiplicatipon with factor, flooring, and"
division by factor
_ = pretty_print_object(obj)
Sloppy that I'm making a module-wide change here...
Determine the coordinates of the intersection rectangle
The intersection of two axis-aligned bounding boxes is always an
axis-aligned bounding box
Compute the area of both AABBs
Compute the intersection over union by taking the intersection
area and dividing it by the sum of prediction + ground-truth
areas - the intersection area.
%% Functions for running commands as subprocesses
%%
%% Test driver for execute_and_print
%% Parallel test driver for execute_command_and_print
Should we use threads (vs. processes) for parallelization?
"Only relevant if n_workers == 1, i.e. if we're not parallelizing"
%% Constants and imports
%% Classes
Collect all names
Make names unique and sort
cast location to string as the entries in locations are strings
Convert classnames to lowercase to simplify comparisons later
Normalize paths to simplify comparisons later
"Make custom replacements in filenames, typically used to"
accommodate changes in root paths after DB construction
## Build useful mappings to facilitate working with the DB
Category ID <--> name
Image filename --> ID
Image ID --> image object
Image ID --> annotations
"Each image can potentially multiple annotations, hence using lists"
...__init__
...class IndexedJsonDb
%% Functions
Find all unique locations
i_location = 0; location = locations[i_location]
Sorting datetimes fails when there are None's in the list.  So instead of sorting datetimes
"directly, sort tuples with a boolean for none-ness, then the datetime itself."
""
https://stackoverflow.com/questions/18411560/sort-list-while-pushing-none-values-to-the-end
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_location[1]
"Start a new sequence if necessary, including the case where this datetime is invalid"
"If this was an invalid datetime, this will record the previous datetime"
"as None, which will force the next image to start a new sequence."
...for each image in this location
Fill in seq_num_frames
...for each location
...create_sequences()
""
cct_to_md.py
""
"""Converts"" a COCO Camera Traps file to a MD results file.  Currently ignores"
"non-bounding-box annotations, and gives all annotations a confidence of 1.0."
""
The only reason to do this is if you are going to add information to an existing
"CCT-formatted dataset, and want to do that in Timelapse."
""
"Currently assumes that width and height are present in the input data, does not"
read them from images.
""
%% Constants and imports
%% Functions
# Validate input
# Read input
# Prepare metadata
ann = d['annotations'][0]
# Process images
im = d['images'][0]
"MegaDetector: [x,y,width,height] (normalized, origin upper-left)"
"CCT: [x,y,width,height] (absolute, origin upper-left)"
...if there's a bounding box
...for each annotation
This field is no longer included in MD output files by default
im_out['max_detection_conf'] = max_detection_conf
...for each image
# Write output
...cct_to_md()
%% Command-line driver
TODO
%% Interactive driver
%%
%%
""
cct_json_to_filename_json.py
""
"Given a .json file in COCO Camera Traps format, outputs a .json-formatted list of"
relative file names present in the CCT file.
""
%% Constants and environment
%% Main function
"json.dump(s,open(outputFilename,'w'))"
%% Command-line driver
%% Interactive driver
%%
""
cct_to_csv.py
""
"""Converts"" a COCO Camera Traps .json file to .csv, in quotes because"
"all kinds of assumptions are made here, and if you have a particular .csv"
"format in mind, YMMV.  Most notably, does not include any bounding box information"
or any non-standard fields that may be present in the .json file.  Does not
propagate information about sequence-level vs. image-level annotations.
""
"Does not assume access to the images, therefore does not open .jpg files to find"
"datetime information if it's not in the metadata, just writes datetime as 'unknown'."
""
%% Imports
%% Main function
#%% Read input
#%% Build internal mappings
annotation = annotations[0]
#%% Write output file
im = images[0]
Write out one line per class:
...for each class name
...for each image
...with open(output_file)
...def cct_to_csv
%% Interactive driver
%%
%% Command-line driver
""
remove_exif.py
""
"Removes all EXIF/IPTC/XMP metadata from a folder of images, without making"
"backup copies, using pyexiv2."
""
%% Imports and constants
%% List files
%% Remove EXIF data (support)
PYEXIV2 IS NOT THREAD SAFE; DO NOT CALL THIS IN PARALLEL FROM A SINGLE PROCESS
data = img.read_exif(); print(data)
%% Debug
%%
%%
%% Remove EXIF data (execution)
fn = image_files[0]
"joblib.Parallel defaults to a process-based backend, but let's be sure"
"results = Parallel(n_jobs=n_exif_threads,verbose=2,prefer='processes')(delayed(remove_exif)(fn) for fn in image_files[0:10])"
""
yolo_to_coco.py
""
Converts a YOLO-formatted dataset to a COCO-formatted dataset.
""
"Currently supports only a single folder (i.e., no recursion).  Treats images without"
corresponding .txt files as empty.
""
%% Imports and constants
from ai4eutils
%% Support functions
Validate input
Class names
Blank lines should only appear at the end
Enumerate images
fn = image_files[0]
Create the image object for this image
Is there an annotation file for this image?
"This is an image with no annotations, currently don't do anything special"
here
s = lines[0]
"COCO: [x_min, y_min, width, height] in absolute coordinates"
"YOLO: [class, x_center, y_center, width, height] in normalized coordinates"
...for each annotation
...if this image has annotations
...for each image
...def yolo_to_coco()
%% Interactive driver
%% Convert YOLO folders to COCO
%% Check DB integrity
%% Preview some images
%% Command-line driver
TODO
""
read_exif.py
""
"Given a folder of images, read relevant metadata (EXIF/IPTC/XMP) fields from all images,"
and write them to  a .json or .csv file.
""
This module can use either PIL (which can only reliably read EXIF data) or exiftool (which
can read everything).  The latter approach expects that exiftool is available on the system
path.  No attempt is made to be consistent in format across the two approaches.
""
%% Imports and constants
From ai4eutils
%% Options
Number of concurrent workers
Should we use threads (vs. processes) for parallelization?
""
Not relevant if n_workers is 1.
Should we use exiftool or pil?
%% Functions
exif_tags = img.info['exif'] if ('exif' in img.info) else None
print('Warning: unrecognized EXIF tag: {}'.format(k))
"-G means ""Print group name for each tag"", e.g. print:"
""
[File]          Bits Per Sample                 : 8
""
...instead of:
""
Bits Per Sample                 : 8
A list of three-element lists (type/tag/value)
line_raw = exif_lines[0]
A typical line:
""
[ExifTool]      ExifTool Version Number         : 12.13
"Split on the first occurrence of "":"""
...for each output line
...which processing library are we using?
...read_exif_tags_for_image()
...populate_exif_data()
Enumerate *relative* paths
Find all EXIF tags that exist in any image
...for each tag in this image
...for each image
Write header
...for each key that *might* be present in this image
...for each image
...with open()
...if we're writing to .json/.csv
https://stackoverflow.com/questions/11210104/check-if-a-program-exists-from-a-python-script
%% Interactive driver
%%
output_file = os.path.expanduser('~/data/test-exif.csv')
options.processing_library = 'pil'
"file_path = os.path.join(input_folder,'KRU_S1_11_R1_IMAG0148.JPG')"
%%
%% Command-line driver
""
"Given a json-formatted list of image filenames, retrieve the width and height of every image."
""
%% Constants and imports
%% Processing functions
Is this image on disk?
"all_results = list(tqdm(pool.imap(process_image, filenames), total=len(filenames)))"
%% Interactive driver
%%
List images in a test folder
%%
"process_list_file(image_list_file,image_size_file,image_prefix=base_dir)"
%% Command-line driver
"python sanity_check_json_db.py ""e:\wildlife_data\wellington_data\wellington_camera_traps.json"" --baseDir ""e:\wildlife_data\wellington_data\images"" --bFindUnusedImages --bCheckImageSizes"
"python sanity_check_json_db.py ""D:/wildlife_data/mcgill_test/mcgill_test.json"" --baseDir ""D:/wildlife_data/mcgill_test"" --bFindUnusedImages --bCheckImageSizes"
"Here the '-u' prevents buffering, which makes tee happier"
""
python -u sanity_check_json_db.py '/datadrive1/nacti_metadata.json' --baseDir '/datadrive1/nactiUnzip/' --bFindUnusedImages --bCheckImageSizes | tee ~/nactiTest.out
""
Partially-formed stub to get from MegaDetector output files to COCO Camera Traps data.
""
"Was actually written to convert *many* MD .json files to a single CCT file, hence"
the loop over .json files.
""
"THIS CODE HAS NEVER BEEN RUN, it was added as a demonstration of how to do this.  YMMV."
""
"You may find a more polished, command-line-ready version of this code at:"
""
https://github.com/StewartWILDlab/mdtools
""
%% Constants and imports
"Images sizes are required to convert between absolute and relative coordinates,"
so we need to read the images.
Only required if you want to write a database preview
%% Create CCT dictionaries
image_ids_to_images = {}
Force the empty category to be ID 0
Load .json annotations for this data set
i_entry = 0; entry = data['images'][i_entry]
""
"PERF: Not exactly trivially parallelizable, but about 100% of the"
time here is spent reading image sizes (which we need to do to get from
"absolute to relative coordinates), so worth parallelizing."
Generate a unique ID from the path
detection = detections[0]
Have we seen this category before?
Create an annotation
"MegaDetector: [x,y,width,height] (normalized, origin upper-left)"
"CCT: [x,y,width,height] (absolute, origin upper-left)"
...for each detection
...for each image
Remove non-reviewed images and associated annotations
%% Create info struct
%% Write .json output
%% Clean start
## Everything after this should work from a clean start ###
%% Validate output
%% Preview animal labels
%% Preview empty labels
"viz_options.classes_to_exclude = ['empty','human']"
""
generate_crops_from_cct.py
""
"Given a .json file in COCO Camera Traps format, create a cropped image for"
each bounding box.
""
%% Imports and constants
%% Functions
# Read and validate input
# Find annotations for each image
"This actually maps image IDs to annotations, but only to annotations"
containing boxes
# Generate crops
TODO: parallelize this loop
im = d['images'][0]
Load the image
Generate crops
i_ann = 0; ann = annotations_this_image[i_ann]
"x/y/w/h, origin at the upper-left"
...for each box
...for each image
...generate_crops_from_cct()
%% Interactive driver
%%
%%
%%
%% Command-line driver
TODO
%% Scrap
%%
""
coco_to_yolo.py
""
Converts a COCO-formatted dataset to a YOLO-formatted dataset.
""
"If the input and output folders are the same, writes .txt files to the input folder,"
and neither moves nor modifies images.
""
"Currently ignores segmentation masks, and errors if an annotation has a"
segmentation polygon but no bbox
""
Has only been tested on a handful of COCO Camera Traps data sets; if you
"use it for more general COCO conversion, YMMV."
""
%% Imports and constants
%% Support functions
Validate input
Read input data
Parse annotations
i_ann = 0; ann = data['annotations'][0]
Make sure no annotations have *only* segmentation data
Re-map class IDs to make sure they run from 0...n-classes-1
""
"TODO: this allows unused categories in the output data set, which I *think* is OK,"
but I'm only 81% sure.
Process images (everything but I/O)
"List of dictionaries with keys 'source_image','dest_image','bboxes','dest_txt'"
i_image = 0; im = data['images'][i_image]
"assert os.path.isfile(source_image), 'Could not find image {}'.format(source_image)"
If this annotation has no bounding boxes...
"This is not entirely clear from the COCO spec, but it seems to be consensus"
"that if you want to specify an image with no objects, you don't include any"
annotations for that image.
We allow empty bbox lists in COCO camera traps; this is typically a negative
"example in a dataset that has bounding boxes, and 0 is typically the empty"
category.
...if this is an empty annotation
"COCO: [x_min, y_min, width, height] in absolute coordinates"
"YOLO: [class, x_center, y_center, width, height] in normalized coordinates"
Convert from COCO coordinates to YOLO coordinates
...for each annotation
...if this image has annotations
...for each image
Write output
Category IDs should range from 0..N-1
TODO: parallelize this loop
""
output_info = images_to_copy[0]
Only write an annotation file if there are bounding boxes.  Images with
"no .txt files are treated as hard negatives, at least by YOLOv5:"
""
https://github.com/ultralytics/yolov5/issues/3218
""
"I think this is also true for images with empty annotation files, but"
"I'm using the convention suggested on that issue, i.e. hard negatives"
are expressed as images without .txt files.
bbox = bboxes[0]
...for each image
...def coco_to_yolo()
%% Interactive driver
%% CCT data
"If preview_export is True, I'm exporting to preview these with BoundingBoxEditor:"
""
https://github.com/mfl28/BoundingBoxEditor
""
"This export will be compatible, other than the fact that you need to move"
"""object.data"" into the ""labels"" folder."
""
"Otherwise I'm exporting for training, in the YOLOv5 flat format."
%% Command-line driver
TODO
""
cct_to_wi.py
""
Converts COCO Camera Traps .json files to the Wildlife Insights
batch upload format
""
Also see:
""
https://github.com/ConservationInternational/Wildlife-Insights----Data-Migration
""
https://data.naturalsciences.org/wildlife-insights/taxonomy/search
""
%% Imports
%% Paths
A COCO camera traps file with information about this dataset
A .json dictionary mapping common names in this dataset to dictionaries with the
"WI taxonomy fields: common_name, wi_taxon_id, class, orer, family, genus, species"
%% Constants
%% Project information
%% Read templates
%% Compare dictionary to template lists
Write the header
Write values
%% Project file
%% Camera file
%% Deployment file
%% Images file
Read .json file with image information
Read taxonomy dictionary
Populate output information
df = pd.DataFrame(columns = images_fields)
annotation = annotations[0]
im = input_data['images'][0]
"We don't have counts, but we can differentiate between zero and 1"
This is the label mapping used for our incoming iMerit annotations
"Only used to parse the incoming annotations. In our database, the string name is used to avoid confusion"
MegaDetector outputs
""
add_bounding_boxes_to_megadb.py
""
"Given COCO-formatted JSONs containing manually labeled bounding box annotations, add them to"
"MegaDB sequence entries, which can then be ingested into MegaDB."
the category map that comes in the COCO JSONs for iMerit batch 12 - to check that each
JSON
"dataset name : (seq_id, frame_num) : [bbox, bbox]"
where bbox is a dict with str 'category' and list 'bbox'
iterate over image_id_to_image rather than image_id_to_annotations so we include
the confirmed empty images
The file_name field in the incoming json looks like
alka_squirrels.seq2020_05_07_25C.frame119221.jpg
there seems to be a bug in the annotations where sometimes there's a
non-empty label along with a label of category_id 5
ignore the empty label (they seem to be actually non-empty)
""
add_bounding_boxes_to_json.py
""
This script takes a image database in the COCO Camera Traps format and merges in a set of bounding
box annotations in the format that iMerit uses (a .json where actually only each row is a valid json).
""
"If you need to update an existing bbox database, please get all the original annotation files and"
re-generate from scratch
""
%% Imports
%% Configurations and paths
images database
output bboxes database
annotation files (pseudo json) obtained from our annotation vendor that contain annotations for this dataset
None or a string or tuple of strings that is the prefix to all file_name of interest / in this dataset in the annotation files
functions for mapping the image_id in the annotation files (pseudo jsons) to the image_id used in the image DB
our img_id doesn't contain frame info
"batch3 - ""file_name"":""ASG0000019_0_S1_B06_R1_PICT0007.JPG"""
"batch5 and 7 - ""file_name"":""datasetsnapshotserengeti.seqASG000002m-frame0.imgS1_B06_R1_PICT0056.JPG"""
sometimes - 'datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG'
"id in DB (old_token): 'S6/J01/J01_R1/S6_J01_R1_IMAG0001', 'S1/B05/B05_R1/S1_B05_R1_PICT0036'"
specify which one to use for your dataset here
%% Load the image database and fill in DB info for the output bbox database
load the images database
%% Find the height and width of images from the annotation files
""
...if they are not available in the images DB
each row in this pseudo-json is a COCO formatted entry for an image sequence
%% Other functions required by specific datasets
the IDFG image database does not include images from unlabeled folders that were annotated with bounding boxes
%% Create the bbox database from all annotation files pertaining to this dataset
"for the incoming annotations, look up by category name (common) and convert to the numerical id used in our databases"
"for each annotation pseudo-json, check that the image it refers to exists in the original database"
each row in this pseudo-json is a COCO formatted entry for an image sequence
check that entry is for this dataset
category map for this entry in the annotation file - usually the same across all entries but just in case
rspb_add_image_entry(img_id)
use the image length and width in the image DB
"[top left x, top left y, width, height] in relative coordinates"
"add all images that have been sent to annotation, some of which may be empty of bounding boxes"
rspb_add_image_entry(db_image_id)
%% Common queries
This query is used when preparing tfrecords for object detector training.
We do not want to get the whole seq obj where at least one image has bbox because
some images in that sequence will not be bbox labeled so will be confusing.
Include images with bbox length 0 - these are confirmed empty by bbox annotators.
"If frame_num is not available, it will not be a field in the result iterable."
"Note that the seq_id is the Cosmos DB assigned ID for that sequence, not the"
"seq_id field, which may contain ""/"" characters."
"For public datasets to be converted to the CCT format, we get the whole seq object because"
sequence level attributes need to be included too. megadb/converters/megadb_to_cct.py handles
the case of bbox-only JSONs with the flag exclude_species_class
Getting all sequences in a dataset - for updating or deleting entries which need the id field
%% Parameters
Use None if querying across all partitions
"The `sequences` table has the `dataset` as the partition key, so if only querying"
"entries from one dataset, set the dataset name here."
"e.g. {'name': '@top_n', 'value': 100} - see query_and_upsert_examples/query_for_data.ipynb"
Use False if do not want all results stored in a single JSON.
%% Script
execute the query
loop through and save the results
MODIFY HERE depending on the query
wiitigers Unicode issue - no good mapping from DB file names to file names in blob URL
build filename
if need to re-download a dataset's images in case of corruption
entries_to_download = {
"filename: entry for filename, entry in entries_to_download.items()"
if entry['dataset'] == DATASET
}
input validation
"existing files, with paths relative to <store_dir>"
parse JSON or TXT file
"create a new storage container client for this dataset,"
and cache it
schema already checks that the min possible value of frame_num is 0
"if there are more than one image item, each needs a frame_num"
checks across all sequence items
per sequence item checks
load the schema
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory
load the schema
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory
checks across all datasets items
check for expiry date of the SAS keys
update the sequences
"`id` is from the database, as well as all attributes starting with _"
"if valuable sequence information is available, add them to the image"
required fields for an image object
add seq-level class labels for this image
add other sequence-level properties to each image too
add other image-level properties
... for im in seq['images']
... for seq in mega_db
consolidate categories
some property names have changed in the new schema
a dummy sequence ID will be generated if the image entry does not have a seq_id field
seq_id only needs to be unique within this dataset; MegaDB does not rely on it as the _id field
"""annotations"" fields are opened and have its sub-field surfaced one level up"
set the `dataset` property on each sequence to the provided dataset_name
check that the location field is the same for all images in a sequence
check which fields in a CCT image entry are sequence-level
image-level properties that really should be sequence-level
need to add (misidentified) seq properties not present for each image in a sequence to img_level_properties
"(some properties act like flags - all have the same value, but not present on each img)"
add the sequence-level properties to the sequence objects
not every sequence have to have all the seq_level_properties
get the value of this sequence-level property from the first image entry
check which fields are really dataset-level and should be included in the dataset table instead.
delete sequence-level properties that should be dataset-level
make all `class` fields lower-case; cast `seq_id` to type string in case they're integers
turn all float NaN values into None so it gets converted to null when serialized
this was an issue in the Snapshot Safari datasets
%% validation
"at first a dict of image_id: image_obj with annotations embedded, then it becomes"
an array of image objects
%% integrate the image DB
takes in image entries and species and other annotations in the image DB
convert the species category to explicit string name
there may be other fields in the annotation object
these fields should already be gotten from the image object
%% integrate the bbox DB
add any images that are not in the image DB
also add any fields in the image object that are not present already
add bbox to the annotations field
for any newly added images
"'bbox_abs': bbox_anno['bbox'],"
not keeping height and width
""
jb_csv_to_json.py
""
Convert a particular .csv file to CCT format.  Images were not available at
"the time I wrote this script, so this is much shorter than other scripts"
in this folder.
""
%% Constants and environment
%% Read source data
%% Confirm filename uniqueness (this data set has one label per image)
"Build up a map from filenames to a list of rows, checking image existence as we go"
%% Create CCT dictionaries
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
imageName = imageFilenames[0]
"As per above, this is convenient and appears to be true; asserting to be safe"
"Filenames look like ""290716114012001a1116.jpg"""
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
%% Create info struct
%% Write output
%% Sanity-check
%% Imports and constants
%% Load data
"with open(image_json,'r') as f:"
data = json.load(f)
%% Sanity-check data
%% Label previews
%% Collect images to annotate
%% Sort by sequence and frame
%% Copy to a folder by GUID
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension
im = images_to_annotate[0]
...for each image
%% Write out the annotation list
%% Constants and imports
%% Enumerate files
edited_image_folder = edited_image_folders[0]
fn = edited_image_files[0]
%% Read metadata and capture location information
i_row = 0; row = df.iloc[i_row]
Sometimes '2017' was just '17' in the date column
%% Read the .json files and build output dictionaries
json_fn = json_files[0]
if 'partial' in json_fn:
continue
line = lines[0]
if image_fn == 'SD1_238_6_26_17_16_76.73.jpg':
asdfad
SD29_079_5_14_2018_17_52.85.jpg
Re-write two-digit years as four-digit years
Sometimes the year was written with two digits instead of 4
assert len(tokens[4]) == 4 and tokens[4].startswith('20')
Have we seen this location already?
"Not a typo, it's actually ""formateddata"""
An image shouldn't be annotated as both empty and non-empty
An image shouldn't be annotated as both empty and non-empty
box = formatteddata[0]
"open_file(os.path.join(base_folder,jpeg_image_folder,image_fn))"
...for each box
...if there are boxes on this image
...for each line
...with open()
...for each json file
%% Prepare the output .json
%% Check DB integrity
%% Print unique locations
SD12_202_6_23_2017_1_31.85.jpg
%% Preview some images
%% Statistics
""
"Import a Snapshot Safari project (one project, one season)"
""
Before running this script:
""
"* Mount the blob container where the images live, or copy the"
images to local storage
""
What this script does:
""
* Creates a .json file
* Creates zip archives of the season without humans.
* Copies animals and humans to separate folders
""
After running this script:
""
* Create or update LILA page
* Push zipfile and unzipped images to LILA
* Push unzipped humans to wildlifeblobssc
* Delete images from UMN uplaod storage
""
"Snapshot Serengeti is handled specially, because we're dealing with bounding"
boxes too.  See snapshot_serengeti_lila.py.
""
%% Imports
From ai4eutils
From CameraTraps
%% Constants
project_name = 'XXX'; season_name = 'S1'; project_friendly_name = 'Snapshot Unknown'
project_name = 'SER'; season_name = 'S1-11'; project_friendly_name = 'Snapshot Serengeti'
project_name = 'KRU'; season_name = 'S1'; project_friendly_name = 'Snapshot Kruger'
project_name = 'CDB'; season_name = 'S1'; project_friendly_name = 'Snapshot Camdeboo'
project_name = 'MTZ'; season_name = 'S1'; project_friendly_name = 'Snapshot Mountain Zebra'
project_name = 'ENO'; season_name = 'S1'; project_friendly_name = 'Snapshot Enonkishu'
project_name = 'KAR'; season_name = 'S1'; project_friendly_name = 'Snapshot Karoo'
project_name = 'KGA'; season_name = 'S1'; project_friendly_name = 'Snapshot Kgalagadi'
%% Folder/file creation
E.g. KRU_S1
E.g. Z:\KRU
E.g. Z:\KRU\KRU_S1
Contains annotations for each capture event (sequence)
Maps image IDs to filenames; each line looks like:
""
"KRU_S1#1#1#2,3,KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0004.JPG"
"Total number of each answer to each question, e.g. total number of times each species was identified"
""
Not used here
Create output folders
Images will be placed in a season-specific folder inside this (the source data includes
this in path names)
%% Load metadata files
%% Convert to dictionaries (prep)
%% Convert to dictionaries (loops)
"TODO: iterrows() is a terrible way to do this, but this is one of those days"
"where I want to get this done, not get better at Python."
irow = 0; row = image_table.iloc[0]
"Loaded as an int64, converting to int here"
...for each row in the image table
Make sure image IDs are what we think they are
...for each row in the annotation table
%% Take a look at categories (just sanity-checking)
print('\nCategories by species:')
pp.pprint(categories_by_species)
%% Fill in some image fields we didn't have when we created the image table
"width, height, corrupt, seq_num_frames, location, datetime"
Every annotation in this list should have the same sequence ID
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;"
there's no reason to do this redundantly for every image
Every image in this sequence should point back to the same equence
Every annotation in this list should have the same location
Every annotation in this list should have the same datetime
Is this image on disk?
iImage = 0; im = images[0]
...for each image
"images_processed = pool.map(process_image, images)"
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))"
%% Count missing/corrupted images
%% Print distribution of sequence lengths (sanity-check)
%% Replicate annotations across images
iAnn = 0; ann = annotations[iAnn]
%% See what files are on disk but not annotated
%% Sanity-check image and annotation uniqueness
%% Minor updates to fields
%% Write .json file
%% Create a list of human files
ann = annotations[0]
%% Create public archive and public/private folders
im = images[0]
E.g. KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0001.JPG
Copy to private output folder
Add to zipfile
Possibly start a new archive
Copy to public output folder
...for each image
%% Sanity-check .json file
"This will produce some validation errors, because this zipfile doesn't include humans"
%% Zip up .json and .csv files
%% When I skip to this part (using a pre-rendered .json file)
%%
%%
ann = annotations[0]
%% Summary prep for LILA
"%% Generate preview, sanity-check labels"
"viz_options.classes_to_include = ['jackalblackbacked','bustardkori']"
%% Scrap
%% Find annotations for a particular image
%% Write a list of missing images
""
save_the_elephants_survey_A.py
""
Convert the .csv file provided for the Save the Elephants Survey A data set to a
COCO-camera-traps .json file
""
%% Constants and environment
Handle all unstructured fields in the source data as extra fields in the annotations
"photo_type really should be an image property, but there are a few conflicts"
that forced me to handle it as an annotation proprerty
%% Read source data
"%% Map filenames to rows, verify image existence"
Maps relative paths to row indices in input_metadata
"Build up a map from filenames to a list of rows, checking image existence as we go"
Ignore directories
%% Make sure the multiple-annotation cases make sense
%%
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
Force the empty category to be ID 0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image]
Example filename:
""
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG'
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG'
i_row = row_indices[0]
There are a small handful of datetime mismatches across annotations
for the same image
assert im['datetime'] == timestamp
Special cases based on the 'photo type' field
Various spellings of 'community'
Have we seen this category before?
Create an annotation
fieldname = list(mapped_fields.keys())[0]
...for each row
...for each image
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
%% Scrap
%% Find unique photo types
##
""
timelapse_csv_set_to_json.py
""
Given a directory full of reasonably-consistent Timelapse-exported
".csvs, assemble a CCT .json."
""
"Assumes that you have a list of all files in the directory tree, including"
image and .csv files.
""
##
%% Constants and imports
Text file with relative paths to all files (images and .csv files)
"%% Read file list, make a list of all image files and all .csv files"
"%% Verify column consistency, create a giant array with all rows from all .csv files"
i_csv = 0; csv_filename = csv_files[0]
%% Prepare some data structures we'll need for mapping image rows in .csv files to actual image files
Enumerate all folders containing image files
"In this data set, a site folder looks like:"
""
Processed Images\\site_name
%% Map .csv files to candidate camera folders
fn = valid_csv_files[0]
"Some site folders appear as ""XXNNNN"", some appear as ""XXNNNN_complete"""
...for each .csv file
%% Map camera folders to candidate image folders
%% Main loop over labels (prep)
Force the empty category to be ID 0
Images that are marked empty and also have a species label
%% Main loop over labels (loop)
i_row = 0; row = input_metadata.iloc[i_row]
"for i_row,row in input_metadata.iterrows():"
"Usually this is just a single folder name, sometimes it's a full path,"
which we don't want
Check whether this file exists on disk
Retrieve image width and height
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each row in the big table of concatenated .csv files
%% Fix cases where an image was annotated as 'unlabeled' and as something else
This annotation is 'unlabeled'
Was there another category associated with this image?
%% Check for un-annnotated images
Enumerate all images
list(relative_path_to_image.keys())[0]
%% Create info struct
%% Write output
%% Sanity-check the database's integrity
%% Render a bunch of images to make sure the labels got carried along correctly
""
auckland_doc_to_json.py
""
Convert Auckland DOC data set to COCO camera traps format.  This was
for a training data set where class names were encoded in path names.
""
%% Constants and imports
Filenames will be stored in the output .json relative to this base dir
%% Enumerate files
%% Assemble dictionaries
Force the empty category to be ID 0
fn = image_files[0]; print(fn)
Typically y:\Maukahuka_Auckland_Island\1_Training\Winter_Trial_2019\cat\cat\eat\20190903_IDdY_34_E3_tmp_201908240051.JPG
"This data set has two top-level folders, ""1_Training"" (which has class names encoded"
"in paths) and ""2_Testing"" (which has no class information)."
...for each image
%% Write output .json
%% Write train/test .jsons
%% Validate .json files
%% Preview labels
checkpoint
""
awc_to_json.py
""
Convert a particular .csv file to CCT format.
""
%% Constants and environment
%% Read source data
%% Main loop over labels
Force the empty category to be ID 0
iRow = 0; row = input_metadata.iloc[iRow]
"ImageID,FileName,FilePath,SpeciesID,CommonName"
"This makes an assumption of one annotation per image, which happens to be"
true in this data set.
Retrieve image width and height
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
%% Check for images that aren't included in the metadata file
Enumerate all images
list(relativePathToImage.keys())[0]
%% Create info struct
%% Write output
%% Sanity-check the database's integrity
%% Render a bunch of images to make sure the labels got carried along correctly
""
idaho-camera-traps.py
""
Prepare the Idaho Camera Traps dataset for release on LILA.
""
%% Imports and constants
Multi-threading for .csv file comparison and image existence validation
"We are going to map the original filenames/locations to obfuscated strings, but once"
"we've done that, we will re-use the mappings every time we run this script."
This is the file to which mappings get saved
The maximum time (in seconds) between images within which two images are considered the
same sequence.
"This is a two-column file, where each line is [string in the original metadata],[category name we want to map it to]"
"The output file, using the original strings"
"The output file, using obfuscated strings for everything but filenamed"
"The output file, using obfuscated strings and obfuscated filenames"
"One time only, I ran MegaDetector on the whole dataset..."
...then set aside any images that *may* have contained humans that had not already been
annotated as such.  Those went in this folder...
...and the ones that *actually* had humans (identified via manual review) got
copied to this folder...
"...which was enumerated to this text file, which is a manually-curated list of"
images that were flagged as human.
Unopinionated .json conversion of the .csv metadata
%% List files (images + .csv)
Ignore .csv files in folders with multiple .csv files
...which would require some extra work to decipher.
fn = csv_files[0]
%% Parse each .csv file into sequences (function)
csv_file = csv_files[-1]
os.startfile(csv_file_absolute)
survey = csv_file.split('\\')[0]
Sample paths from which we need to derive locations:
""
St.Joe_elk\AM99\Trip 1\100RECNX\TimelapseData.csv
Beaverhead_elk\AM34\Trip 1\100RECNX\TimelapseData.csv
""
ClearCreek_mustelids\Winter2015-16\FS-001-P\FS-001-P.csv
ClearCreek_mustelids\Summer2015\FS-001\FS-001.csv
ClearCreek_mustelids\Summer2016\IDFG-016\IDFG-016.csv
""
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017b
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017a
Load .csv file
Validate the opstate column
# Create datetimes
print('Creating datetimes')
i_row = 0; row = df.iloc[i_row]
Make sure data are sorted chronologically
""
"In odd circumstances, they are not... so sort them first, but warn"
Debugging when I was trying to see what was up with the unsorted dates
# Parse into sequences
print('Creating sequences')
i_row = 0; row = df.iloc[i_row]
Start a new sequence if:
""
* This image has no timestamp
* This image has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each row
# Parse labels for each sequence
sequence_id = location_sequences[0]
Row indices in a sequence should be adjacent
sequence_df = df[df['seq_id']==sequence_id]
# Determine what's present
Be conservative; assume humans are present in all maintenance images
The presence columns are *almost* always identical for all images in a sequence
assert single_presence_value
print('Warning: presence value for {} is inconsistent for {}'.format(
"presence_column,sequence_id))"
...for each presence column
Tally up the standard (survey) species
"If no presence columns are marked, all counts should be zero"
count_column = count_columns[0]
Occasionally a count gets entered (correctly) without the presence column being marked
"assert len(values) == 1 and values[0] == 0, 'Non-zero counts with no presence"
columns marked for sequence {}'.format(sequence_id)
"Handle this by virtually checking the ""right"" box"
Make sure we found a match
Handle 'other' tags
column_name = otherpresent_columns[0]
print('Found non-survey counted species column: {}'.format(column_name))
...for each non-empty presence column
"Very rarely, the ""otherpresent"" column is checked, but no more detail is available"
...handling non-survey species
Build the sequence data
i_row = 0; row = sequence_df.iloc[i_row]
Only one folder used a single .csv file for two subfolders
...for each sequence
...def csv_to_sequences()
%% Parse each .csv file into sequences (loop)
%%
%%
i_file = -1; csv_file = csv_files[i_file]
%% Save sequence data
%% Load sequence data
%%
%% Validate file mapping (based on the existing enumeration)
sequences = sequences_by_file[0]
sequence = sequences[0]
"Actually, one folder has relative paths"
assert '\\' not in image_file_relative and '/' not in image_file_relative
os.startfile(csv_folder)
assert os.path.isfile(image_file_absolute)
found_file = os.path.isfile(image_file_absolute)
...for each image
...for each sequence
...for each .csv file
%% Load manual category mappings
The second column is blank when the first column already represents the category name
%% Convert to CCT .json (original strings)
Force the empty category to be ID 0
For each .csv file...
""
sequences = sequences_by_file[0]
For each sequence...
""
sequence = sequences[0]
Find categories for this image
"When 'unknown' is used in combination with another label, use that"
"label; the ""unknown"" here doesn't mean ""another unknown species"", it means"
there is some other unknown property about the main species.
category_name_string = species_present[0]
"This piece of text had a lot of complicated syntax in it, and it would have"
been too complicated to handle in a general way
print('Ignoring category {}'.format(category_name_string))
Don't process redundant labels
category_name = category_names[0]
If we've seen this category before...
If this is a new category...
print('Adding new category for {}'.format(category_name))
...for each category (inner)
...for each category (outer)
...if we do/don't have species in this sequence
"We should have at least one category assigned (which may be ""empty"" or ""unknown"")"
assert len(sequence_category_ids) > 0
Was any image in this sequence manually flagged as human?
print('Flagging sequence {} as human based on manual review'.format(sequence_id))
For each image in this sequence...
""
i_image = 0; im = images[i_image]
Create annotations for this image
...for each image in this sequence
...for each sequence
...for each .csv file
Verify that all images have annotations
ann = ict_data['annotations'][0]
For debugging only
%% Create output (original strings)
%% Validate .json file
%% Preview labels
%% Look for humans that were found by MegaDetector that haven't already been identified as human
This whole step only needed to get run once
%%
Load MD results
Get a list of filenames that MD tagged as human
im = md_results['images'][0]
...for each detection
...for each image
Map images to annotations in ICT
ann = ict_data['annotations'][0]
For every image
im = ict_data['images'][0]
Does this image already have a human annotation?
...for each annotation
...for each image
%% Copy images for review to a new folder
fn = missing_human_images[0]
%% Manual step...
Copy any images from that list that have humans in them to...
%% Create a list of the images we just manually flagged
fn = human_tagged_filenames[0]
E.g. '0000_Beaverhead_elk~AM174~Trip 1~100RECNX~IMG_1397.JPG'
"%% Translate location, image, sequence IDs"
Load mappings if available
Generate mappings
If we've seen this location before...
Otherwise assign a string-formatted int as the ID
If we've seen this sequence before...
Otherwise assign a string-formatted int as the ID
Assign an image ID
...for each image
Assign annotation mappings
Save mappings
"Back this file up, lest we should accidentally re-run this script"
with force_generate_mappings = True and overwrite the mappings we used.
...if we are/aren't re-generating mappings
%% Apply mappings
"%% Write new dictionaries (modified strings, original files)"
"%% Validate .json file (modified strings, original files)"
%% Preview labels (original files)
"viz_options.classes_to_exclude = ['empty','deer','elk']"
viz_options.classes_to_include = ['bobcat']
%% Copy images to final output folder (prep)
ann = d['annotations'][0]
Is this a public or private image?
Generate absolute path
Copy to output
Update the filename reference
...def process_image(im)
%% Copy images to final output folder (execution)
For each image
im = images[0]
Write output .json
%% Make sure the right number of images got there
%% Validate .json file (final filenames)
%% Preview labels (final filenames)
"viz_options.classes_to_exclude = ['empty','deer','elk']"
viz_options.classes_to_include = ['horse']
viz_options.classes_to_include = [viz_options.multiple_categories_tag]
"viz_options.classes_to_include = ['human','vehicle','domestic dog']"
%% Create zipfiles
%% List public files
%% Find the size of each file
fn = all_public_output_files[0]
%% Split into chunks of approximately-equal size
...for each file
%% Create a zipfile for each chunk
...for each filename
with ZipFile()
...def create_zipfile()
i_file_list = 0; file_list = file_lists[i_file_list]
"....if __name__ == ""__main__"""
""
wellington_to_json.py
""
Convert the .csv file provided for the Wellington data set to a
COCO-camera-traps .json file
""
%% Constants and environment
%% Read source data
"Filenames were provided as *.jpg, but images were *.JPG, converting here"
"%% Map filenames to rows, verify image existence"
"Takes ~30 seconds, since it's checking the existence of ~270k images"
"Build up a map from filenames to a list of rows, checking image existence as we go"
"I didn't expect this to be true a priori, but it appears to be true, and"
it saves us the trouble of checking consistency across multiple occurrences
of an image.
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
imageName = imageFilenames[0]
"As per above, this is convenient and appears to be true; asserting to be safe"
"Filenames look like ""290716114012001a1116.jpg"""
This gets imported as an int64
"These appear as ""image1"", ""image2"", etc."
"In the form ""001a"""
Can be in the form '111' or 's46'
"In the form ""7/29/2016 11:40"""
Check image height and width
"Use 'empty', to be consistent with other data on lila"
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
""
mcgill_to_json.py
""
Convert the .csv file provided for the McGill test data set to a
COCO-camera-traps .json file
""
%% Constants and environment
%% Read source data
"%% Map filenames to rows, verify image existence"
Create an additional column for concatenated filenames
Maps relative filenames to rows
"Build up a map from filenames to a list of rows, checking image existence as we go"
row = input_metadata.iloc[0]
"I didn't expect this to be true a priori, but it appears to be true, and"
it saves us the trouble of checking consistency across multiple occurrences
of an image.
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
row = input_metadata.iloc[0]
"Filenames look like ""290716114012001a1116.jpg"""
"In the form ""001a"""
Can be in the form '111' or 's46'
"In the form ""7/29/2016 11:40"""
Check image height and width
NaN is the only thing we should see that's not a string
NaN is the only thing we should see that's not a string
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
""
bellevue_to_json.py
""
"""Bellevue Camera Traps"" is the rather unremarkable camera trap data set"
used by one of the repo's maintainers for testing.  It's organized as:
""
approximate_date/[loose_camera_specifier/]/species
""
E.g.:
""
"""2018.03.30\coyote\DSCF0091.JPG"""
"""2018.07.18\oldcam\empty\DSCF0001.JPG"""
""
%% Constants and imports
from the ai4eutils repo
Filenames will be stored in the output .json relative to this base dir
%% Exif functions
"%% Enumerate files, create image/annotation/category info"
Force the empty category to be ID 0
Keep track of unique camera folders
Each element will be a dictionary with fields:
""
"relative_path, width, height, datetime"
fname = image_files[0]
Corrupt or not an image
Store file info
E.g. 2018.03.30/coyote/DSCF0091.JPG
...for each image file
%% Synthesize sequence information
Sort images by time within each folder
camera_path = camera_folders[0]
previous_datetime = sorted_images_this_camera[0]['datetime']
im = sorted_images_this_camera[1]
Start a new sequence if necessary
...for each image in this camera
...for each camera
Fill in seq_num_frames
%% A little cleanup
%% Write output .json
%% Sanity-check data
%% Label previews
""
snapshot_safar_importer_reprise.py
""
This is a 2023 update to snapshot_safari_importer.py.  We do a bunch of things now that
we didn't do the last time we imported Snapshot data (like updating the big taxonomy)
"file, and we skip a bunch of things now that we used to do (like generating massive"
"zipfiles).  So, new year, new importer."
""
%% Constants and imports
%% List files
"Do a one-time enumeration of the entire drive; this will take a long time,"
but will save a lot of hassle later.
%% Create derived lists
Takes about 60 seconds
CSV files are one of:
""
"_report_lila.csv (this is the one we want to use, with the species/count/etc. for each sequence)"
_report_lila_image_inventory.csv (maps captures to images)
_report_lila_overview.csv (distrubution of species)
%% List project folders
Project folders look like one of these:
""
APN
Snapshot Cameo/DEB
%% Map report and inventory files to codes
fn = csv_files[0]
%% Make sure that every report has a corresponding inventory file
%% Count species based on overview and report files
%% Print counts
%% Make sure that capture IDs in the reports/inventory files match
...and that all the images in the inventory tables are actually present on disk.
assert image_path_relative in all_files_relative_set
Make sure this isn't just a case issue
...for each report on this project
...for each project
"%% For all the files we have on disk, see which are and aren't in the inventory files"
"There aren't any capital-P .PNG files, but if I don't include that"
"in this list, I'll look at this in a year and wonder whether I forgot"
to include it.
fn = all_files_relative[0]
print('Skipping project {}'.format(project_code))
""
plot_wni_giraffes.py
""
Plot keypoints on a random sample of images from the wni-giraffes data set.
""
%% Constants and imports
%% Load and select data
%% Support functions
https://stackoverflow.com/questions/32504246/draw-ellipse-in-python-pil-with-line-thickness
Use a single channel image (mode='L') as mask.
The size of the mask can be increased relative to the imput image
to get smoother looking results.
draw outer shape in white (color) and inner shape in black (transparent)
downsample the mask using PIL.Image.LANCZOS
(a high-quality downsampling filter).
paste outline color to input image through the mask
%% Plot some images
ann = annotations_to_plot[0]
i_tool = 0; tool_name = short_tool_names[i_tool]
Don't plot tools that don't have a consensus annotation
...for each tool
...for each annotation
""
idfg_iwildcam_lila_prep.py
""
Adding class labels (from the private test .csv) to the iWildCam 2019 IDFG
"test set, in preparation for release on LILA."
""
This version works with the public iWildCam release images.
""
"%% ############ Take one, from iWildCam .json files ############"
%% Imports and constants
%% Read input files
Remove the header line
%% Parse annotations
Lines look like:
""
"b005e5b2-2c0b-11e9-bcad-06f1011196c4,1,Private"
%% Minor cleanup re: images
%% Create annotations
%% Prepare info
%% Minor adjustments to categories
Remove unused categories
Name adjustments
%% Create output
%% Write output
%% Validate .json file
%% Preview labels
viz_options.classes_to_exclude = ['test']
"%% ############ Take two, from pre-iWildCam .json files created from IDFG .csv files ############"
%% Imports and constants
%% One-time line break addition
%% Read input files
%% Prepare info
%% Minor adjustments to categories
%% Minor adjustments to annotations
%% Create output
%% Write output
%% Validate .json file
%% Preview labels
viz_options.classes_to_exclude = ['test']
""
missouri_to_json.py
""
Create .json files from the original source files for the Missouri Camera Traps
data set.  Metadata was provided here in two formats:
""
"1) In one subset of the data, folder names indicated species names.  In Set 1,"
there are no empty sequences.  Set 1 has a metadata file to indicate image-level
bounding boxes.
""
2) A subset of the data (overlapping with (1)) was annotated with bounding
"boxes, specified in a whitespace-delimited text file.  In set 2, there are"
"some sequences omitted from the metadata file, which implied emptiness."
""
"In the end, set 2 labels were not reliable enough to publish, so LILA includes only set 1."
""
%% Constants and imports
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Filenames will be stored in the output .json relative to this base dir
This will be a list of filenames that need re-annotation due to redundant boxes
"%% Enumerate files, read image sizes (both sets)"
"Takes a few minutes, since we're reading image sizes."
Each element will be a list of relative path/full path/width/height
"Only process leaf nodes corresponding to sequences, which look like:"
""
E:\wildlife_data\missouri_camera_traps\Set1\1.02-Agouti\SEQ75583
E:\wildlife_data\missouri_camera_traps\Set2\p1d101
""
assert len(files) <= 2
Read the image
Not an image...
Store file info
"...if we didn't hit the max file limit, keep going"
...for each file
%% Add sequence lengths (both sets)
%% Load the set 1 metadata file
"List of lists, length varies according to number of bounding boxes"
""
Preserves original ordering
iLine = 0; line = metadataSet1Lines[0]
"Lines should be filename, number of bounding boxes, boxes (four values per box)"
Make sure we have image info for this image
%% Print missing files from Set 1 metadata
Manual changes I made to the metadata file:
""
'IMG' --> 'IMG_'
Red_Brocket_Deer --> Red_Deer
European-Hare --> European_Hare
Wood-Mouse --> Wood_Mouse
Coiban-Agouti --> Coiban_Agouti
%% Load the set 2 metadata file
"This metadata file contains most (but not all) images, and a class label (person/animal/empty)"
"for each, plus bounding boxes."
"List of lists, length varies according to number of bounding boxes"
""
Preserves original ordering
"Create class IDs for each *sequence*, which we'll use to attach classes to"
images for which we don't have metadata
""
This only contains mappings for sequences that appear in the metadata.
iLine = 0; line = metadataSet2Lines[0]
"Lines should be filename, number of bounding boxes, labeled boxes (five values per box)"
""
Empty images look like filename\t0\t0
E.g. 'Set2\\p1d101\\p1d101s100i10.JPG'
Make sure we don't have mixed classes within an image
"Figure out what class this *sequence* is, so we know how to handle unlabeled"
images from this sequence
Can't un-do a mixed sequence
Previously-empty sequences get the image class label
"If the sequence has a non-empty class, possibly change it"
Make sure we have image info for this image
...for each line in the set 2 metadata file
%% What Set 2 images do I not have metadata for?
These are *mostly* empty images
iImage = 0; imageID = set2ImageIDs[iImage]
%% Create categories and annotations for set 1
"Though we have no empty sequences, we do have empty images in this set"
For each image
""
iImage = 0; imageID = set1ImageIDs[iImage]
E.g. Set1\\1.80-Coiban_Agouti\\SEQ83155\\SEQ83155_IMG_0010.JPG
Find the species name
This image may still be empty...
category['count'] = category['count'] + 1
"If we have bounding boxes, create image-level annotations"
"filename, number of bounding boxes, boxes (four values per box)"
"Make sure the relative filename matches, allowing for the fact that"
some of the filenames in the metadata aren't quite right
This image is non-empty
"Some redundant bounding boxes crept in, don't add them twice"
Check this bbox against previous bboxes
""
Inefficient?  Yes.  In an important way?  No.
"Bounding box values are in absolute coordinates, with the origin"
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1]."
""
"Convert to floats and to x/y/w/h, as per CCT standard"
...for each box
if we do/don't have boxes for this image
Else create a sequence-level annotation
...for each image
%% Write out the list of images with redundant boxes
%% Create categories and annotations for set 2
For each image
""
iImage = 0; imageID = set2ImageIDs[iImage]
E.g. 'Set2\\p1d100\\p1d100s10i1.JPG'
"Find the sequence ID, sanity check filename against what we stored"
"If we have bounding boxes or an explicit empty label, create image-level annotations"
"filename, number of bounding boxes, labeled boxes (five values per box)"
"Make sure the relative filename matches, allowing for the fact that"
some of the filenames in the metadata aren't quite right
"Bounding box values are in absolute coordinates, with the origin"
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1]."
""
Convert to floats and to x/y/w/h
...for each box
...if we do/don't have boxes for this image
Else create a sequence-level annotation
...for each image
"%% The 'count' field isn't really meaningful, delete it"
"It's really the count of image-level annotations, not total images assigned to a class"
%% Write output .json files
%% Sanity-check final set 1 .json file
"python sanity_check_json_db.py --bCheckImageSizes --baseDir ""E:\wildlife_data\missouri_camera_traps"" ""E:\wildlife_data\missouri_camera_traps\missouri_camera_traps_set1.json"""
%% Generate previews
""
filenames_to_json.py
""
Take a directory of images in which species labels are encoded by folder
"names, and produces a COCO-style .json file"
""
%% Constants and imports
from the ai4eutils repo
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Filenames will be stored in the output .json relative to this base dir
"rawClassListFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_classes.csv')"
"classMappingsFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_class_mapping.csv')"
"%% Enumerate files, read image sizes"
Each element will be a list of relative path/full path/width/height
Read the image
Corrupt or not an image
Store file info
Write to output file
...for each image file
...csv file output
...if the file list is/isn't available
%% Enumerate classes
Maps classes to counts
We like 'empty' to be class 0
%% Assemble dictionaries
...for each category
Each element is a list of relative path/full path/width/height/className
...for each image
%% External class mapping
%% Write raw class table
cat = categories[0]
%% Read the mapped class table
"id, source, count, target"
"%% Make classMappings contain *all* classes, not just remapped classes"
cat = categories[0]
%% Create new class list
"Start at 1, explicitly assign 0 to ""empty"""
One-off issue with character encoding
%% Re-map annotations
ann = annotations[0]
%% Write output .json
%% Utilities
%%
Find images with a particular tag
%% Randomly sample annotations
""
umn_to_json.py
""
Prepare images and metadata for the Orinoquía Camera Traps dataset.
""
%% Imports and constants
"For two deployments, we're only processing imagse in the ""detections"" subfolder"
%% Enumerate deployment folders
%% Load ground truth
i_row = 0; row = ground_truth_df.iloc[i_row]
%% Create relative paths for ground truth data
"Some deployment folders have no subfolders, e.g. this is a valid file name:"
""
M00/01010132.JPG
""
"But some deployment folders have subfolders, e.g. this is also a valid file name:"
""
N17/100EK113/07160020.JPG
""
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly"
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths."
deployment_name = list(deployment_folders)[0]
Enumerate all files in this folder
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))"
filename = files[100]
...for each file in this deployment
...for each deployment
%% Add relative paths to our ground truth table
i_row = 0; row = ground_truth_df.iloc[i_row]
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's"
"just the deployment ID and the image name, separated by a slash."
Find the relative path for this image
Make sure this image file exists
...for each row in the ground truth table
%% Take everything out of Pandas
%% Convert string timestamps to Python datetimes
im = ground_truth_dicts[0]
"Filenames look like, e.g., N36/100EK113/06040726.JPG"
%% Synthesize sequence information
Sort images by time within each location
i_location=0; location = locations[i_location]
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_camera[1]
"Timestamp for this image, may be None"
Start a new sequence if:
""
* This image has no timestamp
* This iamge has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each image in this location
...for each location
%% Create category dict and category IDs
im = images[0]
%% Imports and constants (.json generation)
%% Count frames in each sequence
%% Double check images with multiple annotations
im = images[0]
%% Assemble dictionaries
Force the empty category to be ID 0
input_im = images[0]
...for each image
%% Write output .json
%% Validate .json file
%% Map relative paths to annotation categories
ann = data['annotations'][0]
%% Copy images to output
EXCLUDE HUMAN AND MISSING
im = data['images'][0]
im = images[0]
%% Preview labels
viz_options.classes_to_exclude = ['test']
""
nacti_fieldname_adjustments.py
""
"NACTI metadata was posted with ""filename"" in images instead of ""file_name"", and"
"used string (rather than int) category IDs (in categories, but not in annotations)."
""
This script fixes those issues and rev's the version number.
""
%% Constants and environment
%% Read .json file
"%% Rev version number, update field names and types"
%% Write json file
""
cct_field_adjustments.py
""
CCT metadata was posted with int locations instead of strings.
""
This script fixes those issues and rev's the version number.
""
%% Constants and environment
%% Read .json file
"%% Rev version number, update field names and types"
%% Write json file
%% Check output data file
""
carrizo_shrubfree_2018.py
""
Convert the .csv file provided for the Carrizo Mojave data set to a
COCO-camera-traps .json file
""
%% Constants and environment
%% Read source data
Original .csv file had superfluous spaces in column names
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
58 missing files (of 17652)
%% Check for images that aren't included in the metadata file
3012 of 20606 files are not in metadata
%% Create CCT dictionaries
Map categories to integer IDs
""
The category '0' is reserved for 'empty'
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
Each filename should just match one row
Don't include images that don't exist on disk
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
""
snapshot_serengeti_lila.py
""
Create zipfiles of Snapshot Serengeti S1-S11.
""
"Create a metadata file for S1-S10, plus separate metadata files"
"for S1-S11.  At the time this code was written, S11 was under embargo."
""
Create zip archives of each season without humans.
""
Create a human zip archive.
""
%% Constants and imports
import sys; sys.path.append(r'c:\git\ai4eutils')
import sys; sys.path.append(r'c:\git\cameratraps')
assert(os.path.isdir(metadata_base))
"There are two redundant categories, and we re-map ""blank"" to ""empty"" as per CCT convention"
"%% Load metadata files, concatenate into a single table"
iSeason = 1
%% Convert to dictionaries (prep)
%% Convert to dictionaries (loops)
"TODO: iterrows() is a terrible way to do this, but this is one of those days"
"where I want to get this done, not get better at Python."
irow = 0; row = image_table.iloc[0]
"Loaded as an int64, converting to int here"
...for each row in the image table
Make sure image IDs are what we think they are
...for each row in the annotation table
%% Load previously-saved dictionaries when re-starting mid-script
%%
%% Take a look at categories (just sanity-checking)
%%
%% Fill in some image fields we didn't have when we created the image table
"width, height, corrupt, seq_num_frames, location, datetime"
Every annotation in this list should have the same sequence ID
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;"
there's no reason to do this redundantly for every image
Every image in this sequence should point back to the same equence
Every annotation in this list should have the same location
Every annotation in this list should have the same datetime
Is this image on disk?
iImage = 0; im = images[0]
...for each image
"images_processed = pool.map(process_image, images)"
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))"
%%
%% Count missing/corrupted images
%% Print distribution of sequence lengths (sanity-check)
%% Replicate annotations across images
iAnn = 0; ann = annotations[iAnn]
%% See what files are on disk but not annotated (~15 mins)
247370 files not in the database (of 7425810)
%% Load old image database
%% Look for old images not in the new DB and vice-versa
"At the time this was written, ""old"" was S1-S6"
old_im = cct_old['images'][0]
new_im = images[0]
4 old images not in new db
12 new images not in old db
%% Save our work
%% Load our work
%%
%% Examine size mismatches
i_mismatch = -1; old_im = size_mismatches[i_mismatch]
%% Sanity-check image and annotation uniqueness
"%% Split data by seasons, create master list for public seasons"
ann = annotations[0]
%% Minor updates to fields
"%% Write master .json out for S1-10, write individual season .jsons (including S11)"
"Loop over all seasons, plus one iteration for the ""all public data"" iteration, and"
"one for the ""all data"" iteration"
%% Find categories that only exist in S11
List of categories in each season
Category 55 (fire) only in S11
Category 56 (hyenabrown) only in S11
Category 57 (wilddog) only in S11
Category 58 (kudu) only in S11
Category 59 (pangolin) only in S11
Category 60 (lioncub) only in S11
%% Prepare season-specific .csv files
iSeason = 1
%% Create a list of human files
ann = annotations[0]
%% Save our work
%% Load our work
%%
"%% Create archives (human, per-season) (prep)"
im = images[0]
im = images[0]
Don't include humans
Only include files from this season
Possibly start a new archive
...for each image
i_season = 0
"for i_season in range(0,nSeasons):"
create_season_archive(i_season)
%% Create archives (loop)
pool = ThreadPool(nSeasons+1)
"n_images = pool.map(create_archive, range(-1,nSeasons))"
"seasons_to_zip = range(-1,nSeasons)"
...for each season
%% Sanity-check .json files
%logstart -o r'E:\snapshot_temp\python.txt'
%% Zip up .json and .csv files
pool = ThreadPool(len(files_to_zip))
"pool.map(zip_single_file, files_to_zip)"
%% Super-sanity-check that S11 info isn't leaking
im = data_public['images'][0]
ann = data_public['annotations'][0]
iRow = 0; row = annotation_df.iloc[iRow]
iRow = 0; row = image_df.iloc[iRow]
%% Create bounding box archive
i_image = 0; im = data['images'][0]
i_box = 0; boxann = bbox_data['annotations'][0]
%% Sanity-check a few files to make sure bounding boxes are still sensible
import sys; sys.path.append(r'C:\git\CameraTraps')
%% Check categories
%% Summary prep for LILA
""
wi_to_json
""
Prepares CCT-formatted metadata based on a Wildlife Insights data export.
""
"Mostly assumes you have the images also, for validation/QA."
""
%% Imports and constants
"Because WI filenames are GUIDs, it's not practical to page through sequences in an"
image viewer.  So we're going to (optionally) create a copy of the data set where
images are ordered.
%% Load ground truth
%% Take everything out of Pandas
%% Synthesize common names when they're not available
"Blank rows should always have ""Blank"" as the common name"
"print('Warning: missing common name for row {} ({})'.format(i_row,row['filename']))"
%% Convert string timestamps to Python datetimes
im = ground_truth_dicts[0]
"The field called ""location"" in the WI .csv file is a URL, we want to reclaim"
"the ""location"" keyword for CCT output"
"Filenames look like, e.g., N36/100EK113/06040726.JPG"
%% Synthesize sequence information
Sort images by time within each location
i_location=0; location = locations[i_location]
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_camera[1]
"Timestamp for this image, may be None"
Start a new sequence if:
""
* This image has no timestamp
* This iamge has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each image in this location
...for each location
%% Create category dict and category IDs
im = images[0]
%% Count frames in each sequence
%% Build relative paths
im = images[0]
Sample URL:
""
gs://project-asfasdfd/deployment/21444549/asdfasdfd-616a-4d10-a921-45ac456c568a.jpg'
%% Double check images with multiple annotations
im = images[0]
%% Assemble dictionaries
Force the empty category to be ID 0
input_im = images[0]
"print('Warning: image ID {} ({}) has multiple annotations'.format(im['id'],im['id'].replace('_','/')))"
...for each image
%% Write output .json
%% Validate .json file
%% Preview labels
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))"
%%
%% Create ordered dataset
"Because WI filenames are GUIDs, it's not practical to page through sequences in an"
image viewer.  So we're going to create a copy of the data set where images are
ordered.
im = images_out[0]; im
%% Create ordered .json
%% Copy files to their new locations
im = ordered_images[0]
im = data_ordered['images'][0]
%% Preview labels in the ordered dataset
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))"
%% Open an ordered filename from the unordered filename
""
rspb_to_json.py
""
Convert the .csv file provided for the RSPB data set to a
COCO-camera-traps .json file
""
%% Constants and environment
[location] is an obfuscation
%% Create info struct
%% Read source data
metadataTable.columns.values
""
"array(['Project', 'inDir', 'FileName', 'Station', 'Camera',"
"'StationCameraFileName', 'DateTimeOriginal', 'DateReadable',"
"'outDir', 'filename_new', 'fileExistsAlready', 'CopyStatus',"
"'Species'], dtype=object)"
We'll populate these later
keys should be lowercase
"%% Enumerate images, confirm filename uniqueness"
"%% Update metadata filenames to include site and camera folders, check existence"
""
Takes ~1min
iRow = 0; row = metadataTable.iloc[iRow]
There's a bug in the metadata; the 'camera' column isn't correct.
camera = row['Camera']
"These appear as, e.g., '3.22e12'"
camera = str(int(float(camera)))
Let's pull this out of the file name instead
""
Filenames look like one of the following:
""
A1__03224850850507__2015-11-28__10-45-04(1).JPG
Bayama2PH__C05__NA(NA).JPG
assert(os.path.isfile(fullPath))
metadataTable.iloc[iRow] = row
Re-assemble into an updated table
%% Check for images that aren't included in the metadata file
Enumerate all images
Write to a text file
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
iRow = 0; row = metadataTable.iloc[iRow]
A1__03224850850507__2015-11-28__10-45-04(1).JPG
'A1\\03224850850507\\A1__03224850850507__2015-11-28__10-45-04(1).JPG'
Not currently populated
"Often -1, sometimes a semi-meaningful int"
A1
03224850850507
"In variable form, but sometimes '28/11/2015 10:45'"
Check image height and width
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Write output
%% Check database integrity
%% Preview a few images to make sure labels were passed along sensibly
%% One-time processing step: copy images to a flat directory for annotation
%%
""
pc_to_json.py
""
Convert a particular collection of .csv files to CCT format.
""
%% Constants and environment
%% Read and concatenate source data
List files
"List of dataframes, one per .csv file; we'll concatenate later"
i_file = 87; fn = input_files[i_file]
Concatenate into a giant data frame
%% List files
%% Main loop over labels (prep)
Force the empty category to be ID 0
%% Main loop over labels (loop)
iRow = 0; row = input_metadata.iloc[iRow]
"ImageID,FileName,FilePath,SpeciesID,CommonName"
assert os.path.isfile(full_path)
Retrieve image width and height
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
%% See what's up with missing files
s = list(image_relative_paths)[0]
s = missing_files[0]
%% Check for images that aren't included in the metadata file
%% Sample the database
%%
Collect the images we want
%% Create info struct
%% Write output
%% Sanity-check the database's integrity
%% Render a bunch of images to make sure the labels got carried along correctly
options.classes_to_exclude = ['unlabeled']
%% Write out a list of files to annotate
""
save_the_elephants_survey_B.py
""
Convert the .csv file provided for the Save the Elephants Survey B data set to a
COCO-camera-traps .json file
""
%% Constants and environment
input_base = r'/mnt/blobfuse/wildlifeblobssc/ste_2019_08_drop'
output_base = r'/home/gramener/survey_b'
Handle all unstructured fields in the source data as extra fields in the annotations
"photo_type really should be an image property, but there are a few conflicts"
that forced me to handle it as an annotation proprerty
%% Read source data
"%% Map filenames to rows, verify image existence"
"%% Map filenames to rows, verify image existence"
Maps relative paths to row indices in input_metadata
"Build up a map from filenames to a list of rows, checking image existence as we go"
Ignore directories
%% Make sure the multiple-annotation cases make sense
%%
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
Force the empty category to be ID 0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image]
Example filename:
""
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG'
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG'
i_row = row_indices[0]
timestamp = row['Date']
There are a small handful of datetime mismatches across annotations
for the same image
assert im['datetime'] == timestamp
Special cases based on the 'photo type' field
Various spellings of 'community'
Have we seen this category before?
Create an annotation
fieldname = list(mapped_fields.keys())[0]
...for each row
...for each image
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
%% Scrap
%% Find unique photo types
""
ubc_to_json.py
""
Convert the .csv file provided for the UBC data set to a
COCO-camera-traps .json file
""
"Images were provided in eight folders, each of which contained a .csv"
file with annotations.  Those annotations came in two slightly different
"formats, the two formats corresponding to folders starting with ""SC_"" and"
otherwise.
""
%% Constants and environment
Map Excel column names - which vary a little across spreadsheets - to a common set of names
%% Enumerate images
Load from file if we've already enumerated
%% Create CCT dictionaries
Force the empty category to be ID 0
To simplify debugging of the loop below
#%% Create CCT dictionaries (loop)
#%%
Read source data for this folder
Rename columns
Folder name is the first two characters of the filename
""
Create relative path names from the filename itself
Folder name is the camera name
""
Create relative path names from camera name and filename
Which of our images are in the spreadsheet?
i_row = 0; fn = input_metadata['image_relative_path'][i_row]
#%% Check for images that aren't included in the metadata file
Find all the images in this folder
Which of these aren't in the spreadsheet?
#%% Create entries in CCT dictionaries
Only process images we have on disk
"This is redundant, but doing this for clarity, at basically no performance"
cost since we need to *read* the images below to check validity.
i_row = row_indices[0]
"These generally represent zero-byte images in this data set, don't try"
to find the very small handful that might be other kinds of failures we
might want to keep around.
print('Error opening image {}'.format(image_relative_path))
If we've seen this category before...
...make sure it used the same latin --> common mapping
""
"If the previous instance had no mapping, use the new one."
assert common_name == category['common_name']
Create an annotation
...for each annotation we found for this image
...for each image
...for each dataset
Print all of our species mappings
"%% Copy images for which we actually have annotations to a new folder, lowercase everything"
im = images[0]
%% Create info struct
"%% Convert image IDs to lowercase in annotations, tag as sequence level"
"While there isn't any sequence information, the nature of false positives"
"here leads me to believe the images were labeled at the sequence level, so"
we should trust labels more when positives are verified.  Overall false
positive rate looks to be between 1% and 5%.
%% Write output
%% Validate output
%% Preview labels
""
helena_to_cct.py
""
Convert the Helena Detections data set to a COCO-camera-traps .json file
""
%% Constants and environment
This is one time process
%% Create Filenames and timestamps mapping CSV
import pdb;pdb.set_trace()
%% To create CCT JSON for RSPB dataset
%% Read source data
Original Excel file had timestamp in different columns
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
%% Skipping this check because one image has multiple species
assert len(duplicate_rows) == 0
%% Check for images that aren't included in the metadata file
%% Create CCT dictionaries
Map categories to integer IDs
""
The category '0' is reserved for 'empty'
For each image
Don't include images that don't exist on disk
Some filenames will match to multiple rows
assert(len(rows) == 1)
iRow = rows[0]
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
ann['datetime'] = row['datetime']
ann['site'] = row['site']
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
%% Imports and constants
from github.com/microsoft/ai4eutils
from github.com/ecologize/CameraTraps
A list of files in the lilablobssc container for this data set
The raw detection files provided by NOAA
A version of the above with filename columns added
%% Read input .csv
%% Read list of files
%% Convert paths to full paths
i_row = 0; row = df.iloc[i_row]
assert ir_image_path in all_files
...for each row
%% Write results
"%% Load output file, just to be sure"
%% Render annotations on an image
i_image = 2004
%% Download the image
%% Find all the rows (detections) associated with this image
"as l,r,t,b"
%% Render the detections on the image(s)
In pixel coordinates
In pixel coordinates
%% Save images
%% Clean up
""
carrizo_trail_cam_2017.py
""
"Convert the .csv files provided for the ""Trail Cam Carrizo"" 2017 data set to"
a COCO-camera-traps .json file.
""
%% Constants and environment
%% Read source data
Removing the empty records
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
908 missing files (of 60562)
%% Check for images that aren't included in the metadata file
105329 of 164983 files are not in metadata
%% Create CCT dictionaries
Map categories to integer IDs
""
The category '0' is reserved for 'empty'
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
Each filename should just match one row
Don't include images that don't exist on disk
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
""
channel_islands_to_cct.py
""
Convert the Channel Islands data set to a COCO-camera-traps .json file
""
"Uses the command-line tool ExifTool (exiftool.org) to pull EXIF tags from images,"
"because every Python package we tried failed to pull the ""Maker Notes"" field properly."
""
"%% Imports, constants, paths"
# Imports ##
# Constants ##
# Paths ##
Confirm that exiftool is available
"assert which(exiftool_command_name) is not None, 'Could not locate the ExifTool executable'"
%% Load information from every .json file
"Ignore the sample file... actually, first make sure there is a sample file"
...and now ignore that sample file.
json_file = json_files[0]
ann = annotations[0]
...for each annotation in this file
...for each .json file
"%% Confirm URL uniqueness, handle redundant tags"
Have we already added this image?
"One .json file was basically duplicated, but as:"
""
Ellie_2016-2017 SC12.json
Ellie_2016-2017-SC12.json
"If the new image has no output, just leave the old one there"
"If the old image has no output, and the new one has output, default to the one with output"
Don't worry about the cases where someone tagged 'fox' and someone tagged 'fox_partial'
...for each image we've already added
...if this URL is/isn't in the list of URLs we've already processed
...for each image
%% Save progress
%%
%%
%% Download files (functions)
https://www.quickprogrammingtips.com/python/how-to-download-multiple-files-concurrently-in-python.html
"This is returned with a leading slash, remove it"
%% Download files (execution)
%% Read required fields from EXIF data (functions)
"-G means ""Print group name for each tag"", e.g. print:"
""
[File]          Bits Per Sample                 : 8
""
...instead of:
""
Bits Per Sample                 : 8
"If we don't get any EXIF information, this probably isn't an image"
line_raw = exif_lines[0]
"Split on the first occurrence of "":"""
Typically:
""
"'[MakerNotes]    Sequence                        ', '1 of 3']"
Not a typo; we are using serial number as a location
"If there are multiple timestamps, make sure they're *almost* the same"
"If there are multiple timestamps, make sure they're *almost* the same"
...for each line in the exiftool output
"This isn't directly related to the lack of maker notes, but it happens that files that are missing"
maker notes also happen to be missing EXIF date information
...process_exif()
"This is returned with a leading slash, remove it"
Ignore non-image files
%% Read EXIF data (execution)
ann = images[0]
%% Save progress
Use default=str to handle datetime objects
%%
%%
"Not deserializing datetimes yet, will do this if I actually need to run this"
%% Check for EXIF read errors
%% Remove junk
Ignore non-image files
%% Fill in some None values
"...so we can sort by datetime later, and let None's be sorted arbitrarily"
%% Find unique locations
%% Synthesize sequence information
Sort images by time within each location
i_location=0; location = locations[i_location]
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_camera[1]
"Timestamp for this image, may be None"
Start a new sequence if:
""
* This image has no timestamp
* This image has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each image in this location
...for each location
%% Count frames in each sequence
images_this_sequence = [im for im in images if im['seq_id'] == seq_id]
"%% Create output filenames for each image, store original filenames"
i_location = 0; location = locations[i_location]
i_image = 0; im = sorted_images_this_location[i_image]
%% Save progress
Use default=str to handle datetime objects
%%
%%
%% Copy images to their output files (functions)
%% Copy images to output files (execution)
%% Rename the main image list for consistency with other scripts
%% Create CCT dictionaries
Make sure this is really a box
Transform to CCT format
Force the empty category to be ID 0
i_image = 0; input_im = all_image_info[0]
"This issue only impacted one image that wasn't a real image, it was just a screenshot"
"showing ""no images available for this camera"""
Convert datetime if necessary
Process temperature if available
Read width and height if necessary
I don't know what this field is; confirming that it's always None
Process object and bbox
os.startfile(output_image_full_path)
"Zero is hard-coded as the empty category, but check to be safe"
"I can't figure out the 'index' field, but I'm not losing sleep about it"
assert input_annotation['index'] == 1+i_ann
"Some annotators (but not all) included ""_partial"" when animals were partially obscured"
"Annotators *mostly* used 'none', but sometimes 'empty'.  'empty' is CCT-correct."
If we've seen this category before...
If this is a new category...
...if this is an empty/non-empty annotation
Create an annotation
...for each annotation on this image
...for each image
%% Change *two* annotations on images that I discovered contains a human after running MDv4
%% Move human images
ann = annotations[0]
%% Count images by location
%% Write output
%% Validate output
%% Preview labels
viz_options.classes_to_exclude = [0]
""
ena24_to_json_2017.py
""
Convert the ENA24 data set to a COCO-camera-traps .json file
""
%% Constants and environment
Temporary folders for human and non-human images
Clean existing output folders/zipfiles
%% Support functions
%% Read source data
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
Check image height and width
"Each row is category, [box coordinates]"
"If there's just one row, loadtxt reads it as a 1d array; make it a 2d array"
with one row
Each row is a bounding box
Have we seen this category before?
Create an annotation
...for each bounding box
"This was here for debugging; nearly every instance is Human+Horse, Human+Vehicle,"
"or Human+Dog, but there is one Rabbit+Opossium, and a few Deer+Chicken!"
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Create ZIP files for human and non human
%% Validate output
%% Preview labels
%% Imports and constants
configurations and paths
%% Helper functions
"dest_path = copy(source_path, dest_folder)"
num_workers = multiprocessing.cpu_count()
pool = ThreadPool(num_workers)
"results = pool.starmap(_copy_unzip, zip(sources, itertools.repeat(dest_folder)))"
""
print('Waiting for processes to finish...')
pool.close()
pool.join()
sequential
%% Command-line driver
if the blob container is already mounted on the VM
or you can download them using the storage Python SDK
store storage account key in environment variable AZ_STORAGE_KEY
""
eMammal_helpers.py
""
Support functions for processing eMammal metadata
""
%% Constants and imports
%% Support functions
"pad to a total of 3 digits if < 1000, or 4 digits otherwise"
img_frame is a string from the xml tree
"length 4 frame order is returned as is, others are left padded to be 3 digit long"
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels
pad to a total of 4 digits
img_frame is a string from the xml tree
"length 4 frame order is returned as is, others are left padded to be 3 digit long"
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels
others column
summer day hours: 6am - 7pm
others day hours: 7am - 6pm
""
make_eMammal_json.py
""
"Produces the COCO-formatted json database for an eMammal dataset, i.e. a"
"collection of folders, each of which contains a deployment_manifest.xml file."
""
"In this process, each image needs to be loaded to size it."
""
"To add bounding box annotations to the resulting database, use"
add_annotations_to_eMammal_json.py.
""
%% Constants and imports
"Either add the eMammal directory to your path, or run from there"
os.chdir(r'd:\git\CameraTraps\database_tools\eMammal')
import warnings
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
warnings.filterwarnings('ignore')
Should we run the image size retrieval in parallel?
%% Support functions
%% Main loop (metadata processing; image sizes are retrieved later)
deployment = folders[0]
sequence = image_sequences[0]
get species info for this sequence
add each image's info to database
img = images[0]
"some manifests don't have the ImageOrder info, but the info is in the file name"
full_img_id has no frame info
""
frame number only used in requests to iMerit for ordering
...for each image
...for each sequence
...for each deployment
%% Get image sizes
"'tasks' is currently a list of 2-tuples, with each entry as [image dictionary,path]."
""
"Go through that and copy just the image dictionaries to 'db_images', adding size"
information to each entry.  Takes a couple hours.
opening each image seems too fast for this multi-threaded version to be faster than sequential code.
%% Assemble top-level dictionaries
%% Write out .json
""
make_full_SS_json.py
""
Create a COCO-camera-traps .json file for Snapshot Serengeti data from
the original .csv files provided on Dryad.
""
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no"
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated
Snapshot Safari preparation process.
""
%% Imports and constants
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays"
Count the number of images with multiple species
"%% Read image .csv file, format into a dictionary mapping images to capture events"
%% Create CCT-style .json
still need image width and height
...for each image
%% Write output files
""
make_per_season_SS_json.py
""
Create a COCO-camera-traps .json file for each Snapshot Serengeti season from
the original .csv files provided on Dryad.
""
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no"
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated
Snapshot Safari preparation process.
""
%% Imports and constants
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays"
"%% Read image .csv file, format into a dictionary mapping images to capture events"
%% Create CCT-style .json
still need image width and height
...for each image ID
%% Write output files
...for each season
""
generate_lila_per_image_labels.py
""
"Generate a .csv file with one row per annotation, containing full URLs to every"
"camera trap image on LILA, with taxonomically expanded labels."
""
"Typically there will be one row per image, though images with multiple annotations"
will have multiple rows.
""
"Some images may not physically exist, particularly images that are labeled as ""human""."
This script does not validate image URLs.
""
Does not include bounding box annotations.
""
%% Constants and imports
"We'll write images, metadata downloads, and temporary files here"
"Some datasets don't have ""sequence_level_annotation"" fields populated, but we know their"
annotation level
%% Download and parse the metadata file
To select an individual data set for debugging
%% Download and extract metadata for the datasets we're interested in
%% Load taxonomy data
"%% Build a dictionary that maps each [dataset,query] pair to the full taxonomic label set"
i_row = 0; row = taxonomy_df.iloc[i_row]
%% Process annotations for each dataset
ds_name = list(metadata_table.keys())[0]
"Go through annotations, marking each image with the categories that are present"
""
ann = annotations[0]
im = images[10]
This field name was only used for Caltech Camera Traps
raise ValueError('Suspicious date parsing result')
Special case we don't want to print a warning about
"Location, sequence, and image IDs are only guaranteed to be unique within"
"a dataset, so for the output .csv file, include both"
category_name = list(categories_this_image)[0]
Only print a warning the first time we see an unmapped label
...for each category that was applied at least once to this image
...for each image in this dataset
print('Warning: no date information available for this dataset')
print('Warning: no location information available for this dataset')
...for each dataset
...with open()
%% Read the .csv back
%% Do some post-hoc integrity checking
"-1 isn't *really* valid, but we use it sometimes for sequences of unknown length"
i_row = 0; row = df.iloc[i_row]
%% Preview constants
%% Choose images to download
ds_name = list(metadata_table.keys())[2]
Find all rows for this dataset
...for each dataset
%% Download images
i_image = 0; image = images_to_download[i_image]
%% Write preview HTML
im = images_to_download[0]
""
Common constants and functions related to LILA data management/retrieval.
""
%% Imports and constants
LILA camera trap master metadata file
"wildlife_insights_taxonomy_url = 'https://api.wildlifeinsights.org/api/v1/taxonomy?fields=class,order,family,genus,species,authority,taxonomyType,uniqueIdentifier,commonNameEnglish&page[size]={}'.format(wildlife_insights_page_size)"
from ai4eutils
%% Common functions
"We haven't implemented paging, make sure that's not an issue"
d['data'] is a list of items that look like:
Put the master metadata file in the same folder where we're putting images
Read lines from the master metadata file
Parse those lines into a table
Each line in this file is name/sas_url/json_url/[bbox_json_url]
Create a separate entry for bounding boxes if they exist
Unzip if necessary
""
get_lila_category_list.py
""
Generates a .json-formatted dictionary mapping each LILA dataset to all categories
"that exist for that dataset, with counts for the number of occurrences of each category"
"(the number of *annotations* for each category, not the number of *images*)."
""
"Also loads the taxonomy mapping file, to include scientific names for each category."
""
get_lila_category_counts counts the number of *images* for each category in each dataset.
""
%% Constants and imports
array to fill for output
"We'll write images, metadata downloads, and temporary files here"
Created by get_lila_category_list.py... contains counts for each category
%% Load category and taxonomy files
%% Map dataset names and category names to scientific names
i_row = 1; row = taxonomy_df.iloc[i_row]; row
%% Download and parse the metadata file
%% Download and extract metadata for the datasets we're interested in
%% Get category names and counts for each dataset
ds_name = 'NACTI'
Open the metadata file
Collect list of categories and mappings to category name
ann = annotations[0]
c = categories[0]
"Don't do taxonomy mapping for bbox data sets, which are sometimes just binary and are"
always redundant with the class-level data sets.
"As of right now, this is the only quirky case"
...for each dataset
%% Save dict
%% Print the results
ds_name = list(dataset_to_categories.keys())[0]
...for each dataset
""
download_lila_subset.py
""
"Example of how to download a list of files from LILA, e.g. all the files"
in a data set corresponding to a particular species.
""
%% Constants and imports
LILA camera trap master metadata file
"In this example, we're using the Missouri Camera Traps data set and the Caltech Camera Traps dataset"
All lower-case; we'll convert category names to lower-case when comparing
"We'll write images, metadata downloads, and temporary files here"
"We will demonstrate two approaches to downloading, one that loops over files"
"and downloads directly in Python, another that uses AzCopy."
""
AzCopy will generally be more performant and supports resuming if the
transfers are interrupted.  This script assumes that azcopy is on the system path.
Number of concurrent download threads (when not using AzCopy) (AzCopy does its
own magical parallelism)
%% Support functions
remove the leading '/'
%% Download and parse the metadata file
Put the master metadata file in the same folder where we're putting images
Read lines from the master metadata file
Parse those lines into a table
Each line in this file is name/base_url/json_url/[box_url]
%% Download and extract metadata for the datasets we're interested in
Unzip if necessary
...for each dataset of interest
%% List of files we're going to download (for all data sets)
"Flat list or URLS, for use with direct Python downloads"
For use with azcopy
This may or may not be a SAS URL
# Open the metadata file
# Build a list of image files (relative path names) that match the target species
Retrieve all the images that match that category
Retrieve image file names
Convert to URLs
...for each dataset
%% Download those image files
ds_name = 'Caltech Camera Traps'
ds_name = 'SWG Camera Traps'
We want to use the whole relative path for this script (relative to the base of the container)
"to build the output filename, to make sure that different data sets end up in different folders."
This may or may not be a SAS URL
For example:
""
caltech-unzipped/cct_images
swg-camera-traps
Check whether the URL includes a folder
E.g. caltech-unzipped
E.g. cct_images
E.g. swg-camera-traps
"Write out a list of files, and use the azcopy ""list-of-files"" option to download those files."
""
This azcopy feature is unofficially documented at:
""
https://github.com/Azure/azure-storage-azcopy/wiki/Listing-specific-files-to-transfer
""
import clipboard; clipboard.copy(cmd)
Loop over files
""
get_lila_category_counts.py
""
Count the number of images and bounding boxes with each label in one or more LILA datasets.
""
"This script doesn't write these counts out anywhere other than the console, it's just intended"
as a template for doing operations like this on LILA data.  get_lila_category_list.py writes
"information out to a .json file, but it counts *annotations*, not *images*, for each category."
""
%% Constants and imports
"If None, will use all datasets"
"We'll write images, metadata downloads, and temporary files here"
%% Download and parse the metadata file
%% Download and extract metadata for the datasets we're interested in
%% Count categories
ds_name = datasets_of_interest[0]
"Go through annotations, marking each image with the categories that are present"
""
ann = annotations[0]
Now go through images and count categories
im = images[0]
...for each dataset
%% Print the results
...for each dataset
""
add_url_to_database.py
""
"Adds a ""url"" field to a coco-camera-traps .json database, specifically to allow the db to"
be reviewed in the Visipedia annotation tool.
""
""
remove_corrupted_images_from_database.py
""
"Given a coco-camera-traps .json file, checks all images for TF-friendliness and generates"
a new .json file that only contains the non-corrupted images.
""
%% Imports and constants
import multiprocessing
"I leave this at an annoying low number, since by definition weird stuff will"
"be happening in the TF kernel, and it's useful to keep having content in the console."
%% Function definitions
"I sometimes pass in a list of images, sometimes a dict with a single"
element mapping a job ID to the list of images
"We're about to start a lot of TF sessions, and we don't want gobs"
of debugging information printing out for every session.
At some point we were creating a single session and looping over images
"within that session, but the only way I found to reliably not run out"
of GPU memory was to create a session per image and gc.collect() after
each session.
Map Image IDs to boolean (should I keep this image?)
"Convert to lists, append job numbers to the image lists"
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)"
Merge results
%% Interactive driver
%%
base_dir = r'D:\temp\snapshot_serengeti_tfrecord_generation'
Load annotations
Check for corruption
Write out only the uncorrupted data
%% Command-line driver
#######
""
integrity_check_json_db.py
""
"Does some integrity-checking and computes basic statistics on a db, specifically:"
""
* Verifies that required fields are present and have the right types
* Verifies that annotations refer to valid images
* Verifies that annotations refer to valid categories
"* Verifies that image, category, and annotation IDs are unique"
""
* Optionally checks file existence
""
* Finds un-annotated images
* Finds unused categories
""
* Prints a list of categories sorted by count
""
#######
%% Constants and environment
%% Functions
"If baseDir is non-empty, checks image existence"
This is used in a medium-hacky way to share modified options across threads
print('Image path {} does not exist'.format(filePath))
"#%% Read .json file if necessary, integrity-check fields"
info = data['info']
"#%% Build dictionaries, checking ID uniqueness and internal validity as we go"
Confirm that required fields are present
Confirm ID uniqueness
...for each category
image = images[0]
Confirm that required fields are present
Confirm ID uniqueness
We previously supported ints here; this should be strings now
"assert isinstance(image['location'], str) or isinstance(image['location'], int), \"
'Illegal image location type'
Are we checking for unused images?
Recursively enumerate images
print('Image {} is unused'.format(p))
Are we checking file existence and/or image size?
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)"
...for each image
Confirm that required fields are present
Confirm ID uniqueness
Confirm validity
...for each annotation
#%% Print statistics
Find un-annotated images and multi-annotation images
Find unused categories
Prints a list of categories sorted by count
""
https://stackoverflow.com/questions/72899/how-do-i-sort-a-list-of-dictionaries-by-a-value-of-the-dictionary
...def integrity_check_json_db()
%% Command-line driver
%% Interactive driver(s)
%%
Integrity-check .json files for LILA
options.iMaxNumImages = 10
""
combine_two_json_files.py
""
"Merges two coco-camera-traps .json files. In particular, categories are combined and re-numbered."
""
Combined Info
Combined Images
Combined Categories
## categories to merge
Combined Annotations
""
make_detection_db_for_viewing.py
""
"Given a .json file with ground truth bounding boxes, and a .p file containing detections for the same images,"
"creates a new .json file with separate classes for ground truth and detection, suitable for viewing in the Visipedia"
annotation tool.
""
%% Imports and constants
%% Main function
im_id_to_im = {im['id']:im for im in images}
make new categories to distinguish between ground truth and detections
"update all gt annotations to be class ""gt"""
collect all detections by image
keep any detection with score above det_thresh
"need to convert bbox from [x1,y1,x2,y2] to [x,y,w,h]"
"add ""info"" and ""licenses"" for annotation tools to function"
create new db
%% Command-line handling
""
analyze_json_database.py
""
Plots location/class/etc. distributions for classes in a coco-camera-traps .json file.
""
Currently includes some one-off code for specific species.
""
%% Constants and imports
%% Path configuration
%% Load source data
%% Build image/category dictionaries
%% Make plot of category distribution
%% make plots of location distribution
"plt.title('Number of images per location, by category')"
"plt.tight_layout(rect=[0,0,1,0.9])"
#make plot of images per season
%% Make plot of lions per location
%% Make plot of elephants per location
for loc in sorted_by_total[:25]:
"print('Location:' + loc[0] +', Lions: ' + str(len(loc_to_lion_ims[loc[0]])) + ', Elephants: ' + str(len(loc_to_elephant_ims[loc[0]])) + ', Total ims: ' + str(len(loc_to_ims[loc[0]])))"
""
subset_json_db.py
""
Select a subset of images (and associated annotations) from a .json file
in COCO Camera Traps format.
""
"To subset the .json files produced by our batch processing API, see"
subset_json_detector_output.py
""
Sample invocation:
""
"python subset_json_db.py ""E:\Statewide_wolf_container\idfg_20190409.json"" ""E:\Statewide_wolf_container\idfg_20190409_clearcreek.json"" ""clearcreek"" --ignore_case"
""
%% Constants and imports
%% Functions
Load the input file if necessary
Find images matching the query
Find annotations referring to those images
Write the output file if requested
%% Interactive driver
%%
%% Command-line driver
%% Constants and imports
%% Merge functions
i_input_dict = 0; input_dict = input_dicts[i_input_dict]
We will prepend an index to every ID to guarantee uniqueness
Map detection categories from the original data set into the merged data set
...for each category
Merge original image list into the merged data set
Create a unique ID
...for each image
Same for annotations
...for each annotation
"Merge info dicts, don't check completion time fields"
...for each dictionary
Convert merged image dictionaries to a sorted list
%% Driver
""
add_width_and_height_to_database.py
""
Grabs width and height from actual image files for a .json database that is missing w/h.
""
Originally used when we created a .json file for snapshot serengeti from .csv.
""
"This file converts the JSON output of the batch processing API to a pickle file, which can be used by the"
script ./make_classification_dataset.py
"Convert boxes from JSON   [x_min, y_min, width_of_box, height_of_box]"
"to PICKLE [ymin,  xmin,  ymax,         xmax]"
Write detections to file with pickle
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the"
script ./make_classification_dataset.py
Write detections to file with pickle
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the"
script ./make_classification_dataset.py
Assumes that the root of the CameraTrap repo is in the PYTHONPATH
Minimum threshold to put a detection into the output JSON file
Parameter check
Load detections from input
Load COCO style annotations
Build output JSON in format version 1.0
Adding the only known metadata info
"The pickle file does not contain category information, so we assume the default"
For each image with detections
for each detection
"Convert boxes from [ymin, xmin, ymax, xmax] format to"
"[x_min, y_min, width_of_box, height_of_box]"
Write output json
global flag for whether or not we encounter missing images
"- will only print ""missing image"" warning once"
TFRecords variables
1.3 for the cropping during test time and 1.3 for the context that the
CNN requires in the left-over image
Create output directories
Load COCO style annotations from the input dataset
"Get all categories, their names, and create updated ID for the json file"
Prepare the coco-style json files
Split the dataset by locations
Load detections
Write out COCO-style json files to the output directory
Write detections to file with pickle
## Preparations: get all the output tensors
For all images listed in the annotations file
Skip the image if it is annotated with more than one category
"Get ""old"" and ""new"" category ID and category name for this image."
Skip if in excluded categories.
get path to image
"If we already have detection results, we can use them"
Otherwise run detector and add detections to the collection
Only select detections with confidence larger than DETECTION_THRESHOLD
Skip image if no detection selected
whether it belongs to a training or testing location
Skip images that we do not have available right now
- this is useful for processing parts of large datasets
Load image
Run inference
"remove batch dimension, and convert from float32 to appropriate type"
convert normalized bbox coordinates to pixel coordinates
Pad the detected animal to a square box and additionally by
"PADDING_FACTOR. The result will be in crop_boxes. However, we need to make"
sure that its box coordinates are still within the image.
"for each bounding box, crop the image to the padded box and save it"
"Create the file path as it will appear in the annotation json,"
adding the box number if there are multiple boxes
"if the cropped file already exists, verify its size"
Add annotations to the appropriate json
js_keys = ['/'.join(im['file_name'].split('/')[1:])[:-4] for im in js['images']]
for tk in js_keys:
"assert np.isclose(1, np.sum(detections[tk]['detection_scores'] > 0.5))"
""
separate_detections_by_size
""
Not-super-well-maintained script to break a list of API output files up
based on bounding box size.
""
%% Imports and constants
Folder with one or more .json files in it that we want to split up
Enumerate .json files
Define size thresholds and confidence thresholds
"Not used directly in this script, but useful if we want to generate previews"
%% Split by size
For each size threshold...
For each file...
fn = input_files[0]
Just double-checking; we already filtered this out above
Don't reprocess .json files we generated with this script
Load the input file
For each image...
1.1 is the same as infinity here; no box can be bigger than a whole image
What's the smallest detection above threshold?
"[x_min, y_min, width_of_box, height_of_box]"
""
size = w * h
...for each detection
Which list do we put this image on?
...for each image in this file
Make sure the number of images adds up
Write out all files
...for each size threshold
...for each file
""
tile_images.py
""
Split a folder of images into tiles.  Preserves relative folder structure in a
"new output folder, with a/b/c/d.jpg becoming, e.g.:"
""
a/b/c/d_row_0_col_0.jpg
a/b/c/d_row_0_col_1.jpg
""
%% Imports and constants
from ai4eutils
%% Main function
TODO: parallelization
""
i_fn = 2; relative_fn = image_relative_paths[i_fn]
Can we skip this image because we've already generated all the tiles?
TODO: super-sloppy that I'm pasting this code from below
From:
""
https://github.com/whiplashoo/split-image/blob/main/src/split_image/split.py
i_col = 0; i_row = 1
left/top/right/bottom
...for each row
...for each column
...for each image
%% Interactive driver
%%
%%
""
rde_debug.py
""
Some useful cells for comparing the outputs of the repeat detection
"elimination process, specifically to make sure that after optimizations,"
results are the same up to ordering.
""
%% Compare two RDE files
i_dir = 0
break
"Regardless of ordering within a directory, we should have the same"
number of unique detections
Re-sort
Make sure that we have the same number of instances for each detection
Make sure the box values match
""
aggregate_video.py
""
Aggregate results and render output video for a video that's already been run through MD
""
%% Constants
%% Processing
im = d['images'][0]
...for each detection
This is no longer included in output files by default
# Split into frames
# Render output video
## Render detections to images
## Combine into a video
""
ocr_sandbox.py
""
sandbox for experimenting with using OCR to pull metadata from camera trap images
""
The general approach is:
""
"* Crop a fixed percentage from the top and bottom of an image, slightly larger"
than the largest examples we've seen of how much space is used for metadata.
""
"* Refine that crop by blurring a little, then looking for huge peaks in the"
"color histogram suggesting a solid background, then finding rows that are"
mostly that color.
""
"* Crop to the refined crop, then run pytesseract to extract text"
""
"* Use regular expressions to find time and date, in the future can add, e.g.,"
"temperature (which is often present *only* in the images, unlike time/date which"
are also usually in EXIF but often wrong or lost in processing)
""
"The metadata extraction (EXIF, IPTC) here is just sample code that seemed to"
belong in this file.
""
Contact: Dan Morris (cameratraps@lila.science)
""
%% Constants and imports
pip install pytesseract
""
"Also intall tesseract from: https://github.com/UB-Mannheim/tesseract/wiki, and add"
"the installation dir to your path (on Windows, typically C:\Program Files (x86)\Tesseract-OCR)"
pip install IPTCInfo3
from the ai4eutils repo: https://github.com/Microsoft/ai4eutils
""
"Only used for writing out a summary, not important for the core metadata extraction"
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Using a semi-arbitrary metric of how much it feels like we found the
"text-containing region, discard regions that appear to be extraction failures"
Pad each crop with a few pixels to make tesseract happy
Discard text from the top
"When we're looking for pixels that match the background color, allow some"
tolerance around the dominant color
We need to see a consistent color in at least this fraction of pixels in our rough
crop to believe that we actually found a candidate metadata region.
"What fraction of the [top,bottom] of the image should we use for our rough crop?"
A row is considered a probable metadata row if it contains at least this fraction
"of the background color.  This is used only to find the top and bottom of the crop area,"
"so it's not that *every* row needs to hit this criteria, only the rows that are generally"
above and below the text.
%% Support functions
"%% Load some images, pull EXIF and IPTC data for fun"
%% Rough crop
"This will be an nImages x 1 list of 2 x 1 lists (image top, image bottom)"
image = images[0]
"l,t,r,b"
""
"0,0 is upper-left"
"%% Close-crop around the text, return a revised image and success metric"
Did we find a sensible mode that looks like a background value?
"This looks very scientific, right?  Definitely a probability?"
"print('Failed min background fraction test: {} of {}'.format(pBackGroundValue,minBackgroundFraction))"
"Notes to self, things I tried that didn't really go anywhere..."
"analysisImage = cv2.blur(analysisImage, (3,3))"
"analysisImage = cv2.medianBlur(analysisImage,5)"
"analysisImage = cv2.Canny(analysisImage,100,100)"
imagePil = Image.fromarray(analysisImage); imagePil
Use row heuristics to refine the crop
""
This egregious block of code makes me miss my fluency in Matlab.
"print('Cropping to {},{},{},{}'.format(x,y,w,h))"
Crop the image
"For some reason, tesseract doesn't like characters really close to the edge"
imagePil = Image.fromarray(croppedImage); imagePil
%% Go to OCR-town
"An nImages x 2 list of strings, extracted from the top and bottom of each image"
An nImages x 2 list of cropped images
iImage = 0; iRegion = 1; regionSet = imageRegions[iImage]; region = regionSet[iRegion]
text = pytesseract.image_to_string(region)
pil --> cv2
"image = cv2.medianBlur(image, 3)"
"image = cv2.erode(image, None, iterations=2)"
"image = cv2.dilate(image, None, iterations=4)"
"image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]"
"image = cv2.blur(image, (3,3))"
"image = cv2.copyMakeBorder(image,10,10,10,10,cv2.BORDER_CONSTANT,value=[0,0,0])"
"text = pytesseract.image_to_string(imagePil, lang='eng')"
https://github.com/tesseract-ocr/tesseract/wiki/Command-Line-Usage
"psm 6: ""assume a single uniform block of text"""
""
...for each cropped region
...for each image
%% Extract dates and times
s = '1:22 pm'
s = '1:23:44 pm'
%% Write results to a handy html file
Add image name and resized image
Add results and individual region images
"textStyle = ""font-family:calibri,verdana,arial;font-weight:bold;font-size:150%;text-align:left;margin-left:50px;"""
%% Scrap
Alternative approaches to finding the text/background  region
Using findCountours()
imagePil = Image.fromarray(analysisImage); imagePil
"analysisImage = cv2.erode(analysisImage, None, iterations=3)"
"analysisImage = cv2.dilate(analysisImage, None, iterations=3)"
"analysisImage = cv2.threshold(analysisImage, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]"
Find object with the biggest bounding box
Using connectedComponents()
analysisImage = image
print('Found {} components'.format(nb_components))
We just want the *background* image
""
"Takes a .json file with MD results for an individual video, and converts to a .csv that includes"
frame times.  This is very bespoke to animal detection and does not include other classes.
""
%% Imports and constants
Only necessary if you want to extract the sample rate from the video
%% Extract the sample rate if necessary
%% Load results
%% Convert to .csv
i_image = 0; im = results['images'][i_image]
""
umn-pr-analysis.py
""
Precision/recall analysis for UMN data
""
%% Imports and constants
results_file = results_file_filtered
"For two deployments, we're only processing imagse in the ""detections"" subfolder"
String to remove from MegaDetector results
%% Enumerate deployment folders
%% Load MD results
im = md_results['images'][0]
%% Load ground truth
i_row = 0; row = ground_truth_df.iloc[i_row]
%% Create relative paths for ground truth data
"Some deployment folders have no subfolders, e.g. this is a valid file name:"
""
M00/01010132.JPG
""
"But some deployment folders have subfolders, e.g. this is also a valid file name:"
""
N17/100EK113/07160020.JPG
""
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly"
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths."
deployment_name = list(deployment_folders)[0]
Enumerate all files in this folder
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))"
filename = files[100]
...for each file in this deployment
...for each deployment
%% Map relative paths to MD results
%% Add relative paths to our ground truth table
i_row = 0; row = ground_truth_df.iloc[i_row]
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's"
"just the deployment ID and the image name, separated by a slash."
Find the relative path for this image
Make sure we have MegaDetector results for this file
Make sure this image file exists
...for each row in the ground truth table
%% Take everything out of Pandas
%% Some additional error-checking of the ground truth
An early version of the data required consistency between common_name and is_blank
%% Combine MD and ground truth results
d = ground_truth_dicts[0]
Find the maximum confidence for each category
...for each detection
...for each image
%% Precision/recall analysis
...for each image
Thresholds go up throughout precisions/recalls/thresholds; find the last
value where recall is at or above target.  That's our precision @ target recall.
Flatten the confusion matrix
Write precision/recall plot to .png file in output directory
pr_figure_relative_filename = 'prec_recall.png'
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)"
plt.show(block=False)
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')"
%% Find and manually review all images of humans
%%
"...if this image is annotated as ""human"""
...for each image
%% Find and manually review all MegaDetector animal misses
%%
im = merged_images[0]
GT says this is not an animal
GT says this is an animal
%% Convert .json to .csv
%%
""
kga-pr-analysis.py
""
Precision/recall analysis for KGA data
""
%% Imports and constants
%% Load and filter MD results
%% Load and filter ground truth
%% Map images to image-level results
%% Map sequence IDs to images and annotations to images
Verify consistency of annotation within a sequence
TODO
%% Find max confidence values for each category for each sequence
seq_id = list(sequence_id_to_images.keys())[1000]
im = images_this_sequence[0]
det = md_result['detections'][]
...for each detection
...for each image in this sequence
...for each sequence
%% Prepare for precision/recall analysis
seq_id = list(sequence_id_to_images.keys())[1000]
cat_id = list(category_ids_this_sequence)[0]
...for each category in this sequence
...for each sequence
%% Precision/recall analysis
"Confirm that thresholds are increasing, recall is decreasing"
This is not necessarily true
assert np.all(precisions[:-1] <= precisions[1:])
Thresholds go up throughout precisions/recalls/thresholds; find the max
value where recall is at or above target.  That's our precision @ target recall.
"This is very slightly optimistic in its handling of non-monotonic recall curves,"
but is an easy scheme to deal with.
Flatten the confusion matrix
Write precision/recall plot to .png file in output directory
pr_figure_relative_filename = 'prec_recall.png'
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)"
plt.show(block=False)
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')"
%% Scrap
%% Find and manually review all sequence-level MegaDetector animal misses
input_base = '/media/user/lila-01/lila/snapshot-safari/KGA/KGA_public'
i_sequence = 0; seq_id = pr_sequence_ids[i_sequence]
i_seq = 0; seq_id = false_negative_sequences[i_seq]
"sequence_folder = os.path.join(sequence_preview_dir,'seq_{}'.format(str(i_seq).zfill(3)))"
fn = image_files[0]
"print('Copying {} to {}'.format(input_path,output_path))"
...for each file in this sequence.
...for each sequence
%% Image-level postprocessing
parse arguments
check if a GPU is available
load a pretrained embedding model
setup experiment
load the embedding model
setup the target dataset
setup finetuning criterion
setup an active learning environment
create a classifier
the main active learning loop
Active Learning
finetune the embedding model and load new embedding values
gather labeled pool and train the classifier
save a snapshot
Load a checkpoint if necessary
setup the training dataset and the validation dataset
setup data loaders
check if a GPU is available
create a model
setup loss criterion
define optimizer
load a checkpoint if provided
setup a deep learning engine and start running
train the model
train for one epoch
evaluate on validation set
save a checkpoint
open the file
read it
do the substitution
matplotlib.use('Agg')
from UIComponents.DBObjects import *
Initialize Database
# database connection credentials
# try to connect as USER to database DB_NAME through peewee
Load the saved embedding model
dataset_query = Detection.select().limit(5)
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})"
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getalllabels()) , dataset.getallpaths(), {})"
Random examples to start
"random_ids = np.random.choice(dataset.current_set, 1000, replace=False).tolist()"
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)"
print(random_ids)
Move Records
"moveRecords(dataset, DetectionKind.ModelDetection.value, DetectionKind.UserDetection.value, random_ids)"
#print([len(x) for x in dataset.set_indices])
# Finetune the embedding model
#dataset.set_kind(DetectionKind.UserDetection.value)
#dataset.train()
"#train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)"
"#finetune_embedding(model, checkpoint['loss_type'], dataset, 32, 4, 100)"
#save_checkpoint({
"#        'arch': model.arch,"
"#        'state_dict': model.state_dict(),"
"#        'optimizer' : optimizer.state_dict(),"
"#        'loss_type' : loss_type,"
"#        }, False, ""%s%s_%s_%04d.tar""%('finetuned', loss_type, model.arch, len(dataset.set_indices[DetectionKind.UserDetection.value])))"
Get indices of samples to get user to label
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
"kwargs[""already_selected""].extend(indices)"
Train on samples that have been labeled so far
Test on the samples that have not been labeled
"'optimizer' : optimizer.state_dict(),"
num_classes= len(train_dataset.getClassesInfo()[0])
"print(""Num Classes= ""+str(num_classes))"
define loss function (criterion) and optimizer
"optimizer = torch.optim.SGD(params, momentum = 0.9, lr = args.lr, weight_decay = args.weight_decay)"
optimizer.load_state_dict(checkpoint['optimizer'])
train for one epoch
"adjust_lr(optimizer,epoch)"
if epoch % 1 == 0 and epoch > 0:
"a, b, c = e.predict(train_embd_loader, load_info = True, dim = args.feat_dim)"
"plot_embedding(reduce_dimensionality(a), b, c, {})"
evaluate on validation set
--------some stuff needed to get AJAX to work with bottle?--------#
-------------------------------------------------------------------------------- #
PREPARE TO QUEUE IMAGES FOR LABELING
-------------------------------------------------------------------------------- #
# Connect as USER to database DB_NAME through peewee and initialize database proxy
# Load embedding model
---------------------------------------------------------------------- #
CREATE QUEUE OF IMAGES TO LABEL
---------------------------------------------------------------------- #
Use classifier to generate predictions
# Update model predicted class in PostgreSQL database
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
-------------------------------------------------------------------------------- #
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI
-------------------------------------------------------------------------------- #
"# static routes (to serve CSS, etc.)"
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps"
"return bottle.static_file(filename, root='../../../../../../../../../../../.')"
# dynamic routes
if data['display_grayscale']:
indices_to_exclude.update(set(color_indices))
elif not data['display_grayscale']:
indices_to_exclude.update(set(grayscale_indices))
data['display_images'] = {}
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices]
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices]
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices]
data['display_images']['detection_categories'] = []
for i in indices:
if str(dataset.samples[i][1]) == 'None':
data['display_images']['detection_categories'].append('None')
else:
existing_category_entries = {cat.id: cat.name for cat in Category.select()}
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()"
data['display_images']['detection_categories'].append(cat_name)
Use image ids in images_to_label to get the corresponding dataset indices
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
print(set(dataset.set_indices[4]).update(set(indices_to_label)))
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Use image id images_to_label to get the corresponding dataset index
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Train on samples that have been labeled so far
dataset.set_kind(DetectionKind.UserDetection.value)
print(y_train)
Predict on the samples that have not been labeled
print(y_pred)
Update model predicted class in PostgreSQL database
timer = time.time()
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
print('Updating the database took %0.2f seconds'%(time.time() - timer))
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
"once the classifier has been trained the first time, switch to AL sampling"
# sourceMappingURL=bootstrap.bundle.min.js.map
"print(ap_distances.size(),an_distances.size())"
losses = -(((-ap_distances)/128)+1+1e-16).log() - (((-(128-an_distances))/128)+1+1e-16).log()
import pdb
pdb.set_trace()
losses = ap_distances - an_distances + self.margin
print(losses.size())
from UIComponents.DBObjects import *
TODO: should this also change self.kind?
get the embedding representations for all samples (i.e. set current_set to all indices)
"return PILImage.open(os.path.join(self.img_base,path)).convert('RGB')"
"print(self.labels_set, self.n_classes)"
from sklearn.manifold import TSNE
embedding= TSNE(n_components=2).fit_transform(X)
embedding= PCA(n_components=2).fit_transform(X)
return X
"print(dir(event), type(sc))"
"print(label,bgcolor)"
"patches.append(mpatches.Patch(color=indexcolors[i], label=label))"
"plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,"
"ncol=12, mode=""expand"", borderaxespad=0., handles=patches)"
plt.legend(handles=patches)
"plt.xlabel('Dim 1', fontsize=12)"
"plt.ylabel('Dim 2', fontsize=12)"
plt.grid(True)
"plt.xlabel('Dim 1', fontsize=12)"
"plt.ylabel('Dim 2', fontsize=12)"
plt.grid(True)
print(thumb)
"img.thumbnail((16, 12), PILImage.ANTIALIAS)"
plt.show()
Add all negatives for all positive pairs
print(triplets.shape[0])
compute output
measure accuracy
compute loss on this batch
"train on a batch, record loss, and measure accuracy (if calc_accuracy)"
compute output
measure accuracy and record loss
switch to evaluate mode
compute output
switch to evaluate mode
"self.fc13 = nn.Linear(128, 64)"
self.bn2 = nn.BatchNorm1d(64)
x = F.relu(self.fc12(x))
x = F.relu(self.bn1(self.fc13(x)))
x = F.relu(self.fc13(x))
"x = F.dropout(x, training=self.training)"
save features last FC layer
x = F.relu(x)
save features last FC layer
import matplotlib.pyplot as plt
plt.switch_backend('agg')
class EmbeddingNet(nn.Module):
"def __init__(self, architecture, feat_dim, use_pretrained=False):"
"super(EmbeddingNet, self).__init__()"
self.feat_dim= feat_dim
self.inner_model = models.__dict__[architecture](pretrained=use_pretrained)
if architecture.startswith('resnet'):
in_feats= self.inner_model.fc.in_features
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)"
elif architecture.startswith('inception'):
in_feats= self.inner_model.fc.in_features
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)"
if architecture.startswith('densenet'):
in_feats= self.inner_model.classifier.in_features
"self.inner_model.classifier = nn.Linear(in_feats, feat_dim)"
if architecture.startswith('vgg'):
in_feats= self.inner_model.classifier._modules['6'].in_features
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)"
if architecture.startswith('alexnet'):
in_feats= self.inner_model.classifier._modules['6'].in_features
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)"
"def forward(self, x):"
return self.inner_model.forward(x)
class NormalizedEmbeddingNet(EmbeddingNet):
"def __init__(self, architecture, feat_dim, use_pretrained=False):"
"EmbeddingNet.__init__(self, architecture, feat_dim, use_pretrained = use_pretrained)"
"def forward(self, x):"
embedding =  F.normalize(self.inner_model.forward(x))*10.0
"return embedding, embedding"
"def get_random_images(num, image_dir, test_transforms):"
"data = datasets.ImageFolder(image_dir, transform=test_transforms) # slight abuse; this expects subfolders corresponding to classes but we have no classes here"
indices = list(range(len(data)))
np.random.shuffle(indices)
idx = indices[:num]
from torch.utils.data.sampler import SubsetRandomSampler
sampler = SubsetRandomSampler(idx)
"loader = torch.utils.data.DataLoader(data,"
"sampler=sampler, batch_size=num)"
dataiter = iter(loader)
"images, labels = dataiter.next()"
"return images, labels"
"def predict_image(image, model, test_transforms):"
"device = torch.device(""cuda"" if torch.cuda.is_available()"
"else ""cpu"")"
image_tensor = test_transforms(image).float()
image_tensor = image_tensor.unsqueeze_(0)
input = Variable(image_tensor)
input = input.to(device)
output = model(input)[0]
return output.data.cpu().numpy()
Connect to database and initialize db_proxy
# database connection credentials
# load the dataset
Load the saved embedding model from the checkpoint
# update the dataset embedding
# Create a folder for saving embedding visualizations with this model checkpoint
model_emb_dirname = os.path.basename(args.base_model).split('.')[0]
"os.makedirs(model_emb_dirname, exist_ok=True)"
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')"
dataset.embedding_mode()
"assert 2==3, 'break'"
datasetindices = list(range(len(dataset)))
np.random.shuffle(datasetindices)
random_indices = datasetindices[:args.num]
print(random_indices)
"selected_sample_features = np.array([]).reshape(0, 256)"
selected_sample_labels = []
for idx in random_indices:
"selected_sample_features = np.vstack([selected_sample_features, X_train[idx]])"
selected_sample_labels.append(y_train[idx])
img_path = imagepaths[idx].split('.JPG')[0]
image = dataset.loader(img_path)
selected_sample_images.append(image)
# TRY NEAREST NEIGHBORS WALK THROUGH EMBEDDING
nbrs = NearestNeighbors(n_neighbors=args.num).fit(selected_sample_features)
"distances, indices = nbrs.kneighbors(selected_sample_features)"
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')"
"idx_w_closest_nbr = np.where(distances[:,1] == min(distances[:,1]))[0][0]"
order = [idx_w_closest_nbr]
for ii in range(len(distances)):
"distances[ii, 0] = np.inf"
while len(order)<args.num:
curr_idx = order[-1]
curr_neighbors = indices[curr_idx]
curr_dists = list(distances[curr_idx])
# print(min(curr_dists))
next_closest_pos = curr_dists.index(min(curr_dists))
next_closest = curr_neighbors[next_closest_pos]
order.append(next_closest)
# make sure you can't revisit past nodes
for vi in order:
vi_pos = list(indices[next_closest]).index(vi)
"distances[next_closest, vi_pos] = np.inf"
for ii in range(len(order)):
imgidx = order[ii]
image = selected_sample_images[imgidx]
"image.save(model_emb_dirname+""/img""+str(ii)+""_""+str(selected_sample_labels[imgidx])+"".png"")"
# Specify the transformations on the input images before inference
"# test_transforms = transforms.Compose([transforms.Resize([224, 224]), transforms.ToTensor()])"
"test_transforms = transforms.Compose([transforms.Resize([256, 256]), transforms.RandomCrop([224, 224]), transforms.RandomHorizontalFlip(), transforms.ColorJitter(), transforms.ToTensor(), transforms.Normalize([0.407328, 0.407328, 0.407328], [0.118641, 0.118641, 0.118641])])"
"images, labels = get_random_images(num, image_dir, test_transforms)"
"all_features = np.array([]).reshape(0, 256)"
for ii in range(len(images)):
image = to_pil(images[ii])
"features = predict_image(image, model, test_transforms)"
"all_features = np.vstack([all_features, features])"
# for ii in range(len(images)):
#     image = to_pil(images[ii])
"#     image.save(""img""+str(ii)+"".png"")"
# TRY CLUSTERING
kmeans1 = KMeans(n_clusters=5).fit(StandardScaler().fit_transform(all_features))
print(kmeans1.labels_)
for ii in range(len(images)):
image = to_pil(images[ii])
"filename = str(kmeans1.labels_[ii])+""/img""+str(ii)+"".png"""
if not os.path.exists(os.path.dirname(filename)):
os.makedirs(os.path.dirname(filename))
image.save(filename)
class SaveFeatures():
"def __init__(self, module):"
self.hook = module.register_forward_hook(self.hook_fn)
"def hook_fn(self, module, input, output):"
"self.features = torch.tensor(output, requires_grad=True).cuda()"
def close(self):
self.hook.remove()
Load the saved embedding model from the checkpoint
"Get a sample from the database, with eval transforms applied, etc."
Connect to database and sample a dataset
output = model.forward(sample_image.unsqueeze(0))
print(output)
with torch.no_grad():
sample_image_input = sample_image.cuda(non_blocking=True)
"_, output = model(sample_image_input) # compute output"
print(output)
sample_image = PILImage.open(sample_image_path).convert('RGB')
"sample_image = transforms.Compose([Resize([256, 256]), CenterCrop(([[224,224]])), ToTensor(), Normalize([0.369875, 0.388726, 0.347536], [0.136821, 0.143952, 0.145229])])(sample_image)"
print(list(model_inner_resnet.children()))
print(model_inner_resnet.fc)
print(model_inner_resnet.fc0)
# print(model_inner_resnet.layer4[0].conv2)
# print(type(model))
# print(len(list(model_inner_resnet.children())))
# print(list(model.children()))
# print(list(list(model.children())[0].children()))
"img = np.uint8(np.random.uniform(150, 180, (56, 56, 3)))/255"
"img_tensor = torch.unsqueeze(torch.from_numpy(img), 0)"
full_out = model_inner_resnet.forward(img_tensor)
print(full_out)
model(img_tensor)
activations = SaveFeatures(model_inner_resnet.layer4[0].conv2)
print(activations.features)
print(type(activations.features))
activations.close()
Connect to database and sample a dataset
Load the saved embedding model from the checkpoint
Update the dataset embedding
Get a random query image
# # # IMAGES IN THE SAME SEQUENCE # # # #
"assert 2==3, 'break'"
# # # CLOSEST IN (EMBEDDING) FEATURE SPACE # # # #
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Variables for the hierarchical cluster
Variables for the AL algorithm
Data variables
"connectivity = kneighbors_graph(self.transformed_X,max_features)"
Fit cluster and update cluster variables
The sklearn hierarchical clustering algo numbers leaves which correspond
to actual datapoints 0 to n_points - 1 and all internal nodes have
ids greater than n_points - 1 with the root having the highest node id
"If no labels have been observed, simply return uniform distribution"
"If no observations, return worst possible upper lower bounds"
Loop through generations from bottom to top
Update admissible labels for node
Calculate score
Determine if node should be split
Make sure label set for node so that we can flow to children
if necessary
Only split if all ancestors are admissible nodes
This is part  of definition of admissible pruning
Check that pruning covers all leave nodes
Fill in labels
Observe labels for previously recommended batches
TODO(lishal): implement multiple selection methods
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
A list of initialized samplers is allowed as an input because
"for AL_methods that search over different mixtures, may want mixtures to"
have shared AL_methods so that initialization is only performed once for
computation intensive methods like HierarchicalClusteringAL and
states are shared between mixtures.
"If initialized samplers are not provided, initialize them ourselves."
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copy these objects to make sure they are not modified while simulating
trajectories as they are used later by the main run_experiment script.
Assumes that model has already by fit using all labeled data so
the probabilities can be used immediately to hallucinate labels
All models need to have predict_proba method
Hallucinate labels for selected datapoints to be label
using class probabilities from model
"Not saving already_selected here, if saving then should sort"
only for the input to fit but preserve ordering of indices in
already_selected
Useful to know how accuracy compares for model trained on hallucinated
labels vs trained on true labels.  But can remove this train to speed
up simulations.  Won't speed up significantly since many more models
are being trained inside the loop above.
Save trajectory for reference
Delete created copies
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER
Should check model but kernel_svm does not have coef_ so need better
handling here
Make sure that model object fed in did not change during simulations
Return indices based on return type specified
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Set gamma for gaussian kernel to be equal to 1/n_features
kneighbors graph is constructed using k=10
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of"
"another point, make it vice versa"
Graph edges are weighted by applying gaussian kernel to manhattan dist.
"By default, gamma for rbf kernel is equal to 1/n_features but may"
get better results if gamma is tuned.
Define graph density for an observation to be sum of weights for all
edges to the node representing the datapoint.  Normalize sum weights
by total number of neighbors.
"If a neighbor has already been sampled, reduce the graph density"
for its direct neighbors to promote diversity.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update observed reward and arm probabilities
Sample an arm
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
y only used for determining how many clusters there should be
probably not practical to assume we know # of classes before hand
should also probably scale with dimensionality of data
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
TODO(lishal): have MarginSampler and this share margin function
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update min_distances for all examples given new cluster center.
Assumes that the transform function takes in original data and not
flattened data.
Initialize centers with a randomly selected datapoint
New examples should not be in already selected since those points
should have min_distance of zero to a cluster center.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
This is uniform given the remaining pool but biased wrt the entire pool.
sample = [i for i in range(self.X.shape[0]) if i not in already_selected]
return sample[0:N]
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Fields for hierarchical clustering AL
Setting parent and storing nodes in dict for fast access
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Connect to database DB_NAME as USER and initialize tables
Populate Info table
Populate Category table
"# For now, we have a predefined list of species we expect to see in the camera trap database (e.g. maybe from a quick look through the images)"
# TODO: allow user to update the class list through the labeling tool UI as they see different species
Populate Image and Detection tables
Image entry data
Detection entry data
Connect to database DB_NAME as USER and initialize tables
Populate Info table
Populate Category table
# Create category records from species present in the COCO camera trap classification dataset (COCO classes minus any excluded classes)
# TODO: allow user to update the class list through the labeling tool UI as they see different species
Populate Image and Detection tables
"with open(os.path.join(args.crop_dir,'crops.json'), 'r') as infile:"
crops_json = json.load(infile)
counter = 0
timer = time.time()
num_detections = len(crops_json)
for detectionid in crops_json:
counter += 1
detection_data = crops_json[detectionid]
# Image entry data
existing_image_entries = Image.select().where((Image.file_name == detection_data['file_name']))
try:
existing_image_entry = existing_image_entries.get()
except:
"image_entry = Image.create(id=detectionid, file_name=detection_data['file_name'], width=detection_data['width'], height=detection_data['height'], grayscale=detection_data['grayscale'],"
"source_file_name=detection_data['source_file_name'], relative_size=detection_data['relative_size'],"
"seq_id=detection_data['seq_id'], seq_num_frames=detection_data['seq_num_frames'], frame_num=detection_data['frame_num'])"
image_entry.save()
# Detection entry data
"detection_entry = Detection.create(id=detectionid, image=detectionid, bbox_confidence=detection_data['bbox_confidence'],"
"bbox_X1=detection_data['bbox_X1'], bbox_Y1=detection_data['bbox_Y1'], bbox_X2=detection_data['bbox_X2'], bbox_Y2=detection_data['bbox_Y2'],"
kind=DetectionKind.ModelDetection.value)
detection_entry.save()
if counter%100 == 0:
"print('Updated database with Image and Detection table entries for %d out of %d crops in %0.2f seconds'%(counter, num_detections, time.time() - timer))"
# data related to original image
Get class names from .txt list
Initialize Oracle table
Map filenames to classes (NOTE: we assume a single image does not contain more than one class)
"For each detection, use source image path to get class"
TODO update: Assumes that crops have already
been generated for the images using make_active_learning_classification_dataset.py. The created DB contains tables:
- info: information about the dataset
- image: images present in the dataset
- detections: crops of images with detections with confidence greater than a specified threshold
Initialize Database
# database connection credentials
HOST = 'localhost'
PORT = 5432
"# first, make sure the (user, password) has been created"
"# sudo -u postgres psql -c ""CREATE USER <db_user> WITH PASSWORD <db_password>;"""
"# sudo -u postgres psql -c ""CREATE DATABASE <db_name> WITH OWNER <db_user> CONNECTION LIMIT -1;"""
"# sudo -u postgres psql -c ""GRANT CONNECT ON DATABASE <db_name> TO <db_user>;"""
"# sudo -u postgres psql -d <db_name> -c ""CREATE EXTENSION IF NOT EXISTS \""uuid-ossp\"";"""
# Try to connect as USER to database DB_NAME through peewee
Populate Tables
# create Info table
# get class names for Category table
Faster anD available in Python 3.5 and above
# iterate through images in each class folder
killing this process after over 38 hours adding over 500k white-tailed deer crops from emammal
resuming for remaining classes
# get cropped image data for Image table
"if mean of each channel is about the same, image is likely grayscale"
# still have no info on these:
seq_id = CharField(null= True)                # sequence identifier for the original image
seq_num_frames = IntegerField(null = True)    # number of frames in sequence
frame_num = IntegerField(null = True)         # which frame number in sequence
location = CharField(null = True)             # location of camera trap
datetime = DateTimeField(null = True)
# store info about the detection corresponding to this image
# store info about the true labels for the detection
#  - for pretrain dataset this is the same as the detection_category if the detection categories
print(classes)
Connect to database and sample a dataset
Load the saved embedding model from the checkpoint
Update the dataset embedding
save the images
save the features
"with open(os.path.join(args.output_dir, 'lastlayer_features.mat'), 'wb') as f:"
"pickle.dump(sample_features, f)"
"with open(os.path.join(args.output_dir, 'labels.mat'), 'wb') as f:"
"pickle.dump(sample_labels, f)"
"parser.add_argument('--db_name', default='missouricameratraps', type=str, help='Name of the training (target) data Postgres DB.')"
"parser.add_argument('--db_user', default='user', type=str, help='Name of the user accessing the Postgres DB.')"
"parser.add_argument('--db_password', default='password', type=str, help='Password of the user accessing the Postgres DB.')"
"parser.add_argument('--base_model', type=str, help='Path to latest embedding model checkpoint.')"
"parser.add_argument('--output_dir', type=str, help='Output directory for subset of crops')"
Add json entry for this crop
Copy file for this crop to subset dataset crop dir
Copy file for its full-size source image to subset dataset image dir
Write crops.json to subset dataset crop dir
store info about the crops produced in a JSON file
------------------------------------------------------------------------------------------------------------#
COMMENT OUT IF NOT USING A SPECIFIC PROJECT WITHIN ROBERT LONG EMAMMAL DATASET
------------------------------------------------------------------------------------------------------------#
get some information about the source image
------------------------------------------------------------------------------------------------------------#
NOTE: EDIT THIS SECTION BASED ON DATASET SOURCE
get info about sequence the source image belongs to from path and directory
# missouricameratraps:
imgframenum = int(os.path.basename(imgfile).split('.JPG')[0].split('_')[-1])
imgseqid = int(os.path.split(os.path.dirname(imgfile))[-1])
"imgseqnumframes = len([name for name in os.listdir(os.path.dirname(imgfile)) if os.path.isfile(os.path.join(os.path.dirname(imgfile), name))])"
# emammal:
------------------------------------------------------------------------------------------------------------#
utility function
compute output
measure accuracy and record loss
switch to train mode
measure accuracy and record loss
compute output
measure accuracy and record loss
switch to evaluate mode
compute output
switch to evaluate mode
compute output
switch to evaluate mode
"print(self.labels_set, self.n_classes)"
Add all negatives for all positive pairs
print(triplets.shape[0])
constructor
update embedding values after a finetuning
select either the default or active pools
gather test set
gather train set
finetune the embedding model over the labeled pool
a utility function for saving the snapshot
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Variables for the hierarchical cluster
Variables for the AL algorithm
Data variables
"connectivity = kneighbors_graph(self.transformed_X,max_features)"
Fit cluster and update cluster variables
The sklearn hierarchical clustering algo numbers leaves which correspond
to actual datapoints 0 to n_points - 1 and all internal nodes have
ids greater than n_points - 1 with the root having the highest node id
"If no labels have been observed, simply return uniform distribution"
"If no observations, return worst possible upper lower bounds"
Loop through generations from bottom to top
Update admissible labels for node
Calculate score
Determine if node should be split
Make sure label set for node so that we can flow to children
if necessary
Only split if all ancestors are admissible nodes
This is part  of definition of admissible pruning
Check that pruning covers all leave nodes
Fill in labels
Observe labels for previously recommended batches
TODO(lishal): implement multiple selection methods
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
A list of initialized samplers is allowed as an input because
"for AL_methods that search over different mixtures, may want mixtures to"
have shared AL_methods so that initialization is only performed once for
computation intensive methods like HierarchicalClusteringAL and
states are shared between mixtures.
"If initialized samplers are not provided, initialize them ourselves."
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copy these objects to make sure they are not modified while simulating
trajectories as they are used later by the main run_experiment script.
Assumes that model has already by fit using all labeled data so
the probabilities can be used immediately to hallucinate labels
All models need to have predict_proba method
Hallucinate labels for selected datapoints to be label
using class probabilities from model
"Not saving already_selected here, if saving then should sort"
only for the input to fit but preserve ordering of indices in
already_selected
Useful to know how accuracy compares for model trained on hallucinated
labels vs trained on true labels.  But can remove this train to speed
up simulations.  Won't speed up significantly since many more models
are being trained inside the loop above.
Save trajectory for reference
Delete created copies
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER
Should check model but kernel_svm does not have coef_ so need better
handling here
Make sure that model object fed in did not change during simulations
Return indices based on return type specified
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Set gamma for gaussian kernel to be equal to 1/n_features
kneighbors graph is constructed using k=10
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of"
"another point, make it vice versa"
Graph edges are weighted by applying gaussian kernel to manhattan dist.
"By default, gamma for rbf kernel is equal to 1/n_features but may"
get better results if gamma is tuned.
Define graph density for an observation to be sum of weights for all
edges to the node representing the datapoint.  Normalize sum weights
by total number of neighbors.
"If a neighbor has already been sampled, reduce the graph density"
for its direct neighbors to promote diversity.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update observed reward and arm probabilities
Sample an arm
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
y only used for determining how many clusters there should be
probably not practical to assume we know # of classes before hand
should also probably scale with dimensionality of data
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
TODO(lishal): have MarginSampler and this share margin function
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update min_distances for all examples given new cluster center.
Assumes that the transform function takes in original data and not
flattened data.
Initialize centers with a randomly selected datapoint
New examples should not be in already selected since those points
should have min_distance of zero to a cluster center.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
This is uniform given the remaining pool but biased wrt the entire pool.
sample = [i for i in range(self.X.shape[0]) if i not in already_selected]
return sample[0:N]
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Fields for hierarchical clustering AL
Setting parent and storing nodes in dict for fast access
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Faster and available in Python 3.5 and above
transform_list.append(CenterCrop((processed_size)))
transform_list.append(Lambda(lambda X: normalize(X)))
"print(self.labels_set, self.n_classes)"
"from PyQt5 import QtCore, QtWidgets,QtGui"
from collections import deque
from peewee import *
from UIComponents.Tag import Tag
policy.setHeightForWidth(True)
"print(self.tab1.parentWidget(),self)"
self.tab4.add.clicked.connect(self.addSpecies)
self.tab4.update.clicked.connect(self.updateSpecies)
checkpoint= load_checkpoint('../merge/triplet_model_0054.tar')
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))"
"embedding_net = EmbeddingNet(checkpoint['arch'], checkpoint['feat_dim'])"
if checkpoint['loss_type'].lower()=='center':
"model = torch.nn.DataParallel(ClassificationNet(embedding_net, n_classes=14)).cuda()"
else:
model= torch.nn.DataParallel(embedding_net).cuda()
model.load_state_dict(checkpoint['state_dict'])
self.parentWidget().progressBar.setMaximum(len(run_dataset)//2048)
"e=Engine(model,None,None, verbose=True,progressBar= self.parentWidget().progressBar)"
label = [ x[1] for x in run_dataset.samples]
"print(indices,selected_set)"
print(query.sql())
src.delete().where(src.image_id<<rList))
"det= UserDetection.create(category_id=0, id=str(index+label[1][2]),image_id=final[0], bbox_X=label[1][0], bbox_Y=label[1][1], bbox_W=label[1][2], bbox_H=label[1][3])"
for x in self.tab1.grid.tags:
x.delete_instance()
db.create_tables([Detection])
This is simply to show the bar
"p = Process(target=ex.active, args=())"
p.start()
p.join()
ex.active()
ex.centralWidget().setCurrentIndex(1)
main()
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))"
print(row)
"unq_id= ""crops_""+str(uuid.uuid1())"
"print(line,imageWidth,imageHeight)"
"print(""%s,%s,%s,%0.6f,%0.6f,%0.6f,%0.6f,%0.6f""%(line[0], line[1],line[2],float(line[3]),topRel,leftRel,bottomRel,rightRel))"
if not os.path.exists(dest):
os.mkdir(dest)
raise
out.close()
"print length,(i-1)*length,i*length"
matplotlib.use('Agg')
get the inputs
zero the parameter gradients
forward + backward + optimize
print statistics
get the inputs
forward + backward + optimize
print statistics
get the inputs
zero the parameter gradients
forward + backward + optimize
get the inputs
forward + backward + optimize
print statistics
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
"conf.reset()"""""""
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
conf.reset()
for clf in classifiers:
"clf.fit(X_train, y_train)"
preds= clf.predict_proba(X)
uncertainty+= preds.max(axis=1)
print(uncertainty[ind])
print(uncertainty[ind])
true_labels = y[unlabeled_indices]
select up to 5 digit examples that the classifier is most uncertain about
remember best acc@1 and save checkpoint
"completeClassificationLoop(run_dataset, model,num_classes)"
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)"
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd)
labels = db.labels_
"mapp=(find_probablemap(label,labels, K=args.K))"
"print(""Clusters"")"
"for i,x in enumerate(labels):"
labels[i]= mapp[x]
print(np.sum(labels == label)/labels.size)
"print(""Confidence Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning(embd, label, idx)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(""Entropy Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning_entropy(embd, label, idx)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(idx,idx.shape)"
for i in idx:
print(paths[i])
"print(""Silohette active learning"")"
"idx= active_learning2(embd, 1000, args.num_clusters)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
embd= reduce_dimensionality(embd)#[0:10000])
labels= labels[0:10000]
label= label[0:10000]
paths= paths[0:10000]
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])"
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])"
plt.show()
"np.save(args.name_prefix+""_embeddings.npy"",embd)"
"np.save(args.name_prefix+""_labels.npy"",label)"
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")"
del output
define loss function (criterion) and optimizer
"optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay= args.weight_decay)"
optimizer.load_state_dict(checkpoint['optimizer'])
train for one epoch
evaluate on validation set
matplotlib.use('Agg')
selected_set.add(rand_ind[i])
"embedding_net = EmbeddingNet('resnet50', 256, True)"
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)"
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)"
print('Embedding Done')
sys.stdout.flush()
Random examples to start
"random_ids = noveltySamples(unlabeled_dataset.em, unlabeled_dataset.getIDs(), 1000)"
print(random_ids)
Move Records
Finetune the embedding model
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)"
unlabeled_dataset.updateEmbedding(model)
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})"
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()"
matplotlib.use('Agg')
"uncertainty= np.apply_along_axis(stats.entropy,1,probs) * (1 - probs.max(axis=1))"
"copy(paths[srt[i]], ""active"")"
"plot_together( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()), preds, base_ind, dataset.getpaths(), {})"
"return np.random.choice(range(0,prob_list[0].shape[0]), 100, replace=False).tolist()"
selected_set.add(rand_ind[i])
"query= Detection.update(kind = destKind.value).where(Detection.id.in_(rList), Detection.kind == srcKind.value)"
query.execute()
"embedding_net = EmbeddingNet('resnet50', 256, True)"
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)"
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)"
print('Embedding Done')
sys.stdout.flush()
"plot_embedding(dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})"
Random examples to start
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)"
print(random_ids)
Move Records
Finetune the embedding model
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)"
unlabeled_dataset.updateEmbedding(model)
"print(dataset.em[dataset.current_set].shape, np.asarray(dataset.getlabels()).shape, len(dataset.getpaths()))"
"plot_embedding( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})"
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})"
train_eval_classifier()
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()"
"names = [""Linear SVM"", ""RBF SVM"", ""Random Forest"", ""Neural Net"", ""Naive Bayes""]"
"classifiers = [SVC(kernel=""linear"", C=0.025, probability= True, class_weight='balanced'),"
"SVC(gamma=2, C=1, probability= True, class_weight='balanced'),"
"RandomForestClassifier(max_depth=None, n_estimators=100, class_weight='balanced'),"
"MLPClassifier(alpha=1),"
GaussianNB()]
estimators= []
"for name, clf in zip(names, classifiers):"
"estimators.append((name, clf))"
"eclf1 = VotingClassifier(estimators= estimators, voting='hard')"
"eclf2 = VotingClassifier(estimators= estimators, voting='soft')"
"names.append(""ensemble hard"")"
classifiers.append(eclf1)
"names.append(""ensemble soft"")"
classifiers.append(eclf2)
dataset.image_mode()
dataset.updateEmbedding(model)
y_pred= clf.predict(X_test)
"print(confusion_matrix(y_test, y_pred))"
paths= dataset.getpaths()
"for i, (yp, yt) in enumerate(zip(y_pred, y_test)):"
if yp != yt:
"copy(paths[i],""mistakes"")"
"print(yt, yp, paths[i],i)"
"clf_output= clf_e.embedding(eval_loader, dim=48)"
"self.train_transform = transforms.Compose([Resize(raw_size), CenterCrop((processed_size)), ToTensor(), Normalize(mean, std)])"
"print(self.labels_set, self.n_classes)"
"print(line,imageWidth,imageHeight)"
"print length,(i-1)*length,i*length"
#########################################################
## Configuration
#########################################################
## The actual code
Check arguments
Create output directories
Padding around the detected objects when cropping
1.3 for the cropping during test time and 1.3 for
the context that the CNN requires in the left-over
image
Load a (frozen) Tensorflow model into memory.
## Preparations: get all the output tensors
The following processing is only for single image
Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
Follow the convention by adding back the batch dimension
For all images in the image directoryig
Load image
Run inference
"all outputs are float32 numpy arrays, so convert types as appropriate"
Add detections to the collection
Get info about the image
Select detections with a confidence larger than DETECTION_CONFIDENCE
Get these boxes and convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes"
"However, we need to make sure that it box coordinates are still within the image"
"For each detected bounding box with high confidence, we will"
crop the image to the padded box and save it
generate a unique identifier for the detection
"bbox is the detected box, crop_box the padded / enlarged box"
Add numbering to the original file name if there are multiple boxes
The absolute file path where we will store the image
"if COCO_OUTPUT_DIR is set, then we will only use the shape"
of cropped_img in the following code. So instead of reading
cropped_img = np.array(Image.open(out_file))
we can speed everything up by reading only the size of the image
matplotlib.use('Agg')
conf= ConfusionMatrix(24)
get the inputs
zero the parameter gradients
forward + backward + optimize
print statistics
get the inputs
forward + backward + optimize
print statistics
get the inputs
zero the parameter gradients
forward + backward + optimize
get the inputs
forward + backward + optimize
print statistics
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
"conf.reset()"""""""
define loss function (criterion) and optimizer
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
conf.reset()
for clf in classifiers:
"clf.fit(X_train, y_train)"
preds= clf.predict_proba(X)
uncertainty+= preds.max(axis=1)
print(uncertainty[ind])
print(uncertainty[ind])
true_labels = y[unlabeled_indices]
select up to 5 digit examples that the classifier is most uncertain about
"print(indices,selected_set)"
print(query.sql())
remember best acc@1 and save checkpoint
"completeClassificationLoop(run_dataset, model,num_classes)"
"embd, label, paths = extract_embeddings(run_loader, model)"
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)"
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd)
labels = db.labels_
"mapp=(find_probablemap(label,labels, K=args.K))"
"print(""Clusters"")"
"for i,x in enumerate(labels):"
labels[i]= mapp[x]
print(np.sum(labels == label)/labels.size)
"print(""Confidence Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning(embd, label, idx)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(""Entropy Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning_entropy(embd, label, idx)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
"new_selected= selectSamples(embd,paths,3000)"
"print(idx,idx.shape)"
for i in idx:
print(paths[i])
"print(""Silohette active learning"")"
"idx= active_learning2(embd, 1000, args.num_clusters)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(""Random"")"
"idx = np.random.choice(np.arange(len(paths)), 1000, replace=False)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
embd= reduce_dimensionality(embd)#[0:10000])
labels= labels[0:10000]
label= label[0:10000]
paths= paths[0:10000]
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])"
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])"
plt.show()
"np.save(args.name_prefix+""_embeddings.npy"",embd)"
"np.save(args.name_prefix+""_labels.npy"",label)"
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")"
combo.setEnabled(not finalized)
print(self.bbox)
self.addWidget(self.child)
Moving container with arrows
Left - Bottom
Right-Bottom
Left-Top
Right-Top
Left - Bottom
Right - Bottom
Left - Top
Right - Top
check cursor horizontal position
check cursor vertical position
self.resizeEvent=self.onResize
"print(""Parent"", parent, parent.width(), parent.height())"
"self.setGeometry(0,0,410,307)"
"print(w,h,""w,h"")"
"print(""final"",tag.getFinal())"
pass
"self.tags.append(TContainer(self,Category.get(-1),[0,0,0.1,0.1],True, Qt.red))"
label= CharField()
fullname=str(self.model)
"self.name= (fullname[fullname.find("":"")+2:fullname.find("">"")].strip()+'_set').lower()"
"print(""Parent"", self.parentWidget().width(), self.parentWidget().height())"
"print(self.model,self.name,query.sql())"
self.tab4.speciesList.setModel(species)
"self.tab4.speciesList.setRowHidden(len(species.stringList())-1, True)"
self.speciesList.itemChanged.connect(self.itemChanged)
#####
""
video_utils.py
""
"Utilities for splitting, rendering, and assembling videos."
""
#####
"%% Constants, imports, environment"
from ai4eutils
%% Path utilities
%% Function for rendering frames to video and vice-versa
http://tsaith.github.io/combine-images-into-a-video-with-python-3-and-opencv-3.html
Determine the width and height from the first image
Define the codec and create VideoWriter object
"If we're not over-writing, check whether all frame images already exist"
"OpenCV seems to over-report the number of frames by 1 in some cases, or fails"
"to read the last frame; either way, I'm allowing one missing frame."
"print(""Rendering video {}, couldn't find frame {}"".format("
"input_video_file,missing_frame_number))"
...if we need to check whether to skip this video entirely
"for frame_number in tqdm(range(0,n_frames)):"
print('Skipping frame {}'.format(frame_filename))
Recursively enumerate video files
Create the target output folder
Render frames
input_video_file = input_fn_absolute; output_folder = output_folder_video
For each video
""
input_fn_relative = input_files_relative_paths[0]
"process_detection_with_options = partial(process_detection, options=options)"
zero-indexed
Load results
# Break into videos
im = images[0]
# For each video...
video_name = list(video_to_frames.keys())[0]
frame = frames[0]
At most one detection for each category for the whole video
category_id = list(detection_categories.keys())[0]
Find the nth-highest-confidence video to choose a confidence value
Prepare the output representation for this video
'max_detection_conf' is no longer included in output files by default
...for each video
Write the output file
%% Test driver
%% Constants
%% Split videos into frames
"%% List image files, break into folders"
Find unique folders
fn = frame_files[0]
%% Load detector output
%% Render detector frames
folder = list(folders)[0]
d = detection_results_this_folder[0]
...for each file in this folder
...for each folder
%% Render output videos
folder = list(folders)[0]
...for each video
""
run_inference_with_yolov5_val.py
""
Runs a folder of images through MegaDetector (or another YOLOv5 model) with YOLOv5's
"val.py, converting the output to the standard MD format.  The main goal is to leverage"
YOLO's test-time augmentation tools.
""
"YOLOv5's val.py uses each file's base name as a unique identifier, which doesn't work"
when you have typical camera trap images like:
""
a/b/c/RECONYX0001.JPG
d/e/f/RECONYX0001.JPG
""
...so this script jumps through a bunch of hoops to put a symlinks in a flat
"folder, run YOLOv5 on that folder, and map the results back to the real files."
""
"Currently requires the user to supply the path where a working YOLOv5 install lives,"
and assumes that the current conda environment is all set up for YOLOv5.
""
TODO:
""
* Figure out what happens when images are corrupted... right now this is the #1
"reason not to use this script, it may be the case that corrupted images look the"
same as empty images.
""
* Multiple GPU support
""
* Checkpointing
""
* Windows support (I have no idea what all the symlink operations will do on Windows)
""
"* Support alternative class names at the command line (currently defaults to MD classes,"
though other class names can be supplied programmatically)
""
%% Imports
%% Options class
# Required ##
# Optional ##
%% Main function
#%% Path handling
#%% Enumerate images
#%% Create symlinks to give a unique ID to each image
i_image = 0; image_fn = image_files_absolute[i_image]
...for each image
#%% Create the dataset file
Category IDs need to be continuous integers starting at 0
#%% Prepare YOLOv5 command
#%% Run YOLOv5 command
#%% Convert results to MD format
"We'll use the absolute path as a relative path, and pass '/'"
as the base path in this case.
#%% Clean up
...def run_inference_with_yolo_val()
%% Command-line driver
%% Scrap
%% Test driver (folder)
%% Test driver (file)
%% Preview results
options.sample_seed = 0
...for each prediction file
%% Compare results
Choose all pairwise combinations of the files in [filenames]
"%% Constants, imports, environment"
from multiprocessing.pool import ThreadPool as workerpool
Number of images to pre-fetch
Useful hack to force CPU inference.
""
"Need to do this before any PT/TF imports, which happen when we import"
from run_detector.
Numpy FutureWarnings from tensorflow import
%% Support functions for multiprocessing
TODO
""
The queue system is a little more elegant if we start one thread for reading and one
"for processing, and this works fine on Windows, but because we import TF at module load,"
"CUDA will only work in the main process, so currently the consumer function runs here."
""
"To enable proper multi-GPU support, we may need to move the TF import to a separate module"
that isn't loaded until very close to where inference actually happens.
%% Other support funtions
%% Image processing functions
%% Main function
Handle the case where image_file_names is not yet actually a list
"Find the images to score; images can be a directory, may need to recurse"
A json list of image paths
A single image file
Load the detector
Does not count those already processed
Will not add additional entries not in the starter checkpoint
Write a checkpoint if necessary
"Back up any previous checkpoints, to protect against crashes while we're writing"
the checkpoint file.
Write the new checkpoint
Remove the backup checkpoint if it exists
...if it's time to make a checkpoint
"When using multiprocessing, let the workers load the model"
"Results may have been modified in place, but we also return it for"
backwards-compatibility.
The typical case: we need to build the 'info' struct
"If the caller supplied the entire ""info"" struct"
"The 'max_detection_conf' field used to be included by default, and it caused all kinds"
"of headaches, so it's no longer included unless the user explicitly requests it."
%% Interactive driver
%%
%% Command-line driver
This is an experimental hack to allow the use of non-MD YOLOv5 models through
the same infrastructure; it disables the code that enforces MDv5-like class lists.
Load the checkpoint if available
""
Relative file names are only output at the end; all file paths in the checkpoint are
still full paths.
"Find the images to score; images can be a directory, may need to recurse"
A json list of image paths
A single image file
Test that we can write to the output_file's dir if checkpointing requested
"Don't overwrite existing checkpoint files, this is a sure-fire way to eventually"
erase someone's checkpoint.
"Commenting this out for now... the scenario where we are resuming from a checkpoint,"
then immediately overwrite that checkpoint with empty data is higher-risk than the
annoyance of crashing a few minutes after starting a job.
Confirm that we can write to the checkpoint path; this avoids issues where
we crash after several thousand images.
%% Imports
import pre- and post-processing functions from the YOLOv5 repo
scale_coords() became scale_boxes() in later YOLOv5 versions
%% Classes
padded resize
"Image size can be an int (which translates to a square target size) or (h,w)"
...if the caller has specified an image size
NMS
"As of v1.13.0.dev20220824, nms is not implemented for MPS."
""
Send predication back to the CPU to fix.
format detections/bounding boxes
"This is a loop over detection batches, which will always be length 1 in our case,"
since we're not doing batch inference.
Rescale boxes from img_size to im0 size
"normalized center-x, center-y, width and height"
"MegaDetector output format's categories start at 1, but the MD"
model's categories start at 0.
...for each detection in this batch
...if this is a non-empty batch
...for each detection batch
...try
for testing
"MegaDetector was trained with batch size of 1, and the resizing function is a part"
of the inference graph
"change from [y1, x1, y2, x2] to [x1, y1, width, height]"
convert numpy floats to Python floats
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size
"np_images = [np.asarray(image, np.uint8) for image in images]"
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)"
performs inference
our batch size is 1; need to loop the batch dim if supporting batch size > 1
"%% Constants, imports, environment"
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Numpy FutureWarnings from tensorflow import
Useful hack to force CPU inference
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
An enumeration of failure reasons
Number of decimal places to round to for confidence and bbox coordinates
Label mapping for MegaDetector
Should we allow classes that don't look anything like the MegaDetector classes?
"Each version of the detector is associated with some ""typical"" values"
"that are included in output files, so that downstream applications can"
use them as defaults.
%% Classes
Stick this into filenames before the extension for the rendered result
%% Utility functions
mps backend only available in torch >= 1.12.0
%% Main function
Dictionary mapping output file names to a collision-avoidance count.
""
"Since we'll be writing a bunch of files to the same folder, we rename"
as necessary to avoid collisions.
...def input_file_to_detection_file()
Image is modified in place
...for each image
...def load_and_run_detector()
%% Command-line driver
Must specify either an image file or a directory
"but for a single image, args.image_dir is also None"
%% Interactive driver
%%
#####
""
process_video.py
""
"Split a video (or folder of videos) into frames, run the frames through run_detector_batch.py,"
and optionally stitch together results into a new video with detection boxes.
""
TODO: allow video rendering when processing a whole folder
TODO: allow video rendering from existing results
""
#####
"%% Constants, imports, environment"
Only relevant if render_output_video is True
Folder to use for extracted frames
Folder to use for rendered frames (if rendering output video)
Should we render a video with detection boxes?
""
"Only supported when processing a single video, not a folder."
"If we are rendering boxes to a new video, should we keep the temporary"
rendered frames?
Should we keep the extracted frames?
%% Main functions
"TODO: keep track of whether we created this folder, delete if we're deleting the extracted"
"frames and we created the folder, and the output files aren't in the same folder.  For now,"
we're just deleting the extracted frames and leaving the empty folder around in this case.
Render detections to images
"TODO: keep track of whether we created this folder, delete if we're deleting the rendered"
"frames and we created the folder, and the output files aren't in the same folder.  For now,"
we're just deleting the rendered frames and leaving the empty folder around in this case.
Combine into a video
Delete the temporary directory we used for detection images
shutil.rmtree(rendering_output_dir)
(Optionally) delete the frames on which we ran MegaDetector
shutil.rmtree(frame_output_folder)
...process_video()
# Validate options
# Split every video into frames
# Run MegaDetector on the extracted frames
# (Optionally) delete the frames on which we ran MegaDetector
shutil.rmtree(frame_output_folder)
# Convert frame-level results to video-level results
...process_video_folder()
%% Interactive driver
%% Process a folder of videos
import clipboard; clipboard.copy(cmd)
%% Process a single video
import clipboard; clipboard.copy(cmd)
"%% Render a folder of videos, one file at a time"
import clipboard; clipboard.copy(s)
%% Command-line driver
Lint as: python3
Copyright 2020 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TPU is automatically inferred if tpu_name is None and
we are running under cloud ai-platform.
""
copy_checkpoints.py
""
Run this script with specified source_dir and target_dir while the model is training to make a copy
of every checkpoint (checkpoints are kept once an hour by default and is difficult to adjust)
""
do not copy event or evaluation results
input validation
plot the images
adjust the figure
"read in dataset CSV and create merged (dataset, location) col"
map label to label_index
load the splits
only weight the training set by detection confidence
TODO: consider weighting val and test set as well
isotonic regression calibration of MegaDetector confidence
treat each split separately
new_weight[i] = confidence[i] * (n / c) / total_confidence(i's label)
- n = # examples in split (weighted by confidence); c = # labels
- weight allocated to each label is n/c
"- within each label, weigh each example proportional to confidence"
- new weights sum to n
error checking
"maps output label name to set of (dataset, dataset_label) tuples"
find which other label (label_b) has intersection
input validation
create label index JSON
look into sklearn.preprocessing.MultiLabelBinarizer
Note: JSON always saves keys as strings!
"True for ground truth, False for MegaDetector"
always save as .jpg for consistency
get bounding boxes
convert from category ID to category name
"check if crops are already downloaded, and ignore bboxes below the"
confidence threshold
assign all images without location info to 'unknown_location'
remove images from labels that have fewer than min_locs locations
merge dataset and location into a single string '<dataset>/<location>'
"create DataFrame of counts. rows = locations, columns = labels"
label_count: label => number of examples
loc_count: label => number of locs containing that label
generate a new split
score the split
SSE for # of images per label (with 2x weight)
SSE for # of locs per label
label => list of datasets to prioritize for test and validation sets
"merge dataset and location into a tuple (dataset, location)"
sorted smallest to largest
greedily add to test set until it has >= 15% of images
sort the resulting locs
"modify loc_to_size in place, so copy its keys before iterating"
arguments relevant to both creating the dataset CSV and splits.json
arguments only relevant for creating the dataset CSV
arguments only relevant for creating the splits JSON
comment lines starting with '#' are allowed
""
prepare_classification_script.py
""
Notebook-y script used to prepare a series of shell commands to run a classifier
(other than MegaClassifier) on a MegaDetector result set.
""
Differs from prepare_classification_script_mc.py only in the final class mapping step.
""
%% Job options
%% Constants
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
%% Set up environment
commands.append('cd CameraTraps/classification\n')
commands.append('conda activate cameratraps-classifier\n')
%% Crop images
fn = input_files[0]
%% Run classifier
fn = input_files[0]
%% Merge classification and detection outputs
fn = input_files[0]
%% Write everything out
input validation
"Evaluating with accimage is much faster than Pillow or Pillow-SIMD, but accimage"
is Linux-only.
create output directory
override saved params with kwargs
"For now, we don't weight crops by detection confidence during"
evaluation. But consider changing this.
"create model, compile with TorchScript if given checkpoint is not compiled"
"verify that target names matches original ""label names"" from dataset"
"if the dataset does not already have a 'other' category, then the"
'other' category must come last in label_names to avoid conflicting
with an existing label_id
define loss function (criterion)
"this file ends up being huge, so we GZIP compress it"
double check that the accuracy metrics are computed properly
save the confusion matrices to .npz
save per-label statistics
set dropout and BN layers to eval mode
"even if batch contains sample weights, don't use them"
Do target mapping on the outputs (unnormalized logits) instead of
"the normalized (softmax) probabilities, because the loss function"
uses unnormalized logits. Summing probabilities is equivalent to
log-sum-exp of unnormalized logits.
"a confusion matrix C is such that C[i,j] is the # of observations known to"
be in group i and predicted to be in group j.
match pytorch EfficientNet model names
images dataset
"for smaller disk / memory usage, we cache the raw JPEG bytes instead"
of the decoded Tensor
convert JPEG bytes to a 3D uint8 Tensor
"keras EfficientNet already includes normalization from [0, 255] to [0, 1],"
so we don't need to do that here
labels dataset
img_files dataset
weights dataset
define the transforms
efficientnet data preprocessing:
- train:
"1) random crop: aspect_ratio_range=(0.75, 1.33), area_range=(0.08, 1.0)"
2) bicubic resize to img_size
3) random horizontal flip
- test:
1) center crop
2) bicubic resize to img_size
weights sums to:
- if weight_by_detection_conf: (# images in split - conf delta)
- otherwise: (# images in split)
"freeze the base model's weights, including BatchNorm statistics"
https://www.tensorflow.org/guide/keras/transfer_learning#fine-tuning
rebuild output
input validation
set seed
create logdir and save params
create dataloaders and log the index_to_label mapping
define loss function and optimizer
using EfficientNet training defaults
- batch norm momentum: 0.99
"- optimizer: RMSProp, decay 0.9 and momentum 0.9"
- epochs: 350
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs"
- weight decay: 1e-5
TODO: change weighted to False if oversampling minority classes
stop training after 8 epochs without improvement
log metrics
log confusion matrix
log tp/fp/fn images
"tf.summary.image requires input of shape [N, H, W, C]"
false positive for top3_pred[0]
false negative for label
"if evaluating or finetuning, set dropout & BN layers to eval mode"
"for each label, track 5 most-confident and least-confident examples"
"even if batch contains sample weights, don't use them"
we do not track L2-regularization loss in the loss metric
This dictionary will get written out at the end of this process; store
diagnostic variables here
error checking
refresh detection cache
save log of bad images
cache of Detector outputs: dataset name => {img_path => detection_dict}
img_path: <dataset-name>/<img-filename>
get SAS URL for images container
strip image paths of dataset name
save list of dataset names and task IDs for resuming
complete task name: 'detect_for_classifier_caltech_20200722_110816_task01'
HACK! Sleep for 10s between task submissions in the hopes that it
"decreases the chance of backend JSON ""database"" corruption"
task still running => continue
"task finished successfully, save response to disk"
error checking before we download and crop any images
convert from category ID to category name
we need the datasets table for getting SAS keys
"we already did all error checking above, so we don't do any here"
get ContainerClient
get bounding boxes
we must include the dataset <ds> in <crop_path_template> because
'{img_path}' actually gets populated with <img_file> in
load_and_crop()
"get the image, either from disk or from Blob Storage"
inelegant way to close the container_clients
""
prepare_classification_script_mc.py
""
Notebook-y script used to prepare a series of shell commands to run MegaClassifier
on a MegaDetector result set.
""
Differs from prepare_classification_script.py only in the final class mapping step.
""
%% Job options
%% Constants
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
%% Set up environment
commands.append('cd CameraTraps/classification\n')
commands.append('conda activate cameratraps-classifier\n')
%% Crop images
fn = input_files[0]
%% Run classifier
fn = input_files[0]
%% Remap classifier outputs
fn = input_files[0]
%% Merge classification and detection outputs
fn = input_files[0]
%% Write everything out
mean/std values from https://pytorch.org/docs/stable/torchvision/models.html
define the transforms
resizes smaller edge to img_size
weights sums to:
- if weight_by_detection_conf: (# images in split - conf delta)
- otherwise: # images in split
for normal (non-weighted) shuffling
set all parameters to not require gradients except final FC layer
replace final fully-connected layer (which has 1000 ImageNet classes)
"detect GPU, use all if available"
input validation
set seed
create logdir and save params
create dataloaders and log the index_to_label mapping
create model
define loss function and optimizer
using EfficientNet training defaults
- batch norm momentum: 0.99
"- optimizer: RMSProp, decay 0.9 and momentum 0.9"
- epochs: 350
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs"
- weight decay: 1e-5
stop training after 8 epochs without improvement
do a complete evaluation run
log metrics
log confusion matrix
log tp/fp/fn images
"for every image: undo normalization, clamp to [0, 1], CHW -> HWC"
"- cannot be in-place, because the HeapItem might be in multiple heaps"
writer.add_figure() has issues => using add_image() instead
"writer.add_figure(f'{label_name}/{tag}', fig, global_step=epoch)"
false positive for top3_pred[0]
false negative for label
"preds and labels both have shape [N, k]"
"if evaluating or finetuning, set dropout and BN layers to eval mode"
"for each label, track k_extreme most-confident and least-confident images"
"even if batch contains sample weights, don't use them"
lock before changing ImageFile.LOAD_TRUNCATED_IMAGES
input validation
use MegaDB to generate list of images
only keep images that:
"1) end in a supported file extension, and"
2) actually exist in Azure Blob Storage
3) belong to a label with at least min_locs locations
write out log of images / labels that were removed
"save label counts, pre-subsampling"
"save label counts, post-subsampling"
spec_dict['taxa']: list of dict
[
"{'level': 'family', 'name': 'cervidae', 'datasets': ['idfg']},"
"{'level': 'genus',  'name': 'meleagris'}"
]
"spec_dict['dataset_labels']: dict, dataset => list of dataset_label"
{
"""idfg"": [""deer"", ""elk"", ""prong""],"
"""idfg_swwlf_2019"": [""elk"", ""muledeer"", ""whitetaileddeer""]"
}
"maps output label name to set of (dataset, dataset_label) tuples"
"because MegaDB is organized by dataset, we do the same"
ds_to_labels = {
'dataset_name': {
"'dataset_label': [output_label1, output_label2]"
}
}
we need the datasets table for getting full image paths
The line
"[img.class[0], seq.class[0]][0] as class"
selects the image-level class label if available. Otherwise it selects the
"sequence-level class label. This line assumes the following conditions,"
expressed in the WHERE clause:
- at least one of the image or sequence class label is given
- the image and sequence class labels are arrays with length at most 1
- the image class label takes priority over the sequence class label
""
"In Azure Cosmos DB, if a field is not defined, then it is simply excluded"
"from the result. For example, on the following JSON object,"
{
"""dataset"": ""camera_traps"","
"""seq_id"": ""1234"","
"""location"": ""A1"","
"""images"": [{""file"": ""abcd.jpeg""}],"
"""class"": [""deer""],"
}
"the array [img.class[0], seq.class[0]] just gives ['deer'] because"
img.class is undefined and therefore excluded.
"if no path prefix, set it to the empty string '', because"
"os.path.join('', x, '') = '{x}/'"
result keys
"- already has: ['dataset', 'location', 'file', 'class', 'bbox']"
"- add ['label'], remove ['file']"
"if img is mislabeled, but we don't know the correct class, skip it"
"otherwise, update the img with the correct class, but skip the"
img if the correct class is not one we queried for
sort keys for determinism
we need the datasets table for getting SAS keys
strip leading '?' from SAS token
only check Azure Blob Storage
check local directory first before checking Azure Blob Storage
1st pass: populate label_to_locs
"label (tuple of str) => set of (dataset, location)"
2nd pass: eliminate bad images
prioritize is a list of prioritization levels
number of already matching images
main(
"label_spec_json_path='idfg_classes.json',"
"taxonomy_csv_path='../../camera-traps-private/camera_trap_taxonomy_mapping.csv',"
"output_dir='run_idfg',"
json_indent=4)
recursively find all files in cropped_images_dir
only find crops of images from detections JSON
resizes smaller edge to img_size
"Evaluating with accimage is much faster than Pillow or Pillow-SIMD, but accimage"
is Linux-only.
create dataset
create model
set dropout and BN layers to eval mode
load files
dataset => set of img_file
crop_path: <dataset>/<img_file>___cropXX_mdvY.Y.jpg
[----<img_path>----]
error checking
any row with 'correct_class' should be marked 'mislabeled'
filter to only the mislabeled rows
convert '\' to '/'
verify that overlapping indices are the same
"""add"" any new mislabelings"
write out results
error checking
load detections JSON
get detector version
convert from category ID to category name
copy keys to modify dict in-place
This will be removed later when we filter for animals
save log of bad images
"True for ground truth, False for MegaDetector"
always save as .jpg for consistency
"we already did all error checking above, so we don't do any here"
"get the image, either from disk or from Blob Storage"
inelegant way to close the container_client
"crop_path => normalized bbox coordinates [xmin, ymin, width, height]"
"only ground-truth bboxes do not have a ""confidence"" value"
try loading image from local directory
try to download image from Blob Storage
crop the image
"expand box width or height to be square, but limit to img size"
"Image.crop() takes box=[left, upper, right, lower]"
pad to square using 0s
Author: lukemelas (github username)
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch
With adjustments and added comments by workingcoder (github username).
Support the construction of 'efficientnet-l2' without pretrained weights
Expansion phase (Inverted Bottleneck)
"image_size = calculate_output_image_size(image_size, 1) <-- this wouldn't modify image_size"
Depthwise convolution phase
"Squeeze and Excitation layer, if desired"
Pointwise convolution phase
Expansion and Depthwise Convolution
Squeeze and Excitation
Pointwise Convolution
Skip connection and drop connect
The combination of skip connection and drop connect brings about stochastic depth.
Batch norm parameters
Get stem static or dynamic convolution depending on image size
Stem
Build blocks
Update block input and output filters based on depth multiplier.
The first block needs to take care of stride and filter size increase.
"image_size = calculate_output_image_size(image_size, block_args.stride)  # stride = 1"
Head
Final linear layer
Stem
Blocks
Head
Stem
Blocks
Head
Convolution layers
Pooling and final linear layer
Author: lukemelas (github username)
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch
With adjustments and added comments by workingcoder (github username).
###############################################################################
## Help functions for model architecture
###############################################################################
GlobalParams and BlockArgs: Two namedtuples
Swish and MemoryEfficientSwish: Two implementations of the method
round_filters and round_repeats:
Functions to calculate params for scaling model width and depth ! ! !
get_width_and_height_from_size and calculate_output_image_size
drop_connect: A structural design
get_same_padding_conv2d:
Conv2dDynamicSamePadding
Conv2dStaticSamePadding
get_same_padding_maxPool2d:
MaxPool2dDynamicSamePadding
MaxPool2dStaticSamePadding
"It's an additional function, not used in EfficientNet,"
but can be used in other model (such as EfficientDet).
"Parameters for the entire model (stem, all blocks, and head)"
Parameters for an individual model block
Set GlobalParams and BlockArgs's defaults
An ordinary implementation of Swish function
A memory-efficient implementation of Swish function
TODO: modify the params names.
"maybe the names (width_divisor,min_width)"
"are more suitable than (depth_divisor,min_depth)."
follow the formula transferred from official TensorFlow implementation
follow the formula transferred from official TensorFlow implementation
"generate binary_tensor mask according to probability (p for 0, 1-p for 1)"
Note:
The following 'SamePadding' functions make output size equal ceil(input size/stride).
"Only when stride equals 1, can the output size be the same as input size."
Don't be confused by their function names ! ! !
Tips for 'SAME' mode padding.
Given the following:
i: width or height
s: stride
k: kernel size
d: dilation
p: padding
Output after Conv2d:
o = floor((i+p-((k-1)*d+1))/s+1)
"If o equals i, i = floor((i+p-((k-1)*d+1))/s+1),"
=> p = (i-1)*s+((k-1)*d+1)-i
With the same calculation as Conv2dDynamicSamePadding
Calculate padding based on image size and save it
Calculate padding based on image size and save it
###############################################################################
## Helper functions for loading model params
###############################################################################
BlockDecoder: A Class for encoding and decoding BlockArgs
efficientnet_params: A function to query compound coefficient
get_model_params and efficientnet:
Functions to get BlockArgs and GlobalParams for efficientnet
url_map and url_map_advprop: Dicts of url_map for pretrained weights
load_pretrained_weights: A function to load pretrained weights
Check stride
"Coefficients:   width,depth,res,dropout"
Blocks args for the whole model(efficientnet-b0 by default)
It will be modified in the construction of EfficientNet Class according to model
note: all models have drop connect rate = 0.2
ValueError will be raised here if override_params has fields not included in global_params.
train with Standard methods
check more details in paper(EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks)
train with Adversarial Examples(AdvProp)
check more details in paper(Adversarial Examples Improve Image Recognition)
TODO: add the petrained weights url map of 'efficientnet-l2'
AutoAugment or Advprop (different preprocessing)
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils)
from data_management.megadb.schema import sequences_schema_check
resize is for displaying them more quickly
dataset and seq_id are required fields
sort the images in the sequence
"if no class label on the image, show class label on the sequence"
pool = ThreadPool()
"print('len of rendering_info', len(rendering_info))"
"tqdm(pool.imap_unordered(render_image_info_partial, rendering_info), total=len(rendering_info))"
options = write_html_image_list()
options['headerHtml']
print('Checking that the MegaDB entries conform to the schema...')
sequences_schema_check.sequences_schema_check(sequences)
#######
""
visualize_db.py
""
Outputs an HTML page visualizing annotations (class labels and/or bounding boxes)
on a sample of images in a database in the COCO Camera Traps format
""
#######
%% Imports
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils)
Assumes the cameratraps repo root is on the path
%% Settings
Set to None to visualize all images
Target size for rendering; set either dimension to -1 to preserve aspect ratio
"These are mutually exclusive; both are category names, not IDs"
"Special tag used to say ""show me all images with multiple categories"""
We sometimes flatten image directories by replacing a path separator with
another character.  Leave blank for the typical case where this isn't necessary.
Control rendering parallelization
Process-based parallelization in this function is currently unsupported
"due to pickling issues I didn't care to look at, but I'm going to just"
"flip this with a warning, since I intend to support it in the future."
%% Helper functions
"Translate the file name in an image entry in the json database to a path, possibly doing"
some manipulation of path separators
%% Core functions
"Optionally remove all images without bounding boxes, *before* sampling"
"Optionally include/remove images with specific labels, *before* sampling"
Put the annotations in a dataframe so we can select all annotations for a given image
Construct label map
Take a sample of images
Set of dicts representing inputs to render_db_bounding_boxes:
""
"bboxes, boxClasses, image_path"
All the class labels we've seen for this image (with out without bboxes)
Iterate over annotations for this image
iAnn = 0; anno = annos_i.iloc[iAnn]
...for each of this image's annotations
"We're adding html for an image before we render it, so it's possible this image will"
fail to render.  For applications where this script is being used to debua a database
"(the common case?), this is useful behavior, for other applications, this is annoying."
...for each image
...def render_image_info
def process_images(...)
%% Command-line driver
"Copy all fields from a Namespace (i.e., the output from parse_args) to an object."
""
Skips fields starting with _.  Does not check existence in the target object.
Convert to an options object
%% Interactive driver(s)
%%
os.startfile(htmlOutputFile)
%% Constants and imports
convert category ID from int to str
Retry on blob storage read failures
%% Functions
PIL.Image.convert() returns a converted copy of this image
Alter orientation as needed according to EXIF tag 0x112 (274) for Orientation
""
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28
https://www.media.mit.edu/pia/Research/deepview/exif.html
""
Null operation
Aspect ratio as width over height
ar = w / h
h = w / ar
w = ar * h
The following three functions are modified versions of those at:
""
https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py
Convert to pixels so we can use the PIL crop() function
PIL's crop() does surprising things if you provide values outside of
"the image, clip inputs"
...if this detection is above threshold
...for each detection
"list of lists, one list of strings for each bounding box (to accommodate multiple labels)"
for color selection
"Always render objects with a confidence of ""None"", this is typically used"
for ground truth data.
"{} is the default, which means ""show labels with no mapping"", so don't use ""if label_map"" here"
if label_map:
"To avoid duplicate colors with detection-only visualization, offset"
the classification class index by the number of detection classes
...for each classification
...if we have classification results
...if the confidence of this detection is above threshold
...for each detection
"print('Input must be of size [N, 4], but is ' + str(boxes_shape))"
Deliberately trimming to the width of the image only in the case where
"box expansion is turned on.  There's not an obvious correct behavior here,"
but the thinking is that if the caller provided an out-of-range bounding
"box, they meant to do that, but at least in the eyes of the person writing"
"this comment, if you expand a box for visualization reasons, you don't want"
to end up with part of a box.
""
A slightly more sophisticated might check whether it was in fact the expansion
"that made this box larger than the image, but this is the case 99.999% of the time"
"here, so that doesn't seem necessary."
...if we need to expand boxes
If the total height of the display strings added to the top of the bounding
"box exceeds the top of the image, stack the strings below the bounding box"
instead of above.
Each display_str has a top and bottom margin of 0.05x.
Reverse list and print from bottom to top.
Skip empty strings
"list of lists, one list of strings for each bounding box (to accommodate multiple labels)"
need to be a string here because PIL needs to iterate through chars
""
stacked bar charts are made with each segment starting from a y position
"To fit the legend in, shrink current axis by 20%"
Put a legend to the right of the current axis
####
""
visualize_incoming_annotations.py
""
Spot-check the annotations received from iMerit by visualizing annotated bounding
boxes on a sample of images and display them in HTML.
""
"Modified in 2021 March to use the new format (iMerit batch 12 onwards), which is a"
COCO formatted JSON with relative coordinates for the bbox.
""
from data_management.megadb.schema import sequences_schema_check
we used frame_num of 1 when sending out images to annotators when it is not explicitly stored (wcs esp.)
The file_name field in the incoming json looks like
alka_squirrels.seq2020_05_07_25C.frame119221.jpg
"we need to use the dataset, sequence and frame info to find the actual path in blob storage"
using the sequences
category_id 5 is No Object Visible
download the image
Write to HTML
allow forward references in typing annotations
class variables
instance variables
get path to root
taxonomy CSV rows without 'taxonomy_string' entries are excluded
"from the taxonomy graph, but can be included in a classification"
label specification JSON via the 'dataset_labels' key
use the lower parent
special cases
%% Imports
%% Taxnomy checking
taxonomy CSV rows without 'taxonomy_string' entries are excluded
"from the taxonomy graph, but can be included in a classification"
label specification JSON via the 'dataset_labels' key
...for each row in the taxnomy file
%% Command-line driver
%% Interactive driver
%%
which datasets are already processed?
"sequence-level query should be fairly fast, ~1 sec"
cases when the class field is on the image level (images in a sequence
"that had different class labels, 'caltech' dataset is like this)"
"this query may take a long time, >1hr"
"this query should be fairly fast, ~1 sec"
read species presence info from the JSON files for each dataset
has this class name appeared in a previous dataset?
columns to populate the spreadsheet
sort by descending species count
make the spreadsheet
hyperlink Bing search URLs
hyperlink example image SAS URLs
TODO hardcoded columns: change if # of examples or col_order changes
""
map_lila_taxonomy_to_wi_taxonomy.py
""
Loads the LILA category mapping (in which taxonomy information comes from an iNat taxonomy snapshot)
and tries to map each class to the Wildlife Insights taxonomy.
""
%% Constants and imports
Created by get_lila_category_list.py... contains counts for each category
This is a manually-curated file used to store mappings that had to be made manually
This is the main output file from this whole process
%% Load category and taxonomy files
%% Pull everything out of pandas
%% Cache WI taxonomy lookups
This is just a handy lookup table that we'll use to debug mismatches
taxon = wi_taxonomy[21653]; print(taxon)
Look for keywords that don't refer to specific taxa: blank/animal/unknown
Do we have a species name?
"If 'species' is populated, 'genus' should always be populated; one item currently breaks"
this rule.
...for each taxon
%% Find redundant taxa
%% Manual review of redundant taxa
%% Clean up redundant taxa
taxon_name = list(taxon_name_to_preferred_taxon_id.keys())[0]
"If we've gotten this far, we should be choosing from multiple taxa."
""
"This will become untrue if any of these are resolved later, at which point we shoudl"
remove them from taxon_name_to_preferred_id
Choose the preferred taxa
%% Read supplementary mappings
"Each line is [lila query],[WI taxon name],[notes]"
%% Map LILA categories to WI categories
Must be ordered from kingdom --> species
TODO:
"['subspecies','variety']"
i_taxon = 0; taxon = lila_taxonomy[i_taxon]; print(taxon)
"Go from kingdom --> species, choosing the lowest-level description as the query"
"E.g., 'car'"
"print('Made a supplementary mapping from {} to {}'.format(query,wi_taxon['taxon_name']))"
print('No match for {}'.format(query))
...for each LILA taxon
%% Manual mapping
%% Build a dictionary from LILA dataset names and categories to LILA taxa
i_d = 0; d = lila_taxonomy[i_d]
"%% Map LILA datasets to WI taxa, and count the number of each taxon available in each dataset"
dataset_name = list(lila_dataset_to_categories.keys())[0]
dataset_category = dataset_categories[0]
"Write out the dataset name, category name, WI GUID, WI scientific name, WI common name,"
and count
...for each category in this dataset
...for each dataset
...with open()
""
retrieve_sample_image.py
""
"Downloader that retrieves images from Google images, used for verifying taxonomy"
"lookups and looking for egregious mismappings (e.g., ""snake"" being mapped to a fish called"
"""snake"")."
""
"Simple wrapper around simple_image_download, but I've had to swap in and out the underlying"
downloader a few times.
""
%% Imports and environment
%%
%% Main entry point
%% Test driver
%%
""
"Using the taxonomy .csv file, map all LILA datasets to the standard taxonomy"
""
Does not currently produce results; this is just used to confirm that all category names
have mappings in the taxonomy file.
""
%% Constants and imports
Created by get_lila_category_list.py... contains counts for each category
%% Load category and taxonomy files
%% Map dataset names and category names to scientific names
i_row = 1; row = taxonomy_df.iloc[i_row]; row
"%% For each dataset, make sure we can map every category to the taxonomy"
dataset_name = list(lila_dataset_to_categories.keys())[0]
c = categories[0]
""
"Takes the megadb taxonomy mapping, extracts the rows that are relevant to"
"LILA, and does some cleanup."
""
%% Constants and imports
This is a partially-completed taxonomy file that was created from a different set of
"scripts, but covers *most* of LILA as of June 2022"
Created by get_lila_category_list.py
%% Read the input files
Get everything out of pandas
"%% Find all unique dataset names in the input list, compare them with data names from LILA"
d = input_taxonomy_rows[0]
The script that generates this dictionary creates a separate entry for bounding box
"metadata files, but those don't represent new dataset names"
%% Map input columns to output datasets
Make sure all of those datasets actually correspond to datasets on LILA
%% Re-write the input taxonomy file to refer to LILA datasets
Map the string datasetname:token to a taxonomic tree json
mapping = input_taxonomy_rows[0]
Make sure that all occurrences of this mapping_string give us the same output
assert taxonomy_string == taxonomy_mappings[mapping_string]
%% Re-write the input file in the target format
mapping_string = list(taxonomy_mappings.keys())[0]
""
prepare_lila_taxonomy_release.py
""
"Given the private intermediate taxonomy mapping, prepare the public (release)"
taxonomy mapping file.
""
%% Imports and constants
Created by get_lila_category_list.py... contains counts for each category
%% Find out which categories are actually used
dataset_name = datasets_to_map[0]
i_row = 0; row = df.iloc[i_row]; row
%% Generate the final output file
i_row = 0; row = df.iloc[i_row]; row
match_at_level = taxonomic_match[0]
i_row = 0; row = df.iloc[i_row]; row
"E.g.: (43117, 'genus', 'lepus', ['hares and jackrabbits']"
###############
---> CONSTANTS
###############
max_progressbar = count * (list(range(limit+1))[-1]+1)
"bar = progressbar.ProgressBar(maxval=max_progressbar,"
"widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()]).start()"
bar.update(bar.currval + 1)
bar.finish()
""
"Given a subset of LILA datasets, find all the categories, and start the taxonomy"
mapping process.
""
%% Constants and imports
Created by get_lila_category_list.py
'NACTI'
'Channel Islands Camera Traps'
%% Read the list of datasets
The script that generates this dictionary creates a separate entry for bounding box
"metadata files, but those don't represent new dataset names"
%% Find all categories
dataset_name = datasets_to_map[0]
%% Initialize taxonomic lookup
from taxonomy_mapping.species_lookup import (
"get_taxonomic_info, print_taxonomy_matche)"
%% Manual lookup
%%
q = 'white-throated monkey'
raise ValueError('')
%% Match every query against our taxonomies
mapping_string = category_mappings[1]; print(mapping_string)
...for each mapping
%% Write output rows
""
"Does some consistency-checking on the LILA taxonomy file, and generates"
an HTML preview page that we can use to determine whether the mappings
make sense.
""
%% Imports and constants
"lila_taxonomy_file = r""G:\git\agentmorrisprivate\lila-taxonomy\lila-taxonomy-mapping.csv"""
"lila_taxonomy_file = r""G:\temp\lila\lila_additions_2022.06.29.csv"""
%% Support functions
%% Read the taxonomy mapping file
%% Prepare taxonomy lookup
from taxonomy_mapping.species_lookup import (
"get_taxonomic_info, print_taxonomy_matche)"
%% Optionally remap all gbif-based mappings to inat (or vice-versa)
%%
i_row = 1; row = df.iloc[i_row]; row
This should be zero for the release .csv
%%
%% Check for mappings that disagree with the taxonomy string
Look for internal inconsistency
Look for outdated mappings
i_row = 0; row = df.iloc[i_row]
%% List null mappings
""
"These should all be things like ""unidentified"" and ""fire"""
""
i_row = 0; row = df.iloc[i_row]
%% List mappings with scientific names but no common names
%% List mappings that map to different things in different data sets
x = suppress_multiple_matches[-1]
...for each row where we saw this query
...for each row
"%% Verify that nothing ""unidentified"" maps to a species or subspecies"
"E.g., ""unidentified skunk"" should never map to a specific species of skunk"
%% Make sure there are valid source and level values for everything with a mapping
%% Find WCS mappings that aren't species or aren't the same as the input
"WCS used scientific names, so these remappings are slightly more controversial"
then the standard remappings.
row = df.iloc[-500]
"Anything marked ""species"" or ""unknown"" by definition doesn't map to a species,"
so ignore these.
print('WCS query {} ({}) remapped to {} ({})'.format(
"query,common_name,scientific_name,common_names_from_taxonomy))"
%% Download sample images for all scientific names
i_row = 0; row = df.iloc[i_row]
if s != 'mirafra':
continue
Check whether we already have enough images for this query
"print('Skipping query {}, already have {} images'.format(s,len(sizes_above_threshold)))"
Check whether we've already run this query for a previous row
...for each row in the mapping table
%% Rename .jpeg to .jpg
"print('Renaming {} to {}'.format(fn,new_fn))"
%% Choose representative images for each scientific name
s = list(scientific_name_to_paths.keys())[0]
Be suspicious of duplicate sizes
...for each scientific name
%% Delete unused images
%% Produce HTML preview
i_row = 2; row = df.iloc[i_row]
"image_paths = [os.path.relpath(p, output_base) for p in image_paths]"
...for each row
%% Open HTML preview
######
""
species_lookup.py
""
Look up species names (common or scientific) in the GBIF and iNaturalist
taxonomies.
""
Run initialize_taxonomy_lookup() before calling any other function.
""
######
%% Constants and imports
As of 2020.05.12:
""
"GBIF: ~777MB zipped, ~1.6GB taxonomy"
"iNat: ~2.2GB zipped, ~51MB taxonomy"
These are un-initialized globals that must be initialized by
the initialize_taxonomy_lookup() function below.
%% Functions
Initialization function
# Load serialized taxonomy info if we've already saved it
"# If we don't have serialized taxonomy info, create it from scratch."
Download and unzip taxonomy files
taxonomy_name = list(taxonomy_urls.items())[0][0]; zip_url = list(taxonomy_urls.items())[0][1]
Don't download the zipfile if we've already unzipped what we need
Bypasses download if the file exists already
Unzip the files we need
...for each file that we need from this zipfile
Remove the zipfile
os.remove(zipfile_path)
...for each taxonomy
"Create dataframes from each of the taxonomy files, and the GBIF common"
name file
Load iNat taxonomy
Load GBIF taxonomy
Remove questionable rows from the GBIF taxonomy
Load GBIF vernacular name mapping
Only keep English mappings
Convert everything to lowercase
"For each taxonomy table, create a mapping from taxon IDs to rows"
Create name mapping dictionaries
Build iNat dictionaries
row = inat_taxonomy.iloc[0]
Build GBIF dictionaries
"The canonical name is the Latin name; the ""scientific name"""
include the taxonomy name.
""
http://globalnames.org/docs/glossary/
This only seems to happen for really esoteric species that aren't
"likely to apply to our problems, but doing this for completeness."
Don't include taxon IDs that were removed from the master table
Save everything to file
...def initialize_taxonomy_lookup()
"list of dicts: {'source': source_name, 'taxonomy': match_details}"
i_match = 0
"list of (taxon_id, taxonRank, scientific name, [vernacular names])"
corresponding to an exact match and its parents
Walk taxonomy hierarchy
This can happen because we remove questionable rows from the
GBIF taxonomy
"print(f'Warning: no row exists for parent_taxon_id {parent_taxon_id},' + \"
"f'child taxon_id: {taxon_id}, query: {query}')"
The GBIF taxonomy contains unranked entries
...while there is taxonomy left to walk
...for each match
Remove redundant matches
i_tree_a = 0; tree_a = matching_trees[i_tree_a]
i_tree_b = 1; tree_b = matching_trees[i_tree_b]
"If tree a's primary taxon ID is inside tree b, discard tree a"
""
taxonomy_level_b = tree_b['taxonomy'][0]
...for each level in taxonomy B
...for each tree (inner)
...for each tree (outer)
...def traverse_taxonomy()
"print(""Finding taxonomy information for: {0}"".format(query))"
"In GBIF, some queries hit for both common and scientific, make sure we end"
up with unique inputs
"If the species is not found in either taxonomy, return None"
Both GBIF and iNat have a 1-to-1 mapping between taxon_id and row number
Walk both taxonomies
...def get_taxonomic_info()
m = matches[0]
"For example: [(9761484, 'species', 'anas platyrhynchos')]"
...for each taxonomy level
...for each match
...def print_taxonomy_matches()
%% Taxonomy functions that make subjective judgements
query = 'person'
Do we have an iNat match?
"print_taxonomy_matches(inat_matches, verbose=True)"
"print_taxonomy_matches(gbif_matches, verbose=True)"
print('Warning: multiple iNat matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple iNat common names for {query}')
Default to returning the query
"print(f'Matched iNat {query} to {scientific_name},{common_name}')"
...if we had iNat matches
If we either prefer GBIF or didn't have iNat matches
""
Code is deliberately redundant here; I'm expecting some subtleties in how
handle GBIF and iNat.
print('Warning: multiple GBIF matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple GBIF common names for {query}')
Default to returning the query
...if we needed to look in the GBIF taxonomy
...def _get_preferred_taxonomic_match()
%% Interactive drivers and debug
%% Initialization
%% Taxonomic lookup
query = 'lion'
print(matches)
Print the taxonomy in the taxonomy spreadsheet format
%% Directly access the taxonomy tables
%% Command-line driver
Read command line inputs (absolute path)
Read the tokens from the input text file
Loop through each token and get scientific name
""
process_species_by_dataset
""
We generated a list of all the annotations in our universe; this script is
used to (interactively) map them onto the GBIF and iNat taxonomies.  Don't
"try to run this script from top to bottom; it's used like a notebook, not like"
"a script, since manual review steps are required."
""
%% Imports
%autoreload 0
%autoreload -species_lookup
%% Constants
Input file
Output file after automatic remapping
File to which we manually copy that file and do all the manual review; this
should never be programmatically written to
The final output spreadsheet
HTML file generated to facilitate the identificaiton of egregious mismappings
%% Functions
Prefer iNat matches over GBIF matches
query = 'person'
Do we have an iNat match?
"print_taxonomy_matches(inat_matches, verbose=True)"
"print_taxonomy_matches(gbif_matches, verbose=True)"
print('Warning: multiple iNat matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple iNat common names for {query}')
Default to returning the query
"print(f'Matched iNat {query} to {scientific_name},{common_name}')"
...if we had iNat matches
If we either prefer GBIF or didn't have iNat matches
""
Code is deliberately redundant here; I'm expecting some subtleties in how
handle GBIF and iNat.
print('Warning: multiple GBIF matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple GBIF common names for {query}')
Default to returning the query
...if we needed to look in the GBIF taxonomy
...def get_preferred_taxonomic_match()
%% Initialization
%% Test single-query lookup
%%
%%
"q = ""grevy's zebra"""
%% Read the input data
%% Run all our taxonomic lookups
i_row = 0; row = df.iloc[i_row]
query = 'lion'
...for each query
Write to the excel file that we'll use for manual review
%% Download preview images for everything we successfully mapped
uncomment this to load saved output_file
"output_df = pd.read_excel(output_file, keep_default_na=False)"
i_row = 0; row = output_df.iloc[i_row]
...for each query
%% Write HTML file with representative images to scan for obvious mis-mappings
i_row = 0; row = output_df.iloc[i_row]
...for each row
%% Look for redundancy with the master table
Note: `master_table_file` is a CSV file that is the concatenation of the
"manually-remapped files (""manual_remapped.xlsx""), which are the output of"
this script run across from different groups of datasets. The concatenation
"should be done manually. If `master_table_file` doesn't exist yet, skip this"
"code cell. Then, after going through the manual steps below, set the final"
manually-remapped version to be the `master_table_file`.
%% Manual review
Copy the spreadsheet to another file; you're about to do a ton of manual
review work and you don't want that programmatically overwrriten.
""
See manual_review_xlsx above
%% Read back the results of the manual review process
%% Look for manual mapping errors
Manually inspect df for typos in 'dataset_names' and 'taxonomy_level' columns
Identify rows where:
""
- 'taxonomy_level' does not match level of 1st element in 'taxonomy_string'
- 'scientific_name' does not match name of 1st element in 'taxonomy_string'
""
...both of which typically represent manual mapping errors.
i_row = 0; row = df.iloc[i_row]
"I'm not sure why both of these checks are necessary, best guess is that"
the Excel parser was reading blanks as na on one OS/Excel version and as ''
on another.
The taxonomy_string column is a .json-formatted string; expand it into
an object via eval()
"%% Find scientific names that were added manually, and match them to taxonomies"
i_row = 0; row = df.iloc[i_row]
...for each query
%% Write out final version
""
"Generate read-only SAS URLs for all LILA containers, to facilitate partial downloads."
""
The results of this script end up here:
""
http://lila.science/wp-content/uploads/2020/03/lila_sas_urls.txt
""
"Update: that file is manually maintained now, it can't be programmatically generated"
""
%% Imports
Read-only
%% Enumerate containers
%% Generate SAS tokens
%% Generate SAS URLs
%% Write to output file
--------some stuff needed to get AJAX to work with bottle?--------#
-------------------------------------------------------------------------------- #
PREPARE TO QUEUE IMAGES FOR LABELING
-------------------------------------------------------------------------------- #
# Connect as USER to database DB_NAME through peewee and initialize database proxy
# Load embedding model
---------------------------------------------------------------------- #
CREATE QUEUE OF IMAGES TO LABEL
---------------------------------------------------------------------- #
Use classifier to generate predictions
# Update model predicted class in PostgreSQL database
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
-------------------------------------------------------------------------------- #
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI
-------------------------------------------------------------------------------- #
"# static routes (to serve CSS, etc.)"
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps"
"return bottle.static_file(filename, root='../../../../../../../../../../../.')"
# dynamic routes
if data['display_grayscale']:
indices_to_exclude.update(set(color_indices))
elif not data['display_grayscale']:
indices_to_exclude.update(set(grayscale_indices))
data['display_images'] = {}
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices]
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices]
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices]
data['display_images']['detection_categories'] = []
for i in indices:
if str(dataset.samples[i][1]) == 'None':
data['display_images']['detection_categories'].append('None')
else:
existing_category_entries = {cat.id: cat.name for cat in Category.select()}
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()"
data['display_images']['detection_categories'].append(cat_name)
Use image ids in images_to_label to get the corresponding dataset indices
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
print(set(dataset.set_indices[4]).update(set(indices_to_label)))
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Use image id images_to_label to get the corresponding dataset index
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Train on samples that have been labeled so far
dataset.set_kind(DetectionKind.UserDetection.value)
print(y_train)
Predict on the samples that have not been labeled
print(y_pred)
Update model predicted class in PostgreSQL database
timer = time.time()
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
print('Updating the database took %0.2f seconds'%(time.time() - timer))
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
"once the classifier has been trained the first time, switch to AL sampling"
""
make_oneclass_json.py
""
"Takes a coco-camera-traps .json database and collapses species classes to binary,"
optionally removing labels from empty images (to be detector-friendly) (depending on
"""experiment_type"")."
""
"Assumes that empty images are labeled as ""empty""."
""
%% Imports and environment
%% Core conversion function
"We're removing empty images from the annotation list, but not from"
"the ""images"" list; they'll still get used in detector training."
print('Ignoring empty annotation')
%% Interactive driver
%%
Load annotations
Convert from multi-class to one-class
Write out the one-class data
%% Command-line driver
""
plot_bounding_boxes.py
""
Takes a .json database containing bounding boxes and renders those boxes on the
source images.
""
"This assumes annotations in coco-camera-traps format, with absolute bbox"
coordinates.
""
%% Imports and environment
How many images should we process?  Set to -1 to process all images.
Should we randomize the image order?
%%  Read database and build up convenience mappings
Category ID to category name
Image ID to image info
Image ID to image path
Image ID to all annotations referring to this image
"%% Iterate over images, draw bounding boxes, write to file"
For each image
image = images[0]
Build up a list of bounding boxes to draw on this image
Load the image
Create figure and axes
Display the image
ann = imageAnnotations[0]
"For each annotation associated with this image, render bounding box and label"
"In the Rectangle() function, the first argument (""location"") is the bottom-left"
of the rectangle.
""
Origin is the upper-left of the image.
Add the patch to the Axes
Add a class label
This is magic goop that removes whitespace around image plots (sort of)
Write the output image
...for each image
""
plot_imerit_annotations.py
""
Takes a .json file full of bounding box annotations and renders those boxes on the
source images.
""
"This assumes annotations in the format we receive them, specifically:"
""
1) Relative bbox coordinates
"2) A list of .json objects, not a well-formatted .json file"
""
"I.e., don't use this on a COCO-style .json file.  See plot_bounding_boxes.py"
for the same operation performed on a proper COCO-camera-traps database.
""
%% Imports and environment
"os.makedirs(outputBase, exist_ok=True)"
%%  Read all source images and build up a hash table from image name to full path
"This spans training and validation directories, so it's not the same as"
just joining the image name to a base path
"%% Iterate over annotations, draw bounding boxes, write to file"
annData has keys:
""
"annotations, categories, images"
""
Each of these are lists of dictionaries
%% Render all annotations on each image in the sequence
%% Pull out image metadata
Build up a list of bounding boxes to draw on this image
Pull out just the image name from the filename
""
File names look like:
""
seq6efffac2-5567-11e8-b3fe-dca9047ef277.frame1.img59a94e52-23d2-11e8-a6a3-ec086b02610b.jpg
"m = re.findall(r'img(.*\.jpg)$', imgFileName, re.M|re.I)"
print(m)
assert(len(m) == 1)
queryFileName = m[0]
Map this image back to the original directory
"%% Loop over annotations, find annotations that match this image"
%%
"x,y,w,h"
""
"x,y is the bottom-left of the rectangle"
""
"x,y origin is the upper-left"
...for each annotation
%% Render with PIL (scrap)
%% Render with Matplotlib
Create figure and axes
Display the image
Location is the bottom-left of the rect
""
Origin is the upper-left
Add the patch to the Axes
This is magic goop that removes whitespace around image plots (sort of)
os.startfile(outputFileName)
%% Showing figures on-screen during debugging
plt.show()
Various (mostly unsuccessful) approaches to getting the plot window to show up
"in the foreground, which is a backend-specific operation..."
""
fig.canvas.manager.window.activateWindow()
fig.canvas.manager.window.raise_()
fm = plt.get_current_fig_manager()
"fm.window.attributes('-topmost', 1)"
"fm.window.attributes('-topmost', 0)"
""
# This is the one that I found to be most robust... at like 80% robust.
plt.get_current_fig_manager().window.raise_()
%%
...for each image
...for each file
""
convert_imerit_json_to_coco_json.py
""
"Takes a .json file with bounding boxes but no class labels, and a .json file containing the"
"class labels for those images, and creates a new json file with class labels and bounding"
boxes.
""
"The bounding box .json file is in the format returned by our annotators, which is not"
actually a fully-formed .json file; rather it's a series of .json objects
""
"Leaves filenames intact.  Stores imerit ""category IDs"" (empty/human/group/animal) in a new"
"field called ""annotation_type""."
""
%% Imports and constants
%% Configure files and paths
"For Snapshot Serengeti, we stored image IDs in our annotation files as:"
""
S1_B06_R1_PICT0008
""
...but the corresponding ID in the master database is actually:
""
S1\B06\R1\S1_B06_R1_PICT0008
""
"If this is ""True"", we'll expand the former to the latter"
Handling a one-off issue in which .'s were mysteriously replaced with -'s
"in our annotations.  This will be set dynamically, but I keep it here as"
a constant to remind me to remove this code when we clean this issue up.
Used in the (rare) case where a bounding box was added to an image that was originally
annotated as empty
Used in the (rare) case where we added bounding boxes to an image with multiple species
"Should we include ambiguous bounding boxes (with the ""ambiguous"" category label)"
in the output file?  Ambiguous boxes are boxes drawn on images with multiple species.
"%%  Read metadata from the master database, bounding boxes from the annotations file"
"The bounding box .json file is in the format returned by our annotators, which is not"
actually a fully-formed .json file; rather it's a series of .json objects
"Each element of annData is a dictionary corresponding to a single sequence, with keys:"
""
"annotations, categories, images"
sequence = annData[0]
%% Build convenience mappings
Image ID to images
Category ID to categories (referring to the database categories)
"Image ID to categories (i.e., species labels)"
Utility function we'll use to create annotations for images in empty
sequences (empty images in non-empty sequences already have annotations)
"%% Reformat annotations, grabbing category IDs from the master database (prep)"
iSequence = 0; sequence = annData[0]
"%% Reformat annotations, grabbing category IDs from the master database (loop)"
Make a copy here; we're going to manipulate the sequence annotations
when we need to add synthetic annotations for empty images
im = sequenceImages[0]
Are there any annotations in this sequence?
Which images in this sequence have annotations?
For each image in this sequence...
imeritImageID = im['id']
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG
Confirm that the file exists
Hande a one-off issue with our annotations
datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG
""
...had become:
""
datasetsnapshotserengeti.seqASG000001a-frame0.imgS1_B06_R1_PICT0008.JPG
Does it look like we encountered this issue?
Convert:
""
S1_B06_R1_PICT0008
""
...to:
""
S1/B06/B06_R1/S1_B06_R1_PICT0008
Make sure we only see each image once
Create empty annotations for empty images
""
Here we use the *unmodified* file name
Create an empty annotation for this image
Annotations still use the annotation filename (not database ID) at this point;
these will get converted to database IDs below when we process the
whole sequence.
Sanity-check image size
"print('Warning: img {} was listed in DB as {}x{}, annotated as {}x{}, actual size{}x{}'.format("
"old_id,new_im['width'],new_im['height'],im['width'],im['height'],imgObj.width,imgObj.height))"
...for each image in this sequence
For each annotation in this sequence...
ann = sequenceAnnotations[0]
Prepare an annotation using the category ID from the database and
the bounding box from the annotations file
Maintain iMerit's annotation category
Generate an (arbitrary) ID for this annotation; the COCO format has a concept
"of annotation ID, but our annotation files don't"
This was a one-off quirk with our file naming
We'll do special handling of images with multiple categories later
Store the annotation type (group/human/animal/empty)
This annotation has no bounding box but the image wasn't originally
annotated as empty
This annotation has a bounding box but the image was originally
annotated as empty
unnormalize the bbox
... for each annotation in this sequence
... for each sequence
%% Post-processing
Count empty images
...for each file
%% Sanity-check empty images
""
make_ss_annotation_image_folder.py
""
Take a directory full of images with the very long filenames we give annotators:
""
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension
""
"...along with a COCO-camera-traps database referring to those files, and:"
""
1) Creates a new COCO-camera-traps database with the original filenames in them
(copying the annotations)
""
2) Optionally creates a new directory with those images named according to the
"Snapshot Serengeti naming convention, including complete relative paths."
""
See convert_imerit_json_to_coco_json to see how we get from the original annotation
.json to a COCO-camera-traps database.
""
%% Constants and imports
%% Configure files/paths
%% Read the annotations (referring to the old filenames)
"%% Update filenames, optionally copying files"
im = data['images'][0]
For each image...
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG
"Find the image name, e.g. S1_B06_R1_PICT0008"
Convert:
""
S1_B06_R1_PICT0008.JPG
""
...to:
""
S1/B06/B06_R1/S1_B06_R1_PICT0008.JPG
...for each image
%% Write the revised database
""
get_annotation_tool_link.py
""
"Takes a COCO-camera-traps-style .json file with URLs already embedded, and prepares"
a link to the visipedia annotation tool that reviews a subset of those images.
""
""
create_new_annotation_json.py
""
"Creates a subset of a larger .json database, in this case specifically to pick some images"
from Snapshot Serengeti.
""
from utils import get_db_dicts
for seq in already_annotated:
seq_to_ims.pop(seq)
remove already annotated images
add lion images
lion_seqs = cat_to_seqs[cat_to_id['lionMale']] + cat_to_seqs[cat_to_id['lionFemale']]
#print(len(lion_seqs))
lion_seqs = [seq for seq in lion_seqs if seq not in already_annotated]
#print(len(lion_seqs))
"lion_seqs_to_annotate.extend(random.sample(lion_seqs, min(len(lion_seqs),num_lions)))"
print(len(lion_seqs_to_annotate))
num_elephants = 1000
elephant_seqs = cat_to_seqs[cat_to_id['elephant']]
#print(len(lion_seqs))
elephant_seqs = [seq for seq in elephant_seqs if seq not in already_annotated]
#print(len(lion_seqs))
"elephant_seqs_to_annotate = random.sample(elephant_seqs, num_elephants)"
num_empty = 10
empty_seqs_to_annotate = []
for loc in loc_to_seqs:
empty_seqs = cats_per_location[loc][cat_to_id['empty']]
empty_seqs = [seq for seq in empty_seqs if seq not in already_annotated]
empty_seqs = [seq for seq in empty_seqs if seq_to_season[seq] in seasons_to_keep]
"empty_seqs_to_annotate.extend(random.sample(empty_seqs, min(len(empty_seqs),num_empty)))"
ims_to_annotate.extend(empty_ims_to_annotate)
""
filter_database.py
""
"Look through a COCO-ct database and find images matching some crtieria, writing"
a subset of images and annotations to a new file.
""
%% Constants and imports
%% Configuration
%%  Read database and build up convenience mappings
Category ID to category name
Image ID to image info
Image ID to image path
%% Filter
ann = annotations[0]
Is this a tiny box or a group annotation?
"x,y,w,h"
All empty annotations should be classified as either empty or ambiguous
""
"The ambiguous cases are basically minor misses on the annotators' part,"
where two different small animals were present somewhere.
%% Write output file
""
find_images_for_classes.py
""
"Given a .json database, find images that are associated with one or more"
classes.
""
%% Constants and imports
%% Configuration
%%  Read database and build up convenience mappings
Category ID to category name
Image ID to image info
Image ID to image path
%% Look for target-class annotations
ann = annotations[0]
""
create_tfrecords_format.py
""
This script creates a tfrecords file from a dataset in VOTT format.
%% Imports and environment
set up the filenames and annotations
This loop reads the bboxes and corresponding labels and assigns them
the correct image. Kind of slow at the moment...
If needed: merging all classes
bbox_labels = ['Animal' for _ in bbox_labels]
BBox coords are stored in the format
"x_min (of width axis) y_min (of height axis), x_max, y_max"
Coordinate system starts in top left corner
"In this framework, we need ('ymin', 'xmin', 'ymax', 'xmax') format"
print out some stats
To make sure we loaded the bboxes correctly:
self.validate_bboxes()
For each image in the data set...
Make sure all are greater equal 0
%% Main tfrecord generation function
Propagate optional metadata to tfrecords
endfor each annotation for the current image
endfor each image
"Calculate number of shards to get the desired number of images per record,"
ensure it is evenly divisible by the number of threads
%% Command-line driver
%% Driver
""
make_tfrecords_cis_trans.py
""
Given a .json file that contains a three-element list (train/val/test) of image IDs and a .json database that contains
"those image IDs, generates tfrecords whose filenames include ""train""/""val""/""test"""
""
"dataset = json.load(open('/ai4efs/databases/snapshotserengeti/oneclass/SnapshotSerengeti_Seasons_1_to_4_tfrecord_format_valid_ims.json','r'))"
print('Creating trans_val tfrecords')
dataset = [im_id_to_im[idx] for idx in trans_val]
""
create_classification_tfrecords_from_json.py
""
Called from make_tfrecords_cis_trans.py if you're running a classification experiment.
""
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/'
datafile = 'eccv_18_annotation_files_oneclass/CaltechCameraTrapsECCV18'
image_file_root = datafolder+'eccv_18_all_images/'
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/'
datafolder = '/data/iwildcam/'
datafile = 'combined_iwildcam_annotations_oneclass/eccv_train_and_imerit_2'
need consecutive category ids
old_cat_id_to_new_cat_id = {categories[idx]['id']:idx+1 for idx in range(len(categories))}
print(old_cat_id_to_new_cat_id)
remove multiclass images
print(images[0])
print(vis_data[0])
""
make_tfrecords_from_json.py
""
"Given a coco-camera-traps .json file, creates tfrecords"
""
Thin wrapper for create_tfrecords_from_json.
""
%% Constants and imports
%% Constants and imports (interactive)
%%
%% Main tfrecord generation function
"check whether the input file has already been converted to the tfrecords format,"
"if not, convert"
"Calculate number of shards to get the desired number of images per record,"
ensure it is evenly divisible by the number of threads
%% Interactive driver
%%
%% Command-line driver
eMammal_make_splits.py
""
"Based on a tfrecords_format json file of the database, creates 3 splits according to"
the specified fractions based on location (images from the same location should be in
one split) or based on images.
""
"If a previous split is provided (append_to_previous_split is True), the entries in"
"each split will be preserved, and new entries will be appended, so that new models"
can warm start with a model trained on the original splits.
configurations and paths
approximate fraction for the new entries
read in the previous splits of image ID or location ID if available
"find new locations and assign them to a split, without reassigning any previous locations"
"find out which images are new, shuffle and split them"
do NOT sort the IDs to keep the shuffled order
export PYTHONPATH=$PYTHONPATH:tfmodels/research
add empty category
"add all images that don't have annotations, with cat empty"
print(detection_results.keys())
group the detections and gts by image id:
print(image_id)
print(len(scores))
print(len(labels))
recall_thresh = 0.9
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall])
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])"
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall])
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])"
exp_name = 'eccv_train'
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json'
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(len(detection_results['images']))
print(len(seqs))
"print(len(seqs[0]),len(seqs["
print(detection_results.keys())
group the detections by image id:
group the ground truth annotations by image id:
print(seq)
"for image_id, dets in per_image_detections.iteritems():"
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
print(num_gts)
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
if sum(detected_class_labels)>0:
print('detected animal box')
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])"
"print(scores, tp_fp_labels)"
num_total_gts += num_gts
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
print('valid box')
"print(best_im, best_score)"
exp_name = 'eccv_train'
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json'
export PYTHONPATH=$PYTHONPATH:tfmodels/research
need to loop over confidence values
"for each value, check if any detections on the image are > conf"
"If so, that image gets class ""animal"""
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0"
"for image_id, dets in per_image_detections.iteritems():"
"calc prec, rec for this confidence thresh"
group the ground truth annotations by image id:
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(detection_results.keys())
group the detections and gts by image id:
print(image_id)
print(len(scores))
print(len(labels))
recall_thresh = 0.9
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall])
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])"
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall])
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])"
print(sorted_ap)
plt.bar(sorted_ap)
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(detection_results.keys())
group the detections by image id:
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
print(gts)
print(num_gts)
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[:2])"
"print(scores, tp_fp_labels)"
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
export PYTHONPATH=$PYTHONPATH:tfmodels/research
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
print('detected animal box')
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])"
"print(scores, tp_fp_labels)"
num_total_gts += num_gts
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
print('valid box')
best_im = np.argmax(max_im_scores)
"print(best_im, best_score)"
"temp_labels = np.zeros(len(im_detection_labels),  dtype=np.int32)"
"temp_scores = np.zeros(len(im_detection_scores), dtype=np.float32)"
"for j in range(min(im_num_gts, len(im_detection_labels))):"
temp_labels[j] = True #TODO: this currently only works for oneclass?
temp_scores[j] = best_score
im_detection_labels = temp_labels
im_detection_scores = temp_scores
num_total_gts+=im_num_gts
"print(len(detection_scores), len(detection_scores[0]), len(detection_scores[1]))"
exp_name = 'small_balanced_cct'
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(seq)
"for image_id, dets in per_image_detections.iteritems():"
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
seq_num_gts.append(num_gts)
print(num_gts)
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
num_total_gts += 1
print('valid box')
"print(best_im, best_score)"
print('no valid box')
if sum(seq_num_gts)>0:
export PYTHONPATH=$PYTHONPATH:tfmodels/research
need to loop over confidence values
"for each value, check if any detections on the image are > conf"
"If so, that image gets class ""animal"""
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0"
"calc prec, rec for this confidence thresh"
export PYTHONPATH=$PYTHONPATH:tfmodels/research
group the detections by image id:
group the ground truth annotations by image id:
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
print('detected animal box')
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])"
"print(scores, tp_fp_labels)"
num_total_gts += num_gts
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
print('valid box')
"print(best_im, best_score)"
""
evaluate_detections.py
""
Adapted from analyze_detection.py which is now archived.
""
%% Imports and constants
%% Functions
"labels input to compute_object_detection_metrics() needs to start at 0, not 1"
num_detections = len(dets['boxes'])
to prevent 'Invalid dimensions for box data.' error
this box will not match any detections
compute one-class precision/recall/average precision (if every box is just of an object class)
%% Command-line driver
""
detection_eval_utils.py
""
Utility functions used in evaluate_detections.py
""
group the ground truth annotations by image id
#####
""
detect_and_predict_image.py
""
"Functions to load a TensorFlow detection and a classification model, run inference,"
"render bounding boxes on images, and write out the resulting"
images (with bounding boxes and classes).
""
"See the ""test driver"" cell for example invocation."
""
""
#####
"%% Constants, imports, environment"
Minimum detection confidence for showing a bounding box on the output image
Stick this into filenames before the extension for the rendered result
Number of top-scoring classes to show at each bounding box
%% Core detection functions
Load images if they're not already numpy arrays
iImage = 0; image = images[iImage]
"Load the image as an nparray of size h,w,nChannels"
"There was a time when I was loading with PIL and switched to mpimg,"
"but I can't remember why, and converting to RGB is a very good reason"
"to load with PIL, since mpimg doesn't give any indication of color"
"order, which basically breaks all .png files."
""
"So if you find a bug related to using PIL, update this comment"
"to indicate what it was, but also disable .png support."
image = mpimg.imread(image)
This shouldn't be necessarily when loading with PIL and converting to RGB
Actual detection
...for each image
"Currently ""boxes"" is a list of length nImages, where each element is shaped as"
""
"1,nDetections,4"
""
"This implicitly banks on TF giving us back a fixed number of boxes, let's assert on this"
to make sure this doesn't silently break in the future.
iBox = 0; box = boxes[iBox]
"""scores"" is a length-nImages list of elements with size 1,nDetections"
"""classes"" is a length-nImages list of elements with size 1,nDetections"
""
"Still as floats, but really representing ints"
Squeeze out the empty axis
boxes is nImages x nDetections x 4
scores and classes are both nImages x nDetections
Get input and output tensors of classification model
"imsize = cur_image['width'], cur_image['height']"
Select detections with a confidence larger 0.5
Get these boxes and convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes"
"However, we need to make sure that it box coordinates are still within the image"
For convenience:
Create an array with contains the index of the corresponding crop_box for each selected box
i.e. [False False 0 False 1 2 3 False False]
For each box
If this box should be classified
Run inference
if box should not be classified
...for each box
species_scores should have shape len(images) x len(boxes) x num_species
...for each image
...with tf.Session
with classification_graph
species_scores should have shape len(images) x len(boxes) x num_species
%% Rendering functions
Display the image
plt.show()
"top, left, bottom, right"
""
"x,y origin is the upper-left"
Generate bounding box text
Choose color based on class
Location is the bottom-left of the rect
""
Origin is the upper-left
iRight = x + w
iTop = y + h
Add the patch to the Axes
Add class description
First determine best location by finding the corner that is closest to the image center
relative corner coordinates
relative coordinates of image center
Compute pair-wise squared distance and get the index of the one with minimal distance
Get the corresponding coordinates ...
... and alignment for the text box
Plot the text box with background
...for each box
This is magic goop that removes whitespace around image plots (sort of)
"plt.savefig(outputFileName, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)"
os.startfile(outputFileName)
...for each image
...def render_bounding_boxes
Load and run detector on target images
Read the name of all classes
remove empty lines
%% Interactive driver
%%
%%
%% File helper functions
%% Command-line driver
Hack to avoid running on already-detected images
""
"Script for evaluating a frozen graph given a COCO-style dataset, which was generated by"
the ../data_management/databases/classification/make_classification_dataset.py script.
This is useful for debugging as the accuracy reported by this script should match the
accuracy reported by the Tensorflow training.
""
Check that all files exists for easier debugging
Load frozen graph
Collect tensors for input and output
Read image
Run inference
Check if correct
""
Script for evaluating precision/recall on a two-class problem given
"a frozen graph and a COCO-style dataset, which was generated by"
the ../data_management/databases/classification/make_classification_dataset.py script.
We assume the positive class is at index 0 (with a zero-based indexing)
""
Check that all files exists for easier debugging
Load frozen graph
Collect tensors for input and output
Read image
"with open(image_path, 'rb') as fi:"
"image =  sess.run(tf.image.decode_jpeg(fi.read(), channels=3))"
image = image / 255.
Run inference
predicted_class = np.argmax(predictions)
Check if correct
if coco.imgToAnns[image_id][0]['category_id'] == predicted_class:
correct = correct + 1
"In matplotlib < 1.5, plt.fill_between does not have a 'step' argument"
""
Script for selecting testing images of a COCO-style dataset generated by the script
../data_management/databases/classification/make_classification_dataset.py in a consistent
manner and predicting the class for it.
""
Mandatory parameters
Optional parameters
Validate parameters
Derived parameters
"We assume that the dataset was generated with the make_classification_dataset.py script,"
hence the images should be located in the same folder as the json
Make seletion deterministic
Load frozen graph
Get dataset information
Get classes
...and the class list corresponding to the model outputs by assuming
that they are in order of their ids
Get images of each class
Shuffle the image list
Start the image sampling
"Set of avaiable class IDs, will be filled below"
If there are still images left for that class
"Get image for the sampled class, we already shuffled the class images before so"
we can simply pop()
Start prediction
Collect tensors for input and output
Read image
Run inference
Print output to log file
#####
""
api_apply_classifier_single_node.py
""
Takes the JSON file produced by the detection API and
classifies all boxes above a confidence threshold.
""
#####
"%% Constants, imports, environment"
Assumes that the root of the CameraTraps repo is on the PYTHONPATH
Minimum detection confidence for classifying an object
Number of top-scoring classes to show at each bounding box
Enlargment factor applied to boxes before passing them to the classifier
""
Provides more context and can lead to better results
List of detection categories for which we will run the classification
""
"Currently there are {""1"": ""animal"", ""2"": ""person"", ""4"": ""vehicle""}"
""
Should be a list of string-formatted ints.
Number of significant float digits in JSON output
%% Core detection functions
Read the name of all classes
remove empty lines
Create field with name *classification_categories*
Add classes using 0-based indexing
def add_classification_categories
Make sure we have the right json object
Get input and output tensors of classification model
For each image
Read image
"Scale pixel values to [0,1]"
For each box
Skip detections with low confidence
Skip if detection category is not in whitelist
Skip if already classified
"Get current box in relative coordinates and format [x_min, y_min, width_of_box, height_of_box]"
"Convert to [ymin, xmin, ymax, xmax] and store it as 1x4 numpy array so we can"
re-use the generic multi-box padding code
Convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes."
""
"However, we need to make sure that it box coordinates are still within the image."
Get the first (and only) row as our bbox to classify
Get the image data for that box
Run inference
Add an empty list to the json for our predictions
Add the *num_annotated_classes* top scoring classes
...for each box
...for each image
...with tf.Session
with classification_graph
def classify_boxes
Load classification model
Load detector json
Add classes to detector_json
"Run classifier on all images, changes will be writting directly to the json"
Write output json
def load_and_run_classifier
%% Command-line driver
""
Script for selecting testing images of a COCO-style dataset generated by the script
../data_management/databases/classification/make_classification_dataset.py in a consistent
manner and predicting the class for it.
""
Assumes the cameratraps repo root is on the path
Make seletion deterministic
Mandatory parameters
Optional parameters
Validate parameters
Tranfer parameters to post-processing format
Load frozen graph
Reading image list
Reading class list
Image sampling
Start prediction
Collect tensors for input and output
Array for collecting infos for rendering the html
Read image
"Scale pixel values to [0,1]"
Run inference
Add links to all available classes
""
predict_image.py
""
"Given a pointer to a frozen detection graph, runs inference on a single image,"
printing the top classes to the console
""
%% Imports
%% Command-line processing
Check that all files exist for easier debugging
%% Inference
Load frozen graph
Load class list
Remove empty lines
Collect tensors for input and output
Read image
Run inference
Print output
""
Mostly unmodified script for freezing a model
Added for convenience and for possible future optimizations
""
Copyright 2015 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
'input_checkpoint' may be a prefix if we're using Saver V2 format
Remove all the explicit device specifications for this node. This helps to
make the graph more portable.
List of all partition variables. Because the condition is heuristic
"based, the list could include false positives."
This tensor doesn't exist in the graph (for example it's
'global_step' or a similar housekeeping element) so skip it.
`var_list` is required to be a map of variable names to Variable
tensors. Partition variables are Identity tensors that cannot be
handled by Saver.
Models that have been frozen previously do not contain Variables.
optimized_output_graph_def = optimize_for_inference_lib.optimize_for_inference(
"output_graph_def,"
"input_node_names.replace("" "", """").split("",""),"
"output_node_names.replace("" "", """").split("",""),"
tf.float32.as_datatype_enum)
Write GraphDef to file if output path has been given.
""
"Creates a graph description, which is required to create a frozen graph."
Adapted from from ./tf-slim/export_inference_graph.py
Added preprocessing to the definition for easier handling
""
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
tf.app.flags.DEFINE_integer(
"'batch_size', None,"
"'Batch size for the exported model. Defaulted to ""None"" so batch size can '"
'be specified at model runtime.')
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
#####################
Optimization Flags #
#####################
######################
Learning Rate Flags #
######################
######################
Dataset Flags #
######################
####################
Fine-Tuning Flags #
####################
"Note: when num_clones is > 1, this will actually have each clone to go"
over each epoch FLAGS.num_epochs_per_decay times. This is different
behavior from sync replicas and is expected to produce different results.
Warn the user if a checkpoint exists in the train_dir. Then we'll be
ignoring the checkpoint anyway.
TODO(sguada) variables.filter_variables()
######################
Config model_deploy #
######################
Create global_step
#####################
Select the dataset #
#####################
#####################
Select the network #
#####################
####################################
Select the preprocessing function #
####################################
#############################################################
Create a dataset provider that loads data from the dataset #
#############################################################
###################
Define the model #
###################
############################
Specify the loss function #
############################
Gather initial summaries.
"Gather update_ops from the first clone. These contain, for example,"
the updates for the batch_norm variables created by network_fn.
Add summaries for end_points.
Add summaries for losses.
Add summaries for variables.
################################
Configure the moving averages #
################################
########################################
Configure the optimization procedure. #
########################################
"If sync_replicas is enabled, the averaging will be done in the chief"
queue runner.
Update ops executed locally by trainer.
Variables to train.
and returns a train_tensor and summary_op
Add total_loss to summary.
Create gradient updates.
Add the summaries from the first clone. These contain the summaries
created by model_fn and either optimize_clones() or _gather_clone_loss().
Merge all summaries together.
##########################
Kicks off the training. #
##########################
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
#####################
Select the dataset #
#####################
###################
Select the model #
###################
#############################################################
Create a dataset provider that loads data from the dataset #
#############################################################
####################################
Select the preprocessing function #
####################################
###################
Define the model #
###################
Define the metrics:
Print the summaries to screen.
TODO(sguada) use num_epochs=1
This ensures that we make a single pass over all of the data.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Transform the image to floats.
"Randomly crop a [height, width] section of the image."
Randomly flip the image horizontally.
"Because these operations are not commutative, consider randomizing"
the order their operation.
Subtract off the mean and divide by the variance of the pixels.
Transform the image to floats.
Resize and crop if needed.
Subtract off the mean and divide by the variance of the pixels.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Use tf.slice instead of crop_to_bounding box as it accepts tensors to
define the crop size.
Compute the rank assertions.
Create a random bounding box.
""
Use tf.random_uniform and not numpy.random.rand as doing the former would
"generate random numbers at graph eval time, unlike the latter which"
generates random numbers at graph definition time.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Pass the real x only to one of the func calls.
The random_* ops do not necessarily clamp.
"Each bounding box has shape [1, num_boxes, box coords] and"
"the coordinates are ordered [ymin, xmin, ymax, xmax]."
A large fraction of image datasets contain a human-annotated bounding
box delineating the region of the image containing the object of interest.
We choose to create a new bounding box for the object which is a randomly
distorted version of the human-annotated bounding box that obeys an
"allowed range of aspect ratios, sizes and overlap with the human-annotated"
"bounding box. If no box is supplied, then we assume the bounding box is"
the entire image.
Crop the image to the specified bounding box.
"Each bounding box has shape [1, num_boxes, box coords] and"
"the coordinates are ordered [ymin, xmin, ymax, xmax]."
Restore the shape since the dynamic slice based upon the bbox_size loses
the third dimension.
This resizing operation may distort the images because the aspect
ratio is not respected. We select a resize method in a round robin
fashion based on the thread number.
Note that ResizeMethod contains 4 enumerated resizing methods.
We select only 1 case for fast_mode bilinear.
Randomly flip the image horizontally.
Randomly distort the colors. There are 1 or 4 ways to do it.
Crop the central region of the image with an area containing 87.5% of
the original image.
Resize the image to the specified height and width.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(sguada) docstring paragraph by (a) motivating the need for the file and
(b) defining clones.
TODO(sguada) describe the high-level components of model deployment.
"E.g. ""each model deployment is composed of several parts: a DeploymentConfig,"
"which captures A, B and C, an input_fn which loads data.. etc"
Set up DeploymentConfig
Create the global step on the device storing the variables.
Define the inputs
Define the optimizer.
Define the model including the loss.
Run training.
Namedtuple used to represent a clone during deployment.
"Namedtuple used to represent a DeployedModel, returned by deploy()."
Default parameters for DeploymentConfig
Create clones.
The return value.
Individual components of the loss that will need summaries.
Compute and aggregate losses on the clone device.
Add the summaries out of the clone device block.
Only use regularization_losses for the first clone
Compute the total_loss summing all the clones_losses.
Sum the gradients across clones.
Gather initial summaries.
Create Clones.
"Gather update_ops from the first clone. These contain, for example,"
the updates for the batch_norm variables created by model_fn.
Place the global step on the device storing the variables.
Compute the gradients for the clones.
Add summaries to the gradients.
Create gradient updates.
Only use regularization_losses for the first clone
Add the summaries from the first clone. These contain the summaries
created by model_fn and either optimize_clones() or _gather_clone_loss().
Add total_loss to summary.
Merge all summaries together.
Note that each grad_and_vars looks like the following:
"((grad_var0_clone0, var0), ... (grad_varN_cloneN, varN))"
!/usr/bin/python
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Failed to find ""index"" occurrence of item."
pylint: disable=broad-except
pylint: enable=broad-except
Grab the 'index' annotation.
Some images contain bounding box annotations that
"extend outside of the supplied image. See, e.g."
n03127925/n03127925_147.xml
"Additionally, for some bounding boxes, the min > max"
or the box is entirely outside of the image.
Example: <...>/n06470073/n00141669_6790.xml
Determine if the annotation is from an ImageNet Challenge label.
Note: There is a slight bug in the bounding box annotation data.
Many of the dog labels have the human label 'Scottish_deerhound'
instead of the synset ID 'n02092002' in the bbox.label field. As a
"simple hack to overcome this issue, we only exclude bbox labels"
*which are synset ID's* that do not match original synset label for
the XML file.
Guard against improperly specified boxes.
Note bbox.filename occasionally contains '%s' in the name. This is
data set noise that is fixed by just using the basename of the XML file.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The URLs where the MNIST data can be downloaded.
The names of the classes.
"First, process the training data:"
"Next, process the testing data:"
"Finally, write the labels file:"
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The URL where the Flowers data can be downloaded.
The number of images in the validation set.
Seed for repeatability.
The number of shards per dataset split.
Initializes function that decodes RGB JPEG data.
Read the filename:
Divide into train and test:
"First, convert the training and validation sets."
"Finally, write the labels file:"
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(nsilberman): Add tfrecord file type once the script is updated.
"If set to false, will not try to set label_to_names in dataset"
by reading them from labels.txt or github.
n01440764
n01443537
n02119247    black fox
n02119359    silver fox
pylint: disable=g-line-too-long
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The URL where the CIFAR data can be downloaded.
The number of training files.
The height and width of each image.
The names of the classes.
"First, process the training data:"
"Next, process the testing data:"
"Finally, write the labels file:"
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The labels file contains a list of valid labels are held in this file.
Assumes that the file contains entries as such:
n01440764
n01443537
n01484850
where each line corresponds to a label expressed as a synset. We map
each synset contained in the file to an integer (based on the alphabetical
ordering). See below for details.
This file containing mapping from synset to human-readable label.
Assumes each line of the file looks like:
""
n02119247    black fox
n02119359    silver fox
"n02119477    red fox, Vulpes fulva"
""
where each line corresponds to a unique mapping. Note that each line is
formatted as <synset>\t<human readable label>.
This file is the output of process_bounding_box.py
Assumes each line of the file looks like:
""
"n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940"
""
where each line corresponds to one bounding box annotation associated
with an image. Each line can be parsed as:
""
"<JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>"
""
Note that there might exist mulitple bounding box annotations associated
with an image file.
pylint: disable=expression-not-assigned
pylint: enable=expression-not-assigned
Create a single Session to run all image coding calls.
Initializes function that converts PNG to JPEG data.
Initializes function that converts CMYK JPEG data to RGB JPEG data.
Initializes function that decodes RGB JPEG data.
File list from:
https://groups.google.com/forum/embed/?place=forum/torch7#!topic/torch7/fOSTXHIESSU
File list from:
https://github.com/cytsai/ilsvrc-cmyk-image-list
Read the image file.
Clean the dirty data.
1 image is a PNG.
22 JPEG images are in CMYK colorspace.
Decode the RGB JPEG.
Check that image converted to RGB
Each thread produces N shards where N = int(num_shards / num_threads).
"For instance, if num_shards = 128, and the num_threads = 2, then the first"
"thread would produce shards [0, 64)."
"Generate a sharded version of the file name, e.g. 'train-00002-of-00010'"
"Break all images into batches with a [ranges[i][0], ranges[i][1]]."
Launch a thread for each batch.
Create a mechanism for monitoring when all threads are finished.
Create a generic TensorFlow-based utility for converting all image codings.
Wait for all the threads to terminate.
Leave label index 0 empty as a background class.
Construct the list of JPEG files and labels.
Shuffle the ordering of all image files in order to guarantee
random ordering of the images with respect to label in the
saved TFRecord files. Make the randomization repeatable.
Build a map from synset to human-readable label.
Run it!
Allowing None in the signature so that dataset_factory can use the default.
!/usr/bin/python
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Read in the 50000 synsets associated with the validation data set.
Make all sub-directories in the validation data dir.
Move all of the image to the appropriate sub-directory.
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
"The dataset has classes with no images (empty and human), which have ID 0 and 1,"
so we need to specify 49 here despite having only 47 classes with images
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Final pooling and prediction
Global average pooling.
Pooling with a fixed kernel size.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The current_stride variable keeps track of the effective stride of the
activations. This allows us to invoke atrous convolution whenever applying
the next residual unit would result in the activations having stride larger
than the target output_stride.
The atrous convolution rate parameter.
Move stride from the block's last unit to the end of the block.
"If we have reached the target output_stride, then we need to employ"
atrous convolution with stride=1 and multiply the atrous rate by the
current unit's stride for use in subsequent layers.
Collect activations at the block's end before performing subsampling.
Subsampling of the block's output activations.
"The following implies padding='SAME' for pool1, which makes feature"
alignment easier for dense prediction tasks. This is also used in
https://github.com/facebook/fb.resnet.torch. However the accompanying
code of 'Deep Residual Learning for Image Recognition' uses
padding='VALID' for pool1. You can switch to that choice by setting
"slim.arg_scope([slim.max_pool2d], padding='VALID')."
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Collect outputs for conv2d, fully_connected and max_pool2d"
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"inputs has shape [batch, 224, 224, 3]"
"inputs has shape [batch, 513, 513, 3]"
We do not include batch normalization or activation functions in
conv1 because the first ResNet unit will perform these. Cf.
Appendix of [2].
This is needed because the pre-activation variant does not have batch
normalization or activation functions in the residual unit output. See
Appendix of [2].
Convert end_points_collection into a dictionary of end_points.
Global average pooling.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
=============================================================================
"These parameters come from the online port, which don't necessarily match"
those in the paper.
TODO(nsilberman): confirm these values with Philip.
##########
Encoder #
##########
"No normalizer for the first encoder layers as per 'Image-to-Image',"
Section 5.1.1
First layer doesn't use normalizer_fn
Last layer doesn't use activation_fn nor normalizer_fn
##########
Decoder #
##########
"Dropout is used at both train and test time as per 'Image-to-Image',"
Section 2.1 (last paragraph).
The Relu comes BEFORE the upsample op:
Explicitly set the normalizer_fn to None to override any default value
"that may come from an arg_scope, such as pix2pix_arg_scope."
No normalization on the input layer.
Stride 1 on the last layer.
"1-dim logits, stride 1, no activation, no normalization."
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
Alias
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
pylint: disable=unused-import
pylint: enable=unused-import
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
If we are fine tuning a checkpoint we need to start at a lower learning
rate since we are farther along on training.
We can start quantizing immediately if we are finetuning.
We need to wait for the model to train a bit before we quantize if we are
training from scratch.
Call rewriter to produce graph with fake quant ops and folded batch norms
"quant_delay delays start of quantization till quant_delay steps, allowing"
for better model accuracy.
Configure the learning rate using an exponential decay.
"When restoring from a floating point model, the min/max values for"
quantized weights and activations are not present.
We instruct slim to ignore variables that are missing during restoration
by setting ignore_missing_vars=True
"If we are restoring from a floating point model, we need to initialize"
the global step to zero for the exponential decay to result in
reasonable learning rates.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Orignaly, arg_scope = slim.arg_scope and layers = slim, now switch to more"
update-to-date tf.contrib.* API.
Decay for the moving averages.
epsilon to prevent 0s in variance.
Turns off fused batch norm.
collection containing the moving mean and moving variance.
"For inverted pyramid models, we start with gating switched off."
batch_size x 32 x 112 x 112 x 64
Separable conv is slow when used at first conv layer.
batch_size x 32 x 56 x 56 x 64
batch_size x 32 x 56 x 56 x 64
batch_size x 32 x 56 x 56 x 192
batch_size x 32 x 28 x 28 x 192
batch_size x 32 x 28 x 28 x 256
batch_size x 16 x 14 x 14 x 512
batch_size x 16 x 14 x 14 x 512
batch_size x 16 x 14 x 14 x 512
batch_size x 16 x 14 x 14 x 528
batch_size x 16 x 14 x 14 x 832
batch_size x 8 x 7 x 7 x 832
batch_size x 8 x 7 x 7 x 1024
Final pooling and prediction
Temporal average pooling.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
299 x 299 x 3
149 x 149 x 32
147 x 147 x 32
147 x 147 x 64
73 x 73 x 160
71 x 71 x 192
35 x 35 x 384
4 x Inception-A blocks
35 x 35 x 384
Reduction-A block
17 x 17 x 1024
7 x Inception-B blocks
17 x 17 x 1024
Reduction-B block
8 x 8 x 1536
3 x Inception-C blocks
Auxiliary Head logits
17 x 17 x 1024
Final pooling and prediction
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which"
can be set to False to disable pooling here (as in resnet_*()).
8 x 8 x 1536
1 x 1 x 1536
1536
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN
setups need the gradient of gradient FusedBatchNormGrad.
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN
setups need the gradient of gradient FusedBatchNormGrad.
First upscaling is different because it takes the input vector.
Last layer has different normalizer and activation.
Convert to proper channels.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Use clip_by_value to simulate bandpass activation.
Use clip_by_value to simulate bandpass activation.
Use clip_by_value to simulate bandpass activation.
149 x 149 x 32
147 x 147 x 32
147 x 147 x 64
73 x 73 x 64
73 x 73 x 80
71 x 71 x 192
35 x 35 x 192
35 x 35 x 320
TODO(alemi): Register intermediate endpoints
"17 x 17 x 1088 if output_stride == 8,"
33 x 33 x 1088 if output_stride == 16
TODO(alemi): register intermediate endpoints
TODO(gpapan): Properly support output_stride for the rest of the net.
8 x 8 x 2080
TODO(alemi): register intermediate endpoints
8 x 8 x 1536
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which"
can be set to False to disable pooling here (as in resnet_*()).
Set weight_decay for weights in conv2d and fully_connected layers.
Set activation_fn and parameters for batch_norm.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Decay for the moving averages.
epsilon to prevent 0s in variance.
collection containing update_ops.
use fused batch norm if possible.
Set weight_decay for weights in Conv and FC layers.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"inputs has shape [batch, 224, 224, 3]"
"inputs has shape [batch, 513, 513, 3]"
Use clip_by_value to simulate bandpass activation.
Convert end_points_collection into a dictionary of end_points.
Global average pooling.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"end_points will collect relevant activations for external use, for example"
summaries or losses.
299 x 299 x 3
149 x 149 x 32
147 x 147 x 32
147 x 147 x 64
73 x 73 x 64
73 x 73 x 80.
71 x 71 x 192.
35 x 35 x 192.
Inception blocks
mixed: 35 x 35 x 256.
mixed_1: 35 x 35 x 288.
mixed_2: 35 x 35 x 288.
mixed_3: 17 x 17 x 768.
mixed4: 17 x 17 x 768.
mixed_5: 17 x 17 x 768.
mixed_6: 17 x 17 x 768.
mixed_7: 17 x 17 x 768.
mixed_8: 8 x 8 x 1280.
mixed_9: 8 x 8 x 2048.
mixed_10: 8 x 8 x 2048.
Auxiliary Head logits
Shape of feature map before the final layer.
Final pooling and prediction
Global average pooling.
Pooling with a fixed kernel size.
1 x 1 x 2048
2048
1000
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Reflection pad by 1 in spatial dimensions (axes 1, 2 = h, w) to make a 3x3"
'valid' convolution produce an output with the same dimension as the
input.
This corrects 1 pixel offset for images with even width and height.
conv2d is left aligned and conv2d_transpose is right aligned for even
sized images (while doing 'SAME' padding).
Note: This doesn't reflect actual model in paper.
Neither dropout nor batch norm -> dont need is_training
##########
Encoder #
##########
7x7 input stage
##################
Residual Blocks #
##################
##########
Decoder #
##########
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"end_points will collect relevant activations for external use, for example"
summaries or losses.
Used to find thinned depths for each layer.
Note that sizes in the comments below assume an input spatial size of
"224x224, however, the inputs can be of any size greater 32x32."
224 x 224 x 3
depthwise_multiplier here is different from depth_multiplier.
depthwise_multiplier determines the output channels of the initial
"depthwise conv (see docs for tf.nn.separable_conv2d), while"
depth_multiplier controls the # channels of the subsequent 1x1
convolution. Must have
in_channels * depthwise_multipler <= out_channels
so that the separable convolution is not overparameterized.
Use a normal convolution instead of a separable convolution.
112 x 112 x 64
56 x 56 x 64
56 x 56 x 64
56 x 56 x 192
28 x 28 x 192
Inception module.
28 x 28 x 256
28 x 28 x 320
14 x 14 x 576
14 x 14 x 576
14 x 14 x 576
14 x 14 x 576
14 x 14 x 576
7 x 7 x 1024
7 x 7 x 1024
Final pooling and prediction
Global average pooling.
Pooling with a fixed kernel size.
1 x 1 x 1024
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
=============================================================================
Tensorflow mandates these.
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture
Conv defines 3x3 convolution layers
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.
stride is the stride of the convolution
depth is the number of channels or filters in a layer
MOBILENETV1_CONV_DEFS specifies the MobileNet body
Used to find thinned depths for each layer.
The current_stride variable keeps track of the output stride of the
"activations, i.e., the running product of convolution strides up to the"
current network layer. This allows us to invoke atrous convolution
whenever applying the next convolution would result in the activations
having output stride larger than the target output_stride.
The atrous convolution rate parameter.
"If we have reached the target output_stride, then we need to employ"
atrous convolution with stride=1 and multiply the atrous rate by the
current unit's stride for use in subsequent layers.
By passing filters=None
separable_conv2d produces only a depthwise convolution layer
Global average pooling.
Pooling with a fixed kernel size.
1 x 1 x 1024
Set weight_decay for weights in Conv and DepthSepConv layers.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Decay for the moving averages.
epsilon to prevent 0s in variance.
Turns off fused batch norm.
collection containing the moving mean and moving variance.
Final pooling and prediction
Temporal average pooling.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Orignaly, add_arg_scope = slim.add_arg_scope and layers = slim, now switch to"
more update-to-date tf.contrib.* API.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Make sure that round down does not go down by more than 10%.
"Note: want to round down, we adjust each split to match the total."
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts."
which provide numbered scopes.
b1 -> b2 * r -> b2
i -> (o * r) (bottleneck) -> o
"Note in contrast with expansion, we always have"
projection to produce the desired output size.
stride check enforces that we don't add residuals when spatial
dimensions are None
Depth matches
Don't do any splitting if we end up with less than 8 filters
on either side.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Make sure that round down does not go down by more than 10%.
Set conv defs defaults and overrides.
a) Set the tensorflow scope
b) set padding to default: note we might consider removing this
since it is also set by mobilenet_scope
c) set all defaults
d) set all extra overrides.
The current_stride variable keeps track of the output stride of the
"activations, i.e., the running product of convolution strides up to the"
current network layer. This allows us to invoke atrous convolution
whenever applying the next convolution would result in the activations
having output stride larger than the target output_stride.
The atrous convolution rate parameter.
Insert default parameters before the base scope which includes
any custom overrides set in mobilenet.
"If we have reached the target output_stride, then we need to employ"
atrous convolution with stride=1 and multiply the atrous rate by the
current unit's stride for use in subsequent layers.
Update params.
Only insert rate to params if rate > 1.
Set padding
Add all tensors that end with 'output' to
endpoints
1 x 1 x num_classes
Note: legacy scope name.
"Recover output shape, for unknown shape."
the network created will be trainble with dropout/batch norm
initialized appropriately.
Note: do not introduce parameters that would change the inference
"model here (for example whether to use bias), modify conv_def instead."
Set weight_decay for weights in Conv and FC layers.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
pyformat: disable
Architecture: https://arxiv.org/abs/1801.04381
Note: these parameters of batch norm affect the architecture
that's why they are here and not in training_scope.
pyformat: enable
NB: do not set depth_args unless they are provided to avoid overriding
whatever default depth_multiplier might have thanks to arg_scope.
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that
"'finegrain_classification_mode' is set to True, which means the embedding"
layer will not be shrinked when given a depth-multiplier < 1.0.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Notes for training NASNet Cifar Model
-------------------------------------
batch_size: 32
learning rate: 0.025
cosine (single period) learning rate decay
auxiliary head loss weighting: 0.4
clip global norm of all gradients by 5
600 epochs with a batch size of 32
This is used for the drop path probabilities since it needs to increase
the drop out probability over the course of training.
Notes for training large NASNet model on ImageNet
-------------------------------------
batch size (per replica): 16
learning rate: 0.015 * 100
learning rate decay factor: 0.97
num epochs per decay: 2.4
sync sgd with 100 replicas
auxiliary head loss weighting: 0.4
label smoothing: 0.1
clip global norm of all gradients by 10
Notes for training the mobile NASNet ImageNet model
-------------------------------------
batch size (per replica): 32
learning rate: 0.04 * 50
learning rate scaling factor: 0.97
num epochs per decay: 2.4
sync sgd with 50 replicas
auxiliary head weighting: 0.4
label smoothing: 0.1
clip global norm of all gradients by 10
Decay for the moving averages.
epsilon to prevent 0s in variance.
Decay for the moving averages.
epsilon to prevent 0s in variance.
Decay for the moving averages.
epsilon to prevent 0s in variance.
Shape of feature map before the final layer.
149 x 149 x 32
Run the reduction cells
Calculate the total number of cells in the network
Add 2 for the reduction cells
Calculate the total number of cells in the network
Add 2 for the reduction cells
"If ImageNet, then add an additional two for the stem cells"
Calculate the total number of cells in the network
Add 2 for the reduction cells
"If ImageNet, then add an additional two for the stem cells"
Find where to place the reduction cells or stride normal cells
Setup for building in the auxiliary head.
Run the cells
true_cell_num accounts for the stem cells
Final softmax layer
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Find where to place the reduction cells or stride normal cells
pylint: disable=protected-access
pylint: enable=protected-access
Setup for building in the auxiliary head.
Run the cells
true_cell_num accounts for the stem cells
pylint: disable=protected-access
pylint: enable=protected-access
Final softmax layer
pylint: disable=protected-access
pylint: enable=protected-access
Calculate the total number of cells in the network.
There is no distinction between reduction and normal cells in PNAS so the
total number of cells is equal to the number normal cells plus the number
of stem cells (two by default).
pylint: disable=protected-access
pylint: enable=protected-access
Calculate the total number of cells in the network.
There is no distinction between reduction and normal cells in PNAS so the
total number of cells is equal to the number normal cells plus the number
of stem cells (two by default).
Configuration for the PNASNet-5 model.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"The cap for tf.clip_by_value, it's hinted from the activation distribution"
"that the majority of activation values are in the range [-6, 6]."
Skip path 1
Skip path 2
"First pad with 0's on the right and bottom, then shift the filter to"
include those 0's that were added.
"If odd number of filters, add an additional one to the second path."
Concat and apply BN
Set the prev layer to the current layer if it is none
Check to be sure prev layer stuff is setup correctly
num_or_size_splits=1
Apply conv operations
Combine hidden states using 'add'.
Add hiddenstate to the list of hiddenstates we can choose from
Dont stride if this is not one of the original hiddenstates
"Check if a stride is needed, then use a strided 1x1 here"
Determine if a reduction should be applied to make the number of
filters match.
Return the concat of all the states
Scale keep prob by layer number
The added 2 is for the reduction cells
Decrease the keep probability over time
""
Script for generating a two-class dataset in COCO format for training an obscured image classifier
""
Requires Python >= 3.6 because of the glob ** expression
""
Collect images and labels
"Labels: clean = 0, obscured = 1"
""
top_folders_to_bottom.py
""
Given a base folder with files like:
""
A/1/2/a.jpg
B/3/4/b.jpg
""
"...moves the top-level folders to the bottom in a new output folder, i.e., creates:"
""
1/2/A/a.jpg
3/4/B/b.jpg
""
"In practice, this is used to make this:"
""
animal/camera01/image01.jpg
""
...look like:
""
camera01/animal/image01.jpg
""
%% Constants and imports
%% Support functions
%% Main functions
Find top-level folder
Find file/folder names
Move or copy
...def process_file()
Enumerate input folder
Convert absolute paths to relative paths
Standardize delimiters
Make sure each input file maps to a unique output file
relative_filename = relative_files[0]
Loop
...def top_folders_to_bottom()
%% Interactive driver
%%
%%
%% Command-line driver
"python top_folders_to_bottom.py ""g:\temp\separated_images"" ""g:\temp\separated_images_inverted"" --n_threads 100"
Convert to an options object
%% Constants and imports
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils)
Assumes the cameratraps repo root is on the path
%% Options
"Make sure there is no overlap between the two sets, because this will cause"
issues in the code
## Required inputs
## Options
Can be a folder or a SAS URL
# These apply only when we're doing ground-truth comparisons
Classes we'll treat as negative
""
"Include the token ""#NO_LABELS#"" to indicate that an image with no annotations"
should be considered empty.
Classes we'll treat as neither positive nor negative
"A list of output sets that we should count, but not render images for."
""
"Typically used to preview sets with lots of empties, where you don't want to"
"subset but also don't want to render 100,000 empty images."
""
"detections, non_detections"
"detections_animal, detections_person, detections_vehicle"
"By default, choose a confidence threshold based on the detector version"
Used for summary statistics only
"Number of images to sample, -1 for ""all images"""
"Random seed for sampling, or None"
Optionally separate detections into categories (animal/vehicle/human)
""
Currently only supported when ground truth is unavailable
Optionally replace one or more strings in filenames with other strings;
useful for taking a set of results generated for one folder structure
and applying them to a slightly different folder structure.
Allow bypassing API output loading when operating on previously-loaded
results
Should we also split out a separate report about the detections that were
just below our main confidence threshold?
""
Currently only supported when ground truth is unavailable
Control rendering parallelization
...PostProcessingOptions
#%% Helper classes and functions
Anything greater than this isn't clearly positive or negative
image has annotations suggesting both negative and positive
"image is not annotated or is annotated with 'unknown', 'unlabeled', ETC."
image has not yet been assigned a state
"In some analyses, we add an additional class that lets us look at"
detections just below our main confidence threshold
count the # of images with each type of DetectionStatus
Check whether this image has:
- unknown / unassigned-type labels
- negative-type labels
"- positive labels (i.e., labels that are neither unknown nor negative)"
"assert has_unknown_labels is False, '{} has unknown labels'.format(annotations)"
If there are no image annotations...
n_negative += 1
im['_detection_status'] = DetectionStatus.DS_NEGATIVE
"If the image has more than one type of labels, it's ambiguous"
"note: bools are automatically converted to 0/1, so we can sum"
"After the check above, we can be sure it's only one of positive,"
"negative, or unknown."
""
Important: do not merge the following 'unknown' branch with the first
"'unknown' branch above, where we tested 'if len(categories) == 0'"
""
If the image has only unknown labels
If the image has only negative labels
If the images has only positive labels
"Annotate the category, if it is unambiguous"
...for each image
...mark_detection_status()
"Leaving code in place for reading from blob storage, may support this"
in the future.
resize is to display them in this notebook or in the HTML more quickly
os.path.isfile() is slow when mounting remote directories; much faster
to just try/except on the image open.
return ''
Render images to a flat folder... we can use os.sep here because we've
already normalized paths
"Create class labels like ""gt_1"" or ""gt_27"""
"for i_box,box in enumerate(ground_truth_boxes):"
gt_classes.append('_' + str(box[-1]))
"errno.ENAMETOOLONG doesn't get thrown properly on Windows, so"
we awkwardly check against a hard-coded limit
Use slashes regardless of os
Optionally add links back to the original images
...render_bounding_boxes
Count items in each category
Optionally sort by filename before writing to html
Write the individual HTML files
...prepare_html_subpages()
Get unique categories above the threshold for this image
Render an image (with no ground truth information)
"This is a list of [class,confidence] pairs, sorted by confidence"
"If we either don't have a confidence threshold, or we've met our"
confidence threshold
...if this detection has classification info
...for each detection
...def render_image_no_gt()
This should already have been normalized to either '/' or '\'
...def render_image_with_gt()
%% Main function
#%% Expand some options for convenience
#%% Prepare output dir
#%% Load ground truth if available
Mark images in the ground truth as positive or negative
#%% Load detection (and possibly classification) results
"If the caller hasn't supplied results, load them"
Determine confidence thresholds if necessary
Remove failed rows
Convert keys and values to lowercase
"Add column 'pred_detection_label' to indicate predicted detection status,"
not separating out the classes
#%% Pull out descriptive metadata
This is rare; it only happens during debugging when the caller
is supplying already-loaded API results.
"#%% If we have ground truth, remove images we can't match to ground truth"
#%% Sample images for visualization
#%% Fork here depending on whether or not ground truth is available
"If we have ground truth, we'll compute precision/recall and sample tp/fp/tn/fn."
""
Otherwise we'll just visualize detections/non-detections.
#%% Detection evaluation: compute precision/recall
numpy array of detection probabilities
"numpy array of bools (0.0/1.0), and -1 as null value"
Don't include ambiguous/unknown ground truth in precision/recall analysis
"For completeness, include the result at a confidence threshold of 1.0"
Compute and print summary statistics
Thresholds go up throughout precisions/recalls/thresholds; find the last
value where recall is at or above target.  That's our precision @ target recall.
"np.where returns a tuple of arrays, but in this syntax where we're"
"comparing an array with a scalar, there will only be one element."
Convert back to a list
Flatten the confusion matrix
"#%% Collect classification results, if they exist"
Mapping of classnames to idx for the confusion matrix.
""
"The lambda is actually kind of a hack, because we use assume that"
the following code does not reassign classname_to_idx
Confusion matrix as defaultdict of defaultdict
""
"Rows / first index is ground truth, columns / second index is predicted category"
iDetection = 0; fn = detector_files[iDetection]; print(fn)
"If this image has classification predictions, and an unambiguous class"
"annotated, and is a positive image..."
"The unambiguous category, we make this a set for easier handling afterward"
"Compute the accuracy as intersection of union,"
i.e. (# of categories in both prediciton and GT)
divided by (# of categories in either prediction or GT
""
"In case of only one GT category, the result will be 1.0, if"
prediction is one category and this category matches GT
""
"It is 1.0/(# of predicted top-1 categories), if the GT is"
one of the predicted top-1 categories.
""
"It is 0.0, if none of the predicted categories is correct"
Distribute this accuracy across all predicted categories in the
confusion matrix
...for each file in the detection results
If we have classification results
Build confusion matrix as array from classifier_cm
Print some statistics
Prepare confusion matrix output
Get confusion matrix as string
Get fixed-size classname for each idx
Prepend class name on each line and add to the top
Print formatted confusion matrix
"Actually don't, this gets really messy in all but the widest consoles"
Plot confusion matrix
To manually add more space at bottom: plt.rcParams['figure.subplot.bottom'] = 0.1
""
"Add 0.5 to figsize for every class. For two classes, this will result in"
"fig = plt.figure(figsize=[4,4])"
...if we have classification results
#%% Render output
Write p/r table to .csv file in output directory
Write precision/recall plot to .png file in output directory
#%% Sampling
Sample true/false positives/negatives with correct/incorrect top-1
classification and render to html
Accumulate html image structs (in the format expected by write_html_image_lists)
"for each category, e.g. 'tp', 'fp', ..., 'class_bird', ..."
Add default entries by accessing them for the first time
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]"
"Each element will be a three-tuple with elements file,max_conf,detections"
"Assemble the information we need for rendering, so we can parallelize without"
dealing with Pandas
i_row = 0; row = images_to_visualize.iloc[0]
Filenames should already have been normalized to either '/' or '\'
Map all the rendering results in the list rendering_results into the
dictionary images_html
Prepare the individual html image files
Write index.html
Show links to each GT class
""
We could do this without classification results; currently we don't.
Add links to all available classes
Close body and html tags
...for each image
"#%% Otherwise, if we don't have ground truth..."
#%% Sample detections/non-detections
Accumulate html image structs (in the format expected by write_html_image_list)
for each category
Add default entries by accessing them for the first time
"Maps detection categories - e.g. ""human"" - to result set names, e.g."
"""detections_human"""
Add a set of results for each category and combination of categories
Create output directories
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]"
"list of 3-tuples with elements (file, max_conf, detections)"
"Assemble the information we need for rendering, so we can parallelize without"
dealing with Pandas
i_row = 0; row = images_to_visualize.iloc[0]
Filenames should already have been normalized to either '/' or '\'
"render_image_no_gt(file_info,detection_categories_to_results_name,"
"detection_categories,classification_categories)"
Map all the rendering results in the list rendering_results into the
dictionary images_html
Prepare the individual html image files
Write index.html
"We can't just sum these, because image_counts includes images in both their"
detection and classification classes
total_images = sum(image_counts.values())
Don't print classification classes here; we'll do that later with a slightly
different structure
Add links to all available classes
os.startfile(output_html_file)
...if we do/don't have ground truth
...process_batch_results
%% Interactive driver(s)
%%
options.unlabeled_classes = ['human']
os.start(ppresults.output_html_file)
%% Command-line driver
""
load_api_results.py
""
Loads the output of the batch processing API (json) into a pandas dataframe.
""
Also functions to group entries by seq_id.
""
Includes the deprecated functions that worked with the old CSV API output format.
""
%% Constants and imports
%% Functions for grouping by sequence_id
example
input 'file': 'SER/S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG'
output 'id': 'S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG'
%% Functions for loading the result as a Pandas DataFrame
Validate that this is really a detector output file
Fields in the API output json other than 'images'
Normalize paths to simplify comparisons later
"image['file'] = image['file'].replace('\\','/')"
Replace some path tokens to match local paths to original blob structure
"If this is a newer file that doesn't include maximum detection confidence values,"
"add them, because our unofficial internal dataframe format includes this."
Pack the json output into a Pandas DataFrame
Sanity-check that this is really a detector output file
Normalize paths to simplify comparisons later
De-serialize detections
Optionally replace some path tokens to match local paths to the original blob structure
string_to_replace = list(options.detector_output_filename_replacements.keys())[0]
"TODO: hit some silly issues with vectorized str() and escaped characters, vectorize"
this later.
""
"detection_results['image_path'].str.replace(string_to_replace,replacement_string)"
iRow = 0
%% Imports
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils)
%% Constants and support classes
We will confirm that this matches what we load from each file
Process-based parallelization isn't supported yet
%% Main function
"Warn the user if some ""detections"" might not get rendered"
#%% Validate inputs
#%% Load both result sets
assert results_a['detection_categories'] == default_detection_categories
assert results_b['detection_categories'] == default_detection_categories
#%% Make sure they represent the same set of images
#%% Find differences
"Each of these maps a filename to a two-element list (the image in set A, the image in set B)"
""
"Right now, we only handle a very simple notion of class transition, where the detection"
of maximum confidence changes class *and* both images have an above-threshold detection.
fn = filenames_a[0]
We shouldn't have gotten this far if error_on_non_matching_lists is set
det = im_a['detections'][0]
...for each filename
#%% Sample and plot differences
"Render two sets of results (i.e., a comparison) for a single"
image.
...def render_image_pair()
fn = image_filenames[0]
...def render_detection_comparisons()
"For each category, generate comparison images and the"
comparison HTML page.
""
category = 'common_detections'
Choose detection pairs we're going to render for this category
...for each category
#%% Write the top-level HTML file content
...def compare_batch_results()
%% Interactive driver
%% KRU
%% Command-line driver
# TODO
""
"Merge high-confidence detections from one results file into another file,"
when the target file does not detect anything on an image.
""
Does not currently attempt to merge every detection based on whether individual
detections are missing; only merges detections into images that would otherwise
be considered blank.
""
"If you want to literally merge two .json files, see combine_api_outputs.py."
""
%% Constants and imports
%% Structs
Don't bother merging into target images where the max detection is already
higher than this threshold
"If you want to merge only certain categories, specify one"
(but not both) of these.
%% Main function
im = output_data['images'][0]
"Determine whether we should be processing all categories, or just a subset"
of categories.
i_source_file = 0; source_file = source_files[i_source_file]
source_im = source_data['images'][0]
detection_category = list(detection_categories)[0]
"This is already a detection, no need to proceed looking for detections to"
transfer
Boxes are x/y/w/h
source_sizes = [det['bbox'][2]*det['bbox'][3] for det in source_detections_this_category_raw]
Only look at boxes below the size threshold
...for each detection category
"print('Adding {} detections to image {}'.format(len(detections_to_transfer),image_filename))"
Update the max_detection_conf field (if present)
...for each image
...for each source file
%% Test driver
%%
%% Command-line driver (TODO)
""
separate_detections_into_folders.py
""
## Overview
""
"Given a .json file with batch processing results, separate the files in that"
"set of results into folders that contain animals/people/vehicles/nothing,"
according to per-class thresholds.
""
"Image files are copied, not moved."
""
""
## Output structure
""
Preserves relative paths within each of those folders; cannot be used with .json
files that have absolute paths in them.
""
"For example, if your .json file has these images:"
""
a/b/c/1.jpg
a/b/d/2.jpg
a/b/e/3.jpg
a/b/f/4.jpg
a/x/y/5.jpg
""
And let's say:
""
"* The results say that the first three images are empty/person/vehicle, respectively"
"* The fourth image is above threshold for ""animal"" and ""person"""
* The fifth image contains an animal
""
* You specify an output base folder of c:\out
""
You will get the following files:
""
c:\out\empty\a\b\c\1.jpg
c:\out\people\a\b\d\2.jpg
c:\out\vehicles\a\b\e\3.jpg
c:\out\animal_person\a\b\f\4.jpg
c:\out\animals\a\x\y\5.jpg
""
## Rendering bounding boxes
""
"By default, images are just copied to the target output folder.  If you specify --render_boxes,"
bounding boxes will be rendered on the output images.  Because this is no longer strictly
"a copy operation, this may result in the loss of metadata.  More accurately, this *may*"
result in the loss of some EXIF metadata; this *will* result in the loss of IPTC/XMP metadata.
""
Rendering boxes also makes this script a lot slower.
""
## Classification-based separation
""
"If you have a results file with classification data, you can also specify classes to put"
"in their own folders, within the ""animals"" folder, like this:"
""
"--classification_thresholds ""deer=0.75,cow=0.75"""
""
"So, e.g., you might get:"
""
c:\out\animals\deer\a\x\y\5.jpg
""
"In this scenario, the folders within ""animals"" will be:"
""
"deer, cow, multiple, unclassified"
""
"""multiple"" in this case only means ""deer and cow""; if an image is classified as containing a"
"bird and a bear, that would end up in ""unclassified"", since the folder separation is based only"
on the categories you provide at the command line.
""
"No classification-based separation is done within the animal_person, animal_vehicle, or"
animal_person_vehicle folders.
""
%% Constants and imports
Occasionally we have near-zero confidence detections associated with COCO classes that
didn't quite get squeezed out of the model in training.  As long as they're near zero
"confidence, we just ignore them."
%% Options class
"Dictionary mapping categories (plus combinations of categories, and 'empty') to output folders"
Populated only when using classification results
"Originally specified as a string, converted to a dict mapping name:threshold"
...__init__()
...class SeparateDetectionsIntoFoldersOptions
%% Support functions
Find the maximum confidence for each category
""
det = detections[0]
"For zero-confidence detections, we occasionally have leftover goop"
from COCO classes
assert det['conf'] < invalid_category_epsilon
...for each detection on this image
Count the number of thresholds exceeded
...for each category
If this is above multiple thresholds
"TODO: handle species-based separation in, e.g., the animal_person case"
"Are we making species classification folders, and is this an animal?"
Do we need to put this into a specific species folder?
Find the animal-class detections that are above threshold
Count the number of classification categories that are above threshold for at
least one detection
d = valid_animal_detections[0]
classification = d['classifications'][0]
"Do we have a threshold for this category, and if so, is"
this classification above threshold?
...for each classification
...for each detection
...if we have to deal with classification subfolders
...if we have 0/1/more categories above threshold
...if this is/isn't a failure case
Skip this image if it's empty and we're not processing empty images
"At this point, this image is getting copied; we may or may not also need to"
draw bounding boxes.
Do a simple copy operation if we don't need to render any boxes
Open the source image
"Render bounding boxes for each category separately, beacuse"
we allow different thresholds for each category.
"When we're not using classification folders, remove classification"
information to maintain standard detection colors.
...for each category
Read EXIF metadata
"Write output with EXIF metadata if available, and quality='keep' if this is a JPEG"
"image.  Unfortunately, neither parameter likes ""None"", so we get a slightly"
icky cascade of if's here.
Also see:
""
https://discuss.dizzycoding.com/determining-jpg-quality-in-python-pil/
""
...for more ways to preserve jpeg quality if quality='keep' doesn't do the trick.
...if we don't/do need to render boxes
...def process_detections()
%% Main function
Create output folder if necessary
Load detection results
Map class names to output folders
Create all combinations of categories
category_name = category_names[0]
Do we have a custom threshold for this category?
Create folder mappings for each category
Create the actual folders
"Handle species classification thresholds, if specified"
"E.g. deer=0.75,cow=0.75"
token = tokens[0]
...for each token
...if classification thresholds are still in string format
Validate the classes in the threshold list
...if we need to deal with classification categories
i_image = 14; im = images[i_image]; im
...for each image
...def separate_detections_into_folders
%% Interactive driver
%%
%%
%%
%% Testing various command-line invocations
"With boxes, no classification"
"No boxes, no classification (default)"
"With boxes, with classification"
"No boxes, with classification"
%% Command-line driver
"python api\batch_processing\postprocessing\separate_detections_into_folders.py ""d:\temp\rspb_mini.json"" ""d:\temp\demo_images\rspb_2018_2019_mini"" ""d:\temp\separation_test"" --nthreads 2"
Convert to an options object
"print('{} {}'.format(v,name))"
List of category numbers to use in separation; uses all categories if None
"Can be ""size"", ""width"", or ""height"""
For each image...
""
im = images[0]
d = im['detections'][0]
Are there really any detections here?
Is this a category we're supposed to process?
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing#detector-outputs
...for each detection
...for each image
...def categorize_detections_by_size()
""
add_max_conf.py
""
"The MD output format included a ""max_detection_conf"" field with each image"
up to and including version 1.2; it was removed as of version 1.3 (it's
redundant with the individual detection confidence values).
""
"Just in case someone took a dependency on that field, this script allows you"
to add it back to an existing .json file.
""
%% Imports and constants
%% Main function
%% Driver
#######
""
remove_repeat_detections.py
""
"Used after running find_repeat_detections, then manually filtering the results,"
to create a final filtered output file.
""
"If you want to use this script, we recommend that you read the user's guide:"
""
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms
""
#######
%% Constants and imports
%% Main function
%% Interactive driver
%%
%% Command-line driver
#######
""
repeat_detections_core.py
""
Core utilities shared by find_repeat_detections and remove_repeat_detections.
""
#######
%% Imports and environment
"from ai4eutils; this is assumed to be on the path, as per repo convention"
"""PIL cannot read EXIF metainfo for the images"""
"""Metadata Warning, tag 256 had too many entries: 42, expected 1"""
%% Constants
%% Classes
Relevant for rendering the folder of images for filtering
""
"imageBase can also be a SAS URL, in which case some error-checking is"
disabled.
Don't consider detections with confidence lower than this as suspicious
Don't consider detections with confidence higher than this as suspicious
What's the IOU threshold for considering two boxes the same?
How many occurrences of a single location (as defined by the IOU threshold)
are required before we declare it suspicious?
"Ignore ""suspicious"" detections larger than some size; these are often animals"
taking up the whole image.  This is expressed as a fraction of the image size.
"Ignore ""suspicious"" detections smaller than some size"
Ignore folders with more than this many images in them
A list of classes we don't want to treat as suspicious. Each element is an int.
Load detections from a filter file rather than finding them from the detector output
".json file containing detections, should be called detectionIndex.json in the filtering_* folder"
produced in the first pass
(optional) List of filenames remaining after deletion of identified
repeated detections that are actually animals.  This should be a flat
"text file, one relative filename per line.  See enumerate_images()."
Turn on/off optional outputs
Determines whether bounding-box rendering errors (typically network errors) should
be treated as failures
Box rendering options
State variables
"Replace filename tokens after reading, useful when the directory structure"
has changed relative to the structure the detector saw
How many folders up from the leaf nodes should we be going to aggregate images?
An optional function that takes a string (an image file name) and returns
"a string (the corresponding  folder ID), typically used when multiple folders"
actually correspond to the same camera in a manufacturer-specific way (e.g.
a/b/c/RECONYX100 and a/b/c/RECONYX101 may really be the same camera).
Include/exclude specific folders... only one of these may be
"specified; ""including"" folders includes *only* those folders."
"Optionally show *other* detections (i.e., detections other than the"
one the user is evaluating) in a light gray
"If bRenderOtherDetections is True, what color should we use to render the"
(hopefully pretty subtle) non-target detections?
""
"In theory I'd like these ""other detection"" rectangles to be partially"
"transparent, but this is not straightforward, and the alpha is ignored"
"here.  But maybe if I leave it here and wish hard enough, someday it"
will work.
""
otherDetectionsColors = ['dimgray']
Sort detections within a directory so nearby detections are adjacent
"in the list, for faster review."
""
"Can be None, 'xsort', or 'clustersort'"
""
* None sorts detections chronologically by first occurrence
* 'xsort' sorts detections from left to right
* 'clustersort' clusters detections and sorts by cluster
Only relevant if smartSort == 'clustersort'
"The data table (Pandas DataFrame), as loaded from the input json file via"
load_api_results()
"The other fields in the input json file, loaded via load_api_results()"
The data table after modification
dict mapping folder names to whole rows from the data table
dict mapping filenames to rows in the master table
"An array of length nDirs, where each element is a list of DetectionLocation"
objects for that directory that have been flagged as suspicious
"This is a bit of a hack right now, but for future-proofing, I don't want to call this"
"to retrieve anything other than the highest-confidence detection, and I'm assuming this"
"is already sorted, so assert() that."
It's not clear whether it's better to use instances[0].bbox or self.bbox
"here... they should be very similar, unless iouThreshold is very low."
self.bbox is a better representation of the overal DetectionLocation.
%% Helper functions
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t"
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t"
%% Sort a list of candidate detections to make them visually easier to review
Just sort by the X location of each box
"Prepare a list of points to represent each box,"
that's what we'll use for clustering
Upper-left
"points.append([det.bbox[0],det.bbox[1]])"
Center
"Labels *could* be any unique labels according to the docs, but in practice"
they are unique integers from 0:nClusters
Make sure the labels are unique incrementing integers
Store the label assigned to each cluster
"Now sort the clusters by their x coordinate, and re-assign labels"
so the labels are sortable
"Compute the centroid for debugging, but we're only going to use the x"
"coordinate.  This is the centroid of points used to represent detections,"
which may be box centers or box corners.
old_cluster_label_to_new_cluster_label[old_cluster_label] =\
new_cluster_labels[old_cluster_label]
%% Look for matches (one directory)
List of DetectionLocations
candidateDetections = []
Create a tree to store candidate detections
For each image in this directory
""
iDirectoryRow = 0; row = rows.iloc[iDirectoryRow]
""
"iDirectoryRow is a pandas index, so it may not start from zero;"
"for debugging, we maintain i_iteration as a loop index."
print('Searching row {} of {} (index {}) in dir {}'.\
"format(i_iteration,len(rows),iDirectoryRow,dirName))"
Don't bother checking images with no detections above threshold
"Array of dicts, where each element is"
{
"'category': '1',  # str value, category ID"
"'conf': 0.926,  # confidence of this detections"
""
"# (x_min, y_min) is upper-left, all in relative coordinates"
"'bbox': [x_min, y_min, width_of_box, height_of_box]"
""
}
For each detection in this image
"This is no longer strictly true; I sometimes run RDE in stages, so"
some probabilities have already been made negative
""
assert confidence >= 0.0 and confidence <= 1.0
Optionally exclude some classes from consideration as suspicious
Is this detection too big to be suspicious?
print('Illegal zero-size bounding box on image {}'.format(filename))
These are relative coordinates
print('Ignoring very small detection with area {}'.format(area))
print('Ignoring very large detection with area {}'.format(area))
This will return candidates of all classes
For each detection in our candidate list
Don't match across categories
Is this a match?
"If so, add this example to the list for this detection"
We *don't* break here; we allow this instance to possibly
match multiple candidates.  There isn't an obvious right or
wrong here.
...for each detection on our candidate list
"If we found no matches, add this to the candidate list"
candidateDetections.append(candidate)
pyqtree
...for each detection
...for each row
Get all candidate detections
print('Found {} candidate detections for folder {}'.format(
"len(candidateDetections),dirName))"
"For debugging only, it's convenient to have these sorted"
as if they had never gone into a tree structure.  Typically
this is in practce a sort by filename.
...def find_matches_in_directory(dirName)
"%% Update the detection table based on suspicious results, write .csv output"
"An array of length nDirs, where each element is a list of DetectionLocation"
objects for that directory that have been flagged as suspicious
For each directory
For each suspicious detection group in this directory
For each instance of this suspicious detection
This should match the bbox for the detection event
The bbox for this instance should be almost the same as the bbox
"for this detection group, where ""almost"" is defined by the IOU"
threshold.
if iou < options.iouThreshold:
"print('IOU warning: {},{}'.format(iou,options.iouThreshold))"
Make sure the bounding box matches
"Make the probability negative, if it hasn't been switched by"
another bounding box
...for each instance
...for each detection
...for each directory
Update maximum probabilities
For each row...
No longer strictly true; sometimes I run RDE on RDE output
assert maxPOriginal >= 0
We should only be making detections *less* likely in this process
"If there was a meaningful change, count it"
"Negative probabilities should be the only reason maxP changed, so"
we should have found at least one negative value if we reached
this point.
...if there was a meaningful change to the max probability for this row
...for each row
If we're also writing output...
"...def update_detection_table(RepeatDetectionResults,options)"
%% Main function
#%% Input handling
Validate some options
Load the filtering file
Load the same options we used when finding repeat detections
...except for things that explicitly tell this function not to
find repeat detections.
...if we're loading from an existing filtering file
Check early to avoid problems with the output folder
"Load file to a pandas dataframe.  Also populates 'max_detection_conf', even if it's"
not present in the .json file.
detectionResults[detectionResults['failure'].notna()]
"Before doing any real work, make sure we can *probably* access images"
"This is just a cursory check on the first image, but it heads off most"
"problems related to incorrect mount points, etc.  Better to do this before"
spending 20 minutes finding repeat detections.
#%% Separate files into directories
This will be a map from a directory name to smaller data frames
This is a mapping back into the rows of the original table
iRow = 0; row = detectionResults.iloc[0]
Create a new DataFrame with just this row
rowsByDirectory[dirName] = pd.DataFrame(row)
...for each unique detection
Convert lists of rows to proper DataFrames
#% Look for matches (or load them from file)
"Are we actually looking for matches, or just loading from a file?"
length-nDirs list of lists of DetectionLocation objects
We're actually looking for matches...
"We get slightly nicer progress bar behavior using threads, by passing a pbar"
object and letting it get updated.  We can't serialize this object across
processes.
#%% Find suspicious locations based on match results
For each directory
""
iDir = 51
A list of DetectionLocation objects
A list of DetectionLocation objects
occurrenceList is a list of file/detection pairs
Sort the above-threshold detections for easier review
...for each directory
If we're just loading detections from a file...
"We're skipping detection-finding, but to see which images are actually legit false"
"positives, we may be looking for physical files or loading from a text file."
For each directory
iDir = 0; detections = suspiciousDetections[0]
""
"suspiciousDetections is an array of DetectionLocation objects,"
one per directory.
For each detection that was present before filtering
iDetection = 0; detection = detections[iDetection]
Are we checking the directory to see whether detections were actually false
"positives, or reading from a list?"
Is the image still there?
"If not, remove this from the list of suspicious detections"
...for each detection
...for each directory
...if we are/aren't finding detections (vs. loading from file)
Create filtering directory
iDir = 0; suspiciousDetectionsThisDir = suspiciousDetections[iDir]
suspiciousDetectionsThisDir is a list of DetectionLocation objects
iDetection = 0; detection = suspiciousDetectionsThisDir[0]
Sort instances in descending order by confidence
Choose the highest-confidence index
Should we render (typically in a very light color) detections
*other* than the one we're highlighting here?
Render other detections first (typically in a thin+light box)
Now render the example detection (on top of at least one
of the other detections)
This converts the *first* instance to an API standard detection;
"because we just sorted this list in descending order by confidence,"
this is the highest-confidence detection.
...if we are/aren't rendering other bounding boxes
...for each detection in this folder
...for each folder
Write out the detection index
...if we're writing filtering info
...find_repeat_detections()
#######
""
find_repeat_detections.py
""
"If you want to use this script, we recommend that you read the user's guide:"
""
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms
""
"Really, don't try to run this script without reading the user's guide, you'll think"
it's more magical than it is.
""
"This script looks through a sequence of detections in the API output json file, and finds"
"candidates that might be ""repeated false positives"", i.e. that random branch that the"
detector thinks is an animal/person/vehicle.
""
"Typically after running this script, you would do a manual step to remove"
"true positives, then run remove_repeat_detections to produce a final output file."
""
There's no way that statement was self-explanatory; see the user's guide.
""
#######
%% Constants and imports
%% Interactive driver
%%
%% Command-line driver
Convert to an options object
""
If a request has been sent to AML for batch scoring but the monitoring thread of the API was
"interrupted (uncaught exception or having to re-start the API container), we could manually"
"aggregate results from each shard using this script, assuming all jobs submitted to AML have finished."
""
Need to have set environment variables STORAGE_ACCOUNT_NAME and STORAGE_ACCOUNT_KEY to those of the
"storage account backing the API. Also need to adjust the INTERNAL_CONTAINER, AML_CONTAINER and"
AML_CONFIG fields in api_core/orchestrator_api/api_config.py to match the instance of the API that this
request was submitted to.
""
May need to change the import statement in api_core/orchestrator_api/orchestrator.py
"""from sas_blob_utils import SasBlob"" to"
"""from .sas_blob_utils import SasBlob"" to not confuse with the module in AI4Eutils;"
"and change ""import api_config"" to"
"""from api.batch_processing.api_core.orchestrator_api import api_config"""
Execute this script from the root of the repository. You may need to add the repository to PYTHONPATH.
"list_jobs_submitted cannot be serialized (""can't pickle _thread.RLock objects ""), but"
do not need it for aggregating results
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%% helper classes and functions
TODO log exception when we have more telemetry
TODO check that the expiry date of input_container_sas is at least a month
into the future
"if no permission specified explicitly but has an access policy, assumes okay"
TODO - check based on access policy as well
return current UTC time as a string in the ISO 8601 format (so we can query by
timestamp in the Cosmos DB job status table.
example: '2021-02-08T20:02:05.699689Z'
"image_paths will have length at least 1, otherwise would have ended before this step"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
a job moves from created to running/problem after the Batch Job has been submitted
"job_id should be unique across all instances, and is also the partition key"
TODO do not read the entry first to get the call_params when the Cosmos SDK add a
patching functionality:
https://feedback.azure.com/forums/263030-azure-cosmos-db/suggestions/6693091-be-able-to-do-partial-updates-on-document
need to retain other fields in 'status' to be able to restart monitoring thread
retain existing fields; update as needed
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
sentinel should change if new configurations are available
configs have not changed
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Gunicorn logger handler will get attached if needed in server.py
set for all tasks in the job
cannot execute the scoring script that is in the mounted directory; has to be copied to cwd
not luck giving the commandline arguments via formatted string - set as env vars instead
form shards of images and assign each shard to a Task
for persisting stdout and stderr
persist stdout and stderr (will be removed when node removed)
paths are relative to the Task working directory
can also just upload on failure
first try submitting Tasks
retry submitting Tasks
Change the Job's on_all_tasks_complete option to 'terminateJob' so the Job's status changes automatically
after all submitted tasks are done
This is so that we do not take up the quota for active Jobs in the Batch account.
return type: TaskAddCollectionResult
actually we should probably only re-submit if it's a server_error
docs: # https://docs.microsoft.com/en-us/rest/api/batchservice/odata-filters-in-batch#list-tasks
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%% Flask app
reference: https://trstringer.com/logging-flask-gunicorn-the-manageable-way/
%% Helper classes
%% Flask endpoints
required params
can be an URL to a file not hosted in an Azure blob storage container
"if use_url, then images_requested_json_sas is required"
optional params
check model_version is among the available model versions
check request_name has only allowed characters
optional params for telemetry collection - logged to status table for now as part of call_params
All API instances / node pools share a quota on total number of active Jobs;
we cannot accept new Job submissions if we are at the quota
required fields
request_status is either completed or failed
the create_batch_job thread will stop when it wakes up the next time
"Fix for Zooniverse - deleting any ""-"" characters in the job_id"
"If the status is running, it could be a Job submitted before the last restart of this"
"API instance. If that is the case, we should start to monitor its progress again."
WARNING model_version could be wrong (a newer version number gets written to the output file) around
"the time that  the model is updated, if this request was submitted before the model update"
and the API restart; this should be quite rare
conform to previous schemes
%% undocumented endpoints
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Gunicorn logger handler will get attached if needed in server.py
request_name and request_submission_timestamp are for appending to
output file names
image_paths can be a list of strings (Azure blob names or public URLs)
"or a list of length-2 lists where each is a [image_id, metadata] pair"
Case 1: listing all images in the container
- not possible to have attached metadata if listing images in a blob
list all images to process
+ 1 so if the number of images listed > MAX_NUMBER_IMAGES_ACCEPTED_PER_JOB
we will know and not proceed
Case 2: user supplied a list of images to process; can include metadata
filter down to those conforming to the provided prefix and accepted suffixes (image file types)
prefix is case-sensitive; suffix is not
"Although urlparse(p).path preserves the extension on local paths, it will not work for"
"blob file names that contains ""#"", which will be treated as indication of a query."
"If the URL is generated via Azure Blob Storage, the ""#"" char will be properly encoded"
apply the first_n and sample_n filters
OK if first_n > total number of images
sample by shuffling image paths and take the first sample_n images
"upload the image list to the container, which is also mounted on all nodes"
all sharding and scoring use the uploaded list
now request_status moves from created to running
an extra field to allow the monitoring thread to restart after an API restart: total number of tasks
also record the number of images to process for reporting
start the monitor thread with the same name
"both succeeded and failed tasks are marked ""completed"" on Batch"
"preserving format from before, but SAS URL to 'failed_images' and 'images' are no longer provided"
"failures should be contained in the output entries, indicated by an 'error' field"
"when people download this, the timestamp will have : replaced by _"
check if the result blob has already been written (could be another instance of the API / worker thread)
"and if so, skip aggregating and uploading the results, and just generate the SAS URL, which"
could be needed still if the previous request_status was `problem`.
upload the output JSON to the Job folder
%% Helper functions *copied* from ct_utils.py and visualization/visualization_utils.py
"Determine the factor, which shifts the decimal point of x"
just behind the last significant digit
"Shift decimal point by multiplicatipon with factor, flooring, and"
division by factor
PIL.Image.convert() returns a converted copy of this image
alter orientation as needed according to EXIF tag 0x112 (274) for Orientation
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28
https://www.media.mit.edu/pia/Research/deepview/exif.html
"%% TFDetector class, an unmodified *copy* of the class in detection/tf_detector.py,"
so we do not have to import the packages required by run_detector.py
Number of decimal places to round to for confidence and bbox coordinates
"MegaDetector was trained with batch size of 1, and the resizing function is a part"
of the inference graph
An enumeration of failure reasons
"change from [y1, x1, y2, x2] to [x1, y1, width, height]"
convert numpy floats to Python floats
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size
"np_images = [np.asarray(image, np.uint8) for image in images]"
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)"
performs inference
our batch size is 1; need to loop the batch dim if supporting batch size > 1
%% Scoring script
determine if there is metadata attached to each image_id
information to determine input and output locations
other parameters for the task
test that we can write to output path; also in case there is no image to process
list images to process
"items in this list can be strings or [image_id, metadata]"
model path
"Path to .pb TensorFlow detector model file, relative to the"
models/megadetector_copies folder in mounted container
score the images
""
manage_video_batch.py
""
Notebook-esque script to manage the process of running a local batch of videos
through MD.  Defers most of the heavy lifting to manage_local_batch.py .
""
%% Imports and constants
%% Split videos into frames
"%% List frame files, break into folders"
Find unique (relative) folders
fn = frame_files[0]
%% List videos
%% Check for videos that are missing entirely
list(folder_to_frame_files.keys())[0]
video_filenames[0]
fn = video_filenames[0]
%% Check for videos with very few frames
%% Print the list of videos that are problematic
%% Process images like we would for any other camera trap job
"...typically using manage_local_batch.py, but do this however you like, as long"
as you get a results file at the end.
""
"If you do RDE, remember to use the second folder from the bottom, rather than the"
bottom-most folder.
%% Convert frame results to video results
%% Confirm that the videos in the .json file are what we expect them to be
%% Scrap
%% Test a possibly-broken video
%% List videos in a folder
%% Imports
%% Constants
%% Classes
class variables
"instance variables, in order of when they are typically set"
Leaving this commented out to remind us that we don't want this check here; let
the API fail on these images.  It's a huge hassle to remove non-image
files.
""
for path_or_url in images_list:
if not is_image_file_or_url(path_or_url):
raise ValueError('{} is not an image'.format(path_or_url))
Commented out as a reminder: don't check task status (which is a rest API call)
in __repr__; require the caller to explicitly request status
"status=getattr(self, 'status', None))"
estimate # of failed images from failed shards
Download all three JSON urls to memory
Remove files that were submitted but don't appear to be images
assert all(is_image_file_or_url(s) for s in submitted_images)
Diff submitted and processed images
Confirm that the procesed images are a subset of the submitted images
https://www.geeksforgeeks.org/break-list-chunks-size-n-python/
%% Interactive driver
%%
%%
%%
%% Imports and constants
%% Constants I set per script
## Required
base_output_folder_name = os.path.expanduser('~/postprocessing/' + organization_name_short)
Shared Access Signature (SAS) tokens for the Azure Blob Storage container.
Leading question mark is optional.
""
The read-only token is used for accessing images; the write-enabled token is
used for writing file lists.
## Typically left as default
"Pre-pended to all folder names/prefixes, if they're defined below"
"This is how we break the container up into multiple taskgroups, e.g., for"
separate surveys. The typical case is to do the whole container as a single
taskgroup.
"If your ""folders"" are really logical folders corresponding to multiple folders,"
map them here
"A list of .json files to load images from, instead of enumerating.  Formatted as a"
"dictionary, like folder_prefixes."
This is only necessary if you will be performing postprocessing steps that
"don't yet support SAS URLs, specifically the ""subsetting"" step, or in some"
cases the splitting of files into multiple output directories for
empty/animal/vehicle/people.
""
"For those applications, you will need to mount the container to a local drive."
For this case I recommend using rclone whether you are on Windows or Linux;
rclone is much easier than blobfuse for transient mounting.
""
"But most of the time, you can ignore this."
"Supported model_versions: '4', '3', '4_prelim'"
""
Also available at the /supported_model_versions and /default_model_version
endpoints
""
"Unless you have any specific reason to set this to a non-default value, leave"
"it at the default, which as of 2020.04.28 is MegaDetector 4.1"
""
"additional_task_args = {""model_version"":""4_prelim""}"
""
"file_lists_by_folder will contain a list of local JSON file names,"
each JSON file contains a list of blob names corresponding to an API taskgroup
"%% Derived variables, path setup"
local folders
Turn warnings into errors if more than this many images are missing
%% Support functions
"scheme, netloc, path, query, fragment"
%% Read images from lists or enumerate blobs to files
folder_name = folder_names[0]
"Load file lists for this ""folder"""
""
file_list = input_file_lists[folder][0]
Write to file
A flat list of blob paths for each folder
folder_name = folder_names[0]
"If we don't/do have multiple prefixes to enumerate for this ""folder"""
"If this is intended to be a folder, it needs to end in '/', otherwise"
files that start with the same string will match too
...for each prefix
Write to file
...for each folder
%% Some just-to-be-safe double-checking around enumeration
Make sure each folder has at least one image matched; the opposite is usually a sign of a copy/paste issue
...for each image
...for each prefix
...for each folder
Make sure each image comes from one of our folders; the opposite is usually a sign of a bug up above
...for each prefix
...for each folder
...for each image
%% Divide images into chunks for each folder
The JSON file at folder_chunks[i][j] corresponds to task j of taskgroup i
list_file = file_lists_by_folder[0]
"%% Create taskgroups and tasks, and upload image lists to blob storage"
periods not allowed in task names
%% Generate API calls for each task
clipboard.copy(request_strings[0])
clipboard.copy('\n\n'.join(request_strings))
%% Run the tasks (don't run this cell unless you are absolutely sure!)
I really want to make sure I'm sure...
%% Estimate total time
Around 0.8s/image on 16 GPUs
%% Manually create task groups if we ran the tasks manually
%%
"%% Write task information out to disk, in case we need to resume"
%% Status check
print(task.id)
%% Resume jobs if this notebook closes
%% For multiple tasks (use this only when we're merging with another job)
%% For just the one task
%% Load into separate taskgroups
p = task_cache_paths[0]
%% Typically merge everything into one taskgroup
"%% Look for failed shards or missing images, start new tasks if necessary"
List of lists of paths
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup];
"Make a copy, because we append to taskgroup"
i_task = 0; task = tasks[i_task]
Each taskgroup corresponds to one of our folders
Check that we have (almost) all the images
Now look for failed images
Write it out as a flat list as well (without explanation of failures)
...for each task
...for each task group
%% Pull results
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup]; task = taskgroup[0]
Each taskgroup corresponds to one of our folders
...for each task
...for each task group
%% Combine results from task groups into final output files
Check that we have (almost) all the images
The only reason we should ever have a repeated request is the case where an
"image was missing and we reprocessed it, or where it failed and later succeeded"
"There may be non-image files in the request list, ignore those"
Something has gone bonkers if there are images in the results that
aren't in the request
...for each folder
%% Post-processing (no ground truth)
i_folder = 0; folder_name_raw = folder_names[i_folder]
"Omit some pages from the output, useful when animals are rare"
%% Manual processing follows
""
"Everything after this should be considered mostly manual, and no longer includes"
looping over folders.
""
%% RDE (sample directory collapsing)
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images"
"%% Repeat detection elimination, phase 1"
"Deliberately leaving these imports here, rather than at the top, because this cell is not"
typically executed
To invoke custom collapsing of folders for a particular manufacturer's naming scheme
options.customDirNameFunction = remove_overflow_folders
Exclude people and vehicles from RDE
"options.excludeClasses = [2,3]"
%% Manual RDE step
# DELETE THE VALID DETECTIONS ##
%% Re-filtering
%% Post-processing (post-RDE)
i_folder = 0; folder_name_raw = folder_names[i_folder]
"Omit some pages from the output, useful when animals are rare"
"print('No RDE file available for {}, skipping'.format(folder_name))"
continue
%% Create a new category for large boxes
%% Subsetting
i_folder = 0; folder_name = folders[i_folder]
%% String replacement
%% Folder splitting
i_folder = 0; folder_name_raw = folder_names[i_folder]
api_output_file = folder_name_to_combined_output_file[folder_name]
%% Imports and constants
from ai4eutils
To specify a non-default confidence threshold for including detections in the .json file
Turn warnings into errors if more than this many images are missing
Only relevant when we're using a single GPU
"Specify a target image size when running MD... strongly recommended to leave this at ""None"""
Only relevant when running on CPU
OS-specific script line continuation character
OS-specific script comment character
"Prefer threads on Windows, processes on Linux"
"This is for things like image rendering, not for MegaDetector"
Should we use YOLOv5's val.py instead of run_detector_batch.py?
Directory in which to run val.py.  Only relevant if use_yolo_inference_scripts is True.
Should we remove intermediate files used for running YOLOv5's val.py?
""
Only relevant if use_yolo_inference_scripts is True.
Should we apply YOLOv5's augmentation?  Only allowed when use_yolo_inference_scripts
is True.
%% Constants I set per script
Optional descriptor
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v5.0.0/md_v5b.0.0.pt')
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v4.1.0/md_v4.1.0.pb')
"Number of jobs to split data into, typically equal to the number of available GPUs"
Only used to print out a time estimate
"%% Derived variables, constant validation, path setup"
%% Enumerate files
%% Load files from prior enumeration
%% Divide images into chunks
%% Estimate total time
%% Write file lists
%% Generate commands
"A list of the scripts tied to each GPU, as absolute paths.  We'll write this out at"
the end so each GPU's list of commands can be run at once.  Generally only used when
"running lots of small batches via YOLOv5's val.py, which doesn't support checkpointing."
i_task = 0; task = task_info[i_task]
Generate the script to run MD
Check whether this output file exists
Generate the script to resume from the checkpoint (only supported with MD inference code)
...for each task
Write out a script for each GPU that runs all of the commands associated with
that GPU.  Typically only used when running lots of little scripts in lieu
of checkpointing.
...for each GPU
%% Run the tasks
%%% Run the tasks (commented out)
i_task = 0; task = task_info[i_task]
"This will write absolute paths to the file, we'll fix this later"
...for each chunk
...if False
"%% Load results, look for failed or missing images in each task"
i_task = 0; task = task_info[i_task]
im = task_results['images'][0]
...for each task
%% Merge results files and make images relative
im = combined_results['images'][0]
%% Post-processing (pre-RDE)
options.sample_seed = 0
"Omit some pages from the output, useful when animals are rare"
%% RDE (sample directory collapsing)
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images"
%%
%%
%%
relativePath = image_filenames[0]
"%% Repeat detection elimination, phase 1"
"Deliberately leaving these imports here, rather than at the top, because this"
cell is not typically executed
options.minSuspiciousDetectionSize = 0.05
This will cause a very light gray box to get drawn around all the detections
we're *not* considering as suspicious.
options.lineThickness = 5
options.boxExpansion = 8
To invoke custom collapsing of folders for a particular manufacturer's naming scheme
options.customDirNameFunction = remove_overflow_folders
Exclude people and vehicles from RDE
"options.excludeClasses = [2,3]"
options.maxImagesPerFolder = 50000
options.includeFolders = ['a/b/c']
options.excludeFolder = ['a/b/c']
"Can be None, 'xsort', or 'clustersort'"
import clipboard; clipboard.copy(os.path.dirname(suspiciousDetectionResults.filterFile))
path_utils.open_file(os.path.dirname(suspiciousDetectionResults.filterFile))
%% Manual RDE step
# DELETE THE VALID DETECTIONS ##
%% Re-filtering
%% Post-processing (post-RDE)
options.sample_seed = 0
"Omit some pages from the output, useful when animals are rare"
"%% Run MegaClassifier (actually, write out a script that runs MegaClassifier)"
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
#%% Set up environment
commands.append('cd CameraTraps/classification\n')
commands.append('conda activate cameratraps-classifier\n')
#%% Crop images
fn = input_files[0]
#%% Run classifier
fn = input_files[0]
#%% Remap classifier outputs
fn = input_files[0]
#%% Merge classification and detection outputs
fn = input_files[0]
#%% Write  out classification script
"%% Run a non-MegaClassifier classifier (i.e., a classifier with no output mapping)"
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
#%% Set up environment
#%% Crop images
fn = input_files[0]
#%% Run classifier
fn = input_files[0]
#%% Merge classification and detection outputs
fn = input_files[0]
#%% Write everything out
%% Run the classifier(s) via the .sh script(s) or batch file(s) we just wrote
...
%% Within-image classification smoothing
""
Only count detections with a classification confidence threshold above
"*classification_confidence_threshold*, which in practice means we're only"
looking at one category per detection.
""
If an image has at least *min_detections_above_threshold* such detections
"in the most common category, and no more than *max_detections_secondary_class*"
"in the second-most-common category, flip all detections to the most common"
category.
""
"Optionally treat some classes as particularly unreliable, typically used to overwrite an"
"""other"" class."
""
This cell also removes everything but the non-dominant classification for each detection.
""
How many detections do we need above the classification threshold to determine a dominant category
for an image?
"Even if we have a dominant class, if a non-dominant class has at least this many classifications"
"in an image, leave them alone."
"If the dominant class has at least this many classifications, overwrite ""other"" classifications"
What confidence threshold should we use for assessing the dominant category in an image?
Which classifications should we even bother over-writing?
Detection confidence threshold for things we count when determining a dominant class
Which detections should we even bother over-writing?
"Before we do anything else, get rid of everything but the top classification"
for each detection.
...for each detection in this image
...for each image
im = d['images'][0]
...for each classification
...if there are classifications for this detection
...for each detection
"Handle a quirky special case: if the most common category is ""other"" and"
"it's ""tied"" with the second-most-common category, swap them"
secondary_count = category_to_count[keys[1]]
The 'secondary count' is the most common non-other class
If we have at least *min_detections_to_overwrite_other* in a category that isn't
"""other"", change all ""other"" classifications to that category"
...for each classification
...if there are classifications for this detection
...for each detection
"...if we should overwrite all ""other"" classifications"
"At this point, we know we have a dominant category; change all other above-threshold"
"classifications to that category.  That category may have been ""other"", in which"
case we may have already made the relevant changes.
det = detections[0]
...for each classification
...if there are classifications for this detection
...for each detection
...for each image
...for each file we want to smooth
"%% Post-processing (post-classification, post-within-image-smoothing)"
classification_detection_file = classification_detection_files[1]
%% Read EXIF data from all images
%% Prepare COCO-camera-traps-compatible image objects for EXIF results
import dateutil
"This is a standard format for EXIF datetime, and dateutil.parser"
doesn't handle it correctly.
return dateutil.parser.parse(s)
exif_result = exif_results[0]
Currently we assume that each leaf-node folder is a location
"We collected this image this century, but not today, make sure the parsed datetime"
jives with that.
""
The latter check is to make sure we don't repeat a particular pathological approach
"to datetime parsing, where dateutil parses time correctly, but swaps in the current"
date when it's not sure where the date is.
...for each exif image result
%% Assemble into sequences
Make a list of images appearing at each location
im = image_info[0]
%% Load classification results
Map each filename to classification results for that file
%% Smooth classification results over sequences (prep)
These are the only classes to which we're going to switch other classifications
Only switch classifications to the dominant class if we see the dominant class at least
this many times
"If we see more than this many of a class that are above threshold, don't switch those"
classifications to the dominant class.
"If the ratio between a dominant class and a secondary class count is greater than this,"
"regardless of the secondary class count, switch those classificaitons (i.e., ignore"
max_secondary_class_classifications_above_threshold_for_class_smoothing).
""
"This may be different for different dominant classes, e.g. if we see lots of cows, they really"
"tend to be cows.  Less so for canids, so we set a higher ""override ratio"" for canids."
"If there are at least this many classifications for the dominant class in a sequence,"
"regardless of what that class is, convert all 'other' classifications (regardless of"
confidence) to that class.
"If there are at least this many classifications for the dominant class in a sequence,"
"regardless of what that class is, classify all previously-unclassified detections"
as that class.
Only count classifications above this confidence level when determining the dominant
"class, and when deciding whether to switch other classifications."
Confidence values to use when we change a detection's classification (the
original confidence value is irrelevant at that point)
%% Smooth classification results over sequences (supporting functions)
im = images_this_sequence[0]
det = results_this_image['detections'][0]
Only process animal detections
Only process detections with classification information
"We only care about top-1 classifications, remove everything else"
Make sure the list of classifications is already sorted by confidence
...and just keep the first one
"Confidence values should be sorted within a detection; verify this, and ignore"
...for each detection in this image
...for each image in this sequence
...top_classifications_for_sequence()
Count above-threshold classifications in this sequence
Sort the dictionary in descending order by count
"Handle a quirky special case: if the most common category is ""other"" and"
"it's ""tied"" with the second-most-common category, swap them."
...def count_above_threshold_classifications()
%% Smooth classifications at the sequence level (main loop)
Break if this token is contained in a filename (set to None for normal operation)
i_sequence = 0; seq_id = all_sequences[i_sequence]
Count top-1 classifications in this sequence (regardless of confidence)
Handy debugging code for looking at the numbers for a particular sequence
Count above-threshold classifications for each category
"If our dominant category ID isn't something we want to smooth to, don't mess around with this sequence"
"# Smooth ""other"" classifications ##"
"By not re-computing ""max_count"" here, we are making a decision that the count used"
"to decide whether a class should overwrite another class does not include any ""other"""
classifications we changed to be the dominant class.  If we wanted to include those...
""
sorted_category_to_count = count_above_threshold_classifications(classifications_this_sequence)
max_count = get_first_value_from_sorted_dictionary(sorted_category_to_count)
assert dominant_category_id == get_first_key_from_sorted_dictionary(sorted_category_to_count)
# Smooth non-dominant classes ##
Don't flip classes to the dominant class if they have a large number of classifications
"Don't smooth over this class if there are a bunch of them, and the ratio"
if primary to secondary class count isn't too large
Default ratio
Does this dominant class have a custom ratio?
# Smooth unclassified detections ##
...for each sequence
%% Write smoothed classification results
"%% Post-processing (post-classification, post-within-image-and-within-sequence-smoothing)"
%% Zip .json files
%% 99.9% of jobs end here
Everything after this is run ad hoc and/or requires some manual editing.
%% Compare results files for different model versions (or before/after RDE)
Choose all pairwise combinations of the files in [filenames]
%% Merge in high-confidence detections from another results file
%% Create a new category for large boxes
"This is a size threshold, not a confidence threshold"
size_options.categories_to_separate = [3]
%% Preview large boxes
%% .json splitting
options.query = None
options.replacement = None
Reminder: 'n_from_bottom' with a parameter of zero is the same as 'bottom'
%% Custom splitting/subsetting
i_folder = 0; folder_name = folders[i_folder]
"This doesn't do anything in this case, since we're not splitting folders"
options.make_folder_relative = True
%% String replacement
%% Splitting images into folders
%% Generate commands for a subset of tasks
i_task = 8
...for each task
%% End notebook: turn this script into a notebook (how meta!)
Exclude everything before the first cell
Remove the first [first_non_empty_lines] from the list
Add the last cell
""
xmp_integration.py
""
"Tools for loading MegaDetector batch API results into XMP metadata, specifically"
for consumption in digiKam:
""
https://cran.r-project.org/web/packages/camtrapR/vignettes/camtrapr2.html
""
%% Imports and constants
%% Class definitions
Folder where images are stored
.json file containing MegaDetector output
"String to remove from all path names, typically representing a"
prefix that was added during MegaDetector processing
Optionally *rename* (not copy) all images that have no detections
above [rename_conf] for the categories in rename_cats from x.jpg to
x.check.jpg
"Comma-deleted list of category names (or ""all"") to apply the rename_conf"
behavior to.
"Minimum detection threshold (applies to all classes, defaults to None,"
i.e. 0.0
%% Functions
Relative image path
Absolute image path
List of categories to write to XMP metadata
Categories with above-threshold detections present for
this image
Maximum confidence for each category
Have we already added this to the list of categories to
write out to this image?
If we're supposed to compare to a threshold...
Else we treat *any* detection as valid...
Keep track of the highest-confidence detection for this class
If we're doing the rename/.check behavior...
Legacy code to rename files where XMP writing failed
%% Interactive/test driver
%%
%% Command-line driver
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Cosmos DB `batch-api-jobs` table for job status
"aggregate the number of images, country and organization names info from each job"
submitted during yesterday (UTC time)
create the card
""
api_frontend.py
""
"Defines the Flask app, which takes requests (one or more images) from"
"remote callers and pushes the images onto the shared Redis queue, to be processed"
by the main service in api_backend.py .
""
%% Imports
%% Initialization
%% Support functions
Make a dict that the request_processing_function can return to the endpoint
function to notify it of an error
Verify that the content uploaded is not too big
""
request.content_length is the length of the total payload
Verify that the number of images is acceptable
...def check_posted_data(request)
%% Main loop
Check whether the request_processing_function had an error
Write images to temporary files
""
TODO: read from memory rather than using intermediate files
"Submit the image(s) for processing by api_backend.py, who is waiting on this queue"
TODO: convert to a blocking read and eliminate the sleep() statement in this loop
"image = Image.open(os.path.join(temp_direc, image_name))"
...if we do/don't have a request available on the queue
...while(True)
...def detect_sync()
%% Command-line driver
use --non-docker if you are testing without Docker
""
python api_frontend.py --non-docker
# Camera trap real-time API configuration
"Full path to the temporary folder for image storage, only meaningful"
within the Docker container
Upper limit on total content length (all images and parameters)
Minimum confidence threshold for detections
Minimum confidence threshold for showing a bounding box on the output image
Use this when testing without Docker
""
api_backend.py
""
"Defines the model execution service, which pulls requests (one or more images)"
"from the shared Redis queue, and runs them through the TF model."
""
%% Imports
%% Initialization
%% Main loop
TODO: convert to a blocking read and eliminate the sleep() statement in this loop
Filter the detections by the confidence threshold
""
"Each result is [ymin, xmin, ymax, xmax, confidence, category]"
""
"Coordinates are relative, with the origin in the upper-left"
...if serialized_entry
...while(True)
...def detect_process()
%% Command-line driver
use --non-docker if you are testing without Docker
""
python api_frontend.py --non-docker
run detections on a test image to load the model
%%
Importing libraries
%%
%%
%%
%%
GPU configuration: set up GPUs based on availability and user specification
Environment variable setup for numpy multi-threading. It is important to avoid cpu and ram issues.
Load and set configurations from the YAML file
Set a global seed for reproducibility
"If the annotation directory does not have a data split, split the data first"
Replace annotation dir from config with the directory containing the split files
Split the data according to the split type
"Get the path to the annotation files, and we only want to do this if we are not predicting"
Crop test data (most likely we don't need this)
Crop training data
Crop validation data
Dataset and algorithm loading based on the configuration
Logger setup based on the specified logger type
Callbacks for model checkpointing and learning rate monitoring
Trainer configuration in PyTorch Lightning
"Training, validation, or evaluation execution based on the mode"
%%
%%
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
PyTorch imports
"Importing the model, dataset, transformations and utility functions from PytorchWildlife"
Importing the utility function for saving cropped images
Setting the device to use for computations ('cuda' indicates GPU)
Initializing the MegaDetectorV5 model for image detection
Performing batch detection on the images
Saving the detected objects as cropped images
%%
Read the original CSV file
Prepare a list to store new records for the new CSV
Process the data if the name of the file is in the dataframe
Save the crop into a new csv
Add record to the new CSV data
Create a DataFrame from the new records
Define the path for the new CSV file
Save the new DataFrame to CSV
# DATA SPLITTING
Load the data from the csv file
Separate the features and the targets
First split to separate out the test set
Adjust val_size to account for the initial split
Second split to separate out the validation set
"Combine features, labels, and classification back into dataframes"
Create the output directory in case that it does not exist
Save the splits to new CSV files
Return the dataframes
Load the data from the csv file
Calculate train size based on val and test size
Get unique locations
Split locations into train and temp (temporary holding for val and test)
Adjust the proportions for val and test based on the remaining locations
"Allocate images to train, validation, and test sets based on their location"
Save the datasets to CSV files
Return the split datasets
Load the data from the csv file
Convert 'Photo_Time' from string to datetime
Calculate train size based on val and test size
Sort by 'Photo_Time' to ensure chronological order
Group photos into sequences based on a 30-second interval
Assign unique sequence IDs to each group
Get unique sequence IDs
Split sequence IDs into train and temp (temporary holding for val and test)
Adjust the proportions for val and test based on the remaining sequences
"Allocate images to train, validation, and test sets based on their sequence ID"
Save the datasets to CSV files
Return the split datasets
Exportable class names for external use
Applying the ResNet layers and operations
Initialize the network with the specified settings
Selecting the appropriate ResNet architecture and pre-trained weights
self.pretrained_weights = ResNet18_Weights.IMAGENET1K_V1
self.pretrained_weights = ResNet50_Weights.IMAGENET1K_V1
Constructing the feature extractor and classifier
Criterion for binary classification
Load pre-trained weights and adjust for the current model
init_weights = self.pretrained_weights.get_state_dict(progress=True)
Load the weights into the feature extractor
Identify missing and unused keys in the loaded weights
Import necessary libraries
Exportable class names for external use
Define the allowed image extensions
Define normalization mean and standard deviation for image preprocessing
Define data transformations for training and validation datasets
Load data for prediction
"self.data = glob(os.path.join(self.img_root,""*.{}"".format(self.extension)))"
Load data for training/validation
"Load datasets for different modes (training, validation, testing, prediction)"
Calculate class counts and label mappings
Define parameters for the optimizer
Optimizer parameters for feature extraction
Optimizer parameters for the classifier
Setup optimizer and optimizer scheduler
Forward pass
Calculate loss
Forward pass
Forward pass
Concatenate outputs from all test steps
Calculate the metrics and save the output
Forward pass
Concatenate outputs from all predict steps
Compute the confusion matrix from true labels and predictions
Calculate class-wise accuracy (accuracy for each class)
Calculate micro accuracy (overall accuracy)
Calculate macro accuracy (mean of class-wise accuracies)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Importing necessary basic libraries and modules
%%
PyTorch imports for tensor operations
%%
"Importing the models, transformations, and utility functions from PytorchWildlife"
%%
Setting the device to use for computations ('cuda' indicates GPU)
%%
Initializing the MegaDetectorV6 model for image detection
"Valid versions are MDV6-yolov9-c, MDV6-yolov9-e, MDV6-yolov10-c, MDV6-yolov10-e or MDV6-rtdetr-c"
Uncomment the following line to use MegaDetectorV5 instead of MegaDetectorV6
"detection_model = pw_detection.MegaDetectorV5(device=DEVICE, pretrained=True, version=""a"")"
%%
Initializing the model for image classification
%%
Initializing a box annotator for visualizing detections
Processing the video and saving the result with annotated detections and classifications
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Importing necessary basic libraries and modules
PyTorch imports
"Importing the model, dataset, transformations and utility functions from PytorchWildlife"
Importing basic libraries
"Importing the models, dataset, transformations, and utility functions from PytorchWildlife"
Setting the device to use for computations ('cuda' indicates GPU)
Initializing a supervision box annotator for visualizing detections
Create a temp folder
Initializing the detection and classification models
Defining functions for different detection scenarios
Create an exception for custom weights
"If the detection model is HerdNet, use dot annotator, else use box annotator"
Herdnet receives both clf and det confidence thresholds
Only run classifier when detection class is animal
Clean the temp folder if it contains files
Check the contents of the extracted folder
If the detection model is HerdNet set batch_size to 1
Building Gradio UI
The timelapse checkbox is only visible when the detection model is not HerdNet
Show timelapsed checkbox only when detection model is not HerdNet
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Importing necessary basic libraries and modules
PyTorch imports
%%
"Importing the model, dataset, transformations and utility functions from PytorchWildlife"
%%
Setting the device to use for computations ('cuda' indicates GPU)
%%
Initializing the MegaDetectorV6 model for image detection
"Valid versions are MDV6-yolov9-c, MDV6-yolov9-e, MDV6-yolov10-c, MDV6-yolov10-e or MDV6-rtdetr-c"
Uncomment the following line to use MegaDetectorV5 instead of MegaDetectorV6
"detection_model = pw_detection.MegaDetectorV5(device=DEVICE, pretrained=True, version=""a"")"
%% Single image detection
Specifying the path to the target image TODO: Allow argparsing
Performing the detection on the single image
Saving the detection results
Saving the detected objects as cropped images
%% Batch detection
Specifying the folder path containing multiple images for batch detection
Performing batch detection on the images
%% Output to annotated images
Saving the batch detection results as annotated images
%% Output to cropped images
Saving the detected objects as cropped images
%% Output to JSON results
Saving the detection results in JSON format
Saving the detection results in timelapse JSON format
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Importing necessary basic libraries and modules
PyTorch imports
%%
"Importing the model, dataset, transformations and utility functions from PytorchWildlife"
%%
Setting the device to use for computations ('cuda' indicates GPU)
%%
Initializing the HerdNet model for image detection
"If you want to use ennedi dataset weigths, you can use the following line:"
"detection_model = pw_detection.HerdNet(device=DEVICE, version=""ennedi"")"
%% Single image detection
Performing the detection on the single image
%% Output to annotated images
Saving the batch detection results as annotated images
%% Batch image detection
Specifying the folder path containing multiple images for batch detection
Performing batch detection on the images
%% Output to annotated images
Saving the batch detection results as annotated images
Saving the detection results in JSON format
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Importing necessary basic libraries and modules
PyTorch imports
%% Argument parsing
%%
Setting the device to use for computations ('cuda' indicates GPU)
%%
Initializing the MegaDetectorV6 model for image detection
"Valid versions are MDV6-yolov9-c, MDV6-yolov9-e, MDV6-yolov10-c, MDV6-yolov10-e or MDV6-rtdetr-c"
Uncomment the following line to use MegaDetectorV5 instead of MegaDetectorV6
"detection_model = pw_detection.MegaDetectorV5(device=DEVICE, pretrained=True, version=""a"")"
%% Batch detection
Performing batch detection on the images
%% Output to JSON results
Saving the detection results in JSON format
Separate the positive and negative detections through file copying:
This will read version from pyproject.toml
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Making the DetectionImageFolder class available for import from this module
Define the allowed image extensions
Get image filename and path
Load and convert image to RGB
Apply transformation if specified
Only run recognition on animal detections
Get image path and corresponding bbox xyxy for cropping
Load and crop image with supervision
Apply transformation if specified
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Making the provided classes available for import from this module
Convert PIL Image to Torch Tensor
Original shape
New shape
Scale ratio (new / old) and compute padding
Resize image
Pad image
Convert the image to a PyTorch tensor and normalize it
Resize and pad the image using a customized letterbox function.
Normalization constants
Define the sequence of transformations
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
!!! Output paths need to be optimized !!!
!!! Output paths need to be optimized !!!
Category filtering
if not all([x in exclude_category_ids for x in category]):
Category filtering
if not all([x in exclude_category_ids for x in category]):
if not all([x in exclude_category_ids for x in category_id_list]):
Find classifications for this detection
Load JSON data from the file
Ensure the destination directories exist
Process each image detection
Check if there is any category '0' with confidence above the threshold
Construct the source and destination file paths
Copy the file to the appropriate directory
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Importing basic libraries
Placeholder class-level attributes to be defined in derived classes
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Importing basic libraries
self.predictor.args.device = device # Will uncomment later
Creating a DataLoader for batching and parallel processing of the images
Normalize the coordinates for timelapse compatibility
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Importing basic libraries
Creating a DataLoader for batching and parallel processing of the images
Normalize the coordinates for timelapse compatibility
backbone
"self.cls_dla_up = dla_modules.DLAUp(channels[-3:], scales=scales[:3])"
bottleneck conv
localization head
classification head
decode_cls = self.cls_dla_up(encode[-3:])
clsmap = self.cls_head(decode_cls)
Assert that the dataset is either 'general' or 'ennedi'
"checkpoint = load_state_dict_from_url(url, map_location=torch.device(self.device)) # NOTE: This function is not used in the current implementation"
Load the class names and other metadata from the checkpoint
Load the model architecture
Load checkpoint into model
Remove 'model.' prefix from the state_dict keys if the key starts with 'model.'
Load the new state_dict
Creating a Dataloader for batching and parallel processing of the images
Flatten the lists since we know its a single image
Calculate the total number of detections
Pre-allocate based on total possible detections
Loop through each species
Get the detections for this species
Apply the confidence threshold
Fill the preds_array with the valid detections
Call the forward method of the model in evaluation mode
x = self.fc(x)
y = self.softmax(self.up(x))
patches' height & width
unfold on height
if non-perfect division on height
get the residual patch and add it to the fold
unfold on width
"if non-perfect division on width, the same"
reshaping
patches' height & width
lists of pixels numbers
cut into patches to get limits
if non-perfect division on height
if non-perfect division on width
@property
def area(self) -> int:
''' To get area '''
return 1 # always 1 pixel
local maxima
adaptive threshold for counting
negative sample
count
locations and scores
upsample class map
softmax
cat to heatmap
LMDS
step 1 - get patches and limits
step 2 - inference to get maps
step 3 - patch the maps into initial coordinates system
(step 4 - upsample)
outputs = self.model(patch)[0]
cat
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Image size for the Opossum classifier
Class names for prediction
"If pretrained, use the provided URL to fetch the weights"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Making the PlainResNetInference class available for import from this module
Following the ResNet structure to extract features
Initialize the network and weights
... [Missing weight URL definition for ResNet18]
... [Missing weight URL definition for ResNet50]
Print missing and unused keys for debugging purposes
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Image size for the classifier
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Image size for the Opossum classifier
Class names for prediction
"If pretrained, use the provided URL to fetch the weights"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Image size for the Opossum classifier
Class names for prediction
"If pretrained, use the provided URL to fetch the weights"
Configuration file for the Sphinx documentation builder.
""
"For the full list of built-in configuration values, see the documentation:"
https://www.sphinx-doc.org/en/master/usage/configuration.html
-- Project information -----------------------------------------------------
https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information
-- General configuration ---------------------------------------------------
https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration
-- Options for HTML output -------------------------------------------------
https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output
-- Options for todo extension ----------------------------------------------
https://www.sphinx-doc.org/en/master/usage/extensions/todo.html#configuration
"Determine the factor, which shifts the decimal point of x"
just behind the last significant digit
"Shift decimal point by multiplicatipon with factor, flooring, and"
division by factor
_ = pretty_print_object(obj)
Sloppy that I'm making a module-wide change here...
Determine the coordinates of the intersection rectangle
The intersection of two axis-aligned bounding boxes is always an
axis-aligned bounding box
Compute the area of both AABBs
Compute the intersection over union by taking the intersection
area and dividing it by the sum of prediction + ground-truth
areas - the intersection area.
%% Functions for running commands as subprocesses
%%
%% Test driver for execute_and_print
%% Parallel test driver for execute_command_and_print
Should we use threads (vs. processes) for parallelization?
"Only relevant if n_workers == 1, i.e. if we're not parallelizing"
%% Constants and imports
%% Classes
Collect all names
Make names unique and sort
cast location to string as the entries in locations are strings
Convert classnames to lowercase to simplify comparisons later
Normalize paths to simplify comparisons later
"Make custom replacements in filenames, typically used to"
accommodate changes in root paths after DB construction
## Build useful mappings to facilitate working with the DB
Category ID <--> name
Image filename --> ID
Image ID --> image object
Image ID --> annotations
"Each image can potentially multiple annotations, hence using lists"
...__init__
...class IndexedJsonDb
%% Functions
Find all unique locations
i_location = 0; location = locations[i_location]
Sorting datetimes fails when there are None's in the list.  So instead of sorting datetimes
"directly, sort tuples with a boolean for none-ness, then the datetime itself."
""
https://stackoverflow.com/questions/18411560/sort-list-while-pushing-none-values-to-the-end
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_location[1]
"Start a new sequence if necessary, including the case where this datetime is invalid"
"If this was an invalid datetime, this will record the previous datetime"
"as None, which will force the next image to start a new sequence."
...for each image in this location
Fill in seq_num_frames
...for each location
...create_sequences()
""
cct_to_md.py
""
"""Converts"" a COCO Camera Traps file to a MD results file.  Currently ignores"
"non-bounding-box annotations, and gives all annotations a confidence of 1.0."
""
The only reason to do this is if you are going to add information to an existing
"CCT-formatted dataset, and want to do that in Timelapse."
""
"Currently assumes that width and height are present in the input data, does not"
read them from images.
""
%% Constants and imports
%% Functions
# Validate input
# Read input
# Prepare metadata
ann = d['annotations'][0]
# Process images
im = d['images'][0]
"MegaDetector: [x,y,width,height] (normalized, origin upper-left)"
"CCT: [x,y,width,height] (absolute, origin upper-left)"
...if there's a bounding box
...for each annotation
This field is no longer included in MD output files by default
im_out['max_detection_conf'] = max_detection_conf
...for each image
# Write output
...cct_to_md()
%% Command-line driver
TODO
%% Interactive driver
%%
%%
""
cct_json_to_filename_json.py
""
"Given a .json file in COCO Camera Traps format, outputs a .json-formatted list of"
relative file names present in the CCT file.
""
%% Constants and environment
%% Main function
"json.dump(s,open(outputFilename,'w'))"
%% Command-line driver
%% Interactive driver
%%
""
cct_to_csv.py
""
"""Converts"" a COCO Camera Traps .json file to .csv, in quotes because"
"all kinds of assumptions are made here, and if you have a particular .csv"
"format in mind, YMMV.  Most notably, does not include any bounding box information"
or any non-standard fields that may be present in the .json file.  Does not
propagate information about sequence-level vs. image-level annotations.
""
"Does not assume access to the images, therefore does not open .jpg files to find"
"datetime information if it's not in the metadata, just writes datetime as 'unknown'."
""
%% Imports
%% Main function
#%% Read input
#%% Build internal mappings
annotation = annotations[0]
#%% Write output file
im = images[0]
Write out one line per class:
...for each class name
...for each image
...with open(output_file)
...def cct_to_csv
%% Interactive driver
%%
%% Command-line driver
""
remove_exif.py
""
"Removes all EXIF/IPTC/XMP metadata from a folder of images, without making"
"backup copies, using pyexiv2."
""
%% Imports and constants
%% List files
%% Remove EXIF data (support)
PYEXIV2 IS NOT THREAD SAFE; DO NOT CALL THIS IN PARALLEL FROM A SINGLE PROCESS
data = img.read_exif(); print(data)
%% Debug
%%
%%
%% Remove EXIF data (execution)
fn = image_files[0]
"joblib.Parallel defaults to a process-based backend, but let's be sure"
"results = Parallel(n_jobs=n_exif_threads,verbose=2,prefer='processes')(delayed(remove_exif)(fn) for fn in image_files[0:10])"
""
yolo_to_coco.py
""
Converts a YOLO-formatted dataset to a COCO-formatted dataset.
""
"Currently supports only a single folder (i.e., no recursion).  Treats images without"
corresponding .txt files as empty.
""
%% Imports and constants
from ai4eutils
%% Support functions
Validate input
Class names
Blank lines should only appear at the end
Enumerate images
fn = image_files[0]
Create the image object for this image
Is there an annotation file for this image?
"This is an image with no annotations, currently don't do anything special"
here
s = lines[0]
"COCO: [x_min, y_min, width, height] in absolute coordinates"
"YOLO: [class, x_center, y_center, width, height] in normalized coordinates"
...for each annotation
...if this image has annotations
...for each image
...def yolo_to_coco()
%% Interactive driver
%% Convert YOLO folders to COCO
%% Check DB integrity
%% Preview some images
%% Command-line driver
TODO
""
read_exif.py
""
"Given a folder of images, read relevant metadata (EXIF/IPTC/XMP) fields from all images,"
and write them to  a .json or .csv file.
""
This module can use either PIL (which can only reliably read EXIF data) or exiftool (which
can read everything).  The latter approach expects that exiftool is available on the system
path.  No attempt is made to be consistent in format across the two approaches.
""
%% Imports and constants
From ai4eutils
%% Options
Number of concurrent workers
Should we use threads (vs. processes) for parallelization?
""
Not relevant if n_workers is 1.
Should we use exiftool or pil?
%% Functions
exif_tags = img.info['exif'] if ('exif' in img.info) else None
print('Warning: unrecognized EXIF tag: {}'.format(k))
"-G means ""Print group name for each tag"", e.g. print:"
""
[File]          Bits Per Sample                 : 8
""
...instead of:
""
Bits Per Sample                 : 8
A list of three-element lists (type/tag/value)
line_raw = exif_lines[0]
A typical line:
""
[ExifTool]      ExifTool Version Number         : 12.13
"Split on the first occurrence of "":"""
...for each output line
...which processing library are we using?
...read_exif_tags_for_image()
...populate_exif_data()
Enumerate *relative* paths
Find all EXIF tags that exist in any image
...for each tag in this image
...for each image
Write header
...for each key that *might* be present in this image
...for each image
...with open()
...if we're writing to .json/.csv
https://stackoverflow.com/questions/11210104/check-if-a-program-exists-from-a-python-script
%% Interactive driver
%%
output_file = os.path.expanduser('~/data/test-exif.csv')
options.processing_library = 'pil'
"file_path = os.path.join(input_folder,'KRU_S1_11_R1_IMAG0148.JPG')"
%%
%% Command-line driver
""
"Given a json-formatted list of image filenames, retrieve the width and height of every image."
""
%% Constants and imports
%% Processing functions
Is this image on disk?
"all_results = list(tqdm(pool.imap(process_image, filenames), total=len(filenames)))"
%% Interactive driver
%%
List images in a test folder
%%
"process_list_file(image_list_file,image_size_file,image_prefix=base_dir)"
%% Command-line driver
"python sanity_check_json_db.py ""e:\wildlife_data\wellington_data\wellington_camera_traps.json"" --baseDir ""e:\wildlife_data\wellington_data\images"" --bFindUnusedImages --bCheckImageSizes"
"python sanity_check_json_db.py ""D:/wildlife_data/mcgill_test/mcgill_test.json"" --baseDir ""D:/wildlife_data/mcgill_test"" --bFindUnusedImages --bCheckImageSizes"
"Here the '-u' prevents buffering, which makes tee happier"
""
python -u sanity_check_json_db.py '/datadrive1/nacti_metadata.json' --baseDir '/datadrive1/nactiUnzip/' --bFindUnusedImages --bCheckImageSizes | tee ~/nactiTest.out
""
Partially-formed stub to get from MegaDetector output files to COCO Camera Traps data.
""
"Was actually written to convert *many* MD .json files to a single CCT file, hence"
the loop over .json files.
""
"THIS CODE HAS NEVER BEEN RUN, it was added as a demonstration of how to do this.  YMMV."
""
"You may find a more polished, command-line-ready version of this code at:"
""
https://github.com/StewartWILDlab/mdtools
""
%% Constants and imports
"Images sizes are required to convert between absolute and relative coordinates,"
so we need to read the images.
Only required if you want to write a database preview
%% Create CCT dictionaries
image_ids_to_images = {}
Force the empty category to be ID 0
Load .json annotations for this data set
i_entry = 0; entry = data['images'][i_entry]
""
"PERF: Not exactly trivially parallelizable, but about 100% of the"
time here is spent reading image sizes (which we need to do to get from
"absolute to relative coordinates), so worth parallelizing."
Generate a unique ID from the path
detection = detections[0]
Have we seen this category before?
Create an annotation
"MegaDetector: [x,y,width,height] (normalized, origin upper-left)"
"CCT: [x,y,width,height] (absolute, origin upper-left)"
...for each detection
...for each image
Remove non-reviewed images and associated annotations
%% Create info struct
%% Write .json output
%% Clean start
## Everything after this should work from a clean start ###
%% Validate output
%% Preview animal labels
%% Preview empty labels
"viz_options.classes_to_exclude = ['empty','human']"
""
generate_crops_from_cct.py
""
"Given a .json file in COCO Camera Traps format, create a cropped image for"
each bounding box.
""
%% Imports and constants
%% Functions
# Read and validate input
# Find annotations for each image
"This actually maps image IDs to annotations, but only to annotations"
containing boxes
# Generate crops
TODO: parallelize this loop
im = d['images'][0]
Load the image
Generate crops
i_ann = 0; ann = annotations_this_image[i_ann]
"x/y/w/h, origin at the upper-left"
...for each box
...for each image
...generate_crops_from_cct()
%% Interactive driver
%%
%%
%%
%% Command-line driver
TODO
%% Scrap
%%
""
coco_to_yolo.py
""
Converts a COCO-formatted dataset to a YOLO-formatted dataset.
""
"If the input and output folders are the same, writes .txt files to the input folder,"
and neither moves nor modifies images.
""
"Currently ignores segmentation masks, and errors if an annotation has a"
segmentation polygon but no bbox
""
Has only been tested on a handful of COCO Camera Traps data sets; if you
"use it for more general COCO conversion, YMMV."
""
%% Imports and constants
%% Support functions
Validate input
Read input data
Parse annotations
i_ann = 0; ann = data['annotations'][0]
Make sure no annotations have *only* segmentation data
Re-map class IDs to make sure they run from 0...n-classes-1
""
"TODO: this allows unused categories in the output data set, which I *think* is OK,"
but I'm only 81% sure.
Process images (everything but I/O)
"List of dictionaries with keys 'source_image','dest_image','bboxes','dest_txt'"
i_image = 0; im = data['images'][i_image]
"assert os.path.isfile(source_image), 'Could not find image {}'.format(source_image)"
If this annotation has no bounding boxes...
"This is not entirely clear from the COCO spec, but it seems to be consensus"
"that if you want to specify an image with no objects, you don't include any"
annotations for that image.
We allow empty bbox lists in COCO camera traps; this is typically a negative
"example in a dataset that has bounding boxes, and 0 is typically the empty"
category.
...if this is an empty annotation
"COCO: [x_min, y_min, width, height] in absolute coordinates"
"YOLO: [class, x_center, y_center, width, height] in normalized coordinates"
Convert from COCO coordinates to YOLO coordinates
...for each annotation
...if this image has annotations
...for each image
Write output
Category IDs should range from 0..N-1
TODO: parallelize this loop
""
output_info = images_to_copy[0]
Only write an annotation file if there are bounding boxes.  Images with
"no .txt files are treated as hard negatives, at least by YOLOv5:"
""
https://github.com/ultralytics/yolov5/issues/3218
""
"I think this is also true for images with empty annotation files, but"
"I'm using the convention suggested on that issue, i.e. hard negatives"
are expressed as images without .txt files.
bbox = bboxes[0]
...for each image
...def coco_to_yolo()
%% Interactive driver
%% CCT data
"If preview_export is True, I'm exporting to preview these with BoundingBoxEditor:"
""
https://github.com/mfl28/BoundingBoxEditor
""
"This export will be compatible, other than the fact that you need to move"
"""object.data"" into the ""labels"" folder."
""
"Otherwise I'm exporting for training, in the YOLOv5 flat format."
%% Command-line driver
TODO
""
cct_to_wi.py
""
Converts COCO Camera Traps .json files to the Wildlife Insights
batch upload format
""
Also see:
""
https://github.com/ConservationInternational/Wildlife-Insights----Data-Migration
""
https://data.naturalsciences.org/wildlife-insights/taxonomy/search
""
%% Imports
%% Paths
A COCO camera traps file with information about this dataset
A .json dictionary mapping common names in this dataset to dictionaries with the
"WI taxonomy fields: common_name, wi_taxon_id, class, orer, family, genus, species"
%% Constants
%% Project information
%% Read templates
%% Compare dictionary to template lists
Write the header
Write values
%% Project file
%% Camera file
%% Deployment file
%% Images file
Read .json file with image information
Read taxonomy dictionary
Populate output information
df = pd.DataFrame(columns = images_fields)
annotation = annotations[0]
im = input_data['images'][0]
"We don't have counts, but we can differentiate between zero and 1"
This is the label mapping used for our incoming iMerit annotations
"Only used to parse the incoming annotations. In our database, the string name is used to avoid confusion"
MegaDetector outputs
""
add_bounding_boxes_to_megadb.py
""
"Given COCO-formatted JSONs containing manually labeled bounding box annotations, add them to"
"MegaDB sequence entries, which can then be ingested into MegaDB."
the category map that comes in the COCO JSONs for iMerit batch 12 - to check that each
JSON
"dataset name : (seq_id, frame_num) : [bbox, bbox]"
where bbox is a dict with str 'category' and list 'bbox'
iterate over image_id_to_image rather than image_id_to_annotations so we include
the confirmed empty images
The file_name field in the incoming json looks like
alka_squirrels.seq2020_05_07_25C.frame119221.jpg
there seems to be a bug in the annotations where sometimes there's a
non-empty label along with a label of category_id 5
ignore the empty label (they seem to be actually non-empty)
""
add_bounding_boxes_to_json.py
""
This script takes a image database in the COCO Camera Traps format and merges in a set of bounding
box annotations in the format that iMerit uses (a .json where actually only each row is a valid json).
""
"If you need to update an existing bbox database, please get all the original annotation files and"
re-generate from scratch
""
%% Imports
%% Configurations and paths
images database
output bboxes database
annotation files (pseudo json) obtained from our annotation vendor that contain annotations for this dataset
None or a string or tuple of strings that is the prefix to all file_name of interest / in this dataset in the annotation files
functions for mapping the image_id in the annotation files (pseudo jsons) to the image_id used in the image DB
our img_id doesn't contain frame info
"batch3 - ""file_name"":""ASG0000019_0_S1_B06_R1_PICT0007.JPG"""
"batch5 and 7 - ""file_name"":""datasetsnapshotserengeti.seqASG000002m-frame0.imgS1_B06_R1_PICT0056.JPG"""
sometimes - 'datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG'
"id in DB (old_token): 'S6/J01/J01_R1/S6_J01_R1_IMAG0001', 'S1/B05/B05_R1/S1_B05_R1_PICT0036'"
specify which one to use for your dataset here
%% Load the image database and fill in DB info for the output bbox database
load the images database
%% Find the height and width of images from the annotation files
""
...if they are not available in the images DB
each row in this pseudo-json is a COCO formatted entry for an image sequence
%% Other functions required by specific datasets
the IDFG image database does not include images from unlabeled folders that were annotated with bounding boxes
%% Create the bbox database from all annotation files pertaining to this dataset
"for the incoming annotations, look up by category name (common) and convert to the numerical id used in our databases"
"for each annotation pseudo-json, check that the image it refers to exists in the original database"
each row in this pseudo-json is a COCO formatted entry for an image sequence
check that entry is for this dataset
category map for this entry in the annotation file - usually the same across all entries but just in case
rspb_add_image_entry(img_id)
use the image length and width in the image DB
"[top left x, top left y, width, height] in relative coordinates"
"add all images that have been sent to annotation, some of which may be empty of bounding boxes"
rspb_add_image_entry(db_image_id)
%% Common queries
This query is used when preparing tfrecords for object detector training.
We do not want to get the whole seq obj where at least one image has bbox because
some images in that sequence will not be bbox labeled so will be confusing.
Include images with bbox length 0 - these are confirmed empty by bbox annotators.
"If frame_num is not available, it will not be a field in the result iterable."
"Note that the seq_id is the Cosmos DB assigned ID for that sequence, not the"
"seq_id field, which may contain ""/"" characters."
"For public datasets to be converted to the CCT format, we get the whole seq object because"
sequence level attributes need to be included too. megadb/converters/megadb_to_cct.py handles
the case of bbox-only JSONs with the flag exclude_species_class
Getting all sequences in a dataset - for updating or deleting entries which need the id field
%% Parameters
Use None if querying across all partitions
"The `sequences` table has the `dataset` as the partition key, so if only querying"
"entries from one dataset, set the dataset name here."
"e.g. {'name': '@top_n', 'value': 100} - see query_and_upsert_examples/query_for_data.ipynb"
Use False if do not want all results stored in a single JSON.
%% Script
execute the query
loop through and save the results
MODIFY HERE depending on the query
wiitigers Unicode issue - no good mapping from DB file names to file names in blob URL
build filename
if need to re-download a dataset's images in case of corruption
entries_to_download = {
"filename: entry for filename, entry in entries_to_download.items()"
if entry['dataset'] == DATASET
}
input validation
"existing files, with paths relative to <store_dir>"
parse JSON or TXT file
"create a new storage container client for this dataset,"
and cache it
schema already checks that the min possible value of frame_num is 0
"if there are more than one image item, each needs a frame_num"
checks across all sequence items
per sequence item checks
load the schema
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory
load the schema
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory
checks across all datasets items
check for expiry date of the SAS keys
update the sequences
"`id` is from the database, as well as all attributes starting with _"
"if valuable sequence information is available, add them to the image"
required fields for an image object
add seq-level class labels for this image
add other sequence-level properties to each image too
add other image-level properties
... for im in seq['images']
... for seq in mega_db
consolidate categories
some property names have changed in the new schema
a dummy sequence ID will be generated if the image entry does not have a seq_id field
seq_id only needs to be unique within this dataset; MegaDB does not rely on it as the _id field
"""annotations"" fields are opened and have its sub-field surfaced one level up"
set the `dataset` property on each sequence to the provided dataset_name
check that the location field is the same for all images in a sequence
check which fields in a CCT image entry are sequence-level
image-level properties that really should be sequence-level
need to add (misidentified) seq properties not present for each image in a sequence to img_level_properties
"(some properties act like flags - all have the same value, but not present on each img)"
add the sequence-level properties to the sequence objects
not every sequence have to have all the seq_level_properties
get the value of this sequence-level property from the first image entry
check which fields are really dataset-level and should be included in the dataset table instead.
delete sequence-level properties that should be dataset-level
make all `class` fields lower-case; cast `seq_id` to type string in case they're integers
turn all float NaN values into None so it gets converted to null when serialized
this was an issue in the Snapshot Safari datasets
%% validation
"at first a dict of image_id: image_obj with annotations embedded, then it becomes"
an array of image objects
%% integrate the image DB
takes in image entries and species and other annotations in the image DB
convert the species category to explicit string name
there may be other fields in the annotation object
these fields should already be gotten from the image object
%% integrate the bbox DB
add any images that are not in the image DB
also add any fields in the image object that are not present already
add bbox to the annotations field
for any newly added images
"'bbox_abs': bbox_anno['bbox'],"
not keeping height and width
""
jb_csv_to_json.py
""
Convert a particular .csv file to CCT format.  Images were not available at
"the time I wrote this script, so this is much shorter than other scripts"
in this folder.
""
%% Constants and environment
%% Read source data
%% Confirm filename uniqueness (this data set has one label per image)
"Build up a map from filenames to a list of rows, checking image existence as we go"
%% Create CCT dictionaries
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
imageName = imageFilenames[0]
"As per above, this is convenient and appears to be true; asserting to be safe"
"Filenames look like ""290716114012001a1116.jpg"""
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
%% Create info struct
%% Write output
%% Sanity-check
%% Imports and constants
%% Load data
"with open(image_json,'r') as f:"
data = json.load(f)
%% Sanity-check data
%% Label previews
%% Collect images to annotate
%% Sort by sequence and frame
%% Copy to a folder by GUID
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension
im = images_to_annotate[0]
...for each image
%% Write out the annotation list
%% Constants and imports
%% Enumerate files
edited_image_folder = edited_image_folders[0]
fn = edited_image_files[0]
%% Read metadata and capture location information
i_row = 0; row = df.iloc[i_row]
Sometimes '2017' was just '17' in the date column
%% Read the .json files and build output dictionaries
json_fn = json_files[0]
if 'partial' in json_fn:
continue
line = lines[0]
if image_fn == 'SD1_238_6_26_17_16_76.73.jpg':
asdfad
SD29_079_5_14_2018_17_52.85.jpg
Re-write two-digit years as four-digit years
Sometimes the year was written with two digits instead of 4
assert len(tokens[4]) == 4 and tokens[4].startswith('20')
Have we seen this location already?
"Not a typo, it's actually ""formateddata"""
An image shouldn't be annotated as both empty and non-empty
An image shouldn't be annotated as both empty and non-empty
box = formatteddata[0]
"open_file(os.path.join(base_folder,jpeg_image_folder,image_fn))"
...for each box
...if there are boxes on this image
...for each line
...with open()
...for each json file
%% Prepare the output .json
%% Check DB integrity
%% Print unique locations
SD12_202_6_23_2017_1_31.85.jpg
%% Preview some images
%% Statistics
""
"Import a Snapshot Safari project (one project, one season)"
""
Before running this script:
""
"* Mount the blob container where the images live, or copy the"
images to local storage
""
What this script does:
""
* Creates a .json file
* Creates zip archives of the season without humans.
* Copies animals and humans to separate folders
""
After running this script:
""
* Create or update LILA page
* Push zipfile and unzipped images to LILA
* Push unzipped humans to wildlifeblobssc
* Delete images from UMN uplaod storage
""
"Snapshot Serengeti is handled specially, because we're dealing with bounding"
boxes too.  See snapshot_serengeti_lila.py.
""
%% Imports
From ai4eutils
From CameraTraps
%% Constants
project_name = 'XXX'; season_name = 'S1'; project_friendly_name = 'Snapshot Unknown'
project_name = 'SER'; season_name = 'S1-11'; project_friendly_name = 'Snapshot Serengeti'
project_name = 'KRU'; season_name = 'S1'; project_friendly_name = 'Snapshot Kruger'
project_name = 'CDB'; season_name = 'S1'; project_friendly_name = 'Snapshot Camdeboo'
project_name = 'MTZ'; season_name = 'S1'; project_friendly_name = 'Snapshot Mountain Zebra'
project_name = 'ENO'; season_name = 'S1'; project_friendly_name = 'Snapshot Enonkishu'
project_name = 'KAR'; season_name = 'S1'; project_friendly_name = 'Snapshot Karoo'
project_name = 'KGA'; season_name = 'S1'; project_friendly_name = 'Snapshot Kgalagadi'
%% Folder/file creation
E.g. KRU_S1
E.g. Z:\KRU
E.g. Z:\KRU\KRU_S1
Contains annotations for each capture event (sequence)
Maps image IDs to filenames; each line looks like:
""
"KRU_S1#1#1#2,3,KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0004.JPG"
"Total number of each answer to each question, e.g. total number of times each species was identified"
""
Not used here
Create output folders
Images will be placed in a season-specific folder inside this (the source data includes
this in path names)
%% Load metadata files
%% Convert to dictionaries (prep)
%% Convert to dictionaries (loops)
"TODO: iterrows() is a terrible way to do this, but this is one of those days"
"where I want to get this done, not get better at Python."
irow = 0; row = image_table.iloc[0]
"Loaded as an int64, converting to int here"
...for each row in the image table
Make sure image IDs are what we think they are
...for each row in the annotation table
%% Take a look at categories (just sanity-checking)
print('\nCategories by species:')
pp.pprint(categories_by_species)
%% Fill in some image fields we didn't have when we created the image table
"width, height, corrupt, seq_num_frames, location, datetime"
Every annotation in this list should have the same sequence ID
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;"
there's no reason to do this redundantly for every image
Every image in this sequence should point back to the same equence
Every annotation in this list should have the same location
Every annotation in this list should have the same datetime
Is this image on disk?
iImage = 0; im = images[0]
...for each image
"images_processed = pool.map(process_image, images)"
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))"
%% Count missing/corrupted images
%% Print distribution of sequence lengths (sanity-check)
%% Replicate annotations across images
iAnn = 0; ann = annotations[iAnn]
%% See what files are on disk but not annotated
%% Sanity-check image and annotation uniqueness
%% Minor updates to fields
%% Write .json file
%% Create a list of human files
ann = annotations[0]
%% Create public archive and public/private folders
im = images[0]
E.g. KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0001.JPG
Copy to private output folder
Add to zipfile
Possibly start a new archive
Copy to public output folder
...for each image
%% Sanity-check .json file
"This will produce some validation errors, because this zipfile doesn't include humans"
%% Zip up .json and .csv files
%% When I skip to this part (using a pre-rendered .json file)
%%
%%
ann = annotations[0]
%% Summary prep for LILA
"%% Generate preview, sanity-check labels"
"viz_options.classes_to_include = ['jackalblackbacked','bustardkori']"
%% Scrap
%% Find annotations for a particular image
%% Write a list of missing images
""
save_the_elephants_survey_A.py
""
Convert the .csv file provided for the Save the Elephants Survey A data set to a
COCO-camera-traps .json file
""
%% Constants and environment
Handle all unstructured fields in the source data as extra fields in the annotations
"photo_type really should be an image property, but there are a few conflicts"
that forced me to handle it as an annotation proprerty
%% Read source data
"%% Map filenames to rows, verify image existence"
Maps relative paths to row indices in input_metadata
"Build up a map from filenames to a list of rows, checking image existence as we go"
Ignore directories
%% Make sure the multiple-annotation cases make sense
%%
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
Force the empty category to be ID 0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image]
Example filename:
""
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG'
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG'
i_row = row_indices[0]
There are a small handful of datetime mismatches across annotations
for the same image
assert im['datetime'] == timestamp
Special cases based on the 'photo type' field
Various spellings of 'community'
Have we seen this category before?
Create an annotation
fieldname = list(mapped_fields.keys())[0]
...for each row
...for each image
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
%% Scrap
%% Find unique photo types
##
""
timelapse_csv_set_to_json.py
""
Given a directory full of reasonably-consistent Timelapse-exported
".csvs, assemble a CCT .json."
""
"Assumes that you have a list of all files in the directory tree, including"
image and .csv files.
""
##
%% Constants and imports
Text file with relative paths to all files (images and .csv files)
"%% Read file list, make a list of all image files and all .csv files"
"%% Verify column consistency, create a giant array with all rows from all .csv files"
i_csv = 0; csv_filename = csv_files[0]
%% Prepare some data structures we'll need for mapping image rows in .csv files to actual image files
Enumerate all folders containing image files
"In this data set, a site folder looks like:"
""
Processed Images\\site_name
%% Map .csv files to candidate camera folders
fn = valid_csv_files[0]
"Some site folders appear as ""XXNNNN"", some appear as ""XXNNNN_complete"""
...for each .csv file
%% Map camera folders to candidate image folders
%% Main loop over labels (prep)
Force the empty category to be ID 0
Images that are marked empty and also have a species label
%% Main loop over labels (loop)
i_row = 0; row = input_metadata.iloc[i_row]
"for i_row,row in input_metadata.iterrows():"
"Usually this is just a single folder name, sometimes it's a full path,"
which we don't want
Check whether this file exists on disk
Retrieve image width and height
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each row in the big table of concatenated .csv files
%% Fix cases where an image was annotated as 'unlabeled' and as something else
This annotation is 'unlabeled'
Was there another category associated with this image?
%% Check for un-annnotated images
Enumerate all images
list(relative_path_to_image.keys())[0]
%% Create info struct
%% Write output
%% Sanity-check the database's integrity
%% Render a bunch of images to make sure the labels got carried along correctly
""
auckland_doc_to_json.py
""
Convert Auckland DOC data set to COCO camera traps format.  This was
for a training data set where class names were encoded in path names.
""
%% Constants and imports
Filenames will be stored in the output .json relative to this base dir
%% Enumerate files
%% Assemble dictionaries
Force the empty category to be ID 0
fn = image_files[0]; print(fn)
Typically y:\Maukahuka_Auckland_Island\1_Training\Winter_Trial_2019\cat\cat\eat\20190903_IDdY_34_E3_tmp_201908240051.JPG
"This data set has two top-level folders, ""1_Training"" (which has class names encoded"
"in paths) and ""2_Testing"" (which has no class information)."
...for each image
%% Write output .json
%% Write train/test .jsons
%% Validate .json files
%% Preview labels
checkpoint
""
awc_to_json.py
""
Convert a particular .csv file to CCT format.
""
%% Constants and environment
%% Read source data
%% Main loop over labels
Force the empty category to be ID 0
iRow = 0; row = input_metadata.iloc[iRow]
"ImageID,FileName,FilePath,SpeciesID,CommonName"
"This makes an assumption of one annotation per image, which happens to be"
true in this data set.
Retrieve image width and height
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
%% Check for images that aren't included in the metadata file
Enumerate all images
list(relativePathToImage.keys())[0]
%% Create info struct
%% Write output
%% Sanity-check the database's integrity
%% Render a bunch of images to make sure the labels got carried along correctly
""
idaho-camera-traps.py
""
Prepare the Idaho Camera Traps dataset for release on LILA.
""
%% Imports and constants
Multi-threading for .csv file comparison and image existence validation
"We are going to map the original filenames/locations to obfuscated strings, but once"
"we've done that, we will re-use the mappings every time we run this script."
This is the file to which mappings get saved
The maximum time (in seconds) between images within which two images are considered the
same sequence.
"This is a two-column file, where each line is [string in the original metadata],[category name we want to map it to]"
"The output file, using the original strings"
"The output file, using obfuscated strings for everything but filenamed"
"The output file, using obfuscated strings and obfuscated filenames"
"One time only, I ran MegaDetector on the whole dataset..."
...then set aside any images that *may* have contained humans that had not already been
annotated as such.  Those went in this folder...
...and the ones that *actually* had humans (identified via manual review) got
copied to this folder...
"...which was enumerated to this text file, which is a manually-curated list of"
images that were flagged as human.
Unopinionated .json conversion of the .csv metadata
%% List files (images + .csv)
Ignore .csv files in folders with multiple .csv files
...which would require some extra work to decipher.
fn = csv_files[0]
%% Parse each .csv file into sequences (function)
csv_file = csv_files[-1]
os.startfile(csv_file_absolute)
survey = csv_file.split('\\')[0]
Sample paths from which we need to derive locations:
""
St.Joe_elk\AM99\Trip 1\100RECNX\TimelapseData.csv
Beaverhead_elk\AM34\Trip 1\100RECNX\TimelapseData.csv
""
ClearCreek_mustelids\Winter2015-16\FS-001-P\FS-001-P.csv
ClearCreek_mustelids\Summer2015\FS-001\FS-001.csv
ClearCreek_mustelids\Summer2016\IDFG-016\IDFG-016.csv
""
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017b
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017a
Load .csv file
Validate the opstate column
# Create datetimes
print('Creating datetimes')
i_row = 0; row = df.iloc[i_row]
Make sure data are sorted chronologically
""
"In odd circumstances, they are not... so sort them first, but warn"
Debugging when I was trying to see what was up with the unsorted dates
# Parse into sequences
print('Creating sequences')
i_row = 0; row = df.iloc[i_row]
Start a new sequence if:
""
* This image has no timestamp
* This image has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each row
# Parse labels for each sequence
sequence_id = location_sequences[0]
Row indices in a sequence should be adjacent
sequence_df = df[df['seq_id']==sequence_id]
# Determine what's present
Be conservative; assume humans are present in all maintenance images
The presence columns are *almost* always identical for all images in a sequence
assert single_presence_value
print('Warning: presence value for {} is inconsistent for {}'.format(
"presence_column,sequence_id))"
...for each presence column
Tally up the standard (survey) species
"If no presence columns are marked, all counts should be zero"
count_column = count_columns[0]
Occasionally a count gets entered (correctly) without the presence column being marked
"assert len(values) == 1 and values[0] == 0, 'Non-zero counts with no presence"
columns marked for sequence {}'.format(sequence_id)
"Handle this by virtually checking the ""right"" box"
Make sure we found a match
Handle 'other' tags
column_name = otherpresent_columns[0]
print('Found non-survey counted species column: {}'.format(column_name))
...for each non-empty presence column
"Very rarely, the ""otherpresent"" column is checked, but no more detail is available"
...handling non-survey species
Build the sequence data
i_row = 0; row = sequence_df.iloc[i_row]
Only one folder used a single .csv file for two subfolders
...for each sequence
...def csv_to_sequences()
%% Parse each .csv file into sequences (loop)
%%
%%
i_file = -1; csv_file = csv_files[i_file]
%% Save sequence data
%% Load sequence data
%%
%% Validate file mapping (based on the existing enumeration)
sequences = sequences_by_file[0]
sequence = sequences[0]
"Actually, one folder has relative paths"
assert '\\' not in image_file_relative and '/' not in image_file_relative
os.startfile(csv_folder)
assert os.path.isfile(image_file_absolute)
found_file = os.path.isfile(image_file_absolute)
...for each image
...for each sequence
...for each .csv file
%% Load manual category mappings
The second column is blank when the first column already represents the category name
%% Convert to CCT .json (original strings)
Force the empty category to be ID 0
For each .csv file...
""
sequences = sequences_by_file[0]
For each sequence...
""
sequence = sequences[0]
Find categories for this image
"When 'unknown' is used in combination with another label, use that"
"label; the ""unknown"" here doesn't mean ""another unknown species"", it means"
there is some other unknown property about the main species.
category_name_string = species_present[0]
"This piece of text had a lot of complicated syntax in it, and it would have"
been too complicated to handle in a general way
print('Ignoring category {}'.format(category_name_string))
Don't process redundant labels
category_name = category_names[0]
If we've seen this category before...
If this is a new category...
print('Adding new category for {}'.format(category_name))
...for each category (inner)
...for each category (outer)
...if we do/don't have species in this sequence
"We should have at least one category assigned (which may be ""empty"" or ""unknown"")"
assert len(sequence_category_ids) > 0
Was any image in this sequence manually flagged as human?
print('Flagging sequence {} as human based on manual review'.format(sequence_id))
For each image in this sequence...
""
i_image = 0; im = images[i_image]
Create annotations for this image
...for each image in this sequence
...for each sequence
...for each .csv file
Verify that all images have annotations
ann = ict_data['annotations'][0]
For debugging only
%% Create output (original strings)
%% Validate .json file
%% Preview labels
%% Look for humans that were found by MegaDetector that haven't already been identified as human
This whole step only needed to get run once
%%
Load MD results
Get a list of filenames that MD tagged as human
im = md_results['images'][0]
...for each detection
...for each image
Map images to annotations in ICT
ann = ict_data['annotations'][0]
For every image
im = ict_data['images'][0]
Does this image already have a human annotation?
...for each annotation
...for each image
%% Copy images for review to a new folder
fn = missing_human_images[0]
%% Manual step...
Copy any images from that list that have humans in them to...
%% Create a list of the images we just manually flagged
fn = human_tagged_filenames[0]
E.g. '0000_Beaverhead_elk~AM174~Trip 1~100RECNX~IMG_1397.JPG'
"%% Translate location, image, sequence IDs"
Load mappings if available
Generate mappings
If we've seen this location before...
Otherwise assign a string-formatted int as the ID
If we've seen this sequence before...
Otherwise assign a string-formatted int as the ID
Assign an image ID
...for each image
Assign annotation mappings
Save mappings
"Back this file up, lest we should accidentally re-run this script"
with force_generate_mappings = True and overwrite the mappings we used.
...if we are/aren't re-generating mappings
%% Apply mappings
"%% Write new dictionaries (modified strings, original files)"
"%% Validate .json file (modified strings, original files)"
%% Preview labels (original files)
"viz_options.classes_to_exclude = ['empty','deer','elk']"
viz_options.classes_to_include = ['bobcat']
%% Copy images to final output folder (prep)
ann = d['annotations'][0]
Is this a public or private image?
Generate absolute path
Copy to output
Update the filename reference
...def process_image(im)
%% Copy images to final output folder (execution)
For each image
im = images[0]
Write output .json
%% Make sure the right number of images got there
%% Validate .json file (final filenames)
%% Preview labels (final filenames)
"viz_options.classes_to_exclude = ['empty','deer','elk']"
viz_options.classes_to_include = ['horse']
viz_options.classes_to_include = [viz_options.multiple_categories_tag]
"viz_options.classes_to_include = ['human','vehicle','domestic dog']"
%% Create zipfiles
%% List public files
%% Find the size of each file
fn = all_public_output_files[0]
%% Split into chunks of approximately-equal size
...for each file
%% Create a zipfile for each chunk
...for each filename
with ZipFile()
...def create_zipfile()
i_file_list = 0; file_list = file_lists[i_file_list]
"....if __name__ == ""__main__"""
""
wellington_to_json.py
""
Convert the .csv file provided for the Wellington data set to a
COCO-camera-traps .json file
""
%% Constants and environment
%% Read source data
"Filenames were provided as *.jpg, but images were *.JPG, converting here"
"%% Map filenames to rows, verify image existence"
"Takes ~30 seconds, since it's checking the existence of ~270k images"
"Build up a map from filenames to a list of rows, checking image existence as we go"
"I didn't expect this to be true a priori, but it appears to be true, and"
it saves us the trouble of checking consistency across multiple occurrences
of an image.
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
imageName = imageFilenames[0]
"As per above, this is convenient and appears to be true; asserting to be safe"
"Filenames look like ""290716114012001a1116.jpg"""
This gets imported as an int64
"These appear as ""image1"", ""image2"", etc."
"In the form ""001a"""
Can be in the form '111' or 's46'
"In the form ""7/29/2016 11:40"""
Check image height and width
"Use 'empty', to be consistent with other data on lila"
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
""
mcgill_to_json.py
""
Convert the .csv file provided for the McGill test data set to a
COCO-camera-traps .json file
""
%% Constants and environment
%% Read source data
"%% Map filenames to rows, verify image existence"
Create an additional column for concatenated filenames
Maps relative filenames to rows
"Build up a map from filenames to a list of rows, checking image existence as we go"
row = input_metadata.iloc[0]
"I didn't expect this to be true a priori, but it appears to be true, and"
it saves us the trouble of checking consistency across multiple occurrences
of an image.
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
row = input_metadata.iloc[0]
"Filenames look like ""290716114012001a1116.jpg"""
"In the form ""001a"""
Can be in the form '111' or 's46'
"In the form ""7/29/2016 11:40"""
Check image height and width
NaN is the only thing we should see that's not a string
NaN is the only thing we should see that's not a string
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
""
bellevue_to_json.py
""
"""Bellevue Camera Traps"" is the rather unremarkable camera trap data set"
used by one of the repo's maintainers for testing.  It's organized as:
""
approximate_date/[loose_camera_specifier/]/species
""
E.g.:
""
"""2018.03.30\coyote\DSCF0091.JPG"""
"""2018.07.18\oldcam\empty\DSCF0001.JPG"""
""
%% Constants and imports
from the ai4eutils repo
Filenames will be stored in the output .json relative to this base dir
%% Exif functions
"%% Enumerate files, create image/annotation/category info"
Force the empty category to be ID 0
Keep track of unique camera folders
Each element will be a dictionary with fields:
""
"relative_path, width, height, datetime"
fname = image_files[0]
Corrupt or not an image
Store file info
E.g. 2018.03.30/coyote/DSCF0091.JPG
...for each image file
%% Synthesize sequence information
Sort images by time within each folder
camera_path = camera_folders[0]
previous_datetime = sorted_images_this_camera[0]['datetime']
im = sorted_images_this_camera[1]
Start a new sequence if necessary
...for each image in this camera
...for each camera
Fill in seq_num_frames
%% A little cleanup
%% Write output .json
%% Sanity-check data
%% Label previews
""
snapshot_safar_importer_reprise.py
""
This is a 2023 update to snapshot_safari_importer.py.  We do a bunch of things now that
we didn't do the last time we imported Snapshot data (like updating the big taxonomy)
"file, and we skip a bunch of things now that we used to do (like generating massive"
"zipfiles).  So, new year, new importer."
""
%% Constants and imports
%% List files
"Do a one-time enumeration of the entire drive; this will take a long time,"
but will save a lot of hassle later.
%% Create derived lists
Takes about 60 seconds
CSV files are one of:
""
"_report_lila.csv (this is the one we want to use, with the species/count/etc. for each sequence)"
_report_lila_image_inventory.csv (maps captures to images)
_report_lila_overview.csv (distrubution of species)
%% List project folders
Project folders look like one of these:
""
APN
Snapshot Cameo/DEB
%% Map report and inventory files to codes
fn = csv_files[0]
%% Make sure that every report has a corresponding inventory file
%% Count species based on overview and report files
%% Print counts
%% Make sure that capture IDs in the reports/inventory files match
...and that all the images in the inventory tables are actually present on disk.
assert image_path_relative in all_files_relative_set
Make sure this isn't just a case issue
...for each report on this project
...for each project
"%% For all the files we have on disk, see which are and aren't in the inventory files"
"There aren't any capital-P .PNG files, but if I don't include that"
"in this list, I'll look at this in a year and wonder whether I forgot"
to include it.
fn = all_files_relative[0]
print('Skipping project {}'.format(project_code))
""
plot_wni_giraffes.py
""
Plot keypoints on a random sample of images from the wni-giraffes data set.
""
%% Constants and imports
%% Load and select data
%% Support functions
https://stackoverflow.com/questions/32504246/draw-ellipse-in-python-pil-with-line-thickness
Use a single channel image (mode='L') as mask.
The size of the mask can be increased relative to the imput image
to get smoother looking results.
draw outer shape in white (color) and inner shape in black (transparent)
downsample the mask using PIL.Image.LANCZOS
(a high-quality downsampling filter).
paste outline color to input image through the mask
%% Plot some images
ann = annotations_to_plot[0]
i_tool = 0; tool_name = short_tool_names[i_tool]
Don't plot tools that don't have a consensus annotation
...for each tool
...for each annotation
""
idfg_iwildcam_lila_prep.py
""
Adding class labels (from the private test .csv) to the iWildCam 2019 IDFG
"test set, in preparation for release on LILA."
""
This version works with the public iWildCam release images.
""
"%% ############ Take one, from iWildCam .json files ############"
%% Imports and constants
%% Read input files
Remove the header line
%% Parse annotations
Lines look like:
""
"b005e5b2-2c0b-11e9-bcad-06f1011196c4,1,Private"
%% Minor cleanup re: images
%% Create annotations
%% Prepare info
%% Minor adjustments to categories
Remove unused categories
Name adjustments
%% Create output
%% Write output
%% Validate .json file
%% Preview labels
viz_options.classes_to_exclude = ['test']
"%% ############ Take two, from pre-iWildCam .json files created from IDFG .csv files ############"
%% Imports and constants
%% One-time line break addition
%% Read input files
%% Prepare info
%% Minor adjustments to categories
%% Minor adjustments to annotations
%% Create output
%% Write output
%% Validate .json file
%% Preview labels
viz_options.classes_to_exclude = ['test']
""
missouri_to_json.py
""
Create .json files from the original source files for the Missouri Camera Traps
data set.  Metadata was provided here in two formats:
""
"1) In one subset of the data, folder names indicated species names.  In Set 1,"
there are no empty sequences.  Set 1 has a metadata file to indicate image-level
bounding boxes.
""
2) A subset of the data (overlapping with (1)) was annotated with bounding
"boxes, specified in a whitespace-delimited text file.  In set 2, there are"
"some sequences omitted from the metadata file, which implied emptiness."
""
"In the end, set 2 labels were not reliable enough to publish, so LILA includes only set 1."
""
%% Constants and imports
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Filenames will be stored in the output .json relative to this base dir
This will be a list of filenames that need re-annotation due to redundant boxes
"%% Enumerate files, read image sizes (both sets)"
"Takes a few minutes, since we're reading image sizes."
Each element will be a list of relative path/full path/width/height
"Only process leaf nodes corresponding to sequences, which look like:"
""
E:\wildlife_data\missouri_camera_traps\Set1\1.02-Agouti\SEQ75583
E:\wildlife_data\missouri_camera_traps\Set2\p1d101
""
assert len(files) <= 2
Read the image
Not an image...
Store file info
"...if we didn't hit the max file limit, keep going"
...for each file
%% Add sequence lengths (both sets)
%% Load the set 1 metadata file
"List of lists, length varies according to number of bounding boxes"
""
Preserves original ordering
iLine = 0; line = metadataSet1Lines[0]
"Lines should be filename, number of bounding boxes, boxes (four values per box)"
Make sure we have image info for this image
%% Print missing files from Set 1 metadata
Manual changes I made to the metadata file:
""
'IMG' --> 'IMG_'
Red_Brocket_Deer --> Red_Deer
European-Hare --> European_Hare
Wood-Mouse --> Wood_Mouse
Coiban-Agouti --> Coiban_Agouti
%% Load the set 2 metadata file
"This metadata file contains most (but not all) images, and a class label (person/animal/empty)"
"for each, plus bounding boxes."
"List of lists, length varies according to number of bounding boxes"
""
Preserves original ordering
"Create class IDs for each *sequence*, which we'll use to attach classes to"
images for which we don't have metadata
""
This only contains mappings for sequences that appear in the metadata.
iLine = 0; line = metadataSet2Lines[0]
"Lines should be filename, number of bounding boxes, labeled boxes (five values per box)"
""
Empty images look like filename\t0\t0
E.g. 'Set2\\p1d101\\p1d101s100i10.JPG'
Make sure we don't have mixed classes within an image
"Figure out what class this *sequence* is, so we know how to handle unlabeled"
images from this sequence
Can't un-do a mixed sequence
Previously-empty sequences get the image class label
"If the sequence has a non-empty class, possibly change it"
Make sure we have image info for this image
...for each line in the set 2 metadata file
%% What Set 2 images do I not have metadata for?
These are *mostly* empty images
iImage = 0; imageID = set2ImageIDs[iImage]
%% Create categories and annotations for set 1
"Though we have no empty sequences, we do have empty images in this set"
For each image
""
iImage = 0; imageID = set1ImageIDs[iImage]
E.g. Set1\\1.80-Coiban_Agouti\\SEQ83155\\SEQ83155_IMG_0010.JPG
Find the species name
This image may still be empty...
category['count'] = category['count'] + 1
"If we have bounding boxes, create image-level annotations"
"filename, number of bounding boxes, boxes (four values per box)"
"Make sure the relative filename matches, allowing for the fact that"
some of the filenames in the metadata aren't quite right
This image is non-empty
"Some redundant bounding boxes crept in, don't add them twice"
Check this bbox against previous bboxes
""
Inefficient?  Yes.  In an important way?  No.
"Bounding box values are in absolute coordinates, with the origin"
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1]."
""
"Convert to floats and to x/y/w/h, as per CCT standard"
...for each box
if we do/don't have boxes for this image
Else create a sequence-level annotation
...for each image
%% Write out the list of images with redundant boxes
%% Create categories and annotations for set 2
For each image
""
iImage = 0; imageID = set2ImageIDs[iImage]
E.g. 'Set2\\p1d100\\p1d100s10i1.JPG'
"Find the sequence ID, sanity check filename against what we stored"
"If we have bounding boxes or an explicit empty label, create image-level annotations"
"filename, number of bounding boxes, labeled boxes (five values per box)"
"Make sure the relative filename matches, allowing for the fact that"
some of the filenames in the metadata aren't quite right
"Bounding box values are in absolute coordinates, with the origin"
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1]."
""
Convert to floats and to x/y/w/h
...for each box
...if we do/don't have boxes for this image
Else create a sequence-level annotation
...for each image
"%% The 'count' field isn't really meaningful, delete it"
"It's really the count of image-level annotations, not total images assigned to a class"
%% Write output .json files
%% Sanity-check final set 1 .json file
"python sanity_check_json_db.py --bCheckImageSizes --baseDir ""E:\wildlife_data\missouri_camera_traps"" ""E:\wildlife_data\missouri_camera_traps\missouri_camera_traps_set1.json"""
%% Generate previews
""
filenames_to_json.py
""
Take a directory of images in which species labels are encoded by folder
"names, and produces a COCO-style .json file"
""
%% Constants and imports
from the ai4eutils repo
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Filenames will be stored in the output .json relative to this base dir
"rawClassListFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_classes.csv')"
"classMappingsFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_class_mapping.csv')"
"%% Enumerate files, read image sizes"
Each element will be a list of relative path/full path/width/height
Read the image
Corrupt or not an image
Store file info
Write to output file
...for each image file
...csv file output
...if the file list is/isn't available
%% Enumerate classes
Maps classes to counts
We like 'empty' to be class 0
%% Assemble dictionaries
...for each category
Each element is a list of relative path/full path/width/height/className
...for each image
%% External class mapping
%% Write raw class table
cat = categories[0]
%% Read the mapped class table
"id, source, count, target"
"%% Make classMappings contain *all* classes, not just remapped classes"
cat = categories[0]
%% Create new class list
"Start at 1, explicitly assign 0 to ""empty"""
One-off issue with character encoding
%% Re-map annotations
ann = annotations[0]
%% Write output .json
%% Utilities
%%
Find images with a particular tag
%% Randomly sample annotations
""
umn_to_json.py
""
Prepare images and metadata for the Orinoquía Camera Traps dataset.
""
%% Imports and constants
"For two deployments, we're only processing imagse in the ""detections"" subfolder"
%% Enumerate deployment folders
%% Load ground truth
i_row = 0; row = ground_truth_df.iloc[i_row]
%% Create relative paths for ground truth data
"Some deployment folders have no subfolders, e.g. this is a valid file name:"
""
M00/01010132.JPG
""
"But some deployment folders have subfolders, e.g. this is also a valid file name:"
""
N17/100EK113/07160020.JPG
""
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly"
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths."
deployment_name = list(deployment_folders)[0]
Enumerate all files in this folder
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))"
filename = files[100]
...for each file in this deployment
...for each deployment
%% Add relative paths to our ground truth table
i_row = 0; row = ground_truth_df.iloc[i_row]
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's"
"just the deployment ID and the image name, separated by a slash."
Find the relative path for this image
Make sure this image file exists
...for each row in the ground truth table
%% Take everything out of Pandas
%% Convert string timestamps to Python datetimes
im = ground_truth_dicts[0]
"Filenames look like, e.g., N36/100EK113/06040726.JPG"
%% Synthesize sequence information
Sort images by time within each location
i_location=0; location = locations[i_location]
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_camera[1]
"Timestamp for this image, may be None"
Start a new sequence if:
""
* This image has no timestamp
* This iamge has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each image in this location
...for each location
%% Create category dict and category IDs
im = images[0]
%% Imports and constants (.json generation)
%% Count frames in each sequence
%% Double check images with multiple annotations
im = images[0]
%% Assemble dictionaries
Force the empty category to be ID 0
input_im = images[0]
...for each image
%% Write output .json
%% Validate .json file
%% Map relative paths to annotation categories
ann = data['annotations'][0]
%% Copy images to output
EXCLUDE HUMAN AND MISSING
im = data['images'][0]
im = images[0]
%% Preview labels
viz_options.classes_to_exclude = ['test']
""
nacti_fieldname_adjustments.py
""
"NACTI metadata was posted with ""filename"" in images instead of ""file_name"", and"
"used string (rather than int) category IDs (in categories, but not in annotations)."
""
This script fixes those issues and rev's the version number.
""
%% Constants and environment
%% Read .json file
"%% Rev version number, update field names and types"
%% Write json file
""
cct_field_adjustments.py
""
CCT metadata was posted with int locations instead of strings.
""
This script fixes those issues and rev's the version number.
""
%% Constants and environment
%% Read .json file
"%% Rev version number, update field names and types"
%% Write json file
%% Check output data file
""
carrizo_shrubfree_2018.py
""
Convert the .csv file provided for the Carrizo Mojave data set to a
COCO-camera-traps .json file
""
%% Constants and environment
%% Read source data
Original .csv file had superfluous spaces in column names
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
58 missing files (of 17652)
%% Check for images that aren't included in the metadata file
3012 of 20606 files are not in metadata
%% Create CCT dictionaries
Map categories to integer IDs
""
The category '0' is reserved for 'empty'
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
Each filename should just match one row
Don't include images that don't exist on disk
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
""
snapshot_serengeti_lila.py
""
Create zipfiles of Snapshot Serengeti S1-S11.
""
"Create a metadata file for S1-S10, plus separate metadata files"
"for S1-S11.  At the time this code was written, S11 was under embargo."
""
Create zip archives of each season without humans.
""
Create a human zip archive.
""
%% Constants and imports
import sys; sys.path.append(r'c:\git\ai4eutils')
import sys; sys.path.append(r'c:\git\cameratraps')
assert(os.path.isdir(metadata_base))
"There are two redundant categories, and we re-map ""blank"" to ""empty"" as per CCT convention"
"%% Load metadata files, concatenate into a single table"
iSeason = 1
%% Convert to dictionaries (prep)
%% Convert to dictionaries (loops)
"TODO: iterrows() is a terrible way to do this, but this is one of those days"
"where I want to get this done, not get better at Python."
irow = 0; row = image_table.iloc[0]
"Loaded as an int64, converting to int here"
...for each row in the image table
Make sure image IDs are what we think they are
...for each row in the annotation table
%% Load previously-saved dictionaries when re-starting mid-script
%%
%% Take a look at categories (just sanity-checking)
%%
%% Fill in some image fields we didn't have when we created the image table
"width, height, corrupt, seq_num_frames, location, datetime"
Every annotation in this list should have the same sequence ID
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;"
there's no reason to do this redundantly for every image
Every image in this sequence should point back to the same equence
Every annotation in this list should have the same location
Every annotation in this list should have the same datetime
Is this image on disk?
iImage = 0; im = images[0]
...for each image
"images_processed = pool.map(process_image, images)"
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))"
%%
%% Count missing/corrupted images
%% Print distribution of sequence lengths (sanity-check)
%% Replicate annotations across images
iAnn = 0; ann = annotations[iAnn]
%% See what files are on disk but not annotated (~15 mins)
247370 files not in the database (of 7425810)
%% Load old image database
%% Look for old images not in the new DB and vice-versa
"At the time this was written, ""old"" was S1-S6"
old_im = cct_old['images'][0]
new_im = images[0]
4 old images not in new db
12 new images not in old db
%% Save our work
%% Load our work
%%
%% Examine size mismatches
i_mismatch = -1; old_im = size_mismatches[i_mismatch]
%% Sanity-check image and annotation uniqueness
"%% Split data by seasons, create master list for public seasons"
ann = annotations[0]
%% Minor updates to fields
"%% Write master .json out for S1-10, write individual season .jsons (including S11)"
"Loop over all seasons, plus one iteration for the ""all public data"" iteration, and"
"one for the ""all data"" iteration"
%% Find categories that only exist in S11
List of categories in each season
Category 55 (fire) only in S11
Category 56 (hyenabrown) only in S11
Category 57 (wilddog) only in S11
Category 58 (kudu) only in S11
Category 59 (pangolin) only in S11
Category 60 (lioncub) only in S11
%% Prepare season-specific .csv files
iSeason = 1
%% Create a list of human files
ann = annotations[0]
%% Save our work
%% Load our work
%%
"%% Create archives (human, per-season) (prep)"
im = images[0]
im = images[0]
Don't include humans
Only include files from this season
Possibly start a new archive
...for each image
i_season = 0
"for i_season in range(0,nSeasons):"
create_season_archive(i_season)
%% Create archives (loop)
pool = ThreadPool(nSeasons+1)
"n_images = pool.map(create_archive, range(-1,nSeasons))"
"seasons_to_zip = range(-1,nSeasons)"
...for each season
%% Sanity-check .json files
%logstart -o r'E:\snapshot_temp\python.txt'
%% Zip up .json and .csv files
pool = ThreadPool(len(files_to_zip))
"pool.map(zip_single_file, files_to_zip)"
%% Super-sanity-check that S11 info isn't leaking
im = data_public['images'][0]
ann = data_public['annotations'][0]
iRow = 0; row = annotation_df.iloc[iRow]
iRow = 0; row = image_df.iloc[iRow]
%% Create bounding box archive
i_image = 0; im = data['images'][0]
i_box = 0; boxann = bbox_data['annotations'][0]
%% Sanity-check a few files to make sure bounding boxes are still sensible
import sys; sys.path.append(r'C:\git\CameraTraps')
%% Check categories
%% Summary prep for LILA
""
wi_to_json
""
Prepares CCT-formatted metadata based on a Wildlife Insights data export.
""
"Mostly assumes you have the images also, for validation/QA."
""
%% Imports and constants
"Because WI filenames are GUIDs, it's not practical to page through sequences in an"
image viewer.  So we're going to (optionally) create a copy of the data set where
images are ordered.
%% Load ground truth
%% Take everything out of Pandas
%% Synthesize common names when they're not available
"Blank rows should always have ""Blank"" as the common name"
"print('Warning: missing common name for row {} ({})'.format(i_row,row['filename']))"
%% Convert string timestamps to Python datetimes
im = ground_truth_dicts[0]
"The field called ""location"" in the WI .csv file is a URL, we want to reclaim"
"the ""location"" keyword for CCT output"
"Filenames look like, e.g., N36/100EK113/06040726.JPG"
%% Synthesize sequence information
Sort images by time within each location
i_location=0; location = locations[i_location]
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_camera[1]
"Timestamp for this image, may be None"
Start a new sequence if:
""
* This image has no timestamp
* This iamge has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each image in this location
...for each location
%% Create category dict and category IDs
im = images[0]
%% Count frames in each sequence
%% Build relative paths
im = images[0]
Sample URL:
""
gs://project-asfasdfd/deployment/21444549/asdfasdfd-616a-4d10-a921-45ac456c568a.jpg'
%% Double check images with multiple annotations
im = images[0]
%% Assemble dictionaries
Force the empty category to be ID 0
input_im = images[0]
"print('Warning: image ID {} ({}) has multiple annotations'.format(im['id'],im['id'].replace('_','/')))"
...for each image
%% Write output .json
%% Validate .json file
%% Preview labels
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))"
%%
%% Create ordered dataset
"Because WI filenames are GUIDs, it's not practical to page through sequences in an"
image viewer.  So we're going to create a copy of the data set where images are
ordered.
im = images_out[0]; im
%% Create ordered .json
%% Copy files to their new locations
im = ordered_images[0]
im = data_ordered['images'][0]
%% Preview labels in the ordered dataset
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))"
%% Open an ordered filename from the unordered filename
""
rspb_to_json.py
""
Convert the .csv file provided for the RSPB data set to a
COCO-camera-traps .json file
""
%% Constants and environment
[location] is an obfuscation
%% Create info struct
%% Read source data
metadataTable.columns.values
""
"array(['Project', 'inDir', 'FileName', 'Station', 'Camera',"
"'StationCameraFileName', 'DateTimeOriginal', 'DateReadable',"
"'outDir', 'filename_new', 'fileExistsAlready', 'CopyStatus',"
"'Species'], dtype=object)"
We'll populate these later
keys should be lowercase
"%% Enumerate images, confirm filename uniqueness"
"%% Update metadata filenames to include site and camera folders, check existence"
""
Takes ~1min
iRow = 0; row = metadataTable.iloc[iRow]
There's a bug in the metadata; the 'camera' column isn't correct.
camera = row['Camera']
"These appear as, e.g., '3.22e12'"
camera = str(int(float(camera)))
Let's pull this out of the file name instead
""
Filenames look like one of the following:
""
A1__03224850850507__2015-11-28__10-45-04(1).JPG
Bayama2PH__C05__NA(NA).JPG
assert(os.path.isfile(fullPath))
metadataTable.iloc[iRow] = row
Re-assemble into an updated table
%% Check for images that aren't included in the metadata file
Enumerate all images
Write to a text file
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
iRow = 0; row = metadataTable.iloc[iRow]
A1__03224850850507__2015-11-28__10-45-04(1).JPG
'A1\\03224850850507\\A1__03224850850507__2015-11-28__10-45-04(1).JPG'
Not currently populated
"Often -1, sometimes a semi-meaningful int"
A1
03224850850507
"In variable form, but sometimes '28/11/2015 10:45'"
Check image height and width
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Write output
%% Check database integrity
%% Preview a few images to make sure labels were passed along sensibly
%% One-time processing step: copy images to a flat directory for annotation
%%
""
pc_to_json.py
""
Convert a particular collection of .csv files to CCT format.
""
%% Constants and environment
%% Read and concatenate source data
List files
"List of dataframes, one per .csv file; we'll concatenate later"
i_file = 87; fn = input_files[i_file]
Concatenate into a giant data frame
%% List files
%% Main loop over labels (prep)
Force the empty category to be ID 0
%% Main loop over labels (loop)
iRow = 0; row = input_metadata.iloc[iRow]
"ImageID,FileName,FilePath,SpeciesID,CommonName"
assert os.path.isfile(full_path)
Retrieve image width and height
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
%% See what's up with missing files
s = list(image_relative_paths)[0]
s = missing_files[0]
%% Check for images that aren't included in the metadata file
%% Sample the database
%%
Collect the images we want
%% Create info struct
%% Write output
%% Sanity-check the database's integrity
%% Render a bunch of images to make sure the labels got carried along correctly
options.classes_to_exclude = ['unlabeled']
%% Write out a list of files to annotate
""
save_the_elephants_survey_B.py
""
Convert the .csv file provided for the Save the Elephants Survey B data set to a
COCO-camera-traps .json file
""
%% Constants and environment
input_base = r'/mnt/blobfuse/wildlifeblobssc/ste_2019_08_drop'
output_base = r'/home/gramener/survey_b'
Handle all unstructured fields in the source data as extra fields in the annotations
"photo_type really should be an image property, but there are a few conflicts"
that forced me to handle it as an annotation proprerty
%% Read source data
"%% Map filenames to rows, verify image existence"
"%% Map filenames to rows, verify image existence"
Maps relative paths to row indices in input_metadata
"Build up a map from filenames to a list of rows, checking image existence as we go"
Ignore directories
%% Make sure the multiple-annotation cases make sense
%%
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
Force the empty category to be ID 0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image]
Example filename:
""
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG'
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG'
i_row = row_indices[0]
timestamp = row['Date']
There are a small handful of datetime mismatches across annotations
for the same image
assert im['datetime'] == timestamp
Special cases based on the 'photo type' field
Various spellings of 'community'
Have we seen this category before?
Create an annotation
fieldname = list(mapped_fields.keys())[0]
...for each row
...for each image
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
%% Scrap
%% Find unique photo types
""
ubc_to_json.py
""
Convert the .csv file provided for the UBC data set to a
COCO-camera-traps .json file
""
"Images were provided in eight folders, each of which contained a .csv"
file with annotations.  Those annotations came in two slightly different
"formats, the two formats corresponding to folders starting with ""SC_"" and"
otherwise.
""
%% Constants and environment
Map Excel column names - which vary a little across spreadsheets - to a common set of names
%% Enumerate images
Load from file if we've already enumerated
%% Create CCT dictionaries
Force the empty category to be ID 0
To simplify debugging of the loop below
#%% Create CCT dictionaries (loop)
#%%
Read source data for this folder
Rename columns
Folder name is the first two characters of the filename
""
Create relative path names from the filename itself
Folder name is the camera name
""
Create relative path names from camera name and filename
Which of our images are in the spreadsheet?
i_row = 0; fn = input_metadata['image_relative_path'][i_row]
#%% Check for images that aren't included in the metadata file
Find all the images in this folder
Which of these aren't in the spreadsheet?
#%% Create entries in CCT dictionaries
Only process images we have on disk
"This is redundant, but doing this for clarity, at basically no performance"
cost since we need to *read* the images below to check validity.
i_row = row_indices[0]
"These generally represent zero-byte images in this data set, don't try"
to find the very small handful that might be other kinds of failures we
might want to keep around.
print('Error opening image {}'.format(image_relative_path))
If we've seen this category before...
...make sure it used the same latin --> common mapping
""
"If the previous instance had no mapping, use the new one."
assert common_name == category['common_name']
Create an annotation
...for each annotation we found for this image
...for each image
...for each dataset
Print all of our species mappings
"%% Copy images for which we actually have annotations to a new folder, lowercase everything"
im = images[0]
%% Create info struct
"%% Convert image IDs to lowercase in annotations, tag as sequence level"
"While there isn't any sequence information, the nature of false positives"
"here leads me to believe the images were labeled at the sequence level, so"
we should trust labels more when positives are verified.  Overall false
positive rate looks to be between 1% and 5%.
%% Write output
%% Validate output
%% Preview labels
""
helena_to_cct.py
""
Convert the Helena Detections data set to a COCO-camera-traps .json file
""
%% Constants and environment
This is one time process
%% Create Filenames and timestamps mapping CSV
import pdb;pdb.set_trace()
%% To create CCT JSON for RSPB dataset
%% Read source data
Original Excel file had timestamp in different columns
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
%% Skipping this check because one image has multiple species
assert len(duplicate_rows) == 0
%% Check for images that aren't included in the metadata file
%% Create CCT dictionaries
Map categories to integer IDs
""
The category '0' is reserved for 'empty'
For each image
Don't include images that don't exist on disk
Some filenames will match to multiple rows
assert(len(rows) == 1)
iRow = rows[0]
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
ann['datetime'] = row['datetime']
ann['site'] = row['site']
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
%% Imports and constants
from github.com/microsoft/ai4eutils
from github.com/ecologize/CameraTraps
A list of files in the lilablobssc container for this data set
The raw detection files provided by NOAA
A version of the above with filename columns added
%% Read input .csv
%% Read list of files
%% Convert paths to full paths
i_row = 0; row = df.iloc[i_row]
assert ir_image_path in all_files
...for each row
%% Write results
"%% Load output file, just to be sure"
%% Render annotations on an image
i_image = 2004
%% Download the image
%% Find all the rows (detections) associated with this image
"as l,r,t,b"
%% Render the detections on the image(s)
In pixel coordinates
In pixel coordinates
%% Save images
%% Clean up
""
carrizo_trail_cam_2017.py
""
"Convert the .csv files provided for the ""Trail Cam Carrizo"" 2017 data set to"
a COCO-camera-traps .json file.
""
%% Constants and environment
%% Read source data
Removing the empty records
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
908 missing files (of 60562)
%% Check for images that aren't included in the metadata file
105329 of 164983 files are not in metadata
%% Create CCT dictionaries
Map categories to integer IDs
""
The category '0' is reserved for 'empty'
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
Each filename should just match one row
Don't include images that don't exist on disk
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
""
channel_islands_to_cct.py
""
Convert the Channel Islands data set to a COCO-camera-traps .json file
""
"Uses the command-line tool ExifTool (exiftool.org) to pull EXIF tags from images,"
"because every Python package we tried failed to pull the ""Maker Notes"" field properly."
""
"%% Imports, constants, paths"
# Imports ##
# Constants ##
# Paths ##
Confirm that exiftool is available
"assert which(exiftool_command_name) is not None, 'Could not locate the ExifTool executable'"
%% Load information from every .json file
"Ignore the sample file... actually, first make sure there is a sample file"
...and now ignore that sample file.
json_file = json_files[0]
ann = annotations[0]
...for each annotation in this file
...for each .json file
"%% Confirm URL uniqueness, handle redundant tags"
Have we already added this image?
"One .json file was basically duplicated, but as:"
""
Ellie_2016-2017 SC12.json
Ellie_2016-2017-SC12.json
"If the new image has no output, just leave the old one there"
"If the old image has no output, and the new one has output, default to the one with output"
Don't worry about the cases where someone tagged 'fox' and someone tagged 'fox_partial'
...for each image we've already added
...if this URL is/isn't in the list of URLs we've already processed
...for each image
%% Save progress
%%
%%
%% Download files (functions)
https://www.quickprogrammingtips.com/python/how-to-download-multiple-files-concurrently-in-python.html
"This is returned with a leading slash, remove it"
%% Download files (execution)
%% Read required fields from EXIF data (functions)
"-G means ""Print group name for each tag"", e.g. print:"
""
[File]          Bits Per Sample                 : 8
""
...instead of:
""
Bits Per Sample                 : 8
"If we don't get any EXIF information, this probably isn't an image"
line_raw = exif_lines[0]
"Split on the first occurrence of "":"""
Typically:
""
"'[MakerNotes]    Sequence                        ', '1 of 3']"
Not a typo; we are using serial number as a location
"If there are multiple timestamps, make sure they're *almost* the same"
"If there are multiple timestamps, make sure they're *almost* the same"
...for each line in the exiftool output
"This isn't directly related to the lack of maker notes, but it happens that files that are missing"
maker notes also happen to be missing EXIF date information
...process_exif()
"This is returned with a leading slash, remove it"
Ignore non-image files
%% Read EXIF data (execution)
ann = images[0]
%% Save progress
Use default=str to handle datetime objects
%%
%%
"Not deserializing datetimes yet, will do this if I actually need to run this"
%% Check for EXIF read errors
%% Remove junk
Ignore non-image files
%% Fill in some None values
"...so we can sort by datetime later, and let None's be sorted arbitrarily"
%% Find unique locations
%% Synthesize sequence information
Sort images by time within each location
i_location=0; location = locations[i_location]
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_camera[1]
"Timestamp for this image, may be None"
Start a new sequence if:
""
* This image has no timestamp
* This image has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each image in this location
...for each location
%% Count frames in each sequence
images_this_sequence = [im for im in images if im['seq_id'] == seq_id]
"%% Create output filenames for each image, store original filenames"
i_location = 0; location = locations[i_location]
i_image = 0; im = sorted_images_this_location[i_image]
%% Save progress
Use default=str to handle datetime objects
%%
%%
%% Copy images to their output files (functions)
%% Copy images to output files (execution)
%% Rename the main image list for consistency with other scripts
%% Create CCT dictionaries
Make sure this is really a box
Transform to CCT format
Force the empty category to be ID 0
i_image = 0; input_im = all_image_info[0]
"This issue only impacted one image that wasn't a real image, it was just a screenshot"
"showing ""no images available for this camera"""
Convert datetime if necessary
Process temperature if available
Read width and height if necessary
I don't know what this field is; confirming that it's always None
Process object and bbox
os.startfile(output_image_full_path)
"Zero is hard-coded as the empty category, but check to be safe"
"I can't figure out the 'index' field, but I'm not losing sleep about it"
assert input_annotation['index'] == 1+i_ann
"Some annotators (but not all) included ""_partial"" when animals were partially obscured"
"Annotators *mostly* used 'none', but sometimes 'empty'.  'empty' is CCT-correct."
If we've seen this category before...
If this is a new category...
...if this is an empty/non-empty annotation
Create an annotation
...for each annotation on this image
...for each image
%% Change *two* annotations on images that I discovered contains a human after running MDv4
%% Move human images
ann = annotations[0]
%% Count images by location
%% Write output
%% Validate output
%% Preview labels
viz_options.classes_to_exclude = [0]
""
ena24_to_json_2017.py
""
Convert the ENA24 data set to a COCO-camera-traps .json file
""
%% Constants and environment
Temporary folders for human and non-human images
Clean existing output folders/zipfiles
%% Support functions
%% Read source data
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
Check image height and width
"Each row is category, [box coordinates]"
"If there's just one row, loadtxt reads it as a 1d array; make it a 2d array"
with one row
Each row is a bounding box
Have we seen this category before?
Create an annotation
...for each bounding box
"This was here for debugging; nearly every instance is Human+Horse, Human+Vehicle,"
"or Human+Dog, but there is one Rabbit+Opossium, and a few Deer+Chicken!"
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Create ZIP files for human and non human
%% Validate output
%% Preview labels
%% Imports and constants
configurations and paths
%% Helper functions
"dest_path = copy(source_path, dest_folder)"
num_workers = multiprocessing.cpu_count()
pool = ThreadPool(num_workers)
"results = pool.starmap(_copy_unzip, zip(sources, itertools.repeat(dest_folder)))"
""
print('Waiting for processes to finish...')
pool.close()
pool.join()
sequential
%% Command-line driver
if the blob container is already mounted on the VM
or you can download them using the storage Python SDK
store storage account key in environment variable AZ_STORAGE_KEY
""
eMammal_helpers.py
""
Support functions for processing eMammal metadata
""
%% Constants and imports
%% Support functions
"pad to a total of 3 digits if < 1000, or 4 digits otherwise"
img_frame is a string from the xml tree
"length 4 frame order is returned as is, others are left padded to be 3 digit long"
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels
pad to a total of 4 digits
img_frame is a string from the xml tree
"length 4 frame order is returned as is, others are left padded to be 3 digit long"
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels
others column
summer day hours: 6am - 7pm
others day hours: 7am - 6pm
""
make_eMammal_json.py
""
"Produces the COCO-formatted json database for an eMammal dataset, i.e. a"
"collection of folders, each of which contains a deployment_manifest.xml file."
""
"In this process, each image needs to be loaded to size it."
""
"To add bounding box annotations to the resulting database, use"
add_annotations_to_eMammal_json.py.
""
%% Constants and imports
"Either add the eMammal directory to your path, or run from there"
os.chdir(r'd:\git\CameraTraps\database_tools\eMammal')
import warnings
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
warnings.filterwarnings('ignore')
Should we run the image size retrieval in parallel?
%% Support functions
%% Main loop (metadata processing; image sizes are retrieved later)
deployment = folders[0]
sequence = image_sequences[0]
get species info for this sequence
add each image's info to database
img = images[0]
"some manifests don't have the ImageOrder info, but the info is in the file name"
full_img_id has no frame info
""
frame number only used in requests to iMerit for ordering
...for each image
...for each sequence
...for each deployment
%% Get image sizes
"'tasks' is currently a list of 2-tuples, with each entry as [image dictionary,path]."
""
"Go through that and copy just the image dictionaries to 'db_images', adding size"
information to each entry.  Takes a couple hours.
opening each image seems too fast for this multi-threaded version to be faster than sequential code.
%% Assemble top-level dictionaries
%% Write out .json
""
make_full_SS_json.py
""
Create a COCO-camera-traps .json file for Snapshot Serengeti data from
the original .csv files provided on Dryad.
""
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no"
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated
Snapshot Safari preparation process.
""
%% Imports and constants
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays"
Count the number of images with multiple species
"%% Read image .csv file, format into a dictionary mapping images to capture events"
%% Create CCT-style .json
still need image width and height
...for each image
%% Write output files
""
make_per_season_SS_json.py
""
Create a COCO-camera-traps .json file for each Snapshot Serengeti season from
the original .csv files provided on Dryad.
""
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no"
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated
Snapshot Safari preparation process.
""
%% Imports and constants
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays"
"%% Read image .csv file, format into a dictionary mapping images to capture events"
%% Create CCT-style .json
still need image width and height
...for each image ID
%% Write output files
...for each season
""
generate_lila_per_image_labels.py
""
"Generate a .csv file with one row per annotation, containing full URLs to every"
"camera trap image on LILA, with taxonomically expanded labels."
""
"Typically there will be one row per image, though images with multiple annotations"
will have multiple rows.
""
"Some images may not physically exist, particularly images that are labeled as ""human""."
This script does not validate image URLs.
""
Does not include bounding box annotations.
""
%% Constants and imports
"We'll write images, metadata downloads, and temporary files here"
"Some datasets don't have ""sequence_level_annotation"" fields populated, but we know their"
annotation level
%% Download and parse the metadata file
To select an individual data set for debugging
%% Download and extract metadata for the datasets we're interested in
%% Load taxonomy data
"%% Build a dictionary that maps each [dataset,query] pair to the full taxonomic label set"
i_row = 0; row = taxonomy_df.iloc[i_row]
%% Process annotations for each dataset
ds_name = list(metadata_table.keys())[0]
"Go through annotations, marking each image with the categories that are present"
""
ann = annotations[0]
im = images[10]
This field name was only used for Caltech Camera Traps
raise ValueError('Suspicious date parsing result')
Special case we don't want to print a warning about
"Location, sequence, and image IDs are only guaranteed to be unique within"
"a dataset, so for the output .csv file, include both"
category_name = list(categories_this_image)[0]
Only print a warning the first time we see an unmapped label
...for each category that was applied at least once to this image
...for each image in this dataset
print('Warning: no date information available for this dataset')
print('Warning: no location information available for this dataset')
...for each dataset
...with open()
%% Read the .csv back
%% Do some post-hoc integrity checking
"-1 isn't *really* valid, but we use it sometimes for sequences of unknown length"
i_row = 0; row = df.iloc[i_row]
%% Preview constants
%% Choose images to download
ds_name = list(metadata_table.keys())[2]
Find all rows for this dataset
...for each dataset
%% Download images
i_image = 0; image = images_to_download[i_image]
%% Write preview HTML
im = images_to_download[0]
""
Common constants and functions related to LILA data management/retrieval.
""
%% Imports and constants
LILA camera trap master metadata file
"wildlife_insights_taxonomy_url = 'https://api.wildlifeinsights.org/api/v1/taxonomy?fields=class,order,family,genus,species,authority,taxonomyType,uniqueIdentifier,commonNameEnglish&page[size]={}'.format(wildlife_insights_page_size)"
from ai4eutils
%% Common functions
"We haven't implemented paging, make sure that's not an issue"
d['data'] is a list of items that look like:
Put the master metadata file in the same folder where we're putting images
Read lines from the master metadata file
Parse those lines into a table
Each line in this file is name/sas_url/json_url/[bbox_json_url]
Create a separate entry for bounding boxes if they exist
Unzip if necessary
""
get_lila_category_list.py
""
Generates a .json-formatted dictionary mapping each LILA dataset to all categories
"that exist for that dataset, with counts for the number of occurrences of each category"
"(the number of *annotations* for each category, not the number of *images*)."
""
"Also loads the taxonomy mapping file, to include scientific names for each category."
""
get_lila_category_counts counts the number of *images* for each category in each dataset.
""
%% Constants and imports
array to fill for output
"We'll write images, metadata downloads, and temporary files here"
Created by get_lila_category_list.py... contains counts for each category
%% Load category and taxonomy files
%% Map dataset names and category names to scientific names
i_row = 1; row = taxonomy_df.iloc[i_row]; row
%% Download and parse the metadata file
%% Download and extract metadata for the datasets we're interested in
%% Get category names and counts for each dataset
ds_name = 'NACTI'
Open the metadata file
Collect list of categories and mappings to category name
ann = annotations[0]
c = categories[0]
"Don't do taxonomy mapping for bbox data sets, which are sometimes just binary and are"
always redundant with the class-level data sets.
"As of right now, this is the only quirky case"
...for each dataset
%% Save dict
%% Print the results
ds_name = list(dataset_to_categories.keys())[0]
...for each dataset
""
download_lila_subset.py
""
"Example of how to download a list of files from LILA, e.g. all the files"
in a data set corresponding to a particular species.
""
%% Constants and imports
LILA camera trap master metadata file
"In this example, we're using the Missouri Camera Traps data set and the Caltech Camera Traps dataset"
All lower-case; we'll convert category names to lower-case when comparing
"We'll write images, metadata downloads, and temporary files here"
"We will demonstrate two approaches to downloading, one that loops over files"
"and downloads directly in Python, another that uses AzCopy."
""
AzCopy will generally be more performant and supports resuming if the
transfers are interrupted.  This script assumes that azcopy is on the system path.
Number of concurrent download threads (when not using AzCopy) (AzCopy does its
own magical parallelism)
%% Support functions
remove the leading '/'
%% Download and parse the metadata file
Put the master metadata file in the same folder where we're putting images
Read lines from the master metadata file
Parse those lines into a table
Each line in this file is name/base_url/json_url/[box_url]
%% Download and extract metadata for the datasets we're interested in
Unzip if necessary
...for each dataset of interest
%% List of files we're going to download (for all data sets)
"Flat list or URLS, for use with direct Python downloads"
For use with azcopy
This may or may not be a SAS URL
# Open the metadata file
# Build a list of image files (relative path names) that match the target species
Retrieve all the images that match that category
Retrieve image file names
Convert to URLs
...for each dataset
%% Download those image files
ds_name = 'Caltech Camera Traps'
ds_name = 'SWG Camera Traps'
We want to use the whole relative path for this script (relative to the base of the container)
"to build the output filename, to make sure that different data sets end up in different folders."
This may or may not be a SAS URL
For example:
""
caltech-unzipped/cct_images
swg-camera-traps
Check whether the URL includes a folder
E.g. caltech-unzipped
E.g. cct_images
E.g. swg-camera-traps
"Write out a list of files, and use the azcopy ""list-of-files"" option to download those files."
""
This azcopy feature is unofficially documented at:
""
https://github.com/Azure/azure-storage-azcopy/wiki/Listing-specific-files-to-transfer
""
import clipboard; clipboard.copy(cmd)
Loop over files
""
get_lila_category_counts.py
""
Count the number of images and bounding boxes with each label in one or more LILA datasets.
""
"This script doesn't write these counts out anywhere other than the console, it's just intended"
as a template for doing operations like this on LILA data.  get_lila_category_list.py writes
"information out to a .json file, but it counts *annotations*, not *images*, for each category."
""
%% Constants and imports
"If None, will use all datasets"
"We'll write images, metadata downloads, and temporary files here"
%% Download and parse the metadata file
%% Download and extract metadata for the datasets we're interested in
%% Count categories
ds_name = datasets_of_interest[0]
"Go through annotations, marking each image with the categories that are present"
""
ann = annotations[0]
Now go through images and count categories
im = images[0]
...for each dataset
%% Print the results
...for each dataset
""
add_url_to_database.py
""
"Adds a ""url"" field to a coco-camera-traps .json database, specifically to allow the db to"
be reviewed in the Visipedia annotation tool.
""
""
remove_corrupted_images_from_database.py
""
"Given a coco-camera-traps .json file, checks all images for TF-friendliness and generates"
a new .json file that only contains the non-corrupted images.
""
%% Imports and constants
import multiprocessing
"I leave this at an annoying low number, since by definition weird stuff will"
"be happening in the TF kernel, and it's useful to keep having content in the console."
%% Function definitions
"I sometimes pass in a list of images, sometimes a dict with a single"
element mapping a job ID to the list of images
"We're about to start a lot of TF sessions, and we don't want gobs"
of debugging information printing out for every session.
At some point we were creating a single session and looping over images
"within that session, but the only way I found to reliably not run out"
of GPU memory was to create a session per image and gc.collect() after
each session.
Map Image IDs to boolean (should I keep this image?)
"Convert to lists, append job numbers to the image lists"
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)"
Merge results
%% Interactive driver
%%
base_dir = r'D:\temp\snapshot_serengeti_tfrecord_generation'
Load annotations
Check for corruption
Write out only the uncorrupted data
%% Command-line driver
#######
""
integrity_check_json_db.py
""
"Does some integrity-checking and computes basic statistics on a db, specifically:"
""
* Verifies that required fields are present and have the right types
* Verifies that annotations refer to valid images
* Verifies that annotations refer to valid categories
"* Verifies that image, category, and annotation IDs are unique"
""
* Optionally checks file existence
""
* Finds un-annotated images
* Finds unused categories
""
* Prints a list of categories sorted by count
""
#######
%% Constants and environment
%% Functions
"If baseDir is non-empty, checks image existence"
This is used in a medium-hacky way to share modified options across threads
print('Image path {} does not exist'.format(filePath))
"#%% Read .json file if necessary, integrity-check fields"
info = data['info']
"#%% Build dictionaries, checking ID uniqueness and internal validity as we go"
Confirm that required fields are present
Confirm ID uniqueness
...for each category
image = images[0]
Confirm that required fields are present
Confirm ID uniqueness
We previously supported ints here; this should be strings now
"assert isinstance(image['location'], str) or isinstance(image['location'], int), \"
'Illegal image location type'
Are we checking for unused images?
Recursively enumerate images
print('Image {} is unused'.format(p))
Are we checking file existence and/or image size?
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)"
...for each image
Confirm that required fields are present
Confirm ID uniqueness
Confirm validity
...for each annotation
#%% Print statistics
Find un-annotated images and multi-annotation images
Find unused categories
Prints a list of categories sorted by count
""
https://stackoverflow.com/questions/72899/how-do-i-sort-a-list-of-dictionaries-by-a-value-of-the-dictionary
...def integrity_check_json_db()
%% Command-line driver
%% Interactive driver(s)
%%
Integrity-check .json files for LILA
options.iMaxNumImages = 10
""
combine_two_json_files.py
""
"Merges two coco-camera-traps .json files. In particular, categories are combined and re-numbered."
""
Combined Info
Combined Images
Combined Categories
## categories to merge
Combined Annotations
""
make_detection_db_for_viewing.py
""
"Given a .json file with ground truth bounding boxes, and a .p file containing detections for the same images,"
"creates a new .json file with separate classes for ground truth and detection, suitable for viewing in the Visipedia"
annotation tool.
""
%% Imports and constants
%% Main function
im_id_to_im = {im['id']:im for im in images}
make new categories to distinguish between ground truth and detections
"update all gt annotations to be class ""gt"""
collect all detections by image
keep any detection with score above det_thresh
"need to convert bbox from [x1,y1,x2,y2] to [x,y,w,h]"
"add ""info"" and ""licenses"" for annotation tools to function"
create new db
%% Command-line handling
""
analyze_json_database.py
""
Plots location/class/etc. distributions for classes in a coco-camera-traps .json file.
""
Currently includes some one-off code for specific species.
""
%% Constants and imports
%% Path configuration
%% Load source data
%% Build image/category dictionaries
%% Make plot of category distribution
%% make plots of location distribution
"plt.title('Number of images per location, by category')"
"plt.tight_layout(rect=[0,0,1,0.9])"
#make plot of images per season
%% Make plot of lions per location
%% Make plot of elephants per location
for loc in sorted_by_total[:25]:
"print('Location:' + loc[0] +', Lions: ' + str(len(loc_to_lion_ims[loc[0]])) + ', Elephants: ' + str(len(loc_to_elephant_ims[loc[0]])) + ', Total ims: ' + str(len(loc_to_ims[loc[0]])))"
""
subset_json_db.py
""
Select a subset of images (and associated annotations) from a .json file
in COCO Camera Traps format.
""
"To subset the .json files produced by our batch processing API, see"
subset_json_detector_output.py
""
Sample invocation:
""
"python subset_json_db.py ""E:\Statewide_wolf_container\idfg_20190409.json"" ""E:\Statewide_wolf_container\idfg_20190409_clearcreek.json"" ""clearcreek"" --ignore_case"
""
%% Constants and imports
%% Functions
Load the input file if necessary
Find images matching the query
Find annotations referring to those images
Write the output file if requested
%% Interactive driver
%%
%% Command-line driver
%% Constants and imports
%% Merge functions
i_input_dict = 0; input_dict = input_dicts[i_input_dict]
We will prepend an index to every ID to guarantee uniqueness
Map detection categories from the original data set into the merged data set
...for each category
Merge original image list into the merged data set
Create a unique ID
...for each image
Same for annotations
...for each annotation
"Merge info dicts, don't check completion time fields"
...for each dictionary
Convert merged image dictionaries to a sorted list
%% Driver
""
add_width_and_height_to_database.py
""
Grabs width and height from actual image files for a .json database that is missing w/h.
""
Originally used when we created a .json file for snapshot serengeti from .csv.
""
"This file converts the JSON output of the batch processing API to a pickle file, which can be used by the"
script ./make_classification_dataset.py
"Convert boxes from JSON   [x_min, y_min, width_of_box, height_of_box]"
"to PICKLE [ymin,  xmin,  ymax,         xmax]"
Write detections to file with pickle
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the"
script ./make_classification_dataset.py
Write detections to file with pickle
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the"
script ./make_classification_dataset.py
Assumes that the root of the CameraTrap repo is in the PYTHONPATH
Minimum threshold to put a detection into the output JSON file
Parameter check
Load detections from input
Load COCO style annotations
Build output JSON in format version 1.0
Adding the only known metadata info
"The pickle file does not contain category information, so we assume the default"
For each image with detections
for each detection
"Convert boxes from [ymin, xmin, ymax, xmax] format to"
"[x_min, y_min, width_of_box, height_of_box]"
Write output json
global flag for whether or not we encounter missing images
"- will only print ""missing image"" warning once"
TFRecords variables
1.3 for the cropping during test time and 1.3 for the context that the
CNN requires in the left-over image
Create output directories
Load COCO style annotations from the input dataset
"Get all categories, their names, and create updated ID for the json file"
Prepare the coco-style json files
Split the dataset by locations
Load detections
Write out COCO-style json files to the output directory
Write detections to file with pickle
## Preparations: get all the output tensors
For all images listed in the annotations file
Skip the image if it is annotated with more than one category
"Get ""old"" and ""new"" category ID and category name for this image."
Skip if in excluded categories.
get path to image
"If we already have detection results, we can use them"
Otherwise run detector and add detections to the collection
Only select detections with confidence larger than DETECTION_THRESHOLD
Skip image if no detection selected
whether it belongs to a training or testing location
Skip images that we do not have available right now
- this is useful for processing parts of large datasets
Load image
Run inference
"remove batch dimension, and convert from float32 to appropriate type"
convert normalized bbox coordinates to pixel coordinates
Pad the detected animal to a square box and additionally by
"PADDING_FACTOR. The result will be in crop_boxes. However, we need to make"
sure that its box coordinates are still within the image.
"for each bounding box, crop the image to the padded box and save it"
"Create the file path as it will appear in the annotation json,"
adding the box number if there are multiple boxes
"if the cropped file already exists, verify its size"
Add annotations to the appropriate json
js_keys = ['/'.join(im['file_name'].split('/')[1:])[:-4] for im in js['images']]
for tk in js_keys:
"assert np.isclose(1, np.sum(detections[tk]['detection_scores'] > 0.5))"
""
separate_detections_by_size
""
Not-super-well-maintained script to break a list of API output files up
based on bounding box size.
""
%% Imports and constants
Folder with one or more .json files in it that we want to split up
Enumerate .json files
Define size thresholds and confidence thresholds
"Not used directly in this script, but useful if we want to generate previews"
%% Split by size
For each size threshold...
For each file...
fn = input_files[0]
Just double-checking; we already filtered this out above
Don't reprocess .json files we generated with this script
Load the input file
For each image...
1.1 is the same as infinity here; no box can be bigger than a whole image
What's the smallest detection above threshold?
"[x_min, y_min, width_of_box, height_of_box]"
""
size = w * h
...for each detection
Which list do we put this image on?
...for each image in this file
Make sure the number of images adds up
Write out all files
...for each size threshold
...for each file
""
tile_images.py
""
Split a folder of images into tiles.  Preserves relative folder structure in a
"new output folder, with a/b/c/d.jpg becoming, e.g.:"
""
a/b/c/d_row_0_col_0.jpg
a/b/c/d_row_0_col_1.jpg
""
%% Imports and constants
from ai4eutils
%% Main function
TODO: parallelization
""
i_fn = 2; relative_fn = image_relative_paths[i_fn]
Can we skip this image because we've already generated all the tiles?
TODO: super-sloppy that I'm pasting this code from below
From:
""
https://github.com/whiplashoo/split-image/blob/main/src/split_image/split.py
i_col = 0; i_row = 1
left/top/right/bottom
...for each row
...for each column
...for each image
%% Interactive driver
%%
%%
""
rde_debug.py
""
Some useful cells for comparing the outputs of the repeat detection
"elimination process, specifically to make sure that after optimizations,"
results are the same up to ordering.
""
%% Compare two RDE files
i_dir = 0
break
"Regardless of ordering within a directory, we should have the same"
number of unique detections
Re-sort
Make sure that we have the same number of instances for each detection
Make sure the box values match
""
aggregate_video.py
""
Aggregate results and render output video for a video that's already been run through MD
""
%% Constants
%% Processing
im = d['images'][0]
...for each detection
This is no longer included in output files by default
# Split into frames
# Render output video
## Render detections to images
## Combine into a video
""
ocr_sandbox.py
""
sandbox for experimenting with using OCR to pull metadata from camera trap images
""
The general approach is:
""
"* Crop a fixed percentage from the top and bottom of an image, slightly larger"
than the largest examples we've seen of how much space is used for metadata.
""
"* Refine that crop by blurring a little, then looking for huge peaks in the"
"color histogram suggesting a solid background, then finding rows that are"
mostly that color.
""
"* Crop to the refined crop, then run pytesseract to extract text"
""
"* Use regular expressions to find time and date, in the future can add, e.g.,"
"temperature (which is often present *only* in the images, unlike time/date which"
are also usually in EXIF but often wrong or lost in processing)
""
"The metadata extraction (EXIF, IPTC) here is just sample code that seemed to"
belong in this file.
""
Contact: Dan Morris (cameratraps@lila.science)
""
%% Constants and imports
pip install pytesseract
""
"Also intall tesseract from: https://github.com/UB-Mannheim/tesseract/wiki, and add"
"the installation dir to your path (on Windows, typically C:\Program Files (x86)\Tesseract-OCR)"
pip install IPTCInfo3
from the ai4eutils repo: https://github.com/Microsoft/ai4eutils
""
"Only used for writing out a summary, not important for the core metadata extraction"
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Using a semi-arbitrary metric of how much it feels like we found the
"text-containing region, discard regions that appear to be extraction failures"
Pad each crop with a few pixels to make tesseract happy
Discard text from the top
"When we're looking for pixels that match the background color, allow some"
tolerance around the dominant color
We need to see a consistent color in at least this fraction of pixels in our rough
crop to believe that we actually found a candidate metadata region.
"What fraction of the [top,bottom] of the image should we use for our rough crop?"
A row is considered a probable metadata row if it contains at least this fraction
"of the background color.  This is used only to find the top and bottom of the crop area,"
"so it's not that *every* row needs to hit this criteria, only the rows that are generally"
above and below the text.
%% Support functions
"%% Load some images, pull EXIF and IPTC data for fun"
%% Rough crop
"This will be an nImages x 1 list of 2 x 1 lists (image top, image bottom)"
image = images[0]
"l,t,r,b"
""
"0,0 is upper-left"
"%% Close-crop around the text, return a revised image and success metric"
Did we find a sensible mode that looks like a background value?
"This looks very scientific, right?  Definitely a probability?"
"print('Failed min background fraction test: {} of {}'.format(pBackGroundValue,minBackgroundFraction))"
"Notes to self, things I tried that didn't really go anywhere..."
"analysisImage = cv2.blur(analysisImage, (3,3))"
"analysisImage = cv2.medianBlur(analysisImage,5)"
"analysisImage = cv2.Canny(analysisImage,100,100)"
imagePil = Image.fromarray(analysisImage); imagePil
Use row heuristics to refine the crop
""
This egregious block of code makes me miss my fluency in Matlab.
"print('Cropping to {},{},{},{}'.format(x,y,w,h))"
Crop the image
"For some reason, tesseract doesn't like characters really close to the edge"
imagePil = Image.fromarray(croppedImage); imagePil
%% Go to OCR-town
"An nImages x 2 list of strings, extracted from the top and bottom of each image"
An nImages x 2 list of cropped images
iImage = 0; iRegion = 1; regionSet = imageRegions[iImage]; region = regionSet[iRegion]
text = pytesseract.image_to_string(region)
pil --> cv2
"image = cv2.medianBlur(image, 3)"
"image = cv2.erode(image, None, iterations=2)"
"image = cv2.dilate(image, None, iterations=4)"
"image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]"
"image = cv2.blur(image, (3,3))"
"image = cv2.copyMakeBorder(image,10,10,10,10,cv2.BORDER_CONSTANT,value=[0,0,0])"
"text = pytesseract.image_to_string(imagePil, lang='eng')"
https://github.com/tesseract-ocr/tesseract/wiki/Command-Line-Usage
"psm 6: ""assume a single uniform block of text"""
""
...for each cropped region
...for each image
%% Extract dates and times
s = '1:22 pm'
s = '1:23:44 pm'
%% Write results to a handy html file
Add image name and resized image
Add results and individual region images
"textStyle = ""font-family:calibri,verdana,arial;font-weight:bold;font-size:150%;text-align:left;margin-left:50px;"""
%% Scrap
Alternative approaches to finding the text/background  region
Using findCountours()
imagePil = Image.fromarray(analysisImage); imagePil
"analysisImage = cv2.erode(analysisImage, None, iterations=3)"
"analysisImage = cv2.dilate(analysisImage, None, iterations=3)"
"analysisImage = cv2.threshold(analysisImage, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]"
Find object with the biggest bounding box
Using connectedComponents()
analysisImage = image
print('Found {} components'.format(nb_components))
We just want the *background* image
""
"Takes a .json file with MD results for an individual video, and converts to a .csv that includes"
frame times.  This is very bespoke to animal detection and does not include other classes.
""
%% Imports and constants
Only necessary if you want to extract the sample rate from the video
%% Extract the sample rate if necessary
%% Load results
%% Convert to .csv
i_image = 0; im = results['images'][i_image]
""
umn-pr-analysis.py
""
Precision/recall analysis for UMN data
""
%% Imports and constants
results_file = results_file_filtered
"For two deployments, we're only processing imagse in the ""detections"" subfolder"
String to remove from MegaDetector results
%% Enumerate deployment folders
%% Load MD results
im = md_results['images'][0]
%% Load ground truth
i_row = 0; row = ground_truth_df.iloc[i_row]
%% Create relative paths for ground truth data
"Some deployment folders have no subfolders, e.g. this is a valid file name:"
""
M00/01010132.JPG
""
"But some deployment folders have subfolders, e.g. this is also a valid file name:"
""
N17/100EK113/07160020.JPG
""
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly"
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths."
deployment_name = list(deployment_folders)[0]
Enumerate all files in this folder
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))"
filename = files[100]
...for each file in this deployment
...for each deployment
%% Map relative paths to MD results
%% Add relative paths to our ground truth table
i_row = 0; row = ground_truth_df.iloc[i_row]
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's"
"just the deployment ID and the image name, separated by a slash."
Find the relative path for this image
Make sure we have MegaDetector results for this file
Make sure this image file exists
...for each row in the ground truth table
%% Take everything out of Pandas
%% Some additional error-checking of the ground truth
An early version of the data required consistency between common_name and is_blank
%% Combine MD and ground truth results
d = ground_truth_dicts[0]
Find the maximum confidence for each category
...for each detection
...for each image
%% Precision/recall analysis
...for each image
Thresholds go up throughout precisions/recalls/thresholds; find the last
value where recall is at or above target.  That's our precision @ target recall.
Flatten the confusion matrix
Write precision/recall plot to .png file in output directory
pr_figure_relative_filename = 'prec_recall.png'
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)"
plt.show(block=False)
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')"
%% Find and manually review all images of humans
%%
"...if this image is annotated as ""human"""
...for each image
%% Find and manually review all MegaDetector animal misses
%%
im = merged_images[0]
GT says this is not an animal
GT says this is an animal
%% Convert .json to .csv
%%
""
kga-pr-analysis.py
""
Precision/recall analysis for KGA data
""
%% Imports and constants
%% Load and filter MD results
%% Load and filter ground truth
%% Map images to image-level results
%% Map sequence IDs to images and annotations to images
Verify consistency of annotation within a sequence
TODO
%% Find max confidence values for each category for each sequence
seq_id = list(sequence_id_to_images.keys())[1000]
im = images_this_sequence[0]
det = md_result['detections'][]
...for each detection
...for each image in this sequence
...for each sequence
%% Prepare for precision/recall analysis
seq_id = list(sequence_id_to_images.keys())[1000]
cat_id = list(category_ids_this_sequence)[0]
...for each category in this sequence
...for each sequence
%% Precision/recall analysis
"Confirm that thresholds are increasing, recall is decreasing"
This is not necessarily true
assert np.all(precisions[:-1] <= precisions[1:])
Thresholds go up throughout precisions/recalls/thresholds; find the max
value where recall is at or above target.  That's our precision @ target recall.
"This is very slightly optimistic in its handling of non-monotonic recall curves,"
but is an easy scheme to deal with.
Flatten the confusion matrix
Write precision/recall plot to .png file in output directory
pr_figure_relative_filename = 'prec_recall.png'
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)"
plt.show(block=False)
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')"
%% Scrap
%% Find and manually review all sequence-level MegaDetector animal misses
input_base = '/media/user/lila-01/lila/snapshot-safari/KGA/KGA_public'
i_sequence = 0; seq_id = pr_sequence_ids[i_sequence]
i_seq = 0; seq_id = false_negative_sequences[i_seq]
"sequence_folder = os.path.join(sequence_preview_dir,'seq_{}'.format(str(i_seq).zfill(3)))"
fn = image_files[0]
"print('Copying {} to {}'.format(input_path,output_path))"
...for each file in this sequence.
...for each sequence
%% Image-level postprocessing
parse arguments
check if a GPU is available
load a pretrained embedding model
setup experiment
load the embedding model
setup the target dataset
setup finetuning criterion
setup an active learning environment
create a classifier
the main active learning loop
Active Learning
finetune the embedding model and load new embedding values
gather labeled pool and train the classifier
save a snapshot
Load a checkpoint if necessary
setup the training dataset and the validation dataset
setup data loaders
check if a GPU is available
create a model
setup loss criterion
define optimizer
load a checkpoint if provided
setup a deep learning engine and start running
train the model
train for one epoch
evaluate on validation set
save a checkpoint
open the file
read it
do the substitution
matplotlib.use('Agg')
from UIComponents.DBObjects import *
Initialize Database
# database connection credentials
# try to connect as USER to database DB_NAME through peewee
Load the saved embedding model
dataset_query = Detection.select().limit(5)
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})"
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getalllabels()) , dataset.getallpaths(), {})"
Random examples to start
"random_ids = np.random.choice(dataset.current_set, 1000, replace=False).tolist()"
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)"
print(random_ids)
Move Records
"moveRecords(dataset, DetectionKind.ModelDetection.value, DetectionKind.UserDetection.value, random_ids)"
#print([len(x) for x in dataset.set_indices])
# Finetune the embedding model
#dataset.set_kind(DetectionKind.UserDetection.value)
#dataset.train()
"#train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)"
"#finetune_embedding(model, checkpoint['loss_type'], dataset, 32, 4, 100)"
#save_checkpoint({
"#        'arch': model.arch,"
"#        'state_dict': model.state_dict(),"
"#        'optimizer' : optimizer.state_dict(),"
"#        'loss_type' : loss_type,"
"#        }, False, ""%s%s_%s_%04d.tar""%('finetuned', loss_type, model.arch, len(dataset.set_indices[DetectionKind.UserDetection.value])))"
Get indices of samples to get user to label
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
"kwargs[""already_selected""].extend(indices)"
Train on samples that have been labeled so far
Test on the samples that have not been labeled
"'optimizer' : optimizer.state_dict(),"
num_classes= len(train_dataset.getClassesInfo()[0])
"print(""Num Classes= ""+str(num_classes))"
define loss function (criterion) and optimizer
"optimizer = torch.optim.SGD(params, momentum = 0.9, lr = args.lr, weight_decay = args.weight_decay)"
optimizer.load_state_dict(checkpoint['optimizer'])
train for one epoch
"adjust_lr(optimizer,epoch)"
if epoch % 1 == 0 and epoch > 0:
"a, b, c = e.predict(train_embd_loader, load_info = True, dim = args.feat_dim)"
"plot_embedding(reduce_dimensionality(a), b, c, {})"
evaluate on validation set
--------some stuff needed to get AJAX to work with bottle?--------#
-------------------------------------------------------------------------------- #
PREPARE TO QUEUE IMAGES FOR LABELING
-------------------------------------------------------------------------------- #
# Connect as USER to database DB_NAME through peewee and initialize database proxy
# Load embedding model
---------------------------------------------------------------------- #
CREATE QUEUE OF IMAGES TO LABEL
---------------------------------------------------------------------- #
Use classifier to generate predictions
# Update model predicted class in PostgreSQL database
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
-------------------------------------------------------------------------------- #
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI
-------------------------------------------------------------------------------- #
"# static routes (to serve CSS, etc.)"
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps"
"return bottle.static_file(filename, root='../../../../../../../../../../../.')"
# dynamic routes
if data['display_grayscale']:
indices_to_exclude.update(set(color_indices))
elif not data['display_grayscale']:
indices_to_exclude.update(set(grayscale_indices))
data['display_images'] = {}
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices]
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices]
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices]
data['display_images']['detection_categories'] = []
for i in indices:
if str(dataset.samples[i][1]) == 'None':
data['display_images']['detection_categories'].append('None')
else:
existing_category_entries = {cat.id: cat.name for cat in Category.select()}
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()"
data['display_images']['detection_categories'].append(cat_name)
Use image ids in images_to_label to get the corresponding dataset indices
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
print(set(dataset.set_indices[4]).update(set(indices_to_label)))
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Use image id images_to_label to get the corresponding dataset index
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Train on samples that have been labeled so far
dataset.set_kind(DetectionKind.UserDetection.value)
print(y_train)
Predict on the samples that have not been labeled
print(y_pred)
Update model predicted class in PostgreSQL database
timer = time.time()
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
print('Updating the database took %0.2f seconds'%(time.time() - timer))
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
"once the classifier has been trained the first time, switch to AL sampling"
# sourceMappingURL=bootstrap.bundle.min.js.map
"print(ap_distances.size(),an_distances.size())"
losses = -(((-ap_distances)/128)+1+1e-16).log() - (((-(128-an_distances))/128)+1+1e-16).log()
import pdb
pdb.set_trace()
losses = ap_distances - an_distances + self.margin
print(losses.size())
from UIComponents.DBObjects import *
TODO: should this also change self.kind?
get the embedding representations for all samples (i.e. set current_set to all indices)
"return PILImage.open(os.path.join(self.img_base,path)).convert('RGB')"
"print(self.labels_set, self.n_classes)"
from sklearn.manifold import TSNE
embedding= TSNE(n_components=2).fit_transform(X)
embedding= PCA(n_components=2).fit_transform(X)
return X
"print(dir(event), type(sc))"
"print(label,bgcolor)"
"patches.append(mpatches.Patch(color=indexcolors[i], label=label))"
"plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,"
"ncol=12, mode=""expand"", borderaxespad=0., handles=patches)"
plt.legend(handles=patches)
"plt.xlabel('Dim 1', fontsize=12)"
"plt.ylabel('Dim 2', fontsize=12)"
plt.grid(True)
"plt.xlabel('Dim 1', fontsize=12)"
"plt.ylabel('Dim 2', fontsize=12)"
plt.grid(True)
print(thumb)
"img.thumbnail((16, 12), PILImage.ANTIALIAS)"
plt.show()
Add all negatives for all positive pairs
print(triplets.shape[0])
compute output
measure accuracy
compute loss on this batch
"train on a batch, record loss, and measure accuracy (if calc_accuracy)"
compute output
measure accuracy and record loss
switch to evaluate mode
compute output
switch to evaluate mode
"self.fc13 = nn.Linear(128, 64)"
self.bn2 = nn.BatchNorm1d(64)
x = F.relu(self.fc12(x))
x = F.relu(self.bn1(self.fc13(x)))
x = F.relu(self.fc13(x))
"x = F.dropout(x, training=self.training)"
save features last FC layer
x = F.relu(x)
save features last FC layer
import matplotlib.pyplot as plt
plt.switch_backend('agg')
class EmbeddingNet(nn.Module):
"def __init__(self, architecture, feat_dim, use_pretrained=False):"
"super(EmbeddingNet, self).__init__()"
self.feat_dim= feat_dim
self.inner_model = models.__dict__[architecture](pretrained=use_pretrained)
if architecture.startswith('resnet'):
in_feats= self.inner_model.fc.in_features
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)"
elif architecture.startswith('inception'):
in_feats= self.inner_model.fc.in_features
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)"
if architecture.startswith('densenet'):
in_feats= self.inner_model.classifier.in_features
"self.inner_model.classifier = nn.Linear(in_feats, feat_dim)"
if architecture.startswith('vgg'):
in_feats= self.inner_model.classifier._modules['6'].in_features
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)"
if architecture.startswith('alexnet'):
in_feats= self.inner_model.classifier._modules['6'].in_features
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)"
"def forward(self, x):"
return self.inner_model.forward(x)
class NormalizedEmbeddingNet(EmbeddingNet):
"def __init__(self, architecture, feat_dim, use_pretrained=False):"
"EmbeddingNet.__init__(self, architecture, feat_dim, use_pretrained = use_pretrained)"
"def forward(self, x):"
embedding =  F.normalize(self.inner_model.forward(x))*10.0
"return embedding, embedding"
"def get_random_images(num, image_dir, test_transforms):"
"data = datasets.ImageFolder(image_dir, transform=test_transforms) # slight abuse; this expects subfolders corresponding to classes but we have no classes here"
indices = list(range(len(data)))
np.random.shuffle(indices)
idx = indices[:num]
from torch.utils.data.sampler import SubsetRandomSampler
sampler = SubsetRandomSampler(idx)
"loader = torch.utils.data.DataLoader(data,"
"sampler=sampler, batch_size=num)"
dataiter = iter(loader)
"images, labels = dataiter.next()"
"return images, labels"
"def predict_image(image, model, test_transforms):"
"device = torch.device(""cuda"" if torch.cuda.is_available()"
"else ""cpu"")"
image_tensor = test_transforms(image).float()
image_tensor = image_tensor.unsqueeze_(0)
input = Variable(image_tensor)
input = input.to(device)
output = model(input)[0]
return output.data.cpu().numpy()
Connect to database and initialize db_proxy
# database connection credentials
# load the dataset
Load the saved embedding model from the checkpoint
# update the dataset embedding
# Create a folder for saving embedding visualizations with this model checkpoint
model_emb_dirname = os.path.basename(args.base_model).split('.')[0]
"os.makedirs(model_emb_dirname, exist_ok=True)"
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')"
dataset.embedding_mode()
"assert 2==3, 'break'"
datasetindices = list(range(len(dataset)))
np.random.shuffle(datasetindices)
random_indices = datasetindices[:args.num]
print(random_indices)
"selected_sample_features = np.array([]).reshape(0, 256)"
selected_sample_labels = []
for idx in random_indices:
"selected_sample_features = np.vstack([selected_sample_features, X_train[idx]])"
selected_sample_labels.append(y_train[idx])
img_path = imagepaths[idx].split('.JPG')[0]
image = dataset.loader(img_path)
selected_sample_images.append(image)
# TRY NEAREST NEIGHBORS WALK THROUGH EMBEDDING
nbrs = NearestNeighbors(n_neighbors=args.num).fit(selected_sample_features)
"distances, indices = nbrs.kneighbors(selected_sample_features)"
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')"
"idx_w_closest_nbr = np.where(distances[:,1] == min(distances[:,1]))[0][0]"
order = [idx_w_closest_nbr]
for ii in range(len(distances)):
"distances[ii, 0] = np.inf"
while len(order)<args.num:
curr_idx = order[-1]
curr_neighbors = indices[curr_idx]
curr_dists = list(distances[curr_idx])
# print(min(curr_dists))
next_closest_pos = curr_dists.index(min(curr_dists))
next_closest = curr_neighbors[next_closest_pos]
order.append(next_closest)
# make sure you can't revisit past nodes
for vi in order:
vi_pos = list(indices[next_closest]).index(vi)
"distances[next_closest, vi_pos] = np.inf"
for ii in range(len(order)):
imgidx = order[ii]
image = selected_sample_images[imgidx]
"image.save(model_emb_dirname+""/img""+str(ii)+""_""+str(selected_sample_labels[imgidx])+"".png"")"
# Specify the transformations on the input images before inference
"# test_transforms = transforms.Compose([transforms.Resize([224, 224]), transforms.ToTensor()])"
"test_transforms = transforms.Compose([transforms.Resize([256, 256]), transforms.RandomCrop([224, 224]), transforms.RandomHorizontalFlip(), transforms.ColorJitter(), transforms.ToTensor(), transforms.Normalize([0.407328, 0.407328, 0.407328], [0.118641, 0.118641, 0.118641])])"
"images, labels = get_random_images(num, image_dir, test_transforms)"
"all_features = np.array([]).reshape(0, 256)"
for ii in range(len(images)):
image = to_pil(images[ii])
"features = predict_image(image, model, test_transforms)"
"all_features = np.vstack([all_features, features])"
# for ii in range(len(images)):
#     image = to_pil(images[ii])
"#     image.save(""img""+str(ii)+"".png"")"
# TRY CLUSTERING
kmeans1 = KMeans(n_clusters=5).fit(StandardScaler().fit_transform(all_features))
print(kmeans1.labels_)
for ii in range(len(images)):
image = to_pil(images[ii])
"filename = str(kmeans1.labels_[ii])+""/img""+str(ii)+"".png"""
if not os.path.exists(os.path.dirname(filename)):
os.makedirs(os.path.dirname(filename))
image.save(filename)
class SaveFeatures():
"def __init__(self, module):"
self.hook = module.register_forward_hook(self.hook_fn)
"def hook_fn(self, module, input, output):"
"self.features = torch.tensor(output, requires_grad=True).cuda()"
def close(self):
self.hook.remove()
Load the saved embedding model from the checkpoint
"Get a sample from the database, with eval transforms applied, etc."
Connect to database and sample a dataset
output = model.forward(sample_image.unsqueeze(0))
print(output)
with torch.no_grad():
sample_image_input = sample_image.cuda(non_blocking=True)
"_, output = model(sample_image_input) # compute output"
print(output)
sample_image = PILImage.open(sample_image_path).convert('RGB')
"sample_image = transforms.Compose([Resize([256, 256]), CenterCrop(([[224,224]])), ToTensor(), Normalize([0.369875, 0.388726, 0.347536], [0.136821, 0.143952, 0.145229])])(sample_image)"
print(list(model_inner_resnet.children()))
print(model_inner_resnet.fc)
print(model_inner_resnet.fc0)
# print(model_inner_resnet.layer4[0].conv2)
# print(type(model))
# print(len(list(model_inner_resnet.children())))
# print(list(model.children()))
# print(list(list(model.children())[0].children()))
"img = np.uint8(np.random.uniform(150, 180, (56, 56, 3)))/255"
"img_tensor = torch.unsqueeze(torch.from_numpy(img), 0)"
full_out = model_inner_resnet.forward(img_tensor)
print(full_out)
model(img_tensor)
activations = SaveFeatures(model_inner_resnet.layer4[0].conv2)
print(activations.features)
print(type(activations.features))
activations.close()
Connect to database and sample a dataset
Load the saved embedding model from the checkpoint
Update the dataset embedding
Get a random query image
# # # IMAGES IN THE SAME SEQUENCE # # # #
"assert 2==3, 'break'"
# # # CLOSEST IN (EMBEDDING) FEATURE SPACE # # # #
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Variables for the hierarchical cluster
Variables for the AL algorithm
Data variables
"connectivity = kneighbors_graph(self.transformed_X,max_features)"
Fit cluster and update cluster variables
The sklearn hierarchical clustering algo numbers leaves which correspond
to actual datapoints 0 to n_points - 1 and all internal nodes have
ids greater than n_points - 1 with the root having the highest node id
"If no labels have been observed, simply return uniform distribution"
"If no observations, return worst possible upper lower bounds"
Loop through generations from bottom to top
Update admissible labels for node
Calculate score
Determine if node should be split
Make sure label set for node so that we can flow to children
if necessary
Only split if all ancestors are admissible nodes
This is part  of definition of admissible pruning
Check that pruning covers all leave nodes
Fill in labels
Observe labels for previously recommended batches
TODO(lishal): implement multiple selection methods
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
A list of initialized samplers is allowed as an input because
"for AL_methods that search over different mixtures, may want mixtures to"
have shared AL_methods so that initialization is only performed once for
computation intensive methods like HierarchicalClusteringAL and
states are shared between mixtures.
"If initialized samplers are not provided, initialize them ourselves."
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copy these objects to make sure they are not modified while simulating
trajectories as they are used later by the main run_experiment script.
Assumes that model has already by fit using all labeled data so
the probabilities can be used immediately to hallucinate labels
All models need to have predict_proba method
Hallucinate labels for selected datapoints to be label
using class probabilities from model
"Not saving already_selected here, if saving then should sort"
only for the input to fit but preserve ordering of indices in
already_selected
Useful to know how accuracy compares for model trained on hallucinated
labels vs trained on true labels.  But can remove this train to speed
up simulations.  Won't speed up significantly since many more models
are being trained inside the loop above.
Save trajectory for reference
Delete created copies
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER
Should check model but kernel_svm does not have coef_ so need better
handling here
Make sure that model object fed in did not change during simulations
Return indices based on return type specified
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Set gamma for gaussian kernel to be equal to 1/n_features
kneighbors graph is constructed using k=10
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of"
"another point, make it vice versa"
Graph edges are weighted by applying gaussian kernel to manhattan dist.
"By default, gamma for rbf kernel is equal to 1/n_features but may"
get better results if gamma is tuned.
Define graph density for an observation to be sum of weights for all
edges to the node representing the datapoint.  Normalize sum weights
by total number of neighbors.
"If a neighbor has already been sampled, reduce the graph density"
for its direct neighbors to promote diversity.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update observed reward and arm probabilities
Sample an arm
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
y only used for determining how many clusters there should be
probably not practical to assume we know # of classes before hand
should also probably scale with dimensionality of data
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
TODO(lishal): have MarginSampler and this share margin function
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update min_distances for all examples given new cluster center.
Assumes that the transform function takes in original data and not
flattened data.
Initialize centers with a randomly selected datapoint
New examples should not be in already selected since those points
should have min_distance of zero to a cluster center.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
This is uniform given the remaining pool but biased wrt the entire pool.
sample = [i for i in range(self.X.shape[0]) if i not in already_selected]
return sample[0:N]
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Fields for hierarchical clustering AL
Setting parent and storing nodes in dict for fast access
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Connect to database DB_NAME as USER and initialize tables
Populate Info table
Populate Category table
"# For now, we have a predefined list of species we expect to see in the camera trap database (e.g. maybe from a quick look through the images)"
# TODO: allow user to update the class list through the labeling tool UI as they see different species
Populate Image and Detection tables
Image entry data
Detection entry data
Connect to database DB_NAME as USER and initialize tables
Populate Info table
Populate Category table
# Create category records from species present in the COCO camera trap classification dataset (COCO classes minus any excluded classes)
# TODO: allow user to update the class list through the labeling tool UI as they see different species
Populate Image and Detection tables
"with open(os.path.join(args.crop_dir,'crops.json'), 'r') as infile:"
crops_json = json.load(infile)
counter = 0
timer = time.time()
num_detections = len(crops_json)
for detectionid in crops_json:
counter += 1
detection_data = crops_json[detectionid]
# Image entry data
existing_image_entries = Image.select().where((Image.file_name == detection_data['file_name']))
try:
existing_image_entry = existing_image_entries.get()
except:
"image_entry = Image.create(id=detectionid, file_name=detection_data['file_name'], width=detection_data['width'], height=detection_data['height'], grayscale=detection_data['grayscale'],"
"source_file_name=detection_data['source_file_name'], relative_size=detection_data['relative_size'],"
"seq_id=detection_data['seq_id'], seq_num_frames=detection_data['seq_num_frames'], frame_num=detection_data['frame_num'])"
image_entry.save()
# Detection entry data
"detection_entry = Detection.create(id=detectionid, image=detectionid, bbox_confidence=detection_data['bbox_confidence'],"
"bbox_X1=detection_data['bbox_X1'], bbox_Y1=detection_data['bbox_Y1'], bbox_X2=detection_data['bbox_X2'], bbox_Y2=detection_data['bbox_Y2'],"
kind=DetectionKind.ModelDetection.value)
detection_entry.save()
if counter%100 == 0:
"print('Updated database with Image and Detection table entries for %d out of %d crops in %0.2f seconds'%(counter, num_detections, time.time() - timer))"
# data related to original image
Get class names from .txt list
Initialize Oracle table
Map filenames to classes (NOTE: we assume a single image does not contain more than one class)
"For each detection, use source image path to get class"
TODO update: Assumes that crops have already
been generated for the images using make_active_learning_classification_dataset.py. The created DB contains tables:
- info: information about the dataset
- image: images present in the dataset
- detections: crops of images with detections with confidence greater than a specified threshold
Initialize Database
# database connection credentials
HOST = 'localhost'
PORT = 5432
"# first, make sure the (user, password) has been created"
"# sudo -u postgres psql -c ""CREATE USER <db_user> WITH PASSWORD <db_password>;"""
"# sudo -u postgres psql -c ""CREATE DATABASE <db_name> WITH OWNER <db_user> CONNECTION LIMIT -1;"""
"# sudo -u postgres psql -c ""GRANT CONNECT ON DATABASE <db_name> TO <db_user>;"""
"# sudo -u postgres psql -d <db_name> -c ""CREATE EXTENSION IF NOT EXISTS \""uuid-ossp\"";"""
# Try to connect as USER to database DB_NAME through peewee
Populate Tables
# create Info table
# get class names for Category table
Faster anD available in Python 3.5 and above
# iterate through images in each class folder
killing this process after over 38 hours adding over 500k white-tailed deer crops from emammal
resuming for remaining classes
# get cropped image data for Image table
"if mean of each channel is about the same, image is likely grayscale"
# still have no info on these:
seq_id = CharField(null= True)                # sequence identifier for the original image
seq_num_frames = IntegerField(null = True)    # number of frames in sequence
frame_num = IntegerField(null = True)         # which frame number in sequence
location = CharField(null = True)             # location of camera trap
datetime = DateTimeField(null = True)
# store info about the detection corresponding to this image
# store info about the true labels for the detection
#  - for pretrain dataset this is the same as the detection_category if the detection categories
print(classes)
Connect to database and sample a dataset
Load the saved embedding model from the checkpoint
Update the dataset embedding
save the images
save the features
"with open(os.path.join(args.output_dir, 'lastlayer_features.mat'), 'wb') as f:"
"pickle.dump(sample_features, f)"
"with open(os.path.join(args.output_dir, 'labels.mat'), 'wb') as f:"
"pickle.dump(sample_labels, f)"
"parser.add_argument('--db_name', default='missouricameratraps', type=str, help='Name of the training (target) data Postgres DB.')"
"parser.add_argument('--db_user', default='user', type=str, help='Name of the user accessing the Postgres DB.')"
"parser.add_argument('--db_password', default='password', type=str, help='Password of the user accessing the Postgres DB.')"
"parser.add_argument('--base_model', type=str, help='Path to latest embedding model checkpoint.')"
"parser.add_argument('--output_dir', type=str, help='Output directory for subset of crops')"
Add json entry for this crop
Copy file for this crop to subset dataset crop dir
Copy file for its full-size source image to subset dataset image dir
Write crops.json to subset dataset crop dir
store info about the crops produced in a JSON file
------------------------------------------------------------------------------------------------------------#
COMMENT OUT IF NOT USING A SPECIFIC PROJECT WITHIN ROBERT LONG EMAMMAL DATASET
------------------------------------------------------------------------------------------------------------#
get some information about the source image
------------------------------------------------------------------------------------------------------------#
NOTE: EDIT THIS SECTION BASED ON DATASET SOURCE
get info about sequence the source image belongs to from path and directory
# missouricameratraps:
imgframenum = int(os.path.basename(imgfile).split('.JPG')[0].split('_')[-1])
imgseqid = int(os.path.split(os.path.dirname(imgfile))[-1])
"imgseqnumframes = len([name for name in os.listdir(os.path.dirname(imgfile)) if os.path.isfile(os.path.join(os.path.dirname(imgfile), name))])"
# emammal:
------------------------------------------------------------------------------------------------------------#
utility function
compute output
measure accuracy and record loss
switch to train mode
measure accuracy and record loss
compute output
measure accuracy and record loss
switch to evaluate mode
compute output
switch to evaluate mode
compute output
switch to evaluate mode
"print(self.labels_set, self.n_classes)"
Add all negatives for all positive pairs
print(triplets.shape[0])
constructor
update embedding values after a finetuning
select either the default or active pools
gather test set
gather train set
finetune the embedding model over the labeled pool
a utility function for saving the snapshot
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Variables for the hierarchical cluster
Variables for the AL algorithm
Data variables
"connectivity = kneighbors_graph(self.transformed_X,max_features)"
Fit cluster and update cluster variables
The sklearn hierarchical clustering algo numbers leaves which correspond
to actual datapoints 0 to n_points - 1 and all internal nodes have
ids greater than n_points - 1 with the root having the highest node id
"If no labels have been observed, simply return uniform distribution"
"If no observations, return worst possible upper lower bounds"
Loop through generations from bottom to top
Update admissible labels for node
Calculate score
Determine if node should be split
Make sure label set for node so that we can flow to children
if necessary
Only split if all ancestors are admissible nodes
This is part  of definition of admissible pruning
Check that pruning covers all leave nodes
Fill in labels
Observe labels for previously recommended batches
TODO(lishal): implement multiple selection methods
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
A list of initialized samplers is allowed as an input because
"for AL_methods that search over different mixtures, may want mixtures to"
have shared AL_methods so that initialization is only performed once for
computation intensive methods like HierarchicalClusteringAL and
states are shared between mixtures.
"If initialized samplers are not provided, initialize them ourselves."
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copy these objects to make sure they are not modified while simulating
trajectories as they are used later by the main run_experiment script.
Assumes that model has already by fit using all labeled data so
the probabilities can be used immediately to hallucinate labels
All models need to have predict_proba method
Hallucinate labels for selected datapoints to be label
using class probabilities from model
"Not saving already_selected here, if saving then should sort"
only for the input to fit but preserve ordering of indices in
already_selected
Useful to know how accuracy compares for model trained on hallucinated
labels vs trained on true labels.  But can remove this train to speed
up simulations.  Won't speed up significantly since many more models
are being trained inside the loop above.
Save trajectory for reference
Delete created copies
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER
Should check model but kernel_svm does not have coef_ so need better
handling here
Make sure that model object fed in did not change during simulations
Return indices based on return type specified
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Set gamma for gaussian kernel to be equal to 1/n_features
kneighbors graph is constructed using k=10
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of"
"another point, make it vice versa"
Graph edges are weighted by applying gaussian kernel to manhattan dist.
"By default, gamma for rbf kernel is equal to 1/n_features but may"
get better results if gamma is tuned.
Define graph density for an observation to be sum of weights for all
edges to the node representing the datapoint.  Normalize sum weights
by total number of neighbors.
"If a neighbor has already been sampled, reduce the graph density"
for its direct neighbors to promote diversity.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update observed reward and arm probabilities
Sample an arm
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
y only used for determining how many clusters there should be
probably not practical to assume we know # of classes before hand
should also probably scale with dimensionality of data
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
TODO(lishal): have MarginSampler and this share margin function
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update min_distances for all examples given new cluster center.
Assumes that the transform function takes in original data and not
flattened data.
Initialize centers with a randomly selected datapoint
New examples should not be in already selected since those points
should have min_distance of zero to a cluster center.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
This is uniform given the remaining pool but biased wrt the entire pool.
sample = [i for i in range(self.X.shape[0]) if i not in already_selected]
return sample[0:N]
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Fields for hierarchical clustering AL
Setting parent and storing nodes in dict for fast access
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Faster and available in Python 3.5 and above
transform_list.append(CenterCrop((processed_size)))
transform_list.append(Lambda(lambda X: normalize(X)))
"print(self.labels_set, self.n_classes)"
"from PyQt5 import QtCore, QtWidgets,QtGui"
from collections import deque
from peewee import *
from UIComponents.Tag import Tag
policy.setHeightForWidth(True)
"print(self.tab1.parentWidget(),self)"
self.tab4.add.clicked.connect(self.addSpecies)
self.tab4.update.clicked.connect(self.updateSpecies)
checkpoint= load_checkpoint('../merge/triplet_model_0054.tar')
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))"
"embedding_net = EmbeddingNet(checkpoint['arch'], checkpoint['feat_dim'])"
if checkpoint['loss_type'].lower()=='center':
"model = torch.nn.DataParallel(ClassificationNet(embedding_net, n_classes=14)).cuda()"
else:
model= torch.nn.DataParallel(embedding_net).cuda()
model.load_state_dict(checkpoint['state_dict'])
self.parentWidget().progressBar.setMaximum(len(run_dataset)//2048)
"e=Engine(model,None,None, verbose=True,progressBar= self.parentWidget().progressBar)"
label = [ x[1] for x in run_dataset.samples]
"print(indices,selected_set)"
print(query.sql())
src.delete().where(src.image_id<<rList))
"det= UserDetection.create(category_id=0, id=str(index+label[1][2]),image_id=final[0], bbox_X=label[1][0], bbox_Y=label[1][1], bbox_W=label[1][2], bbox_H=label[1][3])"
for x in self.tab1.grid.tags:
x.delete_instance()
db.create_tables([Detection])
This is simply to show the bar
"p = Process(target=ex.active, args=())"
p.start()
p.join()
ex.active()
ex.centralWidget().setCurrentIndex(1)
main()
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))"
print(row)
"unq_id= ""crops_""+str(uuid.uuid1())"
"print(line,imageWidth,imageHeight)"
"print(""%s,%s,%s,%0.6f,%0.6f,%0.6f,%0.6f,%0.6f""%(line[0], line[1],line[2],float(line[3]),topRel,leftRel,bottomRel,rightRel))"
if not os.path.exists(dest):
os.mkdir(dest)
raise
out.close()
"print length,(i-1)*length,i*length"
matplotlib.use('Agg')
get the inputs
zero the parameter gradients
forward + backward + optimize
print statistics
get the inputs
forward + backward + optimize
print statistics
get the inputs
zero the parameter gradients
forward + backward + optimize
get the inputs
forward + backward + optimize
print statistics
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
"conf.reset()"""""""
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
conf.reset()
for clf in classifiers:
"clf.fit(X_train, y_train)"
preds= clf.predict_proba(X)
uncertainty+= preds.max(axis=1)
print(uncertainty[ind])
print(uncertainty[ind])
true_labels = y[unlabeled_indices]
select up to 5 digit examples that the classifier is most uncertain about
remember best acc@1 and save checkpoint
"completeClassificationLoop(run_dataset, model,num_classes)"
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)"
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd)
labels = db.labels_
"mapp=(find_probablemap(label,labels, K=args.K))"
"print(""Clusters"")"
"for i,x in enumerate(labels):"
labels[i]= mapp[x]
print(np.sum(labels == label)/labels.size)
"print(""Confidence Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning(embd, label, idx)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(""Entropy Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning_entropy(embd, label, idx)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(idx,idx.shape)"
for i in idx:
print(paths[i])
"print(""Silohette active learning"")"
"idx= active_learning2(embd, 1000, args.num_clusters)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
embd= reduce_dimensionality(embd)#[0:10000])
labels= labels[0:10000]
label= label[0:10000]
paths= paths[0:10000]
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])"
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])"
plt.show()
"np.save(args.name_prefix+""_embeddings.npy"",embd)"
"np.save(args.name_prefix+""_labels.npy"",label)"
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")"
del output
define loss function (criterion) and optimizer
"optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay= args.weight_decay)"
optimizer.load_state_dict(checkpoint['optimizer'])
train for one epoch
evaluate on validation set
matplotlib.use('Agg')
selected_set.add(rand_ind[i])
"embedding_net = EmbeddingNet('resnet50', 256, True)"
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)"
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)"
print('Embedding Done')
sys.stdout.flush()
Random examples to start
"random_ids = noveltySamples(unlabeled_dataset.em, unlabeled_dataset.getIDs(), 1000)"
print(random_ids)
Move Records
Finetune the embedding model
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)"
unlabeled_dataset.updateEmbedding(model)
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})"
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()"
matplotlib.use('Agg')
"uncertainty= np.apply_along_axis(stats.entropy,1,probs) * (1 - probs.max(axis=1))"
"copy(paths[srt[i]], ""active"")"
"plot_together( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()), preds, base_ind, dataset.getpaths(), {})"
"return np.random.choice(range(0,prob_list[0].shape[0]), 100, replace=False).tolist()"
selected_set.add(rand_ind[i])
"query= Detection.update(kind = destKind.value).where(Detection.id.in_(rList), Detection.kind == srcKind.value)"
query.execute()
"embedding_net = EmbeddingNet('resnet50', 256, True)"
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)"
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)"
print('Embedding Done')
sys.stdout.flush()
"plot_embedding(dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})"
Random examples to start
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)"
print(random_ids)
Move Records
Finetune the embedding model
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)"
unlabeled_dataset.updateEmbedding(model)
"print(dataset.em[dataset.current_set].shape, np.asarray(dataset.getlabels()).shape, len(dataset.getpaths()))"
"plot_embedding( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})"
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})"
train_eval_classifier()
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()"
"names = [""Linear SVM"", ""RBF SVM"", ""Random Forest"", ""Neural Net"", ""Naive Bayes""]"
"classifiers = [SVC(kernel=""linear"", C=0.025, probability= True, class_weight='balanced'),"
"SVC(gamma=2, C=1, probability= True, class_weight='balanced'),"
"RandomForestClassifier(max_depth=None, n_estimators=100, class_weight='balanced'),"
"MLPClassifier(alpha=1),"
GaussianNB()]
estimators= []
"for name, clf in zip(names, classifiers):"
"estimators.append((name, clf))"
"eclf1 = VotingClassifier(estimators= estimators, voting='hard')"
"eclf2 = VotingClassifier(estimators= estimators, voting='soft')"
"names.append(""ensemble hard"")"
classifiers.append(eclf1)
"names.append(""ensemble soft"")"
classifiers.append(eclf2)
dataset.image_mode()
dataset.updateEmbedding(model)
y_pred= clf.predict(X_test)
"print(confusion_matrix(y_test, y_pred))"
paths= dataset.getpaths()
"for i, (yp, yt) in enumerate(zip(y_pred, y_test)):"
if yp != yt:
"copy(paths[i],""mistakes"")"
"print(yt, yp, paths[i],i)"
"clf_output= clf_e.embedding(eval_loader, dim=48)"
"self.train_transform = transforms.Compose([Resize(raw_size), CenterCrop((processed_size)), ToTensor(), Normalize(mean, std)])"
"print(self.labels_set, self.n_classes)"
"print(line,imageWidth,imageHeight)"
"print length,(i-1)*length,i*length"
#########################################################
## Configuration
#########################################################
## The actual code
Check arguments
Create output directories
Padding around the detected objects when cropping
1.3 for the cropping during test time and 1.3 for
the context that the CNN requires in the left-over
image
Load a (frozen) Tensorflow model into memory.
## Preparations: get all the output tensors
The following processing is only for single image
Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
Follow the convention by adding back the batch dimension
For all images in the image directoryig
Load image
Run inference
"all outputs are float32 numpy arrays, so convert types as appropriate"
Add detections to the collection
Get info about the image
Select detections with a confidence larger than DETECTION_CONFIDENCE
Get these boxes and convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes"
"However, we need to make sure that it box coordinates are still within the image"
"For each detected bounding box with high confidence, we will"
crop the image to the padded box and save it
generate a unique identifier for the detection
"bbox is the detected box, crop_box the padded / enlarged box"
Add numbering to the original file name if there are multiple boxes
The absolute file path where we will store the image
"if COCO_OUTPUT_DIR is set, then we will only use the shape"
of cropped_img in the following code. So instead of reading
cropped_img = np.array(Image.open(out_file))
we can speed everything up by reading only the size of the image
matplotlib.use('Agg')
conf= ConfusionMatrix(24)
get the inputs
zero the parameter gradients
forward + backward + optimize
print statistics
get the inputs
forward + backward + optimize
print statistics
get the inputs
zero the parameter gradients
forward + backward + optimize
get the inputs
forward + backward + optimize
print statistics
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
"conf.reset()"""""""
define loss function (criterion) and optimizer
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
conf.reset()
for clf in classifiers:
"clf.fit(X_train, y_train)"
preds= clf.predict_proba(X)
uncertainty+= preds.max(axis=1)
print(uncertainty[ind])
print(uncertainty[ind])
true_labels = y[unlabeled_indices]
select up to 5 digit examples that the classifier is most uncertain about
"print(indices,selected_set)"
print(query.sql())
remember best acc@1 and save checkpoint
"completeClassificationLoop(run_dataset, model,num_classes)"
"embd, label, paths = extract_embeddings(run_loader, model)"
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)"
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd)
labels = db.labels_
"mapp=(find_probablemap(label,labels, K=args.K))"
"print(""Clusters"")"
"for i,x in enumerate(labels):"
labels[i]= mapp[x]
print(np.sum(labels == label)/labels.size)
"print(""Confidence Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning(embd, label, idx)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(""Entropy Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning_entropy(embd, label, idx)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
"new_selected= selectSamples(embd,paths,3000)"
"print(idx,idx.shape)"
for i in idx:
print(paths[i])
"print(""Silohette active learning"")"
"idx= active_learning2(embd, 1000, args.num_clusters)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(""Random"")"
"idx = np.random.choice(np.arange(len(paths)), 1000, replace=False)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
embd= reduce_dimensionality(embd)#[0:10000])
labels= labels[0:10000]
label= label[0:10000]
paths= paths[0:10000]
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])"
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])"
plt.show()
"np.save(args.name_prefix+""_embeddings.npy"",embd)"
"np.save(args.name_prefix+""_labels.npy"",label)"
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")"
combo.setEnabled(not finalized)
print(self.bbox)
self.addWidget(self.child)
Moving container with arrows
Left - Bottom
Right-Bottom
Left-Top
Right-Top
Left - Bottom
Right - Bottom
Left - Top
Right - Top
check cursor horizontal position
check cursor vertical position
self.resizeEvent=self.onResize
"print(""Parent"", parent, parent.width(), parent.height())"
"self.setGeometry(0,0,410,307)"
"print(w,h,""w,h"")"
"print(""final"",tag.getFinal())"
pass
"self.tags.append(TContainer(self,Category.get(-1),[0,0,0.1,0.1],True, Qt.red))"
label= CharField()
fullname=str(self.model)
"self.name= (fullname[fullname.find("":"")+2:fullname.find("">"")].strip()+'_set').lower()"
"print(""Parent"", self.parentWidget().width(), self.parentWidget().height())"
"print(self.model,self.name,query.sql())"
self.tab4.speciesList.setModel(species)
"self.tab4.speciesList.setRowHidden(len(species.stringList())-1, True)"
self.speciesList.itemChanged.connect(self.itemChanged)
#####
""
video_utils.py
""
"Utilities for splitting, rendering, and assembling videos."
""
#####
"%% Constants, imports, environment"
from ai4eutils
%% Path utilities
%% Function for rendering frames to video and vice-versa
http://tsaith.github.io/combine-images-into-a-video-with-python-3-and-opencv-3.html
Determine the width and height from the first image
Define the codec and create VideoWriter object
"If we're not over-writing, check whether all frame images already exist"
"OpenCV seems to over-report the number of frames by 1 in some cases, or fails"
"to read the last frame; either way, I'm allowing one missing frame."
"print(""Rendering video {}, couldn't find frame {}"".format("
"input_video_file,missing_frame_number))"
...if we need to check whether to skip this video entirely
"for frame_number in tqdm(range(0,n_frames)):"
print('Skipping frame {}'.format(frame_filename))
Recursively enumerate video files
Create the target output folder
Render frames
input_video_file = input_fn_absolute; output_folder = output_folder_video
For each video
""
input_fn_relative = input_files_relative_paths[0]
"process_detection_with_options = partial(process_detection, options=options)"
zero-indexed
Load results
# Break into videos
im = images[0]
# For each video...
video_name = list(video_to_frames.keys())[0]
frame = frames[0]
At most one detection for each category for the whole video
category_id = list(detection_categories.keys())[0]
Find the nth-highest-confidence video to choose a confidence value
Prepare the output representation for this video
'max_detection_conf' is no longer included in output files by default
...for each video
Write the output file
%% Test driver
%% Constants
%% Split videos into frames
"%% List image files, break into folders"
Find unique folders
fn = frame_files[0]
%% Load detector output
%% Render detector frames
folder = list(folders)[0]
d = detection_results_this_folder[0]
...for each file in this folder
...for each folder
%% Render output videos
folder = list(folders)[0]
...for each video
""
run_inference_with_yolov5_val.py
""
Runs a folder of images through MegaDetector (or another YOLOv5 model) with YOLOv5's
"val.py, converting the output to the standard MD format.  The main goal is to leverage"
YOLO's test-time augmentation tools.
""
"YOLOv5's val.py uses each file's base name as a unique identifier, which doesn't work"
when you have typical camera trap images like:
""
a/b/c/RECONYX0001.JPG
d/e/f/RECONYX0001.JPG
""
...so this script jumps through a bunch of hoops to put a symlinks in a flat
"folder, run YOLOv5 on that folder, and map the results back to the real files."
""
"Currently requires the user to supply the path where a working YOLOv5 install lives,"
and assumes that the current conda environment is all set up for YOLOv5.
""
TODO:
""
* Figure out what happens when images are corrupted... right now this is the #1
"reason not to use this script, it may be the case that corrupted images look the"
same as empty images.
""
* Multiple GPU support
""
* Checkpointing
""
* Windows support (I have no idea what all the symlink operations will do on Windows)
""
"* Support alternative class names at the command line (currently defaults to MD classes,"
though other class names can be supplied programmatically)
""
%% Imports
%% Options class
# Required ##
# Optional ##
%% Main function
#%% Path handling
#%% Enumerate images
#%% Create symlinks to give a unique ID to each image
i_image = 0; image_fn = image_files_absolute[i_image]
...for each image
#%% Create the dataset file
Category IDs need to be continuous integers starting at 0
#%% Prepare YOLOv5 command
#%% Run YOLOv5 command
#%% Convert results to MD format
"We'll use the absolute path as a relative path, and pass '/'"
as the base path in this case.
#%% Clean up
...def run_inference_with_yolo_val()
%% Command-line driver
%% Scrap
%% Test driver (folder)
%% Test driver (file)
%% Preview results
options.sample_seed = 0
...for each prediction file
%% Compare results
Choose all pairwise combinations of the files in [filenames]
"%% Constants, imports, environment"
from multiprocessing.pool import ThreadPool as workerpool
Number of images to pre-fetch
Useful hack to force CPU inference.
""
"Need to do this before any PT/TF imports, which happen when we import"
from run_detector.
Numpy FutureWarnings from tensorflow import
%% Support functions for multiprocessing
TODO
""
The queue system is a little more elegant if we start one thread for reading and one
"for processing, and this works fine on Windows, but because we import TF at module load,"
"CUDA will only work in the main process, so currently the consumer function runs here."
""
"To enable proper multi-GPU support, we may need to move the TF import to a separate module"
that isn't loaded until very close to where inference actually happens.
%% Other support funtions
%% Image processing functions
%% Main function
Handle the case where image_file_names is not yet actually a list
"Find the images to score; images can be a directory, may need to recurse"
A json list of image paths
A single image file
Load the detector
Does not count those already processed
Will not add additional entries not in the starter checkpoint
Write a checkpoint if necessary
"Back up any previous checkpoints, to protect against crashes while we're writing"
the checkpoint file.
Write the new checkpoint
Remove the backup checkpoint if it exists
...if it's time to make a checkpoint
"When using multiprocessing, let the workers load the model"
"Results may have been modified in place, but we also return it for"
backwards-compatibility.
The typical case: we need to build the 'info' struct
"If the caller supplied the entire ""info"" struct"
"The 'max_detection_conf' field used to be included by default, and it caused all kinds"
"of headaches, so it's no longer included unless the user explicitly requests it."
%% Interactive driver
%%
%% Command-line driver
This is an experimental hack to allow the use of non-MD YOLOv5 models through
the same infrastructure; it disables the code that enforces MDv5-like class lists.
Load the checkpoint if available
""
Relative file names are only output at the end; all file paths in the checkpoint are
still full paths.
"Find the images to score; images can be a directory, may need to recurse"
A json list of image paths
A single image file
Test that we can write to the output_file's dir if checkpointing requested
"Don't overwrite existing checkpoint files, this is a sure-fire way to eventually"
erase someone's checkpoint.
"Commenting this out for now... the scenario where we are resuming from a checkpoint,"
then immediately overwrite that checkpoint with empty data is higher-risk than the
annoyance of crashing a few minutes after starting a job.
Confirm that we can write to the checkpoint path; this avoids issues where
we crash after several thousand images.
%% Imports
import pre- and post-processing functions from the YOLOv5 repo
scale_coords() became scale_boxes() in later YOLOv5 versions
%% Classes
padded resize
"Image size can be an int (which translates to a square target size) or (h,w)"
...if the caller has specified an image size
NMS
"As of v1.13.0.dev20220824, nms is not implemented for MPS."
""
Send predication back to the CPU to fix.
format detections/bounding boxes
"This is a loop over detection batches, which will always be length 1 in our case,"
since we're not doing batch inference.
Rescale boxes from img_size to im0 size
"normalized center-x, center-y, width and height"
"MegaDetector output format's categories start at 1, but the MD"
model's categories start at 0.
...for each detection in this batch
...if this is a non-empty batch
...for each detection batch
...try
for testing
"MegaDetector was trained with batch size of 1, and the resizing function is a part"
of the inference graph
"change from [y1, x1, y2, x2] to [x1, y1, width, height]"
convert numpy floats to Python floats
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size
"np_images = [np.asarray(image, np.uint8) for image in images]"
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)"
performs inference
our batch size is 1; need to loop the batch dim if supporting batch size > 1
"%% Constants, imports, environment"
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Numpy FutureWarnings from tensorflow import
Useful hack to force CPU inference
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
An enumeration of failure reasons
Number of decimal places to round to for confidence and bbox coordinates
Label mapping for MegaDetector
Should we allow classes that don't look anything like the MegaDetector classes?
"Each version of the detector is associated with some ""typical"" values"
"that are included in output files, so that downstream applications can"
use them as defaults.
%% Classes
Stick this into filenames before the extension for the rendered result
%% Utility functions
mps backend only available in torch >= 1.12.0
%% Main function
Dictionary mapping output file names to a collision-avoidance count.
""
"Since we'll be writing a bunch of files to the same folder, we rename"
as necessary to avoid collisions.
...def input_file_to_detection_file()
Image is modified in place
...for each image
...def load_and_run_detector()
%% Command-line driver
Must specify either an image file or a directory
"but for a single image, args.image_dir is also None"
%% Interactive driver
%%
#####
""
process_video.py
""
"Split a video (or folder of videos) into frames, run the frames through run_detector_batch.py,"
and optionally stitch together results into a new video with detection boxes.
""
TODO: allow video rendering when processing a whole folder
TODO: allow video rendering from existing results
""
#####
"%% Constants, imports, environment"
Only relevant if render_output_video is True
Folder to use for extracted frames
Folder to use for rendered frames (if rendering output video)
Should we render a video with detection boxes?
""
"Only supported when processing a single video, not a folder."
"If we are rendering boxes to a new video, should we keep the temporary"
rendered frames?
Should we keep the extracted frames?
%% Main functions
"TODO: keep track of whether we created this folder, delete if we're deleting the extracted"
"frames and we created the folder, and the output files aren't in the same folder.  For now,"
we're just deleting the extracted frames and leaving the empty folder around in this case.
Render detections to images
"TODO: keep track of whether we created this folder, delete if we're deleting the rendered"
"frames and we created the folder, and the output files aren't in the same folder.  For now,"
we're just deleting the rendered frames and leaving the empty folder around in this case.
Combine into a video
Delete the temporary directory we used for detection images
shutil.rmtree(rendering_output_dir)
(Optionally) delete the frames on which we ran MegaDetector
shutil.rmtree(frame_output_folder)
...process_video()
# Validate options
# Split every video into frames
# Run MegaDetector on the extracted frames
# (Optionally) delete the frames on which we ran MegaDetector
shutil.rmtree(frame_output_folder)
# Convert frame-level results to video-level results
...process_video_folder()
%% Interactive driver
%% Process a folder of videos
import clipboard; clipboard.copy(cmd)
%% Process a single video
import clipboard; clipboard.copy(cmd)
"%% Render a folder of videos, one file at a time"
import clipboard; clipboard.copy(s)
%% Command-line driver
Lint as: python3
Copyright 2020 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TPU is automatically inferred if tpu_name is None and
we are running under cloud ai-platform.
""
copy_checkpoints.py
""
Run this script with specified source_dir and target_dir while the model is training to make a copy
of every checkpoint (checkpoints are kept once an hour by default and is difficult to adjust)
""
do not copy event or evaluation results
input validation
plot the images
adjust the figure
"read in dataset CSV and create merged (dataset, location) col"
map label to label_index
load the splits
only weight the training set by detection confidence
TODO: consider weighting val and test set as well
isotonic regression calibration of MegaDetector confidence
treat each split separately
new_weight[i] = confidence[i] * (n / c) / total_confidence(i's label)
- n = # examples in split (weighted by confidence); c = # labels
- weight allocated to each label is n/c
"- within each label, weigh each example proportional to confidence"
- new weights sum to n
error checking
"maps output label name to set of (dataset, dataset_label) tuples"
find which other label (label_b) has intersection
input validation
create label index JSON
look into sklearn.preprocessing.MultiLabelBinarizer
Note: JSON always saves keys as strings!
"True for ground truth, False for MegaDetector"
always save as .jpg for consistency
get bounding boxes
convert from category ID to category name
"check if crops are already downloaded, and ignore bboxes below the"
confidence threshold
assign all images without location info to 'unknown_location'
remove images from labels that have fewer than min_locs locations
merge dataset and location into a single string '<dataset>/<location>'
"create DataFrame of counts. rows = locations, columns = labels"
label_count: label => number of examples
loc_count: label => number of locs containing that label
generate a new split
score the split
SSE for # of images per label (with 2x weight)
SSE for # of locs per label
label => list of datasets to prioritize for test and validation sets
"merge dataset and location into a tuple (dataset, location)"
sorted smallest to largest
greedily add to test set until it has >= 15% of images
sort the resulting locs
"modify loc_to_size in place, so copy its keys before iterating"
arguments relevant to both creating the dataset CSV and splits.json
arguments only relevant for creating the dataset CSV
arguments only relevant for creating the splits JSON
comment lines starting with '#' are allowed
""
prepare_classification_script.py
""
Notebook-y script used to prepare a series of shell commands to run a classifier
(other than MegaClassifier) on a MegaDetector result set.
""
Differs from prepare_classification_script_mc.py only in the final class mapping step.
""
%% Job options
%% Constants
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
%% Set up environment
commands.append('cd CameraTraps/classification\n')
commands.append('conda activate cameratraps-classifier\n')
%% Crop images
fn = input_files[0]
%% Run classifier
fn = input_files[0]
%% Merge classification and detection outputs
fn = input_files[0]
%% Write everything out
input validation
"Evaluating with accimage is much faster than Pillow or Pillow-SIMD, but accimage"
is Linux-only.
create output directory
override saved params with kwargs
"For now, we don't weight crops by detection confidence during"
evaluation. But consider changing this.
"create model, compile with TorchScript if given checkpoint is not compiled"
"verify that target names matches original ""label names"" from dataset"
"if the dataset does not already have a 'other' category, then the"
'other' category must come last in label_names to avoid conflicting
with an existing label_id
define loss function (criterion)
"this file ends up being huge, so we GZIP compress it"
double check that the accuracy metrics are computed properly
save the confusion matrices to .npz
save per-label statistics
set dropout and BN layers to eval mode
"even if batch contains sample weights, don't use them"
Do target mapping on the outputs (unnormalized logits) instead of
"the normalized (softmax) probabilities, because the loss function"
uses unnormalized logits. Summing probabilities is equivalent to
log-sum-exp of unnormalized logits.
"a confusion matrix C is such that C[i,j] is the # of observations known to"
be in group i and predicted to be in group j.
match pytorch EfficientNet model names
images dataset
"for smaller disk / memory usage, we cache the raw JPEG bytes instead"
of the decoded Tensor
convert JPEG bytes to a 3D uint8 Tensor
"keras EfficientNet already includes normalization from [0, 255] to [0, 1],"
so we don't need to do that here
labels dataset
img_files dataset
weights dataset
define the transforms
efficientnet data preprocessing:
- train:
"1) random crop: aspect_ratio_range=(0.75, 1.33), area_range=(0.08, 1.0)"
2) bicubic resize to img_size
3) random horizontal flip
- test:
1) center crop
2) bicubic resize to img_size
weights sums to:
- if weight_by_detection_conf: (# images in split - conf delta)
- otherwise: (# images in split)
"freeze the base model's weights, including BatchNorm statistics"
https://www.tensorflow.org/guide/keras/transfer_learning#fine-tuning
rebuild output
input validation
set seed
create logdir and save params
create dataloaders and log the index_to_label mapping
define loss function and optimizer
using EfficientNet training defaults
- batch norm momentum: 0.99
"- optimizer: RMSProp, decay 0.9 and momentum 0.9"
- epochs: 350
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs"
- weight decay: 1e-5
TODO: change weighted to False if oversampling minority classes
stop training after 8 epochs without improvement
log metrics
log confusion matrix
log tp/fp/fn images
"tf.summary.image requires input of shape [N, H, W, C]"
false positive for top3_pred[0]
false negative for label
"if evaluating or finetuning, set dropout & BN layers to eval mode"
"for each label, track 5 most-confident and least-confident examples"
"even if batch contains sample weights, don't use them"
we do not track L2-regularization loss in the loss metric
This dictionary will get written out at the end of this process; store
diagnostic variables here
error checking
refresh detection cache
save log of bad images
cache of Detector outputs: dataset name => {img_path => detection_dict}
img_path: <dataset-name>/<img-filename>
get SAS URL for images container
strip image paths of dataset name
save list of dataset names and task IDs for resuming
complete task name: 'detect_for_classifier_caltech_20200722_110816_task01'
HACK! Sleep for 10s between task submissions in the hopes that it
"decreases the chance of backend JSON ""database"" corruption"
task still running => continue
"task finished successfully, save response to disk"
error checking before we download and crop any images
convert from category ID to category name
we need the datasets table for getting SAS keys
"we already did all error checking above, so we don't do any here"
get ContainerClient
get bounding boxes
we must include the dataset <ds> in <crop_path_template> because
'{img_path}' actually gets populated with <img_file> in
load_and_crop()
"get the image, either from disk or from Blob Storage"
inelegant way to close the container_clients
""
prepare_classification_script_mc.py
""
Notebook-y script used to prepare a series of shell commands to run MegaClassifier
on a MegaDetector result set.
""
Differs from prepare_classification_script.py only in the final class mapping step.
""
%% Job options
%% Constants
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
%% Set up environment
commands.append('cd CameraTraps/classification\n')
commands.append('conda activate cameratraps-classifier\n')
%% Crop images
fn = input_files[0]
%% Run classifier
fn = input_files[0]
%% Remap classifier outputs
fn = input_files[0]
%% Merge classification and detection outputs
fn = input_files[0]
%% Write everything out
mean/std values from https://pytorch.org/docs/stable/torchvision/models.html
define the transforms
resizes smaller edge to img_size
weights sums to:
- if weight_by_detection_conf: (# images in split - conf delta)
- otherwise: # images in split
for normal (non-weighted) shuffling
set all parameters to not require gradients except final FC layer
replace final fully-connected layer (which has 1000 ImageNet classes)
"detect GPU, use all if available"
input validation
set seed
create logdir and save params
create dataloaders and log the index_to_label mapping
create model
define loss function and optimizer
using EfficientNet training defaults
- batch norm momentum: 0.99
"- optimizer: RMSProp, decay 0.9 and momentum 0.9"
- epochs: 350
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs"
- weight decay: 1e-5
stop training after 8 epochs without improvement
do a complete evaluation run
log metrics
log confusion matrix
log tp/fp/fn images
"for every image: undo normalization, clamp to [0, 1], CHW -> HWC"
"- cannot be in-place, because the HeapItem might be in multiple heaps"
writer.add_figure() has issues => using add_image() instead
"writer.add_figure(f'{label_name}/{tag}', fig, global_step=epoch)"
false positive for top3_pred[0]
false negative for label
"preds and labels both have shape [N, k]"
"if evaluating or finetuning, set dropout and BN layers to eval mode"
"for each label, track k_extreme most-confident and least-confident images"
"even if batch contains sample weights, don't use them"
lock before changing ImageFile.LOAD_TRUNCATED_IMAGES
input validation
use MegaDB to generate list of images
only keep images that:
"1) end in a supported file extension, and"
2) actually exist in Azure Blob Storage
3) belong to a label with at least min_locs locations
write out log of images / labels that were removed
"save label counts, pre-subsampling"
"save label counts, post-subsampling"
spec_dict['taxa']: list of dict
[
"{'level': 'family', 'name': 'cervidae', 'datasets': ['idfg']},"
"{'level': 'genus',  'name': 'meleagris'}"
]
"spec_dict['dataset_labels']: dict, dataset => list of dataset_label"
{
"""idfg"": [""deer"", ""elk"", ""prong""],"
"""idfg_swwlf_2019"": [""elk"", ""muledeer"", ""whitetaileddeer""]"
}
"maps output label name to set of (dataset, dataset_label) tuples"
"because MegaDB is organized by dataset, we do the same"
ds_to_labels = {
'dataset_name': {
"'dataset_label': [output_label1, output_label2]"
}
}
we need the datasets table for getting full image paths
The line
"[img.class[0], seq.class[0]][0] as class"
selects the image-level class label if available. Otherwise it selects the
"sequence-level class label. This line assumes the following conditions,"
expressed in the WHERE clause:
- at least one of the image or sequence class label is given
- the image and sequence class labels are arrays with length at most 1
- the image class label takes priority over the sequence class label
""
"In Azure Cosmos DB, if a field is not defined, then it is simply excluded"
"from the result. For example, on the following JSON object,"
{
"""dataset"": ""camera_traps"","
"""seq_id"": ""1234"","
"""location"": ""A1"","
"""images"": [{""file"": ""abcd.jpeg""}],"
"""class"": [""deer""],"
}
"the array [img.class[0], seq.class[0]] just gives ['deer'] because"
img.class is undefined and therefore excluded.
"if no path prefix, set it to the empty string '', because"
"os.path.join('', x, '') = '{x}/'"
result keys
"- already has: ['dataset', 'location', 'file', 'class', 'bbox']"
"- add ['label'], remove ['file']"
"if img is mislabeled, but we don't know the correct class, skip it"
"otherwise, update the img with the correct class, but skip the"
img if the correct class is not one we queried for
sort keys for determinism
we need the datasets table for getting SAS keys
strip leading '?' from SAS token
only check Azure Blob Storage
check local directory first before checking Azure Blob Storage
1st pass: populate label_to_locs
"label (tuple of str) => set of (dataset, location)"
2nd pass: eliminate bad images
prioritize is a list of prioritization levels
number of already matching images
main(
"label_spec_json_path='idfg_classes.json',"
"taxonomy_csv_path='../../camera-traps-private/camera_trap_taxonomy_mapping.csv',"
"output_dir='run_idfg',"
json_indent=4)
recursively find all files in cropped_images_dir
only find crops of images from detections JSON
resizes smaller edge to img_size
"Evaluating with accimage is much faster than Pillow or Pillow-SIMD, but accimage"
is Linux-only.
create dataset
create model
set dropout and BN layers to eval mode
load files
dataset => set of img_file
crop_path: <dataset>/<img_file>___cropXX_mdvY.Y.jpg
[----<img_path>----]
error checking
any row with 'correct_class' should be marked 'mislabeled'
filter to only the mislabeled rows
convert '\' to '/'
verify that overlapping indices are the same
"""add"" any new mislabelings"
write out results
error checking
load detections JSON
get detector version
convert from category ID to category name
copy keys to modify dict in-place
This will be removed later when we filter for animals
save log of bad images
"True for ground truth, False for MegaDetector"
always save as .jpg for consistency
"we already did all error checking above, so we don't do any here"
"get the image, either from disk or from Blob Storage"
inelegant way to close the container_client
"crop_path => normalized bbox coordinates [xmin, ymin, width, height]"
"only ground-truth bboxes do not have a ""confidence"" value"
try loading image from local directory
try to download image from Blob Storage
crop the image
"expand box width or height to be square, but limit to img size"
"Image.crop() takes box=[left, upper, right, lower]"
pad to square using 0s
Author: lukemelas (github username)
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch
With adjustments and added comments by workingcoder (github username).
Support the construction of 'efficientnet-l2' without pretrained weights
Expansion phase (Inverted Bottleneck)
"image_size = calculate_output_image_size(image_size, 1) <-- this wouldn't modify image_size"
Depthwise convolution phase
"Squeeze and Excitation layer, if desired"
Pointwise convolution phase
Expansion and Depthwise Convolution
Squeeze and Excitation
Pointwise Convolution
Skip connection and drop connect
The combination of skip connection and drop connect brings about stochastic depth.
Batch norm parameters
Get stem static or dynamic convolution depending on image size
Stem
Build blocks
Update block input and output filters based on depth multiplier.
The first block needs to take care of stride and filter size increase.
"image_size = calculate_output_image_size(image_size, block_args.stride)  # stride = 1"
Head
Final linear layer
Stem
Blocks
Head
Stem
Blocks
Head
Convolution layers
Pooling and final linear layer
Author: lukemelas (github username)
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch
With adjustments and added comments by workingcoder (github username).
###############################################################################
## Help functions for model architecture
###############################################################################
GlobalParams and BlockArgs: Two namedtuples
Swish and MemoryEfficientSwish: Two implementations of the method
round_filters and round_repeats:
Functions to calculate params for scaling model width and depth ! ! !
get_width_and_height_from_size and calculate_output_image_size
drop_connect: A structural design
get_same_padding_conv2d:
Conv2dDynamicSamePadding
Conv2dStaticSamePadding
get_same_padding_maxPool2d:
MaxPool2dDynamicSamePadding
MaxPool2dStaticSamePadding
"It's an additional function, not used in EfficientNet,"
but can be used in other model (such as EfficientDet).
"Parameters for the entire model (stem, all blocks, and head)"
Parameters for an individual model block
Set GlobalParams and BlockArgs's defaults
An ordinary implementation of Swish function
A memory-efficient implementation of Swish function
TODO: modify the params names.
"maybe the names (width_divisor,min_width)"
"are more suitable than (depth_divisor,min_depth)."
follow the formula transferred from official TensorFlow implementation
follow the formula transferred from official TensorFlow implementation
"generate binary_tensor mask according to probability (p for 0, 1-p for 1)"
Note:
The following 'SamePadding' functions make output size equal ceil(input size/stride).
"Only when stride equals 1, can the output size be the same as input size."
Don't be confused by their function names ! ! !
Tips for 'SAME' mode padding.
Given the following:
i: width or height
s: stride
k: kernel size
d: dilation
p: padding
Output after Conv2d:
o = floor((i+p-((k-1)*d+1))/s+1)
"If o equals i, i = floor((i+p-((k-1)*d+1))/s+1),"
=> p = (i-1)*s+((k-1)*d+1)-i
With the same calculation as Conv2dDynamicSamePadding
Calculate padding based on image size and save it
Calculate padding based on image size and save it
###############################################################################
## Helper functions for loading model params
###############################################################################
BlockDecoder: A Class for encoding and decoding BlockArgs
efficientnet_params: A function to query compound coefficient
get_model_params and efficientnet:
Functions to get BlockArgs and GlobalParams for efficientnet
url_map and url_map_advprop: Dicts of url_map for pretrained weights
load_pretrained_weights: A function to load pretrained weights
Check stride
"Coefficients:   width,depth,res,dropout"
Blocks args for the whole model(efficientnet-b0 by default)
It will be modified in the construction of EfficientNet Class according to model
note: all models have drop connect rate = 0.2
ValueError will be raised here if override_params has fields not included in global_params.
train with Standard methods
check more details in paper(EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks)
train with Adversarial Examples(AdvProp)
check more details in paper(Adversarial Examples Improve Image Recognition)
TODO: add the petrained weights url map of 'efficientnet-l2'
AutoAugment or Advprop (different preprocessing)
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils)
from data_management.megadb.schema import sequences_schema_check
resize is for displaying them more quickly
dataset and seq_id are required fields
sort the images in the sequence
"if no class label on the image, show class label on the sequence"
pool = ThreadPool()
"print('len of rendering_info', len(rendering_info))"
"tqdm(pool.imap_unordered(render_image_info_partial, rendering_info), total=len(rendering_info))"
options = write_html_image_list()
options['headerHtml']
print('Checking that the MegaDB entries conform to the schema...')
sequences_schema_check.sequences_schema_check(sequences)
#######
""
visualize_db.py
""
Outputs an HTML page visualizing annotations (class labels and/or bounding boxes)
on a sample of images in a database in the COCO Camera Traps format
""
#######
%% Imports
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils)
Assumes the cameratraps repo root is on the path
%% Settings
Set to None to visualize all images
Target size for rendering; set either dimension to -1 to preserve aspect ratio
"These are mutually exclusive; both are category names, not IDs"
"Special tag used to say ""show me all images with multiple categories"""
We sometimes flatten image directories by replacing a path separator with
another character.  Leave blank for the typical case where this isn't necessary.
Control rendering parallelization
Process-based parallelization in this function is currently unsupported
"due to pickling issues I didn't care to look at, but I'm going to just"
"flip this with a warning, since I intend to support it in the future."
%% Helper functions
"Translate the file name in an image entry in the json database to a path, possibly doing"
some manipulation of path separators
%% Core functions
"Optionally remove all images without bounding boxes, *before* sampling"
"Optionally include/remove images with specific labels, *before* sampling"
Put the annotations in a dataframe so we can select all annotations for a given image
Construct label map
Take a sample of images
Set of dicts representing inputs to render_db_bounding_boxes:
""
"bboxes, boxClasses, image_path"
All the class labels we've seen for this image (with out without bboxes)
Iterate over annotations for this image
iAnn = 0; anno = annos_i.iloc[iAnn]
...for each of this image's annotations
"We're adding html for an image before we render it, so it's possible this image will"
fail to render.  For applications where this script is being used to debua a database
"(the common case?), this is useful behavior, for other applications, this is annoying."
...for each image
...def render_image_info
def process_images(...)
%% Command-line driver
"Copy all fields from a Namespace (i.e., the output from parse_args) to an object."
""
Skips fields starting with _.  Does not check existence in the target object.
Convert to an options object
%% Interactive driver(s)
%%
os.startfile(htmlOutputFile)
%% Constants and imports
convert category ID from int to str
Retry on blob storage read failures
%% Functions
PIL.Image.convert() returns a converted copy of this image
Alter orientation as needed according to EXIF tag 0x112 (274) for Orientation
""
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28
https://www.media.mit.edu/pia/Research/deepview/exif.html
""
Null operation
Aspect ratio as width over height
ar = w / h
h = w / ar
w = ar * h
The following three functions are modified versions of those at:
""
https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py
Convert to pixels so we can use the PIL crop() function
PIL's crop() does surprising things if you provide values outside of
"the image, clip inputs"
...if this detection is above threshold
...for each detection
"list of lists, one list of strings for each bounding box (to accommodate multiple labels)"
for color selection
"Always render objects with a confidence of ""None"", this is typically used"
for ground truth data.
"{} is the default, which means ""show labels with no mapping"", so don't use ""if label_map"" here"
if label_map:
"To avoid duplicate colors with detection-only visualization, offset"
the classification class index by the number of detection classes
...for each classification
...if we have classification results
...if the confidence of this detection is above threshold
...for each detection
"print('Input must be of size [N, 4], but is ' + str(boxes_shape))"
Deliberately trimming to the width of the image only in the case where
"box expansion is turned on.  There's not an obvious correct behavior here,"
but the thinking is that if the caller provided an out-of-range bounding
"box, they meant to do that, but at least in the eyes of the person writing"
"this comment, if you expand a box for visualization reasons, you don't want"
to end up with part of a box.
""
A slightly more sophisticated might check whether it was in fact the expansion
"that made this box larger than the image, but this is the case 99.999% of the time"
"here, so that doesn't seem necessary."
...if we need to expand boxes
If the total height of the display strings added to the top of the bounding
"box exceeds the top of the image, stack the strings below the bounding box"
instead of above.
Each display_str has a top and bottom margin of 0.05x.
Reverse list and print from bottom to top.
Skip empty strings
"list of lists, one list of strings for each bounding box (to accommodate multiple labels)"
need to be a string here because PIL needs to iterate through chars
""
stacked bar charts are made with each segment starting from a y position
"To fit the legend in, shrink current axis by 20%"
Put a legend to the right of the current axis
####
""
visualize_incoming_annotations.py
""
Spot-check the annotations received from iMerit by visualizing annotated bounding
boxes on a sample of images and display them in HTML.
""
"Modified in 2021 March to use the new format (iMerit batch 12 onwards), which is a"
COCO formatted JSON with relative coordinates for the bbox.
""
from data_management.megadb.schema import sequences_schema_check
we used frame_num of 1 when sending out images to annotators when it is not explicitly stored (wcs esp.)
The file_name field in the incoming json looks like
alka_squirrels.seq2020_05_07_25C.frame119221.jpg
"we need to use the dataset, sequence and frame info to find the actual path in blob storage"
using the sequences
category_id 5 is No Object Visible
download the image
Write to HTML
allow forward references in typing annotations
class variables
instance variables
get path to root
taxonomy CSV rows without 'taxonomy_string' entries are excluded
"from the taxonomy graph, but can be included in a classification"
label specification JSON via the 'dataset_labels' key
use the lower parent
special cases
%% Imports
%% Taxnomy checking
taxonomy CSV rows without 'taxonomy_string' entries are excluded
"from the taxonomy graph, but can be included in a classification"
label specification JSON via the 'dataset_labels' key
...for each row in the taxnomy file
%% Command-line driver
%% Interactive driver
%%
which datasets are already processed?
"sequence-level query should be fairly fast, ~1 sec"
cases when the class field is on the image level (images in a sequence
"that had different class labels, 'caltech' dataset is like this)"
"this query may take a long time, >1hr"
"this query should be fairly fast, ~1 sec"
read species presence info from the JSON files for each dataset
has this class name appeared in a previous dataset?
columns to populate the spreadsheet
sort by descending species count
make the spreadsheet
hyperlink Bing search URLs
hyperlink example image SAS URLs
TODO hardcoded columns: change if # of examples or col_order changes
""
map_lila_taxonomy_to_wi_taxonomy.py
""
Loads the LILA category mapping (in which taxonomy information comes from an iNat taxonomy snapshot)
and tries to map each class to the Wildlife Insights taxonomy.
""
%% Constants and imports
Created by get_lila_category_list.py... contains counts for each category
This is a manually-curated file used to store mappings that had to be made manually
This is the main output file from this whole process
%% Load category and taxonomy files
%% Pull everything out of pandas
%% Cache WI taxonomy lookups
This is just a handy lookup table that we'll use to debug mismatches
taxon = wi_taxonomy[21653]; print(taxon)
Look for keywords that don't refer to specific taxa: blank/animal/unknown
Do we have a species name?
"If 'species' is populated, 'genus' should always be populated; one item currently breaks"
this rule.
...for each taxon
%% Find redundant taxa
%% Manual review of redundant taxa
%% Clean up redundant taxa
taxon_name = list(taxon_name_to_preferred_taxon_id.keys())[0]
"If we've gotten this far, we should be choosing from multiple taxa."
""
"This will become untrue if any of these are resolved later, at which point we shoudl"
remove them from taxon_name_to_preferred_id
Choose the preferred taxa
%% Read supplementary mappings
"Each line is [lila query],[WI taxon name],[notes]"
%% Map LILA categories to WI categories
Must be ordered from kingdom --> species
TODO:
"['subspecies','variety']"
i_taxon = 0; taxon = lila_taxonomy[i_taxon]; print(taxon)
"Go from kingdom --> species, choosing the lowest-level description as the query"
"E.g., 'car'"
"print('Made a supplementary mapping from {} to {}'.format(query,wi_taxon['taxon_name']))"
print('No match for {}'.format(query))
...for each LILA taxon
%% Manual mapping
%% Build a dictionary from LILA dataset names and categories to LILA taxa
i_d = 0; d = lila_taxonomy[i_d]
"%% Map LILA datasets to WI taxa, and count the number of each taxon available in each dataset"
dataset_name = list(lila_dataset_to_categories.keys())[0]
dataset_category = dataset_categories[0]
"Write out the dataset name, category name, WI GUID, WI scientific name, WI common name,"
and count
...for each category in this dataset
...for each dataset
...with open()
""
retrieve_sample_image.py
""
"Downloader that retrieves images from Google images, used for verifying taxonomy"
"lookups and looking for egregious mismappings (e.g., ""snake"" being mapped to a fish called"
"""snake"")."
""
"Simple wrapper around simple_image_download, but I've had to swap in and out the underlying"
downloader a few times.
""
%% Imports and environment
%%
%% Main entry point
%% Test driver
%%
""
"Using the taxonomy .csv file, map all LILA datasets to the standard taxonomy"
""
Does not currently produce results; this is just used to confirm that all category names
have mappings in the taxonomy file.
""
%% Constants and imports
Created by get_lila_category_list.py... contains counts for each category
%% Load category and taxonomy files
%% Map dataset names and category names to scientific names
i_row = 1; row = taxonomy_df.iloc[i_row]; row
"%% For each dataset, make sure we can map every category to the taxonomy"
dataset_name = list(lila_dataset_to_categories.keys())[0]
c = categories[0]
""
"Takes the megadb taxonomy mapping, extracts the rows that are relevant to"
"LILA, and does some cleanup."
""
%% Constants and imports
This is a partially-completed taxonomy file that was created from a different set of
"scripts, but covers *most* of LILA as of June 2022"
Created by get_lila_category_list.py
%% Read the input files
Get everything out of pandas
"%% Find all unique dataset names in the input list, compare them with data names from LILA"
d = input_taxonomy_rows[0]
The script that generates this dictionary creates a separate entry for bounding box
"metadata files, but those don't represent new dataset names"
%% Map input columns to output datasets
Make sure all of those datasets actually correspond to datasets on LILA
%% Re-write the input taxonomy file to refer to LILA datasets
Map the string datasetname:token to a taxonomic tree json
mapping = input_taxonomy_rows[0]
Make sure that all occurrences of this mapping_string give us the same output
assert taxonomy_string == taxonomy_mappings[mapping_string]
%% Re-write the input file in the target format
mapping_string = list(taxonomy_mappings.keys())[0]
""
prepare_lila_taxonomy_release.py
""
"Given the private intermediate taxonomy mapping, prepare the public (release)"
taxonomy mapping file.
""
%% Imports and constants
Created by get_lila_category_list.py... contains counts for each category
%% Find out which categories are actually used
dataset_name = datasets_to_map[0]
i_row = 0; row = df.iloc[i_row]; row
%% Generate the final output file
i_row = 0; row = df.iloc[i_row]; row
match_at_level = taxonomic_match[0]
i_row = 0; row = df.iloc[i_row]; row
"E.g.: (43117, 'genus', 'lepus', ['hares and jackrabbits']"
###############
---> CONSTANTS
###############
max_progressbar = count * (list(range(limit+1))[-1]+1)
"bar = progressbar.ProgressBar(maxval=max_progressbar,"
"widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()]).start()"
bar.update(bar.currval + 1)
bar.finish()
""
"Given a subset of LILA datasets, find all the categories, and start the taxonomy"
mapping process.
""
%% Constants and imports
Created by get_lila_category_list.py
'NACTI'
'Channel Islands Camera Traps'
%% Read the list of datasets
The script that generates this dictionary creates a separate entry for bounding box
"metadata files, but those don't represent new dataset names"
%% Find all categories
dataset_name = datasets_to_map[0]
%% Initialize taxonomic lookup
from taxonomy_mapping.species_lookup import (
"get_taxonomic_info, print_taxonomy_matche)"
%% Manual lookup
%%
q = 'white-throated monkey'
raise ValueError('')
%% Match every query against our taxonomies
mapping_string = category_mappings[1]; print(mapping_string)
...for each mapping
%% Write output rows
""
"Does some consistency-checking on the LILA taxonomy file, and generates"
an HTML preview page that we can use to determine whether the mappings
make sense.
""
%% Imports and constants
"lila_taxonomy_file = r""G:\git\agentmorrisprivate\lila-taxonomy\lila-taxonomy-mapping.csv"""
"lila_taxonomy_file = r""G:\temp\lila\lila_additions_2022.06.29.csv"""
%% Support functions
%% Read the taxonomy mapping file
%% Prepare taxonomy lookup
from taxonomy_mapping.species_lookup import (
"get_taxonomic_info, print_taxonomy_matche)"
%% Optionally remap all gbif-based mappings to inat (or vice-versa)
%%
i_row = 1; row = df.iloc[i_row]; row
This should be zero for the release .csv
%%
%% Check for mappings that disagree with the taxonomy string
Look for internal inconsistency
Look for outdated mappings
i_row = 0; row = df.iloc[i_row]
%% List null mappings
""
"These should all be things like ""unidentified"" and ""fire"""
""
i_row = 0; row = df.iloc[i_row]
%% List mappings with scientific names but no common names
%% List mappings that map to different things in different data sets
x = suppress_multiple_matches[-1]
...for each row where we saw this query
...for each row
"%% Verify that nothing ""unidentified"" maps to a species or subspecies"
"E.g., ""unidentified skunk"" should never map to a specific species of skunk"
%% Make sure there are valid source and level values for everything with a mapping
%% Find WCS mappings that aren't species or aren't the same as the input
"WCS used scientific names, so these remappings are slightly more controversial"
then the standard remappings.
row = df.iloc[-500]
"Anything marked ""species"" or ""unknown"" by definition doesn't map to a species,"
so ignore these.
print('WCS query {} ({}) remapped to {} ({})'.format(
"query,common_name,scientific_name,common_names_from_taxonomy))"
%% Download sample images for all scientific names
i_row = 0; row = df.iloc[i_row]
if s != 'mirafra':
continue
Check whether we already have enough images for this query
"print('Skipping query {}, already have {} images'.format(s,len(sizes_above_threshold)))"
Check whether we've already run this query for a previous row
...for each row in the mapping table
%% Rename .jpeg to .jpg
"print('Renaming {} to {}'.format(fn,new_fn))"
%% Choose representative images for each scientific name
s = list(scientific_name_to_paths.keys())[0]
Be suspicious of duplicate sizes
...for each scientific name
%% Delete unused images
%% Produce HTML preview
i_row = 2; row = df.iloc[i_row]
"image_paths = [os.path.relpath(p, output_base) for p in image_paths]"
...for each row
%% Open HTML preview
######
""
species_lookup.py
""
Look up species names (common or scientific) in the GBIF and iNaturalist
taxonomies.
""
Run initialize_taxonomy_lookup() before calling any other function.
""
######
%% Constants and imports
As of 2020.05.12:
""
"GBIF: ~777MB zipped, ~1.6GB taxonomy"
"iNat: ~2.2GB zipped, ~51MB taxonomy"
These are un-initialized globals that must be initialized by
the initialize_taxonomy_lookup() function below.
%% Functions
Initialization function
# Load serialized taxonomy info if we've already saved it
"# If we don't have serialized taxonomy info, create it from scratch."
Download and unzip taxonomy files
taxonomy_name = list(taxonomy_urls.items())[0][0]; zip_url = list(taxonomy_urls.items())[0][1]
Don't download the zipfile if we've already unzipped what we need
Bypasses download if the file exists already
Unzip the files we need
...for each file that we need from this zipfile
Remove the zipfile
os.remove(zipfile_path)
...for each taxonomy
"Create dataframes from each of the taxonomy files, and the GBIF common"
name file
Load iNat taxonomy
Load GBIF taxonomy
Remove questionable rows from the GBIF taxonomy
Load GBIF vernacular name mapping
Only keep English mappings
Convert everything to lowercase
"For each taxonomy table, create a mapping from taxon IDs to rows"
Create name mapping dictionaries
Build iNat dictionaries
row = inat_taxonomy.iloc[0]
Build GBIF dictionaries
"The canonical name is the Latin name; the ""scientific name"""
include the taxonomy name.
""
http://globalnames.org/docs/glossary/
This only seems to happen for really esoteric species that aren't
"likely to apply to our problems, but doing this for completeness."
Don't include taxon IDs that were removed from the master table
Save everything to file
...def initialize_taxonomy_lookup()
"list of dicts: {'source': source_name, 'taxonomy': match_details}"
i_match = 0
"list of (taxon_id, taxonRank, scientific name, [vernacular names])"
corresponding to an exact match and its parents
Walk taxonomy hierarchy
This can happen because we remove questionable rows from the
GBIF taxonomy
"print(f'Warning: no row exists for parent_taxon_id {parent_taxon_id},' + \"
"f'child taxon_id: {taxon_id}, query: {query}')"
The GBIF taxonomy contains unranked entries
...while there is taxonomy left to walk
...for each match
Remove redundant matches
i_tree_a = 0; tree_a = matching_trees[i_tree_a]
i_tree_b = 1; tree_b = matching_trees[i_tree_b]
"If tree a's primary taxon ID is inside tree b, discard tree a"
""
taxonomy_level_b = tree_b['taxonomy'][0]
...for each level in taxonomy B
...for each tree (inner)
...for each tree (outer)
...def traverse_taxonomy()
"print(""Finding taxonomy information for: {0}"".format(query))"
"In GBIF, some queries hit for both common and scientific, make sure we end"
up with unique inputs
"If the species is not found in either taxonomy, return None"
Both GBIF and iNat have a 1-to-1 mapping between taxon_id and row number
Walk both taxonomies
...def get_taxonomic_info()
m = matches[0]
"For example: [(9761484, 'species', 'anas platyrhynchos')]"
...for each taxonomy level
...for each match
...def print_taxonomy_matches()
%% Taxonomy functions that make subjective judgements
query = 'person'
Do we have an iNat match?
"print_taxonomy_matches(inat_matches, verbose=True)"
"print_taxonomy_matches(gbif_matches, verbose=True)"
print('Warning: multiple iNat matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple iNat common names for {query}')
Default to returning the query
"print(f'Matched iNat {query} to {scientific_name},{common_name}')"
...if we had iNat matches
If we either prefer GBIF or didn't have iNat matches
""
Code is deliberately redundant here; I'm expecting some subtleties in how
handle GBIF and iNat.
print('Warning: multiple GBIF matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple GBIF common names for {query}')
Default to returning the query
...if we needed to look in the GBIF taxonomy
...def _get_preferred_taxonomic_match()
%% Interactive drivers and debug
%% Initialization
%% Taxonomic lookup
query = 'lion'
print(matches)
Print the taxonomy in the taxonomy spreadsheet format
%% Directly access the taxonomy tables
%% Command-line driver
Read command line inputs (absolute path)
Read the tokens from the input text file
Loop through each token and get scientific name
""
process_species_by_dataset
""
We generated a list of all the annotations in our universe; this script is
used to (interactively) map them onto the GBIF and iNat taxonomies.  Don't
"try to run this script from top to bottom; it's used like a notebook, not like"
"a script, since manual review steps are required."
""
%% Imports
%autoreload 0
%autoreload -species_lookup
%% Constants
Input file
Output file after automatic remapping
File to which we manually copy that file and do all the manual review; this
should never be programmatically written to
The final output spreadsheet
HTML file generated to facilitate the identificaiton of egregious mismappings
%% Functions
Prefer iNat matches over GBIF matches
query = 'person'
Do we have an iNat match?
"print_taxonomy_matches(inat_matches, verbose=True)"
"print_taxonomy_matches(gbif_matches, verbose=True)"
print('Warning: multiple iNat matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple iNat common names for {query}')
Default to returning the query
"print(f'Matched iNat {query} to {scientific_name},{common_name}')"
...if we had iNat matches
If we either prefer GBIF or didn't have iNat matches
""
Code is deliberately redundant here; I'm expecting some subtleties in how
handle GBIF and iNat.
print('Warning: multiple GBIF matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple GBIF common names for {query}')
Default to returning the query
...if we needed to look in the GBIF taxonomy
...def get_preferred_taxonomic_match()
%% Initialization
%% Test single-query lookup
%%
%%
"q = ""grevy's zebra"""
%% Read the input data
%% Run all our taxonomic lookups
i_row = 0; row = df.iloc[i_row]
query = 'lion'
...for each query
Write to the excel file that we'll use for manual review
%% Download preview images for everything we successfully mapped
uncomment this to load saved output_file
"output_df = pd.read_excel(output_file, keep_default_na=False)"
i_row = 0; row = output_df.iloc[i_row]
...for each query
%% Write HTML file with representative images to scan for obvious mis-mappings
i_row = 0; row = output_df.iloc[i_row]
...for each row
%% Look for redundancy with the master table
Note: `master_table_file` is a CSV file that is the concatenation of the
"manually-remapped files (""manual_remapped.xlsx""), which are the output of"
this script run across from different groups of datasets. The concatenation
"should be done manually. If `master_table_file` doesn't exist yet, skip this"
"code cell. Then, after going through the manual steps below, set the final"
manually-remapped version to be the `master_table_file`.
%% Manual review
Copy the spreadsheet to another file; you're about to do a ton of manual
review work and you don't want that programmatically overwrriten.
""
See manual_review_xlsx above
%% Read back the results of the manual review process
%% Look for manual mapping errors
Manually inspect df for typos in 'dataset_names' and 'taxonomy_level' columns
Identify rows where:
""
- 'taxonomy_level' does not match level of 1st element in 'taxonomy_string'
- 'scientific_name' does not match name of 1st element in 'taxonomy_string'
""
...both of which typically represent manual mapping errors.
i_row = 0; row = df.iloc[i_row]
"I'm not sure why both of these checks are necessary, best guess is that"
the Excel parser was reading blanks as na on one OS/Excel version and as ''
on another.
The taxonomy_string column is a .json-formatted string; expand it into
an object via eval()
"%% Find scientific names that were added manually, and match them to taxonomies"
i_row = 0; row = df.iloc[i_row]
...for each query
%% Write out final version
""
"Generate read-only SAS URLs for all LILA containers, to facilitate partial downloads."
""
The results of this script end up here:
""
http://lila.science/wp-content/uploads/2020/03/lila_sas_urls.txt
""
"Update: that file is manually maintained now, it can't be programmatically generated"
""
%% Imports
Read-only
%% Enumerate containers
%% Generate SAS tokens
%% Generate SAS URLs
%% Write to output file
--------some stuff needed to get AJAX to work with bottle?--------#
-------------------------------------------------------------------------------- #
PREPARE TO QUEUE IMAGES FOR LABELING
-------------------------------------------------------------------------------- #
# Connect as USER to database DB_NAME through peewee and initialize database proxy
# Load embedding model
---------------------------------------------------------------------- #
CREATE QUEUE OF IMAGES TO LABEL
---------------------------------------------------------------------- #
Use classifier to generate predictions
# Update model predicted class in PostgreSQL database
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
-------------------------------------------------------------------------------- #
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI
-------------------------------------------------------------------------------- #
"# static routes (to serve CSS, etc.)"
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps"
"return bottle.static_file(filename, root='../../../../../../../../../../../.')"
# dynamic routes
if data['display_grayscale']:
indices_to_exclude.update(set(color_indices))
elif not data['display_grayscale']:
indices_to_exclude.update(set(grayscale_indices))
data['display_images'] = {}
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices]
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices]
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices]
data['display_images']['detection_categories'] = []
for i in indices:
if str(dataset.samples[i][1]) == 'None':
data['display_images']['detection_categories'].append('None')
else:
existing_category_entries = {cat.id: cat.name for cat in Category.select()}
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()"
data['display_images']['detection_categories'].append(cat_name)
Use image ids in images_to_label to get the corresponding dataset indices
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
print(set(dataset.set_indices[4]).update(set(indices_to_label)))
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Use image id images_to_label to get the corresponding dataset index
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Train on samples that have been labeled so far
dataset.set_kind(DetectionKind.UserDetection.value)
print(y_train)
Predict on the samples that have not been labeled
print(y_pred)
Update model predicted class in PostgreSQL database
timer = time.time()
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
print('Updating the database took %0.2f seconds'%(time.time() - timer))
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
"once the classifier has been trained the first time, switch to AL sampling"
""
make_oneclass_json.py
""
"Takes a coco-camera-traps .json database and collapses species classes to binary,"
optionally removing labels from empty images (to be detector-friendly) (depending on
"""experiment_type"")."
""
"Assumes that empty images are labeled as ""empty""."
""
%% Imports and environment
%% Core conversion function
"We're removing empty images from the annotation list, but not from"
"the ""images"" list; they'll still get used in detector training."
print('Ignoring empty annotation')
%% Interactive driver
%%
Load annotations
Convert from multi-class to one-class
Write out the one-class data
%% Command-line driver
""
plot_bounding_boxes.py
""
Takes a .json database containing bounding boxes and renders those boxes on the
source images.
""
"This assumes annotations in coco-camera-traps format, with absolute bbox"
coordinates.
""
%% Imports and environment
How many images should we process?  Set to -1 to process all images.
Should we randomize the image order?
%%  Read database and build up convenience mappings
Category ID to category name
Image ID to image info
Image ID to image path
Image ID to all annotations referring to this image
"%% Iterate over images, draw bounding boxes, write to file"
For each image
image = images[0]
Build up a list of bounding boxes to draw on this image
Load the image
Create figure and axes
Display the image
ann = imageAnnotations[0]
"For each annotation associated with this image, render bounding box and label"
"In the Rectangle() function, the first argument (""location"") is the bottom-left"
of the rectangle.
""
Origin is the upper-left of the image.
Add the patch to the Axes
Add a class label
This is magic goop that removes whitespace around image plots (sort of)
Write the output image
...for each image
""
plot_imerit_annotations.py
""
Takes a .json file full of bounding box annotations and renders those boxes on the
source images.
""
"This assumes annotations in the format we receive them, specifically:"
""
1) Relative bbox coordinates
"2) A list of .json objects, not a well-formatted .json file"
""
"I.e., don't use this on a COCO-style .json file.  See plot_bounding_boxes.py"
for the same operation performed on a proper COCO-camera-traps database.
""
%% Imports and environment
"os.makedirs(outputBase, exist_ok=True)"
%%  Read all source images and build up a hash table from image name to full path
"This spans training and validation directories, so it's not the same as"
just joining the image name to a base path
"%% Iterate over annotations, draw bounding boxes, write to file"
annData has keys:
""
"annotations, categories, images"
""
Each of these are lists of dictionaries
%% Render all annotations on each image in the sequence
%% Pull out image metadata
Build up a list of bounding boxes to draw on this image
Pull out just the image name from the filename
""
File names look like:
""
seq6efffac2-5567-11e8-b3fe-dca9047ef277.frame1.img59a94e52-23d2-11e8-a6a3-ec086b02610b.jpg
"m = re.findall(r'img(.*\.jpg)$', imgFileName, re.M|re.I)"
print(m)
assert(len(m) == 1)
queryFileName = m[0]
Map this image back to the original directory
"%% Loop over annotations, find annotations that match this image"
%%
"x,y,w,h"
""
"x,y is the bottom-left of the rectangle"
""
"x,y origin is the upper-left"
...for each annotation
%% Render with PIL (scrap)
%% Render with Matplotlib
Create figure and axes
Display the image
Location is the bottom-left of the rect
""
Origin is the upper-left
Add the patch to the Axes
This is magic goop that removes whitespace around image plots (sort of)
os.startfile(outputFileName)
%% Showing figures on-screen during debugging
plt.show()
Various (mostly unsuccessful) approaches to getting the plot window to show up
"in the foreground, which is a backend-specific operation..."
""
fig.canvas.manager.window.activateWindow()
fig.canvas.manager.window.raise_()
fm = plt.get_current_fig_manager()
"fm.window.attributes('-topmost', 1)"
"fm.window.attributes('-topmost', 0)"
""
# This is the one that I found to be most robust... at like 80% robust.
plt.get_current_fig_manager().window.raise_()
%%
...for each image
...for each file
""
convert_imerit_json_to_coco_json.py
""
"Takes a .json file with bounding boxes but no class labels, and a .json file containing the"
"class labels for those images, and creates a new json file with class labels and bounding"
boxes.
""
"The bounding box .json file is in the format returned by our annotators, which is not"
actually a fully-formed .json file; rather it's a series of .json objects
""
"Leaves filenames intact.  Stores imerit ""category IDs"" (empty/human/group/animal) in a new"
"field called ""annotation_type""."
""
%% Imports and constants
%% Configure files and paths
"For Snapshot Serengeti, we stored image IDs in our annotation files as:"
""
S1_B06_R1_PICT0008
""
...but the corresponding ID in the master database is actually:
""
S1\B06\R1\S1_B06_R1_PICT0008
""
"If this is ""True"", we'll expand the former to the latter"
Handling a one-off issue in which .'s were mysteriously replaced with -'s
"in our annotations.  This will be set dynamically, but I keep it here as"
a constant to remind me to remove this code when we clean this issue up.
Used in the (rare) case where a bounding box was added to an image that was originally
annotated as empty
Used in the (rare) case where we added bounding boxes to an image with multiple species
"Should we include ambiguous bounding boxes (with the ""ambiguous"" category label)"
in the output file?  Ambiguous boxes are boxes drawn on images with multiple species.
"%%  Read metadata from the master database, bounding boxes from the annotations file"
"The bounding box .json file is in the format returned by our annotators, which is not"
actually a fully-formed .json file; rather it's a series of .json objects
"Each element of annData is a dictionary corresponding to a single sequence, with keys:"
""
"annotations, categories, images"
sequence = annData[0]
%% Build convenience mappings
Image ID to images
Category ID to categories (referring to the database categories)
"Image ID to categories (i.e., species labels)"
Utility function we'll use to create annotations for images in empty
sequences (empty images in non-empty sequences already have annotations)
"%% Reformat annotations, grabbing category IDs from the master database (prep)"
iSequence = 0; sequence = annData[0]
"%% Reformat annotations, grabbing category IDs from the master database (loop)"
Make a copy here; we're going to manipulate the sequence annotations
when we need to add synthetic annotations for empty images
im = sequenceImages[0]
Are there any annotations in this sequence?
Which images in this sequence have annotations?
For each image in this sequence...
imeritImageID = im['id']
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG
Confirm that the file exists
Hande a one-off issue with our annotations
datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG
""
...had become:
""
datasetsnapshotserengeti.seqASG000001a-frame0.imgS1_B06_R1_PICT0008.JPG
Does it look like we encountered this issue?
Convert:
""
S1_B06_R1_PICT0008
""
...to:
""
S1/B06/B06_R1/S1_B06_R1_PICT0008
Make sure we only see each image once
Create empty annotations for empty images
""
Here we use the *unmodified* file name
Create an empty annotation for this image
Annotations still use the annotation filename (not database ID) at this point;
these will get converted to database IDs below when we process the
whole sequence.
Sanity-check image size
"print('Warning: img {} was listed in DB as {}x{}, annotated as {}x{}, actual size{}x{}'.format("
"old_id,new_im['width'],new_im['height'],im['width'],im['height'],imgObj.width,imgObj.height))"
...for each image in this sequence
For each annotation in this sequence...
ann = sequenceAnnotations[0]
Prepare an annotation using the category ID from the database and
the bounding box from the annotations file
Maintain iMerit's annotation category
Generate an (arbitrary) ID for this annotation; the COCO format has a concept
"of annotation ID, but our annotation files don't"
This was a one-off quirk with our file naming
We'll do special handling of images with multiple categories later
Store the annotation type (group/human/animal/empty)
This annotation has no bounding box but the image wasn't originally
annotated as empty
This annotation has a bounding box but the image was originally
annotated as empty
unnormalize the bbox
... for each annotation in this sequence
... for each sequence
%% Post-processing
Count empty images
...for each file
%% Sanity-check empty images
""
make_ss_annotation_image_folder.py
""
Take a directory full of images with the very long filenames we give annotators:
""
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension
""
"...along with a COCO-camera-traps database referring to those files, and:"
""
1) Creates a new COCO-camera-traps database with the original filenames in them
(copying the annotations)
""
2) Optionally creates a new directory with those images named according to the
"Snapshot Serengeti naming convention, including complete relative paths."
""
See convert_imerit_json_to_coco_json to see how we get from the original annotation
.json to a COCO-camera-traps database.
""
%% Constants and imports
%% Configure files/paths
%% Read the annotations (referring to the old filenames)
"%% Update filenames, optionally copying files"
im = data['images'][0]
For each image...
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG
"Find the image name, e.g. S1_B06_R1_PICT0008"
Convert:
""
S1_B06_R1_PICT0008.JPG
""
...to:
""
S1/B06/B06_R1/S1_B06_R1_PICT0008.JPG
...for each image
%% Write the revised database
""
get_annotation_tool_link.py
""
"Takes a COCO-camera-traps-style .json file with URLs already embedded, and prepares"
a link to the visipedia annotation tool that reviews a subset of those images.
""
""
create_new_annotation_json.py
""
"Creates a subset of a larger .json database, in this case specifically to pick some images"
from Snapshot Serengeti.
""
from utils import get_db_dicts
for seq in already_annotated:
seq_to_ims.pop(seq)
remove already annotated images
add lion images
lion_seqs = cat_to_seqs[cat_to_id['lionMale']] + cat_to_seqs[cat_to_id['lionFemale']]
#print(len(lion_seqs))
lion_seqs = [seq for seq in lion_seqs if seq not in already_annotated]
#print(len(lion_seqs))
"lion_seqs_to_annotate.extend(random.sample(lion_seqs, min(len(lion_seqs),num_lions)))"
print(len(lion_seqs_to_annotate))
num_elephants = 1000
elephant_seqs = cat_to_seqs[cat_to_id['elephant']]
#print(len(lion_seqs))
elephant_seqs = [seq for seq in elephant_seqs if seq not in already_annotated]
#print(len(lion_seqs))
"elephant_seqs_to_annotate = random.sample(elephant_seqs, num_elephants)"
num_empty = 10
empty_seqs_to_annotate = []
for loc in loc_to_seqs:
empty_seqs = cats_per_location[loc][cat_to_id['empty']]
empty_seqs = [seq for seq in empty_seqs if seq not in already_annotated]
empty_seqs = [seq for seq in empty_seqs if seq_to_season[seq] in seasons_to_keep]
"empty_seqs_to_annotate.extend(random.sample(empty_seqs, min(len(empty_seqs),num_empty)))"
ims_to_annotate.extend(empty_ims_to_annotate)
""
filter_database.py
""
"Look through a COCO-ct database and find images matching some crtieria, writing"
a subset of images and annotations to a new file.
""
%% Constants and imports
%% Configuration
%%  Read database and build up convenience mappings
Category ID to category name
Image ID to image info
Image ID to image path
%% Filter
ann = annotations[0]
Is this a tiny box or a group annotation?
"x,y,w,h"
All empty annotations should be classified as either empty or ambiguous
""
"The ambiguous cases are basically minor misses on the annotators' part,"
where two different small animals were present somewhere.
%% Write output file
""
find_images_for_classes.py
""
"Given a .json database, find images that are associated with one or more"
classes.
""
%% Constants and imports
%% Configuration
%%  Read database and build up convenience mappings
Category ID to category name
Image ID to image info
Image ID to image path
%% Look for target-class annotations
ann = annotations[0]
""
create_tfrecords_format.py
""
This script creates a tfrecords file from a dataset in VOTT format.
%% Imports and environment
set up the filenames and annotations
This loop reads the bboxes and corresponding labels and assigns them
the correct image. Kind of slow at the moment...
If needed: merging all classes
bbox_labels = ['Animal' for _ in bbox_labels]
BBox coords are stored in the format
"x_min (of width axis) y_min (of height axis), x_max, y_max"
Coordinate system starts in top left corner
"In this framework, we need ('ymin', 'xmin', 'ymax', 'xmax') format"
print out some stats
To make sure we loaded the bboxes correctly:
self.validate_bboxes()
For each image in the data set...
Make sure all are greater equal 0
%% Main tfrecord generation function
Propagate optional metadata to tfrecords
endfor each annotation for the current image
endfor each image
"Calculate number of shards to get the desired number of images per record,"
ensure it is evenly divisible by the number of threads
%% Command-line driver
%% Driver
""
make_tfrecords_cis_trans.py
""
Given a .json file that contains a three-element list (train/val/test) of image IDs and a .json database that contains
"those image IDs, generates tfrecords whose filenames include ""train""/""val""/""test"""
""
"dataset = json.load(open('/ai4efs/databases/snapshotserengeti/oneclass/SnapshotSerengeti_Seasons_1_to_4_tfrecord_format_valid_ims.json','r'))"
print('Creating trans_val tfrecords')
dataset = [im_id_to_im[idx] for idx in trans_val]
""
create_classification_tfrecords_from_json.py
""
Called from make_tfrecords_cis_trans.py if you're running a classification experiment.
""
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/'
datafile = 'eccv_18_annotation_files_oneclass/CaltechCameraTrapsECCV18'
image_file_root = datafolder+'eccv_18_all_images/'
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/'
datafolder = '/data/iwildcam/'
datafile = 'combined_iwildcam_annotations_oneclass/eccv_train_and_imerit_2'
need consecutive category ids
old_cat_id_to_new_cat_id = {categories[idx]['id']:idx+1 for idx in range(len(categories))}
print(old_cat_id_to_new_cat_id)
remove multiclass images
print(images[0])
print(vis_data[0])
""
make_tfrecords_from_json.py
""
"Given a coco-camera-traps .json file, creates tfrecords"
""
Thin wrapper for create_tfrecords_from_json.
""
%% Constants and imports
%% Constants and imports (interactive)
%%
%% Main tfrecord generation function
"check whether the input file has already been converted to the tfrecords format,"
"if not, convert"
"Calculate number of shards to get the desired number of images per record,"
ensure it is evenly divisible by the number of threads
%% Interactive driver
%%
%% Command-line driver
eMammal_make_splits.py
""
"Based on a tfrecords_format json file of the database, creates 3 splits according to"
the specified fractions based on location (images from the same location should be in
one split) or based on images.
""
"If a previous split is provided (append_to_previous_split is True), the entries in"
"each split will be preserved, and new entries will be appended, so that new models"
can warm start with a model trained on the original splits.
configurations and paths
approximate fraction for the new entries
read in the previous splits of image ID or location ID if available
"find new locations and assign them to a split, without reassigning any previous locations"
"find out which images are new, shuffle and split them"
do NOT sort the IDs to keep the shuffled order
export PYTHONPATH=$PYTHONPATH:tfmodels/research
add empty category
"add all images that don't have annotations, with cat empty"
print(detection_results.keys())
group the detections and gts by image id:
print(image_id)
print(len(scores))
print(len(labels))
recall_thresh = 0.9
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall])
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])"
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall])
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])"
exp_name = 'eccv_train'
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json'
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(len(detection_results['images']))
print(len(seqs))
"print(len(seqs[0]),len(seqs["
print(detection_results.keys())
group the detections by image id:
group the ground truth annotations by image id:
print(seq)
"for image_id, dets in per_image_detections.iteritems():"
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
print(num_gts)
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
if sum(detected_class_labels)>0:
print('detected animal box')
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])"
"print(scores, tp_fp_labels)"
num_total_gts += num_gts
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
print('valid box')
"print(best_im, best_score)"
exp_name = 'eccv_train'
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json'
export PYTHONPATH=$PYTHONPATH:tfmodels/research
need to loop over confidence values
"for each value, check if any detections on the image are > conf"
"If so, that image gets class ""animal"""
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0"
"for image_id, dets in per_image_detections.iteritems():"
"calc prec, rec for this confidence thresh"
group the ground truth annotations by image id:
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(detection_results.keys())
group the detections and gts by image id:
print(image_id)
print(len(scores))
print(len(labels))
recall_thresh = 0.9
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall])
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])"
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall])
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])"
print(sorted_ap)
plt.bar(sorted_ap)
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(detection_results.keys())
group the detections by image id:
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
print(gts)
print(num_gts)
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[:2])"
"print(scores, tp_fp_labels)"
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
export PYTHONPATH=$PYTHONPATH:tfmodels/research
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
print('detected animal box')
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])"
"print(scores, tp_fp_labels)"
num_total_gts += num_gts
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
print('valid box')
best_im = np.argmax(max_im_scores)
"print(best_im, best_score)"
"temp_labels = np.zeros(len(im_detection_labels),  dtype=np.int32)"
"temp_scores = np.zeros(len(im_detection_scores), dtype=np.float32)"
"for j in range(min(im_num_gts, len(im_detection_labels))):"
temp_labels[j] = True #TODO: this currently only works for oneclass?
temp_scores[j] = best_score
im_detection_labels = temp_labels
im_detection_scores = temp_scores
num_total_gts+=im_num_gts
"print(len(detection_scores), len(detection_scores[0]), len(detection_scores[1]))"
exp_name = 'small_balanced_cct'
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(seq)
"for image_id, dets in per_image_detections.iteritems():"
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
seq_num_gts.append(num_gts)
print(num_gts)
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
num_total_gts += 1
print('valid box')
"print(best_im, best_score)"
print('no valid box')
if sum(seq_num_gts)>0:
export PYTHONPATH=$PYTHONPATH:tfmodels/research
need to loop over confidence values
"for each value, check if any detections on the image are > conf"
"If so, that image gets class ""animal"""
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0"
"calc prec, rec for this confidence thresh"
export PYTHONPATH=$PYTHONPATH:tfmodels/research
group the detections by image id:
group the ground truth annotations by image id:
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
print('detected animal box')
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])"
"print(scores, tp_fp_labels)"
num_total_gts += num_gts
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
print('valid box')
"print(best_im, best_score)"
""
evaluate_detections.py
""
Adapted from analyze_detection.py which is now archived.
""
%% Imports and constants
%% Functions
"labels input to compute_object_detection_metrics() needs to start at 0, not 1"
num_detections = len(dets['boxes'])
to prevent 'Invalid dimensions for box data.' error
this box will not match any detections
compute one-class precision/recall/average precision (if every box is just of an object class)
%% Command-line driver
""
detection_eval_utils.py
""
Utility functions used in evaluate_detections.py
""
group the ground truth annotations by image id
#####
""
detect_and_predict_image.py
""
"Functions to load a TensorFlow detection and a classification model, run inference,"
"render bounding boxes on images, and write out the resulting"
images (with bounding boxes and classes).
""
"See the ""test driver"" cell for example invocation."
""
""
#####
"%% Constants, imports, environment"
Minimum detection confidence for showing a bounding box on the output image
Stick this into filenames before the extension for the rendered result
Number of top-scoring classes to show at each bounding box
%% Core detection functions
Load images if they're not already numpy arrays
iImage = 0; image = images[iImage]
"Load the image as an nparray of size h,w,nChannels"
"There was a time when I was loading with PIL and switched to mpimg,"
"but I can't remember why, and converting to RGB is a very good reason"
"to load with PIL, since mpimg doesn't give any indication of color"
"order, which basically breaks all .png files."
""
"So if you find a bug related to using PIL, update this comment"
"to indicate what it was, but also disable .png support."
image = mpimg.imread(image)
This shouldn't be necessarily when loading with PIL and converting to RGB
Actual detection
...for each image
"Currently ""boxes"" is a list of length nImages, where each element is shaped as"
""
"1,nDetections,4"
""
"This implicitly banks on TF giving us back a fixed number of boxes, let's assert on this"
to make sure this doesn't silently break in the future.
iBox = 0; box = boxes[iBox]
"""scores"" is a length-nImages list of elements with size 1,nDetections"
"""classes"" is a length-nImages list of elements with size 1,nDetections"
""
"Still as floats, but really representing ints"
Squeeze out the empty axis
boxes is nImages x nDetections x 4
scores and classes are both nImages x nDetections
Get input and output tensors of classification model
"imsize = cur_image['width'], cur_image['height']"
Select detections with a confidence larger 0.5
Get these boxes and convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes"
"However, we need to make sure that it box coordinates are still within the image"
For convenience:
Create an array with contains the index of the corresponding crop_box for each selected box
i.e. [False False 0 False 1 2 3 False False]
For each box
If this box should be classified
Run inference
if box should not be classified
...for each box
species_scores should have shape len(images) x len(boxes) x num_species
...for each image
...with tf.Session
with classification_graph
species_scores should have shape len(images) x len(boxes) x num_species
%% Rendering functions
Display the image
plt.show()
"top, left, bottom, right"
""
"x,y origin is the upper-left"
Generate bounding box text
Choose color based on class
Location is the bottom-left of the rect
""
Origin is the upper-left
iRight = x + w
iTop = y + h
Add the patch to the Axes
Add class description
First determine best location by finding the corner that is closest to the image center
relative corner coordinates
relative coordinates of image center
Compute pair-wise squared distance and get the index of the one with minimal distance
Get the corresponding coordinates ...
... and alignment for the text box
Plot the text box with background
...for each box
This is magic goop that removes whitespace around image plots (sort of)
"plt.savefig(outputFileName, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)"
os.startfile(outputFileName)
...for each image
...def render_bounding_boxes
Load and run detector on target images
Read the name of all classes
remove empty lines
%% Interactive driver
%%
%%
%% File helper functions
%% Command-line driver
Hack to avoid running on already-detected images
""
"Script for evaluating a frozen graph given a COCO-style dataset, which was generated by"
the ../data_management/databases/classification/make_classification_dataset.py script.
This is useful for debugging as the accuracy reported by this script should match the
accuracy reported by the Tensorflow training.
""
Check that all files exists for easier debugging
Load frozen graph
Collect tensors for input and output
Read image
Run inference
Check if correct
""
Script for evaluating precision/recall on a two-class problem given
"a frozen graph and a COCO-style dataset, which was generated by"
the ../data_management/databases/classification/make_classification_dataset.py script.
We assume the positive class is at index 0 (with a zero-based indexing)
""
Check that all files exists for easier debugging
Load frozen graph
Collect tensors for input and output
Read image
"with open(image_path, 'rb') as fi:"
"image =  sess.run(tf.image.decode_jpeg(fi.read(), channels=3))"
image = image / 255.
Run inference
predicted_class = np.argmax(predictions)
Check if correct
if coco.imgToAnns[image_id][0]['category_id'] == predicted_class:
correct = correct + 1
"In matplotlib < 1.5, plt.fill_between does not have a 'step' argument"
""
Script for selecting testing images of a COCO-style dataset generated by the script
../data_management/databases/classification/make_classification_dataset.py in a consistent
manner and predicting the class for it.
""
Mandatory parameters
Optional parameters
Validate parameters
Derived parameters
"We assume that the dataset was generated with the make_classification_dataset.py script,"
hence the images should be located in the same folder as the json
Make seletion deterministic
Load frozen graph
Get dataset information
Get classes
...and the class list corresponding to the model outputs by assuming
that they are in order of their ids
Get images of each class
Shuffle the image list
Start the image sampling
"Set of avaiable class IDs, will be filled below"
If there are still images left for that class
"Get image for the sampled class, we already shuffled the class images before so"
we can simply pop()
Start prediction
Collect tensors for input and output
Read image
Run inference
Print output to log file
#####
""
api_apply_classifier_single_node.py
""
Takes the JSON file produced by the detection API and
classifies all boxes above a confidence threshold.
""
#####
"%% Constants, imports, environment"
Assumes that the root of the CameraTraps repo is on the PYTHONPATH
Minimum detection confidence for classifying an object
Number of top-scoring classes to show at each bounding box
Enlargment factor applied to boxes before passing them to the classifier
""
Provides more context and can lead to better results
List of detection categories for which we will run the classification
""
"Currently there are {""1"": ""animal"", ""2"": ""person"", ""4"": ""vehicle""}"
""
Should be a list of string-formatted ints.
Number of significant float digits in JSON output
%% Core detection functions
Read the name of all classes
remove empty lines
Create field with name *classification_categories*
Add classes using 0-based indexing
def add_classification_categories
Make sure we have the right json object
Get input and output tensors of classification model
For each image
Read image
"Scale pixel values to [0,1]"
For each box
Skip detections with low confidence
Skip if detection category is not in whitelist
Skip if already classified
"Get current box in relative coordinates and format [x_min, y_min, width_of_box, height_of_box]"
"Convert to [ymin, xmin, ymax, xmax] and store it as 1x4 numpy array so we can"
re-use the generic multi-box padding code
Convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes."
""
"However, we need to make sure that it box coordinates are still within the image."
Get the first (and only) row as our bbox to classify
Get the image data for that box
Run inference
Add an empty list to the json for our predictions
Add the *num_annotated_classes* top scoring classes
...for each box
...for each image
...with tf.Session
with classification_graph
def classify_boxes
Load classification model
Load detector json
Add classes to detector_json
"Run classifier on all images, changes will be writting directly to the json"
Write output json
def load_and_run_classifier
%% Command-line driver
""
Script for selecting testing images of a COCO-style dataset generated by the script
../data_management/databases/classification/make_classification_dataset.py in a consistent
manner and predicting the class for it.
""
Assumes the cameratraps repo root is on the path
Make seletion deterministic
Mandatory parameters
Optional parameters
Validate parameters
Tranfer parameters to post-processing format
Load frozen graph
Reading image list
Reading class list
Image sampling
Start prediction
Collect tensors for input and output
Array for collecting infos for rendering the html
Read image
"Scale pixel values to [0,1]"
Run inference
Add links to all available classes
""
predict_image.py
""
"Given a pointer to a frozen detection graph, runs inference on a single image,"
printing the top classes to the console
""
%% Imports
%% Command-line processing
Check that all files exist for easier debugging
%% Inference
Load frozen graph
Load class list
Remove empty lines
Collect tensors for input and output
Read image
Run inference
Print output
""
Mostly unmodified script for freezing a model
Added for convenience and for possible future optimizations
""
Copyright 2015 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
'input_checkpoint' may be a prefix if we're using Saver V2 format
Remove all the explicit device specifications for this node. This helps to
make the graph more portable.
List of all partition variables. Because the condition is heuristic
"based, the list could include false positives."
This tensor doesn't exist in the graph (for example it's
'global_step' or a similar housekeeping element) so skip it.
`var_list` is required to be a map of variable names to Variable
tensors. Partition variables are Identity tensors that cannot be
handled by Saver.
Models that have been frozen previously do not contain Variables.
optimized_output_graph_def = optimize_for_inference_lib.optimize_for_inference(
"output_graph_def,"
"input_node_names.replace("" "", """").split("",""),"
"output_node_names.replace("" "", """").split("",""),"
tf.float32.as_datatype_enum)
Write GraphDef to file if output path has been given.
""
"Creates a graph description, which is required to create a frozen graph."
Adapted from from ./tf-slim/export_inference_graph.py
Added preprocessing to the definition for easier handling
""
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
tf.app.flags.DEFINE_integer(
"'batch_size', None,"
"'Batch size for the exported model. Defaulted to ""None"" so batch size can '"
'be specified at model runtime.')
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
#####################
Optimization Flags #
#####################
######################
Learning Rate Flags #
######################
######################
Dataset Flags #
######################
####################
Fine-Tuning Flags #
####################
"Note: when num_clones is > 1, this will actually have each clone to go"
over each epoch FLAGS.num_epochs_per_decay times. This is different
behavior from sync replicas and is expected to produce different results.
Warn the user if a checkpoint exists in the train_dir. Then we'll be
ignoring the checkpoint anyway.
TODO(sguada) variables.filter_variables()
######################
Config model_deploy #
######################
Create global_step
#####################
Select the dataset #
#####################
#####################
Select the network #
#####################
####################################
Select the preprocessing function #
####################################
#############################################################
Create a dataset provider that loads data from the dataset #
#############################################################
###################
Define the model #
###################
############################
Specify the loss function #
############################
Gather initial summaries.
"Gather update_ops from the first clone. These contain, for example,"
the updates for the batch_norm variables created by network_fn.
Add summaries for end_points.
Add summaries for losses.
Add summaries for variables.
################################
Configure the moving averages #
################################
########################################
Configure the optimization procedure. #
########################################
"If sync_replicas is enabled, the averaging will be done in the chief"
queue runner.
Update ops executed locally by trainer.
Variables to train.
and returns a train_tensor and summary_op
Add total_loss to summary.
Create gradient updates.
Add the summaries from the first clone. These contain the summaries
created by model_fn and either optimize_clones() or _gather_clone_loss().
Merge all summaries together.
##########################
Kicks off the training. #
##########################
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
#####################
Select the dataset #
#####################
###################
Select the model #
###################
#############################################################
Create a dataset provider that loads data from the dataset #
#############################################################
####################################
Select the preprocessing function #
####################################
###################
Define the model #
###################
Define the metrics:
Print the summaries to screen.
TODO(sguada) use num_epochs=1
This ensures that we make a single pass over all of the data.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Transform the image to floats.
"Randomly crop a [height, width] section of the image."
Randomly flip the image horizontally.
"Because these operations are not commutative, consider randomizing"
the order their operation.
Subtract off the mean and divide by the variance of the pixels.
Transform the image to floats.
Resize and crop if needed.
Subtract off the mean and divide by the variance of the pixels.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Use tf.slice instead of crop_to_bounding box as it accepts tensors to
define the crop size.
Compute the rank assertions.
Create a random bounding box.
""
Use tf.random_uniform and not numpy.random.rand as doing the former would
"generate random numbers at graph eval time, unlike the latter which"
generates random numbers at graph definition time.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Pass the real x only to one of the func calls.
The random_* ops do not necessarily clamp.
"Each bounding box has shape [1, num_boxes, box coords] and"
"the coordinates are ordered [ymin, xmin, ymax, xmax]."
A large fraction of image datasets contain a human-annotated bounding
box delineating the region of the image containing the object of interest.
We choose to create a new bounding box for the object which is a randomly
distorted version of the human-annotated bounding box that obeys an
"allowed range of aspect ratios, sizes and overlap with the human-annotated"
"bounding box. If no box is supplied, then we assume the bounding box is"
the entire image.
Crop the image to the specified bounding box.
"Each bounding box has shape [1, num_boxes, box coords] and"
"the coordinates are ordered [ymin, xmin, ymax, xmax]."
Restore the shape since the dynamic slice based upon the bbox_size loses
the third dimension.
This resizing operation may distort the images because the aspect
ratio is not respected. We select a resize method in a round robin
fashion based on the thread number.
Note that ResizeMethod contains 4 enumerated resizing methods.
We select only 1 case for fast_mode bilinear.
Randomly flip the image horizontally.
Randomly distort the colors. There are 1 or 4 ways to do it.
Crop the central region of the image with an area containing 87.5% of
the original image.
Resize the image to the specified height and width.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(sguada) docstring paragraph by (a) motivating the need for the file and
(b) defining clones.
TODO(sguada) describe the high-level components of model deployment.
"E.g. ""each model deployment is composed of several parts: a DeploymentConfig,"
"which captures A, B and C, an input_fn which loads data.. etc"
Set up DeploymentConfig
Create the global step on the device storing the variables.
Define the inputs
Define the optimizer.
Define the model including the loss.
Run training.
Namedtuple used to represent a clone during deployment.
"Namedtuple used to represent a DeployedModel, returned by deploy()."
Default parameters for DeploymentConfig
Create clones.
The return value.
Individual components of the loss that will need summaries.
Compute and aggregate losses on the clone device.
Add the summaries out of the clone device block.
Only use regularization_losses for the first clone
Compute the total_loss summing all the clones_losses.
Sum the gradients across clones.
Gather initial summaries.
Create Clones.
"Gather update_ops from the first clone. These contain, for example,"
the updates for the batch_norm variables created by model_fn.
Place the global step on the device storing the variables.
Compute the gradients for the clones.
Add summaries to the gradients.
Create gradient updates.
Only use regularization_losses for the first clone
Add the summaries from the first clone. These contain the summaries
created by model_fn and either optimize_clones() or _gather_clone_loss().
Add total_loss to summary.
Merge all summaries together.
Note that each grad_and_vars looks like the following:
"((grad_var0_clone0, var0), ... (grad_varN_cloneN, varN))"
!/usr/bin/python
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Failed to find ""index"" occurrence of item."
pylint: disable=broad-except
pylint: enable=broad-except
Grab the 'index' annotation.
Some images contain bounding box annotations that
"extend outside of the supplied image. See, e.g."
n03127925/n03127925_147.xml
"Additionally, for some bounding boxes, the min > max"
or the box is entirely outside of the image.
Example: <...>/n06470073/n00141669_6790.xml
Determine if the annotation is from an ImageNet Challenge label.
Note: There is a slight bug in the bounding box annotation data.
Many of the dog labels have the human label 'Scottish_deerhound'
instead of the synset ID 'n02092002' in the bbox.label field. As a
"simple hack to overcome this issue, we only exclude bbox labels"
*which are synset ID's* that do not match original synset label for
the XML file.
Guard against improperly specified boxes.
Note bbox.filename occasionally contains '%s' in the name. This is
data set noise that is fixed by just using the basename of the XML file.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The URLs where the MNIST data can be downloaded.
The names of the classes.
"First, process the training data:"
"Next, process the testing data:"
"Finally, write the labels file:"
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The URL where the Flowers data can be downloaded.
The number of images in the validation set.
Seed for repeatability.
The number of shards per dataset split.
Initializes function that decodes RGB JPEG data.
Read the filename:
Divide into train and test:
"First, convert the training and validation sets."
"Finally, write the labels file:"
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(nsilberman): Add tfrecord file type once the script is updated.
"If set to false, will not try to set label_to_names in dataset"
by reading them from labels.txt or github.
n01440764
n01443537
n02119247    black fox
n02119359    silver fox
pylint: disable=g-line-too-long
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The URL where the CIFAR data can be downloaded.
The number of training files.
The height and width of each image.
The names of the classes.
"First, process the training data:"
"Next, process the testing data:"
"Finally, write the labels file:"
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The labels file contains a list of valid labels are held in this file.
Assumes that the file contains entries as such:
n01440764
n01443537
n01484850
where each line corresponds to a label expressed as a synset. We map
each synset contained in the file to an integer (based on the alphabetical
ordering). See below for details.
This file containing mapping from synset to human-readable label.
Assumes each line of the file looks like:
""
n02119247    black fox
n02119359    silver fox
"n02119477    red fox, Vulpes fulva"
""
where each line corresponds to a unique mapping. Note that each line is
formatted as <synset>\t<human readable label>.
This file is the output of process_bounding_box.py
Assumes each line of the file looks like:
""
"n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940"
""
where each line corresponds to one bounding box annotation associated
with an image. Each line can be parsed as:
""
"<JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>"
""
Note that there might exist mulitple bounding box annotations associated
with an image file.
pylint: disable=expression-not-assigned
pylint: enable=expression-not-assigned
Create a single Session to run all image coding calls.
Initializes function that converts PNG to JPEG data.
Initializes function that converts CMYK JPEG data to RGB JPEG data.
Initializes function that decodes RGB JPEG data.
File list from:
https://groups.google.com/forum/embed/?place=forum/torch7#!topic/torch7/fOSTXHIESSU
File list from:
https://github.com/cytsai/ilsvrc-cmyk-image-list
Read the image file.
Clean the dirty data.
1 image is a PNG.
22 JPEG images are in CMYK colorspace.
Decode the RGB JPEG.
Check that image converted to RGB
Each thread produces N shards where N = int(num_shards / num_threads).
"For instance, if num_shards = 128, and the num_threads = 2, then the first"
"thread would produce shards [0, 64)."
"Generate a sharded version of the file name, e.g. 'train-00002-of-00010'"
"Break all images into batches with a [ranges[i][0], ranges[i][1]]."
Launch a thread for each batch.
Create a mechanism for monitoring when all threads are finished.
Create a generic TensorFlow-based utility for converting all image codings.
Wait for all the threads to terminate.
Leave label index 0 empty as a background class.
Construct the list of JPEG files and labels.
Shuffle the ordering of all image files in order to guarantee
random ordering of the images with respect to label in the
saved TFRecord files. Make the randomization repeatable.
Build a map from synset to human-readable label.
Run it!
Allowing None in the signature so that dataset_factory can use the default.
!/usr/bin/python
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Read in the 50000 synsets associated with the validation data set.
Make all sub-directories in the validation data dir.
Move all of the image to the appropriate sub-directory.
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
"The dataset has classes with no images (empty and human), which have ID 0 and 1,"
so we need to specify 49 here despite having only 47 classes with images
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Final pooling and prediction
Global average pooling.
Pooling with a fixed kernel size.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The current_stride variable keeps track of the effective stride of the
activations. This allows us to invoke atrous convolution whenever applying
the next residual unit would result in the activations having stride larger
than the target output_stride.
The atrous convolution rate parameter.
Move stride from the block's last unit to the end of the block.
"If we have reached the target output_stride, then we need to employ"
atrous convolution with stride=1 and multiply the atrous rate by the
current unit's stride for use in subsequent layers.
Collect activations at the block's end before performing subsampling.
Subsampling of the block's output activations.
"The following implies padding='SAME' for pool1, which makes feature"
alignment easier for dense prediction tasks. This is also used in
https://github.com/facebook/fb.resnet.torch. However the accompanying
code of 'Deep Residual Learning for Image Recognition' uses
padding='VALID' for pool1. You can switch to that choice by setting
"slim.arg_scope([slim.max_pool2d], padding='VALID')."
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Collect outputs for conv2d, fully_connected and max_pool2d"
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"inputs has shape [batch, 224, 224, 3]"
"inputs has shape [batch, 513, 513, 3]"
We do not include batch normalization or activation functions in
conv1 because the first ResNet unit will perform these. Cf.
Appendix of [2].
This is needed because the pre-activation variant does not have batch
normalization or activation functions in the residual unit output. See
Appendix of [2].
Convert end_points_collection into a dictionary of end_points.
Global average pooling.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
=============================================================================
"These parameters come from the online port, which don't necessarily match"
those in the paper.
TODO(nsilberman): confirm these values with Philip.
##########
Encoder #
##########
"No normalizer for the first encoder layers as per 'Image-to-Image',"
Section 5.1.1
First layer doesn't use normalizer_fn
Last layer doesn't use activation_fn nor normalizer_fn
##########
Decoder #
##########
"Dropout is used at both train and test time as per 'Image-to-Image',"
Section 2.1 (last paragraph).
The Relu comes BEFORE the upsample op:
Explicitly set the normalizer_fn to None to override any default value
"that may come from an arg_scope, such as pix2pix_arg_scope."
No normalization on the input layer.
Stride 1 on the last layer.
"1-dim logits, stride 1, no activation, no normalization."
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
Alias
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
pylint: disable=unused-import
pylint: enable=unused-import
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
If we are fine tuning a checkpoint we need to start at a lower learning
rate since we are farther along on training.
We can start quantizing immediately if we are finetuning.
We need to wait for the model to train a bit before we quantize if we are
training from scratch.
Call rewriter to produce graph with fake quant ops and folded batch norms
"quant_delay delays start of quantization till quant_delay steps, allowing"
for better model accuracy.
Configure the learning rate using an exponential decay.
"When restoring from a floating point model, the min/max values for"
quantized weights and activations are not present.
We instruct slim to ignore variables that are missing during restoration
by setting ignore_missing_vars=True
"If we are restoring from a floating point model, we need to initialize"
the global step to zero for the exponential decay to result in
reasonable learning rates.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Orignaly, arg_scope = slim.arg_scope and layers = slim, now switch to more"
update-to-date tf.contrib.* API.
Decay for the moving averages.
epsilon to prevent 0s in variance.
Turns off fused batch norm.
collection containing the moving mean and moving variance.
"For inverted pyramid models, we start with gating switched off."
batch_size x 32 x 112 x 112 x 64
Separable conv is slow when used at first conv layer.
batch_size x 32 x 56 x 56 x 64
batch_size x 32 x 56 x 56 x 64
batch_size x 32 x 56 x 56 x 192
batch_size x 32 x 28 x 28 x 192
batch_size x 32 x 28 x 28 x 256
batch_size x 16 x 14 x 14 x 512
batch_size x 16 x 14 x 14 x 512
batch_size x 16 x 14 x 14 x 512
batch_size x 16 x 14 x 14 x 528
batch_size x 16 x 14 x 14 x 832
batch_size x 8 x 7 x 7 x 832
batch_size x 8 x 7 x 7 x 1024
Final pooling and prediction
Temporal average pooling.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
299 x 299 x 3
149 x 149 x 32
147 x 147 x 32
147 x 147 x 64
73 x 73 x 160
71 x 71 x 192
35 x 35 x 384
4 x Inception-A blocks
35 x 35 x 384
Reduction-A block
17 x 17 x 1024
7 x Inception-B blocks
17 x 17 x 1024
Reduction-B block
8 x 8 x 1536
3 x Inception-C blocks
Auxiliary Head logits
17 x 17 x 1024
Final pooling and prediction
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which"
can be set to False to disable pooling here (as in resnet_*()).
8 x 8 x 1536
1 x 1 x 1536
1536
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN
setups need the gradient of gradient FusedBatchNormGrad.
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN
setups need the gradient of gradient FusedBatchNormGrad.
First upscaling is different because it takes the input vector.
Last layer has different normalizer and activation.
Convert to proper channels.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Use clip_by_value to simulate bandpass activation.
Use clip_by_value to simulate bandpass activation.
Use clip_by_value to simulate bandpass activation.
149 x 149 x 32
147 x 147 x 32
147 x 147 x 64
73 x 73 x 64
73 x 73 x 80
71 x 71 x 192
35 x 35 x 192
35 x 35 x 320
TODO(alemi): Register intermediate endpoints
"17 x 17 x 1088 if output_stride == 8,"
33 x 33 x 1088 if output_stride == 16
TODO(alemi): register intermediate endpoints
TODO(gpapan): Properly support output_stride for the rest of the net.
8 x 8 x 2080
TODO(alemi): register intermediate endpoints
8 x 8 x 1536
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which"
can be set to False to disable pooling here (as in resnet_*()).
Set weight_decay for weights in conv2d and fully_connected layers.
Set activation_fn and parameters for batch_norm.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Decay for the moving averages.
epsilon to prevent 0s in variance.
collection containing update_ops.
use fused batch norm if possible.
Set weight_decay for weights in Conv and FC layers.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"inputs has shape [batch, 224, 224, 3]"
"inputs has shape [batch, 513, 513, 3]"
Use clip_by_value to simulate bandpass activation.
Convert end_points_collection into a dictionary of end_points.
Global average pooling.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"end_points will collect relevant activations for external use, for example"
summaries or losses.
299 x 299 x 3
149 x 149 x 32
147 x 147 x 32
147 x 147 x 64
73 x 73 x 64
73 x 73 x 80.
71 x 71 x 192.
35 x 35 x 192.
Inception blocks
mixed: 35 x 35 x 256.
mixed_1: 35 x 35 x 288.
mixed_2: 35 x 35 x 288.
mixed_3: 17 x 17 x 768.
mixed4: 17 x 17 x 768.
mixed_5: 17 x 17 x 768.
mixed_6: 17 x 17 x 768.
mixed_7: 17 x 17 x 768.
mixed_8: 8 x 8 x 1280.
mixed_9: 8 x 8 x 2048.
mixed_10: 8 x 8 x 2048.
Auxiliary Head logits
Shape of feature map before the final layer.
Final pooling and prediction
Global average pooling.
Pooling with a fixed kernel size.
1 x 1 x 2048
2048
1000
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Reflection pad by 1 in spatial dimensions (axes 1, 2 = h, w) to make a 3x3"
'valid' convolution produce an output with the same dimension as the
input.
This corrects 1 pixel offset for images with even width and height.
conv2d is left aligned and conv2d_transpose is right aligned for even
sized images (while doing 'SAME' padding).
Note: This doesn't reflect actual model in paper.
Neither dropout nor batch norm -> dont need is_training
##########
Encoder #
##########
7x7 input stage
##################
Residual Blocks #
##################
##########
Decoder #
##########
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"end_points will collect relevant activations for external use, for example"
summaries or losses.
Used to find thinned depths for each layer.
Note that sizes in the comments below assume an input spatial size of
"224x224, however, the inputs can be of any size greater 32x32."
224 x 224 x 3
depthwise_multiplier here is different from depth_multiplier.
depthwise_multiplier determines the output channels of the initial
"depthwise conv (see docs for tf.nn.separable_conv2d), while"
depth_multiplier controls the # channels of the subsequent 1x1
convolution. Must have
in_channels * depthwise_multipler <= out_channels
so that the separable convolution is not overparameterized.
Use a normal convolution instead of a separable convolution.
112 x 112 x 64
56 x 56 x 64
56 x 56 x 64
56 x 56 x 192
28 x 28 x 192
Inception module.
28 x 28 x 256
28 x 28 x 320
14 x 14 x 576
14 x 14 x 576
14 x 14 x 576
14 x 14 x 576
14 x 14 x 576
7 x 7 x 1024
7 x 7 x 1024
Final pooling and prediction
Global average pooling.
Pooling with a fixed kernel size.
1 x 1 x 1024
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
=============================================================================
Tensorflow mandates these.
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture
Conv defines 3x3 convolution layers
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.
stride is the stride of the convolution
depth is the number of channels or filters in a layer
MOBILENETV1_CONV_DEFS specifies the MobileNet body
Used to find thinned depths for each layer.
The current_stride variable keeps track of the output stride of the
"activations, i.e., the running product of convolution strides up to the"
current network layer. This allows us to invoke atrous convolution
whenever applying the next convolution would result in the activations
having output stride larger than the target output_stride.
The atrous convolution rate parameter.
"If we have reached the target output_stride, then we need to employ"
atrous convolution with stride=1 and multiply the atrous rate by the
current unit's stride for use in subsequent layers.
By passing filters=None
separable_conv2d produces only a depthwise convolution layer
Global average pooling.
Pooling with a fixed kernel size.
1 x 1 x 1024
Set weight_decay for weights in Conv and DepthSepConv layers.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Decay for the moving averages.
epsilon to prevent 0s in variance.
Turns off fused batch norm.
collection containing the moving mean and moving variance.
Final pooling and prediction
Temporal average pooling.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Orignaly, add_arg_scope = slim.add_arg_scope and layers = slim, now switch to"
more update-to-date tf.contrib.* API.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Make sure that round down does not go down by more than 10%.
"Note: want to round down, we adjust each split to match the total."
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts."
which provide numbered scopes.
b1 -> b2 * r -> b2
i -> (o * r) (bottleneck) -> o
"Note in contrast with expansion, we always have"
projection to produce the desired output size.
stride check enforces that we don't add residuals when spatial
dimensions are None
Depth matches
Don't do any splitting if we end up with less than 8 filters
on either side.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Make sure that round down does not go down by more than 10%.
Set conv defs defaults and overrides.
a) Set the tensorflow scope
b) set padding to default: note we might consider removing this
since it is also set by mobilenet_scope
c) set all defaults
d) set all extra overrides.
The current_stride variable keeps track of the output stride of the
"activations, i.e., the running product of convolution strides up to the"
current network layer. This allows us to invoke atrous convolution
whenever applying the next convolution would result in the activations
having output stride larger than the target output_stride.
The atrous convolution rate parameter.
Insert default parameters before the base scope which includes
any custom overrides set in mobilenet.
"If we have reached the target output_stride, then we need to employ"
atrous convolution with stride=1 and multiply the atrous rate by the
current unit's stride for use in subsequent layers.
Update params.
Only insert rate to params if rate > 1.
Set padding
Add all tensors that end with 'output' to
endpoints
1 x 1 x num_classes
Note: legacy scope name.
"Recover output shape, for unknown shape."
the network created will be trainble with dropout/batch norm
initialized appropriately.
Note: do not introduce parameters that would change the inference
"model here (for example whether to use bias), modify conv_def instead."
Set weight_decay for weights in Conv and FC layers.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
pyformat: disable
Architecture: https://arxiv.org/abs/1801.04381
Note: these parameters of batch norm affect the architecture
that's why they are here and not in training_scope.
pyformat: enable
NB: do not set depth_args unless they are provided to avoid overriding
whatever default depth_multiplier might have thanks to arg_scope.
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that
"'finegrain_classification_mode' is set to True, which means the embedding"
layer will not be shrinked when given a depth-multiplier < 1.0.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Notes for training NASNet Cifar Model
-------------------------------------
batch_size: 32
learning rate: 0.025
cosine (single period) learning rate decay
auxiliary head loss weighting: 0.4
clip global norm of all gradients by 5
600 epochs with a batch size of 32
This is used for the drop path probabilities since it needs to increase
the drop out probability over the course of training.
Notes for training large NASNet model on ImageNet
-------------------------------------
batch size (per replica): 16
learning rate: 0.015 * 100
learning rate decay factor: 0.97
num epochs per decay: 2.4
sync sgd with 100 replicas
auxiliary head loss weighting: 0.4
label smoothing: 0.1
clip global norm of all gradients by 10
Notes for training the mobile NASNet ImageNet model
-------------------------------------
batch size (per replica): 32
learning rate: 0.04 * 50
learning rate scaling factor: 0.97
num epochs per decay: 2.4
sync sgd with 50 replicas
auxiliary head weighting: 0.4
label smoothing: 0.1
clip global norm of all gradients by 10
Decay for the moving averages.
epsilon to prevent 0s in variance.
Decay for the moving averages.
epsilon to prevent 0s in variance.
Decay for the moving averages.
epsilon to prevent 0s in variance.
Shape of feature map before the final layer.
149 x 149 x 32
Run the reduction cells
Calculate the total number of cells in the network
Add 2 for the reduction cells
Calculate the total number of cells in the network
Add 2 for the reduction cells
"If ImageNet, then add an additional two for the stem cells"
Calculate the total number of cells in the network
Add 2 for the reduction cells
"If ImageNet, then add an additional two for the stem cells"
Find where to place the reduction cells or stride normal cells
Setup for building in the auxiliary head.
Run the cells
true_cell_num accounts for the stem cells
Final softmax layer
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Find where to place the reduction cells or stride normal cells
pylint: disable=protected-access
pylint: enable=protected-access
Setup for building in the auxiliary head.
Run the cells
true_cell_num accounts for the stem cells
pylint: disable=protected-access
pylint: enable=protected-access
Final softmax layer
pylint: disable=protected-access
pylint: enable=protected-access
Calculate the total number of cells in the network.
There is no distinction between reduction and normal cells in PNAS so the
total number of cells is equal to the number normal cells plus the number
of stem cells (two by default).
pylint: disable=protected-access
pylint: enable=protected-access
Calculate the total number of cells in the network.
There is no distinction between reduction and normal cells in PNAS so the
total number of cells is equal to the number normal cells plus the number
of stem cells (two by default).
Configuration for the PNASNet-5 model.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"The cap for tf.clip_by_value, it's hinted from the activation distribution"
"that the majority of activation values are in the range [-6, 6]."
Skip path 1
Skip path 2
"First pad with 0's on the right and bottom, then shift the filter to"
include those 0's that were added.
"If odd number of filters, add an additional one to the second path."
Concat and apply BN
Set the prev layer to the current layer if it is none
Check to be sure prev layer stuff is setup correctly
num_or_size_splits=1
Apply conv operations
Combine hidden states using 'add'.
Add hiddenstate to the list of hiddenstates we can choose from
Dont stride if this is not one of the original hiddenstates
"Check if a stride is needed, then use a strided 1x1 here"
Determine if a reduction should be applied to make the number of
filters match.
Return the concat of all the states
Scale keep prob by layer number
The added 2 is for the reduction cells
Decrease the keep probability over time
""
Script for generating a two-class dataset in COCO format for training an obscured image classifier
""
Requires Python >= 3.6 because of the glob ** expression
""
Collect images and labels
"Labels: clean = 0, obscured = 1"
""
top_folders_to_bottom.py
""
Given a base folder with files like:
""
A/1/2/a.jpg
B/3/4/b.jpg
""
"...moves the top-level folders to the bottom in a new output folder, i.e., creates:"
""
1/2/A/a.jpg
3/4/B/b.jpg
""
"In practice, this is used to make this:"
""
animal/camera01/image01.jpg
""
...look like:
""
camera01/animal/image01.jpg
""
%% Constants and imports
%% Support functions
%% Main functions
Find top-level folder
Find file/folder names
Move or copy
...def process_file()
Enumerate input folder
Convert absolute paths to relative paths
Standardize delimiters
Make sure each input file maps to a unique output file
relative_filename = relative_files[0]
Loop
...def top_folders_to_bottom()
%% Interactive driver
%%
%%
%% Command-line driver
"python top_folders_to_bottom.py ""g:\temp\separated_images"" ""g:\temp\separated_images_inverted"" --n_threads 100"
Convert to an options object
%% Constants and imports
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils)
Assumes the cameratraps repo root is on the path
%% Options
"Make sure there is no overlap between the two sets, because this will cause"
issues in the code
## Required inputs
## Options
Can be a folder or a SAS URL
# These apply only when we're doing ground-truth comparisons
Classes we'll treat as negative
""
"Include the token ""#NO_LABELS#"" to indicate that an image with no annotations"
should be considered empty.
Classes we'll treat as neither positive nor negative
"A list of output sets that we should count, but not render images for."
""
"Typically used to preview sets with lots of empties, where you don't want to"
"subset but also don't want to render 100,000 empty images."
""
"detections, non_detections"
"detections_animal, detections_person, detections_vehicle"
"By default, choose a confidence threshold based on the detector version"
Used for summary statistics only
"Number of images to sample, -1 for ""all images"""
"Random seed for sampling, or None"
Optionally separate detections into categories (animal/vehicle/human)
""
Currently only supported when ground truth is unavailable
Optionally replace one or more strings in filenames with other strings;
useful for taking a set of results generated for one folder structure
and applying them to a slightly different folder structure.
Allow bypassing API output loading when operating on previously-loaded
results
Should we also split out a separate report about the detections that were
just below our main confidence threshold?
""
Currently only supported when ground truth is unavailable
Control rendering parallelization
...PostProcessingOptions
#%% Helper classes and functions
Anything greater than this isn't clearly positive or negative
image has annotations suggesting both negative and positive
"image is not annotated or is annotated with 'unknown', 'unlabeled', ETC."
image has not yet been assigned a state
"In some analyses, we add an additional class that lets us look at"
detections just below our main confidence threshold
count the # of images with each type of DetectionStatus
Check whether this image has:
- unknown / unassigned-type labels
- negative-type labels
"- positive labels (i.e., labels that are neither unknown nor negative)"
"assert has_unknown_labels is False, '{} has unknown labels'.format(annotations)"
If there are no image annotations...
n_negative += 1
im['_detection_status'] = DetectionStatus.DS_NEGATIVE
"If the image has more than one type of labels, it's ambiguous"
"note: bools are automatically converted to 0/1, so we can sum"
"After the check above, we can be sure it's only one of positive,"
"negative, or unknown."
""
Important: do not merge the following 'unknown' branch with the first
"'unknown' branch above, where we tested 'if len(categories) == 0'"
""
If the image has only unknown labels
If the image has only negative labels
If the images has only positive labels
"Annotate the category, if it is unambiguous"
...for each image
...mark_detection_status()
"Leaving code in place for reading from blob storage, may support this"
in the future.
resize is to display them in this notebook or in the HTML more quickly
os.path.isfile() is slow when mounting remote directories; much faster
to just try/except on the image open.
return ''
Render images to a flat folder... we can use os.sep here because we've
already normalized paths
"Create class labels like ""gt_1"" or ""gt_27"""
"for i_box,box in enumerate(ground_truth_boxes):"
gt_classes.append('_' + str(box[-1]))
"errno.ENAMETOOLONG doesn't get thrown properly on Windows, so"
we awkwardly check against a hard-coded limit
Use slashes regardless of os
Optionally add links back to the original images
...render_bounding_boxes
Count items in each category
Optionally sort by filename before writing to html
Write the individual HTML files
...prepare_html_subpages()
Get unique categories above the threshold for this image
Render an image (with no ground truth information)
"This is a list of [class,confidence] pairs, sorted by confidence"
"If we either don't have a confidence threshold, or we've met our"
confidence threshold
...if this detection has classification info
...for each detection
...def render_image_no_gt()
This should already have been normalized to either '/' or '\'
...def render_image_with_gt()
%% Main function
#%% Expand some options for convenience
#%% Prepare output dir
#%% Load ground truth if available
Mark images in the ground truth as positive or negative
#%% Load detection (and possibly classification) results
"If the caller hasn't supplied results, load them"
Determine confidence thresholds if necessary
Remove failed rows
Convert keys and values to lowercase
"Add column 'pred_detection_label' to indicate predicted detection status,"
not separating out the classes
#%% Pull out descriptive metadata
This is rare; it only happens during debugging when the caller
is supplying already-loaded API results.
"#%% If we have ground truth, remove images we can't match to ground truth"
#%% Sample images for visualization
#%% Fork here depending on whether or not ground truth is available
"If we have ground truth, we'll compute precision/recall and sample tp/fp/tn/fn."
""
Otherwise we'll just visualize detections/non-detections.
#%% Detection evaluation: compute precision/recall
numpy array of detection probabilities
"numpy array of bools (0.0/1.0), and -1 as null value"
Don't include ambiguous/unknown ground truth in precision/recall analysis
"For completeness, include the result at a confidence threshold of 1.0"
Compute and print summary statistics
Thresholds go up throughout precisions/recalls/thresholds; find the last
value where recall is at or above target.  That's our precision @ target recall.
"np.where returns a tuple of arrays, but in this syntax where we're"
"comparing an array with a scalar, there will only be one element."
Convert back to a list
Flatten the confusion matrix
"#%% Collect classification results, if they exist"
Mapping of classnames to idx for the confusion matrix.
""
"The lambda is actually kind of a hack, because we use assume that"
the following code does not reassign classname_to_idx
Confusion matrix as defaultdict of defaultdict
""
"Rows / first index is ground truth, columns / second index is predicted category"
iDetection = 0; fn = detector_files[iDetection]; print(fn)
"If this image has classification predictions, and an unambiguous class"
"annotated, and is a positive image..."
"The unambiguous category, we make this a set for easier handling afterward"
"Compute the accuracy as intersection of union,"
i.e. (# of categories in both prediciton and GT)
divided by (# of categories in either prediction or GT
""
"In case of only one GT category, the result will be 1.0, if"
prediction is one category and this category matches GT
""
"It is 1.0/(# of predicted top-1 categories), if the GT is"
one of the predicted top-1 categories.
""
"It is 0.0, if none of the predicted categories is correct"
Distribute this accuracy across all predicted categories in the
confusion matrix
...for each file in the detection results
If we have classification results
Build confusion matrix as array from classifier_cm
Print some statistics
Prepare confusion matrix output
Get confusion matrix as string
Get fixed-size classname for each idx
Prepend class name on each line and add to the top
Print formatted confusion matrix
"Actually don't, this gets really messy in all but the widest consoles"
Plot confusion matrix
To manually add more space at bottom: plt.rcParams['figure.subplot.bottom'] = 0.1
""
"Add 0.5 to figsize for every class. For two classes, this will result in"
"fig = plt.figure(figsize=[4,4])"
...if we have classification results
#%% Render output
Write p/r table to .csv file in output directory
Write precision/recall plot to .png file in output directory
#%% Sampling
Sample true/false positives/negatives with correct/incorrect top-1
classification and render to html
Accumulate html image structs (in the format expected by write_html_image_lists)
"for each category, e.g. 'tp', 'fp', ..., 'class_bird', ..."
Add default entries by accessing them for the first time
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]"
"Each element will be a three-tuple with elements file,max_conf,detections"
"Assemble the information we need for rendering, so we can parallelize without"
dealing with Pandas
i_row = 0; row = images_to_visualize.iloc[0]
Filenames should already have been normalized to either '/' or '\'
Map all the rendering results in the list rendering_results into the
dictionary images_html
Prepare the individual html image files
Write index.html
Show links to each GT class
""
We could do this without classification results; currently we don't.
Add links to all available classes
Close body and html tags
...for each image
"#%% Otherwise, if we don't have ground truth..."
#%% Sample detections/non-detections
Accumulate html image structs (in the format expected by write_html_image_list)
for each category
Add default entries by accessing them for the first time
"Maps detection categories - e.g. ""human"" - to result set names, e.g."
"""detections_human"""
Add a set of results for each category and combination of categories
Create output directories
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]"
"list of 3-tuples with elements (file, max_conf, detections)"
"Assemble the information we need for rendering, so we can parallelize without"
dealing with Pandas
i_row = 0; row = images_to_visualize.iloc[0]
Filenames should already have been normalized to either '/' or '\'
"render_image_no_gt(file_info,detection_categories_to_results_name,"
"detection_categories,classification_categories)"
Map all the rendering results in the list rendering_results into the
dictionary images_html
Prepare the individual html image files
Write index.html
"We can't just sum these, because image_counts includes images in both their"
detection and classification classes
total_images = sum(image_counts.values())
Don't print classification classes here; we'll do that later with a slightly
different structure
Add links to all available classes
os.startfile(output_html_file)
...if we do/don't have ground truth
...process_batch_results
%% Interactive driver(s)
%%
options.unlabeled_classes = ['human']
os.start(ppresults.output_html_file)
%% Command-line driver
""
load_api_results.py
""
Loads the output of the batch processing API (json) into a pandas dataframe.
""
Also functions to group entries by seq_id.
""
Includes the deprecated functions that worked with the old CSV API output format.
""
%% Constants and imports
%% Functions for grouping by sequence_id
example
input 'file': 'SER/S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG'
output 'id': 'S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG'
%% Functions for loading the result as a Pandas DataFrame
Validate that this is really a detector output file
Fields in the API output json other than 'images'
Normalize paths to simplify comparisons later
"image['file'] = image['file'].replace('\\','/')"
Replace some path tokens to match local paths to original blob structure
"If this is a newer file that doesn't include maximum detection confidence values,"
"add them, because our unofficial internal dataframe format includes this."
Pack the json output into a Pandas DataFrame
Sanity-check that this is really a detector output file
Normalize paths to simplify comparisons later
De-serialize detections
Optionally replace some path tokens to match local paths to the original blob structure
string_to_replace = list(options.detector_output_filename_replacements.keys())[0]
"TODO: hit some silly issues with vectorized str() and escaped characters, vectorize"
this later.
""
"detection_results['image_path'].str.replace(string_to_replace,replacement_string)"
iRow = 0
%% Imports
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils)
%% Constants and support classes
We will confirm that this matches what we load from each file
Process-based parallelization isn't supported yet
%% Main function
"Warn the user if some ""detections"" might not get rendered"
#%% Validate inputs
#%% Load both result sets
assert results_a['detection_categories'] == default_detection_categories
assert results_b['detection_categories'] == default_detection_categories
#%% Make sure they represent the same set of images
#%% Find differences
"Each of these maps a filename to a two-element list (the image in set A, the image in set B)"
""
"Right now, we only handle a very simple notion of class transition, where the detection"
of maximum confidence changes class *and* both images have an above-threshold detection.
fn = filenames_a[0]
We shouldn't have gotten this far if error_on_non_matching_lists is set
det = im_a['detections'][0]
...for each filename
#%% Sample and plot differences
"Render two sets of results (i.e., a comparison) for a single"
image.
...def render_image_pair()
fn = image_filenames[0]
...def render_detection_comparisons()
"For each category, generate comparison images and the"
comparison HTML page.
""
category = 'common_detections'
Choose detection pairs we're going to render for this category
...for each category
#%% Write the top-level HTML file content
...def compare_batch_results()
%% Interactive driver
%% KRU
%% Command-line driver
# TODO
""
"Merge high-confidence detections from one results file into another file,"
when the target file does not detect anything on an image.
""
Does not currently attempt to merge every detection based on whether individual
detections are missing; only merges detections into images that would otherwise
be considered blank.
""
"If you want to literally merge two .json files, see combine_api_outputs.py."
""
%% Constants and imports
%% Structs
Don't bother merging into target images where the max detection is already
higher than this threshold
"If you want to merge only certain categories, specify one"
(but not both) of these.
%% Main function
im = output_data['images'][0]
"Determine whether we should be processing all categories, or just a subset"
of categories.
i_source_file = 0; source_file = source_files[i_source_file]
source_im = source_data['images'][0]
detection_category = list(detection_categories)[0]
"This is already a detection, no need to proceed looking for detections to"
transfer
Boxes are x/y/w/h
source_sizes = [det['bbox'][2]*det['bbox'][3] for det in source_detections_this_category_raw]
Only look at boxes below the size threshold
...for each detection category
"print('Adding {} detections to image {}'.format(len(detections_to_transfer),image_filename))"
Update the max_detection_conf field (if present)
...for each image
...for each source file
%% Test driver
%%
%% Command-line driver (TODO)
""
separate_detections_into_folders.py
""
## Overview
""
"Given a .json file with batch processing results, separate the files in that"
"set of results into folders that contain animals/people/vehicles/nothing,"
according to per-class thresholds.
""
"Image files are copied, not moved."
""
""
## Output structure
""
Preserves relative paths within each of those folders; cannot be used with .json
files that have absolute paths in them.
""
"For example, if your .json file has these images:"
""
a/b/c/1.jpg
a/b/d/2.jpg
a/b/e/3.jpg
a/b/f/4.jpg
a/x/y/5.jpg
""
And let's say:
""
"* The results say that the first three images are empty/person/vehicle, respectively"
"* The fourth image is above threshold for ""animal"" and ""person"""
* The fifth image contains an animal
""
* You specify an output base folder of c:\out
""
You will get the following files:
""
c:\out\empty\a\b\c\1.jpg
c:\out\people\a\b\d\2.jpg
c:\out\vehicles\a\b\e\3.jpg
c:\out\animal_person\a\b\f\4.jpg
c:\out\animals\a\x\y\5.jpg
""
## Rendering bounding boxes
""
"By default, images are just copied to the target output folder.  If you specify --render_boxes,"
bounding boxes will be rendered on the output images.  Because this is no longer strictly
"a copy operation, this may result in the loss of metadata.  More accurately, this *may*"
result in the loss of some EXIF metadata; this *will* result in the loss of IPTC/XMP metadata.
""
Rendering boxes also makes this script a lot slower.
""
## Classification-based separation
""
"If you have a results file with classification data, you can also specify classes to put"
"in their own folders, within the ""animals"" folder, like this:"
""
"--classification_thresholds ""deer=0.75,cow=0.75"""
""
"So, e.g., you might get:"
""
c:\out\animals\deer\a\x\y\5.jpg
""
"In this scenario, the folders within ""animals"" will be:"
""
"deer, cow, multiple, unclassified"
""
"""multiple"" in this case only means ""deer and cow""; if an image is classified as containing a"
"bird and a bear, that would end up in ""unclassified"", since the folder separation is based only"
on the categories you provide at the command line.
""
"No classification-based separation is done within the animal_person, animal_vehicle, or"
animal_person_vehicle folders.
""
%% Constants and imports
Occasionally we have near-zero confidence detections associated with COCO classes that
didn't quite get squeezed out of the model in training.  As long as they're near zero
"confidence, we just ignore them."
%% Options class
"Dictionary mapping categories (plus combinations of categories, and 'empty') to output folders"
Populated only when using classification results
"Originally specified as a string, converted to a dict mapping name:threshold"
...__init__()
...class SeparateDetectionsIntoFoldersOptions
%% Support functions
Find the maximum confidence for each category
""
det = detections[0]
"For zero-confidence detections, we occasionally have leftover goop"
from COCO classes
assert det['conf'] < invalid_category_epsilon
...for each detection on this image
Count the number of thresholds exceeded
...for each category
If this is above multiple thresholds
"TODO: handle species-based separation in, e.g., the animal_person case"
"Are we making species classification folders, and is this an animal?"
Do we need to put this into a specific species folder?
Find the animal-class detections that are above threshold
Count the number of classification categories that are above threshold for at
least one detection
d = valid_animal_detections[0]
classification = d['classifications'][0]
"Do we have a threshold for this category, and if so, is"
this classification above threshold?
...for each classification
...for each detection
...if we have to deal with classification subfolders
...if we have 0/1/more categories above threshold
...if this is/isn't a failure case
Skip this image if it's empty and we're not processing empty images
"At this point, this image is getting copied; we may or may not also need to"
draw bounding boxes.
Do a simple copy operation if we don't need to render any boxes
Open the source image
"Render bounding boxes for each category separately, beacuse"
we allow different thresholds for each category.
"When we're not using classification folders, remove classification"
information to maintain standard detection colors.
...for each category
Read EXIF metadata
"Write output with EXIF metadata if available, and quality='keep' if this is a JPEG"
"image.  Unfortunately, neither parameter likes ""None"", so we get a slightly"
icky cascade of if's here.
Also see:
""
https://discuss.dizzycoding.com/determining-jpg-quality-in-python-pil/
""
...for more ways to preserve jpeg quality if quality='keep' doesn't do the trick.
...if we don't/do need to render boxes
...def process_detections()
%% Main function
Create output folder if necessary
Load detection results
Map class names to output folders
Create all combinations of categories
category_name = category_names[0]
Do we have a custom threshold for this category?
Create folder mappings for each category
Create the actual folders
"Handle species classification thresholds, if specified"
"E.g. deer=0.75,cow=0.75"
token = tokens[0]
...for each token
...if classification thresholds are still in string format
Validate the classes in the threshold list
...if we need to deal with classification categories
i_image = 14; im = images[i_image]; im
...for each image
...def separate_detections_into_folders
%% Interactive driver
%%
%%
%%
%% Testing various command-line invocations
"With boxes, no classification"
"No boxes, no classification (default)"
"With boxes, with classification"
"No boxes, with classification"
%% Command-line driver
"python api\batch_processing\postprocessing\separate_detections_into_folders.py ""d:\temp\rspb_mini.json"" ""d:\temp\demo_images\rspb_2018_2019_mini"" ""d:\temp\separation_test"" --nthreads 2"
Convert to an options object
"print('{} {}'.format(v,name))"
List of category numbers to use in separation; uses all categories if None
"Can be ""size"", ""width"", or ""height"""
For each image...
""
im = images[0]
d = im['detections'][0]
Are there really any detections here?
Is this a category we're supposed to process?
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing#detector-outputs
...for each detection
...for each image
...def categorize_detections_by_size()
""
add_max_conf.py
""
"The MD output format included a ""max_detection_conf"" field with each image"
up to and including version 1.2; it was removed as of version 1.3 (it's
redundant with the individual detection confidence values).
""
"Just in case someone took a dependency on that field, this script allows you"
to add it back to an existing .json file.
""
%% Imports and constants
%% Main function
%% Driver
#######
""
remove_repeat_detections.py
""
"Used after running find_repeat_detections, then manually filtering the results,"
to create a final filtered output file.
""
"If you want to use this script, we recommend that you read the user's guide:"
""
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms
""
#######
%% Constants and imports
%% Main function
%% Interactive driver
%%
%% Command-line driver
#######
""
repeat_detections_core.py
""
Core utilities shared by find_repeat_detections and remove_repeat_detections.
""
#######
%% Imports and environment
"from ai4eutils; this is assumed to be on the path, as per repo convention"
"""PIL cannot read EXIF metainfo for the images"""
"""Metadata Warning, tag 256 had too many entries: 42, expected 1"""
%% Constants
%% Classes
Relevant for rendering the folder of images for filtering
""
"imageBase can also be a SAS URL, in which case some error-checking is"
disabled.
Don't consider detections with confidence lower than this as suspicious
Don't consider detections with confidence higher than this as suspicious
What's the IOU threshold for considering two boxes the same?
How many occurrences of a single location (as defined by the IOU threshold)
are required before we declare it suspicious?
"Ignore ""suspicious"" detections larger than some size; these are often animals"
taking up the whole image.  This is expressed as a fraction of the image size.
"Ignore ""suspicious"" detections smaller than some size"
Ignore folders with more than this many images in them
A list of classes we don't want to treat as suspicious. Each element is an int.
Load detections from a filter file rather than finding them from the detector output
".json file containing detections, should be called detectionIndex.json in the filtering_* folder"
produced in the first pass
(optional) List of filenames remaining after deletion of identified
repeated detections that are actually animals.  This should be a flat
"text file, one relative filename per line.  See enumerate_images()."
Turn on/off optional outputs
Determines whether bounding-box rendering errors (typically network errors) should
be treated as failures
Box rendering options
State variables
"Replace filename tokens after reading, useful when the directory structure"
has changed relative to the structure the detector saw
How many folders up from the leaf nodes should we be going to aggregate images?
An optional function that takes a string (an image file name) and returns
"a string (the corresponding  folder ID), typically used when multiple folders"
actually correspond to the same camera in a manufacturer-specific way (e.g.
a/b/c/RECONYX100 and a/b/c/RECONYX101 may really be the same camera).
Include/exclude specific folders... only one of these may be
"specified; ""including"" folders includes *only* those folders."
"Optionally show *other* detections (i.e., detections other than the"
one the user is evaluating) in a light gray
"If bRenderOtherDetections is True, what color should we use to render the"
(hopefully pretty subtle) non-target detections?
""
"In theory I'd like these ""other detection"" rectangles to be partially"
"transparent, but this is not straightforward, and the alpha is ignored"
"here.  But maybe if I leave it here and wish hard enough, someday it"
will work.
""
otherDetectionsColors = ['dimgray']
Sort detections within a directory so nearby detections are adjacent
"in the list, for faster review."
""
"Can be None, 'xsort', or 'clustersort'"
""
* None sorts detections chronologically by first occurrence
* 'xsort' sorts detections from left to right
* 'clustersort' clusters detections and sorts by cluster
Only relevant if smartSort == 'clustersort'
"The data table (Pandas DataFrame), as loaded from the input json file via"
load_api_results()
"The other fields in the input json file, loaded via load_api_results()"
The data table after modification
dict mapping folder names to whole rows from the data table
dict mapping filenames to rows in the master table
"An array of length nDirs, where each element is a list of DetectionLocation"
objects for that directory that have been flagged as suspicious
"This is a bit of a hack right now, but for future-proofing, I don't want to call this"
"to retrieve anything other than the highest-confidence detection, and I'm assuming this"
"is already sorted, so assert() that."
It's not clear whether it's better to use instances[0].bbox or self.bbox
"here... they should be very similar, unless iouThreshold is very low."
self.bbox is a better representation of the overal DetectionLocation.
%% Helper functions
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t"
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t"
%% Sort a list of candidate detections to make them visually easier to review
Just sort by the X location of each box
"Prepare a list of points to represent each box,"
that's what we'll use for clustering
Upper-left
"points.append([det.bbox[0],det.bbox[1]])"
Center
"Labels *could* be any unique labels according to the docs, but in practice"
they are unique integers from 0:nClusters
Make sure the labels are unique incrementing integers
Store the label assigned to each cluster
"Now sort the clusters by their x coordinate, and re-assign labels"
so the labels are sortable
"Compute the centroid for debugging, but we're only going to use the x"
"coordinate.  This is the centroid of points used to represent detections,"
which may be box centers or box corners.
old_cluster_label_to_new_cluster_label[old_cluster_label] =\
new_cluster_labels[old_cluster_label]
%% Look for matches (one directory)
List of DetectionLocations
candidateDetections = []
Create a tree to store candidate detections
For each image in this directory
""
iDirectoryRow = 0; row = rows.iloc[iDirectoryRow]
""
"iDirectoryRow is a pandas index, so it may not start from zero;"
"for debugging, we maintain i_iteration as a loop index."
print('Searching row {} of {} (index {}) in dir {}'.\
"format(i_iteration,len(rows),iDirectoryRow,dirName))"
Don't bother checking images with no detections above threshold
"Array of dicts, where each element is"
{
"'category': '1',  # str value, category ID"
"'conf': 0.926,  # confidence of this detections"
""
"# (x_min, y_min) is upper-left, all in relative coordinates"
"'bbox': [x_min, y_min, width_of_box, height_of_box]"
""
}
For each detection in this image
"This is no longer strictly true; I sometimes run RDE in stages, so"
some probabilities have already been made negative
""
assert confidence >= 0.0 and confidence <= 1.0
Optionally exclude some classes from consideration as suspicious
Is this detection too big to be suspicious?
print('Illegal zero-size bounding box on image {}'.format(filename))
These are relative coordinates
print('Ignoring very small detection with area {}'.format(area))
print('Ignoring very large detection with area {}'.format(area))
This will return candidates of all classes
For each detection in our candidate list
Don't match across categories
Is this a match?
"If so, add this example to the list for this detection"
We *don't* break here; we allow this instance to possibly
match multiple candidates.  There isn't an obvious right or
wrong here.
...for each detection on our candidate list
"If we found no matches, add this to the candidate list"
candidateDetections.append(candidate)
pyqtree
...for each detection
...for each row
Get all candidate detections
print('Found {} candidate detections for folder {}'.format(
"len(candidateDetections),dirName))"
"For debugging only, it's convenient to have these sorted"
as if they had never gone into a tree structure.  Typically
this is in practce a sort by filename.
...def find_matches_in_directory(dirName)
"%% Update the detection table based on suspicious results, write .csv output"
"An array of length nDirs, where each element is a list of DetectionLocation"
objects for that directory that have been flagged as suspicious
For each directory
For each suspicious detection group in this directory
For each instance of this suspicious detection
This should match the bbox for the detection event
The bbox for this instance should be almost the same as the bbox
"for this detection group, where ""almost"" is defined by the IOU"
threshold.
if iou < options.iouThreshold:
"print('IOU warning: {},{}'.format(iou,options.iouThreshold))"
Make sure the bounding box matches
"Make the probability negative, if it hasn't been switched by"
another bounding box
...for each instance
...for each detection
...for each directory
Update maximum probabilities
For each row...
No longer strictly true; sometimes I run RDE on RDE output
assert maxPOriginal >= 0
We should only be making detections *less* likely in this process
"If there was a meaningful change, count it"
"Negative probabilities should be the only reason maxP changed, so"
we should have found at least one negative value if we reached
this point.
...if there was a meaningful change to the max probability for this row
...for each row
If we're also writing output...
"...def update_detection_table(RepeatDetectionResults,options)"
%% Main function
#%% Input handling
Validate some options
Load the filtering file
Load the same options we used when finding repeat detections
...except for things that explicitly tell this function not to
find repeat detections.
...if we're loading from an existing filtering file
Check early to avoid problems with the output folder
"Load file to a pandas dataframe.  Also populates 'max_detection_conf', even if it's"
not present in the .json file.
detectionResults[detectionResults['failure'].notna()]
"Before doing any real work, make sure we can *probably* access images"
"This is just a cursory check on the first image, but it heads off most"
"problems related to incorrect mount points, etc.  Better to do this before"
spending 20 minutes finding repeat detections.
#%% Separate files into directories
This will be a map from a directory name to smaller data frames
This is a mapping back into the rows of the original table
iRow = 0; row = detectionResults.iloc[0]
Create a new DataFrame with just this row
rowsByDirectory[dirName] = pd.DataFrame(row)
...for each unique detection
Convert lists of rows to proper DataFrames
#% Look for matches (or load them from file)
"Are we actually looking for matches, or just loading from a file?"
length-nDirs list of lists of DetectionLocation objects
We're actually looking for matches...
"We get slightly nicer progress bar behavior using threads, by passing a pbar"
object and letting it get updated.  We can't serialize this object across
processes.
#%% Find suspicious locations based on match results
For each directory
""
iDir = 51
A list of DetectionLocation objects
A list of DetectionLocation objects
occurrenceList is a list of file/detection pairs
Sort the above-threshold detections for easier review
...for each directory
If we're just loading detections from a file...
"We're skipping detection-finding, but to see which images are actually legit false"
"positives, we may be looking for physical files or loading from a text file."
For each directory
iDir = 0; detections = suspiciousDetections[0]
""
"suspiciousDetections is an array of DetectionLocation objects,"
one per directory.
For each detection that was present before filtering
iDetection = 0; detection = detections[iDetection]
Are we checking the directory to see whether detections were actually false
"positives, or reading from a list?"
Is the image still there?
"If not, remove this from the list of suspicious detections"
...for each detection
...for each directory
...if we are/aren't finding detections (vs. loading from file)
Create filtering directory
iDir = 0; suspiciousDetectionsThisDir = suspiciousDetections[iDir]
suspiciousDetectionsThisDir is a list of DetectionLocation objects
iDetection = 0; detection = suspiciousDetectionsThisDir[0]
Sort instances in descending order by confidence
Choose the highest-confidence index
Should we render (typically in a very light color) detections
*other* than the one we're highlighting here?
Render other detections first (typically in a thin+light box)
Now render the example detection (on top of at least one
of the other detections)
This converts the *first* instance to an API standard detection;
"because we just sorted this list in descending order by confidence,"
this is the highest-confidence detection.
...if we are/aren't rendering other bounding boxes
...for each detection in this folder
...for each folder
Write out the detection index
...if we're writing filtering info
...find_repeat_detections()
#######
""
find_repeat_detections.py
""
"If you want to use this script, we recommend that you read the user's guide:"
""
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms
""
"Really, don't try to run this script without reading the user's guide, you'll think"
it's more magical than it is.
""
"This script looks through a sequence of detections in the API output json file, and finds"
"candidates that might be ""repeated false positives"", i.e. that random branch that the"
detector thinks is an animal/person/vehicle.
""
"Typically after running this script, you would do a manual step to remove"
"true positives, then run remove_repeat_detections to produce a final output file."
""
There's no way that statement was self-explanatory; see the user's guide.
""
#######
%% Constants and imports
%% Interactive driver
%%
%% Command-line driver
Convert to an options object
""
If a request has been sent to AML for batch scoring but the monitoring thread of the API was
"interrupted (uncaught exception or having to re-start the API container), we could manually"
"aggregate results from each shard using this script, assuming all jobs submitted to AML have finished."
""
Need to have set environment variables STORAGE_ACCOUNT_NAME and STORAGE_ACCOUNT_KEY to those of the
"storage account backing the API. Also need to adjust the INTERNAL_CONTAINER, AML_CONTAINER and"
AML_CONFIG fields in api_core/orchestrator_api/api_config.py to match the instance of the API that this
request was submitted to.
""
May need to change the import statement in api_core/orchestrator_api/orchestrator.py
"""from sas_blob_utils import SasBlob"" to"
"""from .sas_blob_utils import SasBlob"" to not confuse with the module in AI4Eutils;"
"and change ""import api_config"" to"
"""from api.batch_processing.api_core.orchestrator_api import api_config"""
Execute this script from the root of the repository. You may need to add the repository to PYTHONPATH.
"list_jobs_submitted cannot be serialized (""can't pickle _thread.RLock objects ""), but"
do not need it for aggregating results
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%% helper classes and functions
TODO log exception when we have more telemetry
TODO check that the expiry date of input_container_sas is at least a month
into the future
"if no permission specified explicitly but has an access policy, assumes okay"
TODO - check based on access policy as well
return current UTC time as a string in the ISO 8601 format (so we can query by
timestamp in the Cosmos DB job status table.
example: '2021-02-08T20:02:05.699689Z'
"image_paths will have length at least 1, otherwise would have ended before this step"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
a job moves from created to running/problem after the Batch Job has been submitted
"job_id should be unique across all instances, and is also the partition key"
TODO do not read the entry first to get the call_params when the Cosmos SDK add a
patching functionality:
https://feedback.azure.com/forums/263030-azure-cosmos-db/suggestions/6693091-be-able-to-do-partial-updates-on-document
need to retain other fields in 'status' to be able to restart monitoring thread
retain existing fields; update as needed
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
sentinel should change if new configurations are available
configs have not changed
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Gunicorn logger handler will get attached if needed in server.py
set for all tasks in the job
cannot execute the scoring script that is in the mounted directory; has to be copied to cwd
not luck giving the commandline arguments via formatted string - set as env vars instead
form shards of images and assign each shard to a Task
for persisting stdout and stderr
persist stdout and stderr (will be removed when node removed)
paths are relative to the Task working directory
can also just upload on failure
first try submitting Tasks
retry submitting Tasks
Change the Job's on_all_tasks_complete option to 'terminateJob' so the Job's status changes automatically
after all submitted tasks are done
This is so that we do not take up the quota for active Jobs in the Batch account.
return type: TaskAddCollectionResult
actually we should probably only re-submit if it's a server_error
docs: # https://docs.microsoft.com/en-us/rest/api/batchservice/odata-filters-in-batch#list-tasks
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%% Flask app
reference: https://trstringer.com/logging-flask-gunicorn-the-manageable-way/
%% Helper classes
%% Flask endpoints
required params
can be an URL to a file not hosted in an Azure blob storage container
"if use_url, then images_requested_json_sas is required"
optional params
check model_version is among the available model versions
check request_name has only allowed characters
optional params for telemetry collection - logged to status table for now as part of call_params
All API instances / node pools share a quota on total number of active Jobs;
we cannot accept new Job submissions if we are at the quota
required fields
request_status is either completed or failed
the create_batch_job thread will stop when it wakes up the next time
"Fix for Zooniverse - deleting any ""-"" characters in the job_id"
"If the status is running, it could be a Job submitted before the last restart of this"
"API instance. If that is the case, we should start to monitor its progress again."
WARNING model_version could be wrong (a newer version number gets written to the output file) around
"the time that  the model is updated, if this request was submitted before the model update"
and the API restart; this should be quite rare
conform to previous schemes
%% undocumented endpoints
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Gunicorn logger handler will get attached if needed in server.py
request_name and request_submission_timestamp are for appending to
output file names
image_paths can be a list of strings (Azure blob names or public URLs)
"or a list of length-2 lists where each is a [image_id, metadata] pair"
Case 1: listing all images in the container
- not possible to have attached metadata if listing images in a blob
list all images to process
+ 1 so if the number of images listed > MAX_NUMBER_IMAGES_ACCEPTED_PER_JOB
we will know and not proceed
Case 2: user supplied a list of images to process; can include metadata
filter down to those conforming to the provided prefix and accepted suffixes (image file types)
prefix is case-sensitive; suffix is not
"Although urlparse(p).path preserves the extension on local paths, it will not work for"
"blob file names that contains ""#"", which will be treated as indication of a query."
"If the URL is generated via Azure Blob Storage, the ""#"" char will be properly encoded"
apply the first_n and sample_n filters
OK if first_n > total number of images
sample by shuffling image paths and take the first sample_n images
"upload the image list to the container, which is also mounted on all nodes"
all sharding and scoring use the uploaded list
now request_status moves from created to running
an extra field to allow the monitoring thread to restart after an API restart: total number of tasks
also record the number of images to process for reporting
start the monitor thread with the same name
"both succeeded and failed tasks are marked ""completed"" on Batch"
"preserving format from before, but SAS URL to 'failed_images' and 'images' are no longer provided"
"failures should be contained in the output entries, indicated by an 'error' field"
"when people download this, the timestamp will have : replaced by _"
check if the result blob has already been written (could be another instance of the API / worker thread)
"and if so, skip aggregating and uploading the results, and just generate the SAS URL, which"
could be needed still if the previous request_status was `problem`.
upload the output JSON to the Job folder
%% Helper functions *copied* from ct_utils.py and visualization/visualization_utils.py
"Determine the factor, which shifts the decimal point of x"
just behind the last significant digit
"Shift decimal point by multiplicatipon with factor, flooring, and"
division by factor
PIL.Image.convert() returns a converted copy of this image
alter orientation as needed according to EXIF tag 0x112 (274) for Orientation
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28
https://www.media.mit.edu/pia/Research/deepview/exif.html
"%% TFDetector class, an unmodified *copy* of the class in detection/tf_detector.py,"
so we do not have to import the packages required by run_detector.py
Number of decimal places to round to for confidence and bbox coordinates
"MegaDetector was trained with batch size of 1, and the resizing function is a part"
of the inference graph
An enumeration of failure reasons
"change from [y1, x1, y2, x2] to [x1, y1, width, height]"
convert numpy floats to Python floats
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size
"np_images = [np.asarray(image, np.uint8) for image in images]"
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)"
performs inference
our batch size is 1; need to loop the batch dim if supporting batch size > 1
%% Scoring script
determine if there is metadata attached to each image_id
information to determine input and output locations
other parameters for the task
test that we can write to output path; also in case there is no image to process
list images to process
"items in this list can be strings or [image_id, metadata]"
model path
"Path to .pb TensorFlow detector model file, relative to the"
models/megadetector_copies folder in mounted container
score the images
""
manage_video_batch.py
""
Notebook-esque script to manage the process of running a local batch of videos
through MD.  Defers most of the heavy lifting to manage_local_batch.py .
""
%% Imports and constants
%% Split videos into frames
"%% List frame files, break into folders"
Find unique (relative) folders
fn = frame_files[0]
%% List videos
%% Check for videos that are missing entirely
list(folder_to_frame_files.keys())[0]
video_filenames[0]
fn = video_filenames[0]
%% Check for videos with very few frames
%% Print the list of videos that are problematic
%% Process images like we would for any other camera trap job
"...typically using manage_local_batch.py, but do this however you like, as long"
as you get a results file at the end.
""
"If you do RDE, remember to use the second folder from the bottom, rather than the"
bottom-most folder.
%% Convert frame results to video results
%% Confirm that the videos in the .json file are what we expect them to be
%% Scrap
%% Test a possibly-broken video
%% List videos in a folder
%% Imports
%% Constants
%% Classes
class variables
"instance variables, in order of when they are typically set"
Leaving this commented out to remind us that we don't want this check here; let
the API fail on these images.  It's a huge hassle to remove non-image
files.
""
for path_or_url in images_list:
if not is_image_file_or_url(path_or_url):
raise ValueError('{} is not an image'.format(path_or_url))
Commented out as a reminder: don't check task status (which is a rest API call)
in __repr__; require the caller to explicitly request status
"status=getattr(self, 'status', None))"
estimate # of failed images from failed shards
Download all three JSON urls to memory
Remove files that were submitted but don't appear to be images
assert all(is_image_file_or_url(s) for s in submitted_images)
Diff submitted and processed images
Confirm that the procesed images are a subset of the submitted images
https://www.geeksforgeeks.org/break-list-chunks-size-n-python/
%% Interactive driver
%%
%%
%%
%% Imports and constants
%% Constants I set per script
## Required
base_output_folder_name = os.path.expanduser('~/postprocessing/' + organization_name_short)
Shared Access Signature (SAS) tokens for the Azure Blob Storage container.
Leading question mark is optional.
""
The read-only token is used for accessing images; the write-enabled token is
used for writing file lists.
## Typically left as default
"Pre-pended to all folder names/prefixes, if they're defined below"
"This is how we break the container up into multiple taskgroups, e.g., for"
separate surveys. The typical case is to do the whole container as a single
taskgroup.
"If your ""folders"" are really logical folders corresponding to multiple folders,"
map them here
"A list of .json files to load images from, instead of enumerating.  Formatted as a"
"dictionary, like folder_prefixes."
This is only necessary if you will be performing postprocessing steps that
"don't yet support SAS URLs, specifically the ""subsetting"" step, or in some"
cases the splitting of files into multiple output directories for
empty/animal/vehicle/people.
""
"For those applications, you will need to mount the container to a local drive."
For this case I recommend using rclone whether you are on Windows or Linux;
rclone is much easier than blobfuse for transient mounting.
""
"But most of the time, you can ignore this."
"Supported model_versions: '4', '3', '4_prelim'"
""
Also available at the /supported_model_versions and /default_model_version
endpoints
""
"Unless you have any specific reason to set this to a non-default value, leave"
"it at the default, which as of 2020.04.28 is MegaDetector 4.1"
""
"additional_task_args = {""model_version"":""4_prelim""}"
""
"file_lists_by_folder will contain a list of local JSON file names,"
each JSON file contains a list of blob names corresponding to an API taskgroup
"%% Derived variables, path setup"
local folders
Turn warnings into errors if more than this many images are missing
%% Support functions
"scheme, netloc, path, query, fragment"
%% Read images from lists or enumerate blobs to files
folder_name = folder_names[0]
"Load file lists for this ""folder"""
""
file_list = input_file_lists[folder][0]
Write to file
A flat list of blob paths for each folder
folder_name = folder_names[0]
"If we don't/do have multiple prefixes to enumerate for this ""folder"""
"If this is intended to be a folder, it needs to end in '/', otherwise"
files that start with the same string will match too
...for each prefix
Write to file
...for each folder
%% Some just-to-be-safe double-checking around enumeration
Make sure each folder has at least one image matched; the opposite is usually a sign of a copy/paste issue
...for each image
...for each prefix
...for each folder
Make sure each image comes from one of our folders; the opposite is usually a sign of a bug up above
...for each prefix
...for each folder
...for each image
%% Divide images into chunks for each folder
The JSON file at folder_chunks[i][j] corresponds to task j of taskgroup i
list_file = file_lists_by_folder[0]
"%% Create taskgroups and tasks, and upload image lists to blob storage"
periods not allowed in task names
%% Generate API calls for each task
clipboard.copy(request_strings[0])
clipboard.copy('\n\n'.join(request_strings))
%% Run the tasks (don't run this cell unless you are absolutely sure!)
I really want to make sure I'm sure...
%% Estimate total time
Around 0.8s/image on 16 GPUs
%% Manually create task groups if we ran the tasks manually
%%
"%% Write task information out to disk, in case we need to resume"
%% Status check
print(task.id)
%% Resume jobs if this notebook closes
%% For multiple tasks (use this only when we're merging with another job)
%% For just the one task
%% Load into separate taskgroups
p = task_cache_paths[0]
%% Typically merge everything into one taskgroup
"%% Look for failed shards or missing images, start new tasks if necessary"
List of lists of paths
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup];
"Make a copy, because we append to taskgroup"
i_task = 0; task = tasks[i_task]
Each taskgroup corresponds to one of our folders
Check that we have (almost) all the images
Now look for failed images
Write it out as a flat list as well (without explanation of failures)
...for each task
...for each task group
%% Pull results
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup]; task = taskgroup[0]
Each taskgroup corresponds to one of our folders
...for each task
...for each task group
%% Combine results from task groups into final output files
Check that we have (almost) all the images
The only reason we should ever have a repeated request is the case where an
"image was missing and we reprocessed it, or where it failed and later succeeded"
"There may be non-image files in the request list, ignore those"
Something has gone bonkers if there are images in the results that
aren't in the request
...for each folder
%% Post-processing (no ground truth)
i_folder = 0; folder_name_raw = folder_names[i_folder]
"Omit some pages from the output, useful when animals are rare"
%% Manual processing follows
""
"Everything after this should be considered mostly manual, and no longer includes"
looping over folders.
""
%% RDE (sample directory collapsing)
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images"
"%% Repeat detection elimination, phase 1"
"Deliberately leaving these imports here, rather than at the top, because this cell is not"
typically executed
To invoke custom collapsing of folders for a particular manufacturer's naming scheme
options.customDirNameFunction = remove_overflow_folders
Exclude people and vehicles from RDE
"options.excludeClasses = [2,3]"
%% Manual RDE step
# DELETE THE VALID DETECTIONS ##
%% Re-filtering
%% Post-processing (post-RDE)
i_folder = 0; folder_name_raw = folder_names[i_folder]
"Omit some pages from the output, useful when animals are rare"
"print('No RDE file available for {}, skipping'.format(folder_name))"
continue
%% Create a new category for large boxes
%% Subsetting
i_folder = 0; folder_name = folders[i_folder]
%% String replacement
%% Folder splitting
i_folder = 0; folder_name_raw = folder_names[i_folder]
api_output_file = folder_name_to_combined_output_file[folder_name]
%% Imports and constants
from ai4eutils
To specify a non-default confidence threshold for including detections in the .json file
Turn warnings into errors if more than this many images are missing
Only relevant when we're using a single GPU
"Specify a target image size when running MD... strongly recommended to leave this at ""None"""
Only relevant when running on CPU
OS-specific script line continuation character
OS-specific script comment character
"Prefer threads on Windows, processes on Linux"
"This is for things like image rendering, not for MegaDetector"
Should we use YOLOv5's val.py instead of run_detector_batch.py?
Directory in which to run val.py.  Only relevant if use_yolo_inference_scripts is True.
Should we remove intermediate files used for running YOLOv5's val.py?
""
Only relevant if use_yolo_inference_scripts is True.
Should we apply YOLOv5's augmentation?  Only allowed when use_yolo_inference_scripts
is True.
%% Constants I set per script
Optional descriptor
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v5.0.0/md_v5b.0.0.pt')
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v4.1.0/md_v4.1.0.pb')
"Number of jobs to split data into, typically equal to the number of available GPUs"
Only used to print out a time estimate
"%% Derived variables, constant validation, path setup"
%% Enumerate files
%% Load files from prior enumeration
%% Divide images into chunks
%% Estimate total time
%% Write file lists
%% Generate commands
"A list of the scripts tied to each GPU, as absolute paths.  We'll write this out at"
the end so each GPU's list of commands can be run at once.  Generally only used when
"running lots of small batches via YOLOv5's val.py, which doesn't support checkpointing."
i_task = 0; task = task_info[i_task]
Generate the script to run MD
Check whether this output file exists
Generate the script to resume from the checkpoint (only supported with MD inference code)
...for each task
Write out a script for each GPU that runs all of the commands associated with
that GPU.  Typically only used when running lots of little scripts in lieu
of checkpointing.
...for each GPU
%% Run the tasks
%%% Run the tasks (commented out)
i_task = 0; task = task_info[i_task]
"This will write absolute paths to the file, we'll fix this later"
...for each chunk
...if False
"%% Load results, look for failed or missing images in each task"
i_task = 0; task = task_info[i_task]
im = task_results['images'][0]
...for each task
%% Merge results files and make images relative
im = combined_results['images'][0]
%% Post-processing (pre-RDE)
options.sample_seed = 0
"Omit some pages from the output, useful when animals are rare"
%% RDE (sample directory collapsing)
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images"
%%
%%
%%
relativePath = image_filenames[0]
"%% Repeat detection elimination, phase 1"
"Deliberately leaving these imports here, rather than at the top, because this"
cell is not typically executed
options.minSuspiciousDetectionSize = 0.05
This will cause a very light gray box to get drawn around all the detections
we're *not* considering as suspicious.
options.lineThickness = 5
options.boxExpansion = 8
To invoke custom collapsing of folders for a particular manufacturer's naming scheme
options.customDirNameFunction = remove_overflow_folders
Exclude people and vehicles from RDE
"options.excludeClasses = [2,3]"
options.maxImagesPerFolder = 50000
options.includeFolders = ['a/b/c']
options.excludeFolder = ['a/b/c']
"Can be None, 'xsort', or 'clustersort'"
import clipboard; clipboard.copy(os.path.dirname(suspiciousDetectionResults.filterFile))
path_utils.open_file(os.path.dirname(suspiciousDetectionResults.filterFile))
%% Manual RDE step
# DELETE THE VALID DETECTIONS ##
%% Re-filtering
%% Post-processing (post-RDE)
options.sample_seed = 0
"Omit some pages from the output, useful when animals are rare"
"%% Run MegaClassifier (actually, write out a script that runs MegaClassifier)"
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
#%% Set up environment
commands.append('cd CameraTraps/classification\n')
commands.append('conda activate cameratraps-classifier\n')
#%% Crop images
fn = input_files[0]
#%% Run classifier
fn = input_files[0]
#%% Remap classifier outputs
fn = input_files[0]
#%% Merge classification and detection outputs
fn = input_files[0]
#%% Write  out classification script
"%% Run a non-MegaClassifier classifier (i.e., a classifier with no output mapping)"
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
#%% Set up environment
#%% Crop images
fn = input_files[0]
#%% Run classifier
fn = input_files[0]
#%% Merge classification and detection outputs
fn = input_files[0]
#%% Write everything out
%% Run the classifier(s) via the .sh script(s) or batch file(s) we just wrote
...
%% Within-image classification smoothing
""
Only count detections with a classification confidence threshold above
"*classification_confidence_threshold*, which in practice means we're only"
looking at one category per detection.
""
If an image has at least *min_detections_above_threshold* such detections
"in the most common category, and no more than *max_detections_secondary_class*"
"in the second-most-common category, flip all detections to the most common"
category.
""
"Optionally treat some classes as particularly unreliable, typically used to overwrite an"
"""other"" class."
""
This cell also removes everything but the non-dominant classification for each detection.
""
How many detections do we need above the classification threshold to determine a dominant category
for an image?
"Even if we have a dominant class, if a non-dominant class has at least this many classifications"
"in an image, leave them alone."
"If the dominant class has at least this many classifications, overwrite ""other"" classifications"
What confidence threshold should we use for assessing the dominant category in an image?
Which classifications should we even bother over-writing?
Detection confidence threshold for things we count when determining a dominant class
Which detections should we even bother over-writing?
"Before we do anything else, get rid of everything but the top classification"
for each detection.
...for each detection in this image
...for each image
im = d['images'][0]
...for each classification
...if there are classifications for this detection
...for each detection
"Handle a quirky special case: if the most common category is ""other"" and"
"it's ""tied"" with the second-most-common category, swap them"
secondary_count = category_to_count[keys[1]]
The 'secondary count' is the most common non-other class
If we have at least *min_detections_to_overwrite_other* in a category that isn't
"""other"", change all ""other"" classifications to that category"
...for each classification
...if there are classifications for this detection
...for each detection
"...if we should overwrite all ""other"" classifications"
"At this point, we know we have a dominant category; change all other above-threshold"
"classifications to that category.  That category may have been ""other"", in which"
case we may have already made the relevant changes.
det = detections[0]
...for each classification
...if there are classifications for this detection
...for each detection
...for each image
...for each file we want to smooth
"%% Post-processing (post-classification, post-within-image-smoothing)"
classification_detection_file = classification_detection_files[1]
%% Read EXIF data from all images
%% Prepare COCO-camera-traps-compatible image objects for EXIF results
import dateutil
"This is a standard format for EXIF datetime, and dateutil.parser"
doesn't handle it correctly.
return dateutil.parser.parse(s)
exif_result = exif_results[0]
Currently we assume that each leaf-node folder is a location
"We collected this image this century, but not today, make sure the parsed datetime"
jives with that.
""
The latter check is to make sure we don't repeat a particular pathological approach
"to datetime parsing, where dateutil parses time correctly, but swaps in the current"
date when it's not sure where the date is.
...for each exif image result
%% Assemble into sequences
Make a list of images appearing at each location
im = image_info[0]
%% Load classification results
Map each filename to classification results for that file
%% Smooth classification results over sequences (prep)
These are the only classes to which we're going to switch other classifications
Only switch classifications to the dominant class if we see the dominant class at least
this many times
"If we see more than this many of a class that are above threshold, don't switch those"
classifications to the dominant class.
"If the ratio between a dominant class and a secondary class count is greater than this,"
"regardless of the secondary class count, switch those classificaitons (i.e., ignore"
max_secondary_class_classifications_above_threshold_for_class_smoothing).
""
"This may be different for different dominant classes, e.g. if we see lots of cows, they really"
"tend to be cows.  Less so for canids, so we set a higher ""override ratio"" for canids."
"If there are at least this many classifications for the dominant class in a sequence,"
"regardless of what that class is, convert all 'other' classifications (regardless of"
confidence) to that class.
"If there are at least this many classifications for the dominant class in a sequence,"
"regardless of what that class is, classify all previously-unclassified detections"
as that class.
Only count classifications above this confidence level when determining the dominant
"class, and when deciding whether to switch other classifications."
Confidence values to use when we change a detection's classification (the
original confidence value is irrelevant at that point)
%% Smooth classification results over sequences (supporting functions)
im = images_this_sequence[0]
det = results_this_image['detections'][0]
Only process animal detections
Only process detections with classification information
"We only care about top-1 classifications, remove everything else"
Make sure the list of classifications is already sorted by confidence
...and just keep the first one
"Confidence values should be sorted within a detection; verify this, and ignore"
...for each detection in this image
...for each image in this sequence
...top_classifications_for_sequence()
Count above-threshold classifications in this sequence
Sort the dictionary in descending order by count
"Handle a quirky special case: if the most common category is ""other"" and"
"it's ""tied"" with the second-most-common category, swap them."
...def count_above_threshold_classifications()
%% Smooth classifications at the sequence level (main loop)
Break if this token is contained in a filename (set to None for normal operation)
i_sequence = 0; seq_id = all_sequences[i_sequence]
Count top-1 classifications in this sequence (regardless of confidence)
Handy debugging code for looking at the numbers for a particular sequence
Count above-threshold classifications for each category
"If our dominant category ID isn't something we want to smooth to, don't mess around with this sequence"
"# Smooth ""other"" classifications ##"
"By not re-computing ""max_count"" here, we are making a decision that the count used"
"to decide whether a class should overwrite another class does not include any ""other"""
classifications we changed to be the dominant class.  If we wanted to include those...
""
sorted_category_to_count = count_above_threshold_classifications(classifications_this_sequence)
max_count = get_first_value_from_sorted_dictionary(sorted_category_to_count)
assert dominant_category_id == get_first_key_from_sorted_dictionary(sorted_category_to_count)
# Smooth non-dominant classes ##
Don't flip classes to the dominant class if they have a large number of classifications
"Don't smooth over this class if there are a bunch of them, and the ratio"
if primary to secondary class count isn't too large
Default ratio
Does this dominant class have a custom ratio?
# Smooth unclassified detections ##
...for each sequence
%% Write smoothed classification results
"%% Post-processing (post-classification, post-within-image-and-within-sequence-smoothing)"
%% Zip .json files
%% 99.9% of jobs end here
Everything after this is run ad hoc and/or requires some manual editing.
%% Compare results files for different model versions (or before/after RDE)
Choose all pairwise combinations of the files in [filenames]
%% Merge in high-confidence detections from another results file
%% Create a new category for large boxes
"This is a size threshold, not a confidence threshold"
size_options.categories_to_separate = [3]
%% Preview large boxes
%% .json splitting
options.query = None
options.replacement = None
Reminder: 'n_from_bottom' with a parameter of zero is the same as 'bottom'
%% Custom splitting/subsetting
i_folder = 0; folder_name = folders[i_folder]
"This doesn't do anything in this case, since we're not splitting folders"
options.make_folder_relative = True
%% String replacement
%% Splitting images into folders
%% Generate commands for a subset of tasks
i_task = 8
...for each task
%% End notebook: turn this script into a notebook (how meta!)
Exclude everything before the first cell
Remove the first [first_non_empty_lines] from the list
Add the last cell
""
xmp_integration.py
""
"Tools for loading MegaDetector batch API results into XMP metadata, specifically"
for consumption in digiKam:
""
https://cran.r-project.org/web/packages/camtrapR/vignettes/camtrapr2.html
""
%% Imports and constants
%% Class definitions
Folder where images are stored
.json file containing MegaDetector output
"String to remove from all path names, typically representing a"
prefix that was added during MegaDetector processing
Optionally *rename* (not copy) all images that have no detections
above [rename_conf] for the categories in rename_cats from x.jpg to
x.check.jpg
"Comma-deleted list of category names (or ""all"") to apply the rename_conf"
behavior to.
"Minimum detection threshold (applies to all classes, defaults to None,"
i.e. 0.0
%% Functions
Relative image path
Absolute image path
List of categories to write to XMP metadata
Categories with above-threshold detections present for
this image
Maximum confidence for each category
Have we already added this to the list of categories to
write out to this image?
If we're supposed to compare to a threshold...
Else we treat *any* detection as valid...
Keep track of the highest-confidence detection for this class
If we're doing the rename/.check behavior...
Legacy code to rename files where XMP writing failed
%% Interactive/test driver
%%
%% Command-line driver
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Cosmos DB `batch-api-jobs` table for job status
"aggregate the number of images, country and organization names info from each job"
submitted during yesterday (UTC time)
create the card
""
api_frontend.py
""
"Defines the Flask app, which takes requests (one or more images) from"
"remote callers and pushes the images onto the shared Redis queue, to be processed"
by the main service in api_backend.py .
""
%% Imports
%% Initialization
%% Support functions
Make a dict that the request_processing_function can return to the endpoint
function to notify it of an error
Verify that the content uploaded is not too big
""
request.content_length is the length of the total payload
Verify that the number of images is acceptable
...def check_posted_data(request)
%% Main loop
Check whether the request_processing_function had an error
Write images to temporary files
""
TODO: read from memory rather than using intermediate files
"Submit the image(s) for processing by api_backend.py, who is waiting on this queue"
TODO: convert to a blocking read and eliminate the sleep() statement in this loop
"image = Image.open(os.path.join(temp_direc, image_name))"
...if we do/don't have a request available on the queue
...while(True)
...def detect_sync()
%% Command-line driver
use --non-docker if you are testing without Docker
""
python api_frontend.py --non-docker
# Camera trap real-time API configuration
"Full path to the temporary folder for image storage, only meaningful"
within the Docker container
Upper limit on total content length (all images and parameters)
Minimum confidence threshold for detections
Minimum confidence threshold for showing a bounding box on the output image
Use this when testing without Docker
""
api_backend.py
""
"Defines the model execution service, which pulls requests (one or more images)"
"from the shared Redis queue, and runs them through the TF model."
""
%% Imports
%% Initialization
%% Main loop
TODO: convert to a blocking read and eliminate the sleep() statement in this loop
Filter the detections by the confidence threshold
""
"Each result is [ymin, xmin, ymax, xmax, confidence, category]"
""
"Coordinates are relative, with the origin in the upper-left"
...if serialized_entry
...while(True)
...def detect_process()
%% Command-line driver
use --non-docker if you are testing without Docker
""
python api_frontend.py --non-docker
run detections on a test image to load the model
%%
Importing libraries
%%
%%
%%
%%
GPU configuration: set up GPUs based on availability and user specification
Environment variable setup for numpy multi-threading. It is important to avoid cpu and ram issues.
Load and set configurations from the YAML file
Set a global seed for reproducibility
"If the annotation directory does not have a data split, split the data first"
Replace annotation dir from config with the directory containing the split files
Split the data according to the split type
Get the path to the annotation files
Crop training data
Crop validation data
Crop test data (most likely we don't need this)
Dataset and algorithm loading based on the configuration
Logger setup based on the specified logger type
Callbacks for model checkpointing and learning rate monitoring
Trainer configuration in PyTorch Lightning
"Training, validation, or evaluation execution based on the mode"
%%
%%
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
PyTorch imports
"Importing the model, dataset, transformations and utility functions from PytorchWildlife"
Importing the utility function for saving cropped images
Setting the device to use for computations ('cuda' indicates GPU)
Initializing the MegaDetectorV5 model for image detection
Creating a dataset of images with the specified transform
Creating a DataLoader for batching and parallel processing of the images
Performing batch detection on the images
Saving the detected objects as cropped images
%%
Read the original CSV file
Prepare a list to store new records for the new CSV
Process the data if the name of the file is in the dataframe
Save the crop into a new csv
Add record to the new CSV data
Create a DataFrame from the new records
Define the path for the new CSV file
Save the new DataFrame to CSV
# DATA SPLITTING
Load the data from the csv file
Separate the features and the targets
First split to separate out the test set
Adjust val_size to account for the initial split
Second split to separate out the validation set
"Combine features, labels, and classification back into dataframes"
Create the output directory in case that it does not exist
Save the splits to new CSV files
Return the dataframes
Load the data from the csv file
Calculate train size based on val and test size
Get unique locations
Split locations into train and temp (temporary holding for val and test)
Adjust the proportions for val and test based on the remaining locations
"Allocate images to train, validation, and test sets based on their location"
Save the datasets to CSV files
Return the split datasets
Load the data from the csv file
Convert 'Photo_Time' from string to datetime
Calculate train size based on val and test size
Sort by 'Photo_Time' to ensure chronological order
Group photos into sequences based on a 30-second interval
Assign unique sequence IDs to each group
Get unique sequence IDs
Split sequence IDs into train and temp (temporary holding for val and test)
Adjust the proportions for val and test based on the remaining sequences
"Allocate images to train, validation, and test sets based on their sequence ID"
Save the datasets to CSV files
Return the split datasets
Exportable class names for external use
Applying the ResNet layers and operations
Initialize the network with the specified settings
Selecting the appropriate ResNet architecture and pre-trained weights
self.pretrained_weights = ResNet18_Weights.IMAGENET1K_V1
self.pretrained_weights = ResNet50_Weights.IMAGENET1K_V1
Constructing the feature extractor and classifier
Criterion for binary classification
Load pre-trained weights and adjust for the current model
init_weights = self.pretrained_weights.get_state_dict(progress=True)
Load the weights into the feature extractor
Identify missing and unused keys in the loaded weights
Import necessary libraries
Exportable class names for external use
Define normalization mean and standard deviation for image preprocessing
Define data transformations for training and validation datasets
Load data for prediction
Load data for training/validation
"Load datasets for different modes (training, validation, testing, prediction)"
Calculate class counts and label mappings
Define parameters for the optimizer
Optimizer parameters for feature extraction
Optimizer parameters for the classifier
Setup optimizer and optimizer scheduler
Forward pass
Calculate loss
Forward pass
Forward pass
Concatenate outputs from all test steps
Calculate the metrics and save the output
Forward pass
Concatenate outputs from all predict steps
Compute the confusion matrix from true labels and predictions
Calculate class-wise accuracy (accuracy for each class)
Calculate micro accuracy (overall accuracy)
Calculate macro accuracy (mean of class-wise accuracies)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Importing necessary basic libraries and modules
%%
PyTorch imports for tensor operations
%%
"Importing the models, transformations, and utility functions from PytorchWildlife"
%%
Setting the device to use for computations ('cuda' indicates GPU)
%%
Initializing the MegaDetectorV6 model for image detection
Uncomment the following line to use MegaDetectorV5 instead of MegaDetectorV6
"detection_model = pw_detection.MegaDetectorV5(device=DEVICE, pretrained=True, version=""a"")"
%%
Initializing the model for image classification
%%
Initializing a box annotator for visualizing detections
Processing the video and saving the result with annotated detections and classifications
%%
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Importing necessary basic libraries and modules
PyTorch imports
"Importing the model, dataset, transformations and utility functions from PytorchWildlife"
Importing basic libraries
"Importing the models, dataset, transformations, and utility functions from PytorchWildlife"
Setting the device to use for computations ('cuda' indicates GPU)
Initializing a supervision box annotator for visualizing detections
Create a temp folder
Initializing the detection and classification models
Defining functions for different detection scenarios
Create an exception for custom weights
"If the detection model is HerdNet, use dot annotator, else use box annotator"
Herdnet receives both clf and det confidence thresholds
Only run classifier when detection class is animal
Clean the temp folder if it contains files
Check the contents of the extracted folder
If the detection model is HerdNet set batch_size to 1
Building Gradio UI
The timelapse checkbox is only visible when the detection model is not HerdNet
Show timelapsed checkbox only when detection model is not HerdNet
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Importing necessary basic libraries and modules
PyTorch imports
%%
"Importing the model, dataset, transformations and utility functions from PytorchWildlife"
%%
Setting the device to use for computations ('cuda' indicates GPU)
%%
Initializing the MegaDetectorV6 model for image detection
Uncomment the following line to use MegaDetectorV5 instead of MegaDetectorV6
"detection_model = pw_detection.MegaDetectorV5(device=DEVICE, pretrained=True, version=""a"")"
%% Single image detection
Specifying the path to the target image TODO: Allow argparsing
Performing the detection on the single image
Saving the detection results
Saving the detected objects as cropped images
%% Batch detection
Specifying the folder path containing multiple images for batch detection
Performing batch detection on the images
%% Output to annotated images
Saving the batch detection results as annotated images
%% Output to cropped images
Saving the detected objects as cropped images
%% Output to JSON results
Saving the detection results in JSON format
Saving the detection results in timelapse JSON format
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Importing necessary basic libraries and modules
PyTorch imports
%%
"Importing the model, dataset, transformations and utility functions from PytorchWildlife"
%%
Setting the device to use for computations ('cuda' indicates GPU)
%%
Initializing the HerdNet model for image detection
"If you want to use ennedi dataset weigths, you can use the following line:"
"detection_model = pw_detection.HerdNet(device=DEVICE, dataset=""ennedi"")"
%% Single image detection
Performing the detection on the single image
%% Output to annotated images
Saving the batch detection results as annotated images
%% Batch image detection
Specifying the folder path containing multiple images for batch detection
Performing batch detection on the images
%% Output to annotated images
Saving the batch detection results as annotated images
Saving the detection results in JSON format
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Importing necessary basic libraries and modules
PyTorch imports
%% Argument parsing
%%
Setting the device to use for computations ('cuda' indicates GPU)
%%
Initializing the MegaDetectorV6 model for image detection
Uncomment the following line to use MegaDetectorV5 instead of MegaDetectorV6
"detection_model = pw_detection.MegaDetectorV5(device=DEVICE, pretrained=True, version=""a"")"
%% Batch detection
Performing batch detection on the images
%% Output to JSON results
Saving the detection results in JSON format
Separate the positive and negative detections through file copying:
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Making the DetectionImageFolder class available for import from this module
Listing and sorting all image files in the specified directory
Get image filename and path
Load and convert image to RGB
Apply transformation if specified
Only run recognition on animal detections
Get image path and corresponding bbox xyxy for cropping
Load and crop image with supervision
Apply transformation if specified
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
from yolov5.utils.augmentations import letterbox
Making the provided classes available for import from this module
Convert PIL Image to Torch Tensor
Original shape
New shape
Scale ratio (new / old) and compute padding
Resize image
Pad image
Convert the image to a PyTorch tensor and normalize it
Resize and pad the image using a customized letterbox function.
Normalization constants
Define the sequence of transformations
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
!!! Output paths need to be optimized !!!
!!! Output paths need to be optimized !!!
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Importing basic libraries
Placeholder class-level attributes to be defined in derived classes
"If there are size differences in the input images, use a for loop instead of matrix processing for scaling"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Image size for the Opossum classifier
Class names for prediction
"If pretrained, use the provided URL to fetch the weights"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Making the PlainResNetInference class available for import from this module
Following the ResNet structure to extract features
Initialize the network and weights
... [Missing weight URL definition for ResNet18]
... [Missing weight URL definition for ResNet50]
Print missing and unused keys for debugging purposes
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Image size for the Opossum classifier
Class names for prediction
"If pretrained, use the provided URL to fetch the weights"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Image size for the Opossum classifier
Class names for prediction
"If pretrained, use the provided URL to fetch the weights"
Configuration file for the Sphinx documentation builder.
""
"For the full list of built-in configuration values, see the documentation:"
https://www.sphinx-doc.org/en/master/usage/configuration.html
-- Project information -----------------------------------------------------
https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information
-- General configuration ---------------------------------------------------
https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration
-- Options for HTML output -------------------------------------------------
https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output
-- Options for todo extension ----------------------------------------------
https://www.sphinx-doc.org/en/master/usage/extensions/todo.html#configuration
"Determine the factor, which shifts the decimal point of x"
just behind the last significant digit
"Shift decimal point by multiplicatipon with factor, flooring, and"
division by factor
_ = pretty_print_object(obj)
Sloppy that I'm making a module-wide change here...
Determine the coordinates of the intersection rectangle
The intersection of two axis-aligned bounding boxes is always an
axis-aligned bounding box
Compute the area of both AABBs
Compute the intersection over union by taking the intersection
area and dividing it by the sum of prediction + ground-truth
areas - the intersection area.
%% Functions for running commands as subprocesses
%%
%% Test driver for execute_and_print
%% Parallel test driver for execute_command_and_print
Should we use threads (vs. processes) for parallelization?
"Only relevant if n_workers == 1, i.e. if we're not parallelizing"
%% Constants and imports
%% Classes
Collect all names
Make names unique and sort
cast location to string as the entries in locations are strings
Convert classnames to lowercase to simplify comparisons later
Normalize paths to simplify comparisons later
"Make custom replacements in filenames, typically used to"
accommodate changes in root paths after DB construction
## Build useful mappings to facilitate working with the DB
Category ID <--> name
Image filename --> ID
Image ID --> image object
Image ID --> annotations
"Each image can potentially multiple annotations, hence using lists"
...__init__
...class IndexedJsonDb
%% Functions
Find all unique locations
i_location = 0; location = locations[i_location]
Sorting datetimes fails when there are None's in the list.  So instead of sorting datetimes
"directly, sort tuples with a boolean for none-ness, then the datetime itself."
""
https://stackoverflow.com/questions/18411560/sort-list-while-pushing-none-values-to-the-end
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_location[1]
"Start a new sequence if necessary, including the case where this datetime is invalid"
"If this was an invalid datetime, this will record the previous datetime"
"as None, which will force the next image to start a new sequence."
...for each image in this location
Fill in seq_num_frames
...for each location
...create_sequences()
""
cct_to_md.py
""
"""Converts"" a COCO Camera Traps file to a MD results file.  Currently ignores"
"non-bounding-box annotations, and gives all annotations a confidence of 1.0."
""
The only reason to do this is if you are going to add information to an existing
"CCT-formatted dataset, and want to do that in Timelapse."
""
"Currently assumes that width and height are present in the input data, does not"
read them from images.
""
%% Constants and imports
%% Functions
# Validate input
# Read input
# Prepare metadata
ann = d['annotations'][0]
# Process images
im = d['images'][0]
"MegaDetector: [x,y,width,height] (normalized, origin upper-left)"
"CCT: [x,y,width,height] (absolute, origin upper-left)"
...if there's a bounding box
...for each annotation
This field is no longer included in MD output files by default
im_out['max_detection_conf'] = max_detection_conf
...for each image
# Write output
...cct_to_md()
%% Command-line driver
TODO
%% Interactive driver
%%
%%
""
cct_json_to_filename_json.py
""
"Given a .json file in COCO Camera Traps format, outputs a .json-formatted list of"
relative file names present in the CCT file.
""
%% Constants and environment
%% Main function
"json.dump(s,open(outputFilename,'w'))"
%% Command-line driver
%% Interactive driver
%%
""
cct_to_csv.py
""
"""Converts"" a COCO Camera Traps .json file to .csv, in quotes because"
"all kinds of assumptions are made here, and if you have a particular .csv"
"format in mind, YMMV.  Most notably, does not include any bounding box information"
or any non-standard fields that may be present in the .json file.  Does not
propagate information about sequence-level vs. image-level annotations.
""
"Does not assume access to the images, therefore does not open .jpg files to find"
"datetime information if it's not in the metadata, just writes datetime as 'unknown'."
""
%% Imports
%% Main function
#%% Read input
#%% Build internal mappings
annotation = annotations[0]
#%% Write output file
im = images[0]
Write out one line per class:
...for each class name
...for each image
...with open(output_file)
...def cct_to_csv
%% Interactive driver
%%
%% Command-line driver
""
remove_exif.py
""
"Removes all EXIF/IPTC/XMP metadata from a folder of images, without making"
"backup copies, using pyexiv2."
""
%% Imports and constants
%% List files
%% Remove EXIF data (support)
PYEXIV2 IS NOT THREAD SAFE; DO NOT CALL THIS IN PARALLEL FROM A SINGLE PROCESS
data = img.read_exif(); print(data)
%% Debug
%%
%%
%% Remove EXIF data (execution)
fn = image_files[0]
"joblib.Parallel defaults to a process-based backend, but let's be sure"
"results = Parallel(n_jobs=n_exif_threads,verbose=2,prefer='processes')(delayed(remove_exif)(fn) for fn in image_files[0:10])"
""
yolo_to_coco.py
""
Converts a YOLO-formatted dataset to a COCO-formatted dataset.
""
"Currently supports only a single folder (i.e., no recursion).  Treats images without"
corresponding .txt files as empty.
""
%% Imports and constants
from ai4eutils
%% Support functions
Validate input
Class names
Blank lines should only appear at the end
Enumerate images
fn = image_files[0]
Create the image object for this image
Is there an annotation file for this image?
"This is an image with no annotations, currently don't do anything special"
here
s = lines[0]
"COCO: [x_min, y_min, width, height] in absolute coordinates"
"YOLO: [class, x_center, y_center, width, height] in normalized coordinates"
...for each annotation
...if this image has annotations
...for each image
...def yolo_to_coco()
%% Interactive driver
%% Convert YOLO folders to COCO
%% Check DB integrity
%% Preview some images
%% Command-line driver
TODO
""
read_exif.py
""
"Given a folder of images, read relevant metadata (EXIF/IPTC/XMP) fields from all images,"
and write them to  a .json or .csv file.
""
This module can use either PIL (which can only reliably read EXIF data) or exiftool (which
can read everything).  The latter approach expects that exiftool is available on the system
path.  No attempt is made to be consistent in format across the two approaches.
""
%% Imports and constants
From ai4eutils
%% Options
Number of concurrent workers
Should we use threads (vs. processes) for parallelization?
""
Not relevant if n_workers is 1.
Should we use exiftool or pil?
%% Functions
exif_tags = img.info['exif'] if ('exif' in img.info) else None
print('Warning: unrecognized EXIF tag: {}'.format(k))
"-G means ""Print group name for each tag"", e.g. print:"
""
[File]          Bits Per Sample                 : 8
""
...instead of:
""
Bits Per Sample                 : 8
A list of three-element lists (type/tag/value)
line_raw = exif_lines[0]
A typical line:
""
[ExifTool]      ExifTool Version Number         : 12.13
"Split on the first occurrence of "":"""
...for each output line
...which processing library are we using?
...read_exif_tags_for_image()
...populate_exif_data()
Enumerate *relative* paths
Find all EXIF tags that exist in any image
...for each tag in this image
...for each image
Write header
...for each key that *might* be present in this image
...for each image
...with open()
...if we're writing to .json/.csv
https://stackoverflow.com/questions/11210104/check-if-a-program-exists-from-a-python-script
%% Interactive driver
%%
output_file = os.path.expanduser('~/data/test-exif.csv')
options.processing_library = 'pil'
"file_path = os.path.join(input_folder,'KRU_S1_11_R1_IMAG0148.JPG')"
%%
%% Command-line driver
""
"Given a json-formatted list of image filenames, retrieve the width and height of every image."
""
%% Constants and imports
%% Processing functions
Is this image on disk?
"all_results = list(tqdm(pool.imap(process_image, filenames), total=len(filenames)))"
%% Interactive driver
%%
List images in a test folder
%%
"process_list_file(image_list_file,image_size_file,image_prefix=base_dir)"
%% Command-line driver
"python sanity_check_json_db.py ""e:\wildlife_data\wellington_data\wellington_camera_traps.json"" --baseDir ""e:\wildlife_data\wellington_data\images"" --bFindUnusedImages --bCheckImageSizes"
"python sanity_check_json_db.py ""D:/wildlife_data/mcgill_test/mcgill_test.json"" --baseDir ""D:/wildlife_data/mcgill_test"" --bFindUnusedImages --bCheckImageSizes"
"Here the '-u' prevents buffering, which makes tee happier"
""
python -u sanity_check_json_db.py '/datadrive1/nacti_metadata.json' --baseDir '/datadrive1/nactiUnzip/' --bFindUnusedImages --bCheckImageSizes | tee ~/nactiTest.out
""
Partially-formed stub to get from MegaDetector output files to COCO Camera Traps data.
""
"Was actually written to convert *many* MD .json files to a single CCT file, hence"
the loop over .json files.
""
"THIS CODE HAS NEVER BEEN RUN, it was added as a demonstration of how to do this.  YMMV."
""
"You may find a more polished, command-line-ready version of this code at:"
""
https://github.com/StewartWILDlab/mdtools
""
%% Constants and imports
"Images sizes are required to convert between absolute and relative coordinates,"
so we need to read the images.
Only required if you want to write a database preview
%% Create CCT dictionaries
image_ids_to_images = {}
Force the empty category to be ID 0
Load .json annotations for this data set
i_entry = 0; entry = data['images'][i_entry]
""
"PERF: Not exactly trivially parallelizable, but about 100% of the"
time here is spent reading image sizes (which we need to do to get from
"absolute to relative coordinates), so worth parallelizing."
Generate a unique ID from the path
detection = detections[0]
Have we seen this category before?
Create an annotation
"MegaDetector: [x,y,width,height] (normalized, origin upper-left)"
"CCT: [x,y,width,height] (absolute, origin upper-left)"
...for each detection
...for each image
Remove non-reviewed images and associated annotations
%% Create info struct
%% Write .json output
%% Clean start
## Everything after this should work from a clean start ###
%% Validate output
%% Preview animal labels
%% Preview empty labels
"viz_options.classes_to_exclude = ['empty','human']"
""
generate_crops_from_cct.py
""
"Given a .json file in COCO Camera Traps format, create a cropped image for"
each bounding box.
""
%% Imports and constants
%% Functions
# Read and validate input
# Find annotations for each image
"This actually maps image IDs to annotations, but only to annotations"
containing boxes
# Generate crops
TODO: parallelize this loop
im = d['images'][0]
Load the image
Generate crops
i_ann = 0; ann = annotations_this_image[i_ann]
"x/y/w/h, origin at the upper-left"
...for each box
...for each image
...generate_crops_from_cct()
%% Interactive driver
%%
%%
%%
%% Command-line driver
TODO
%% Scrap
%%
""
coco_to_yolo.py
""
Converts a COCO-formatted dataset to a YOLO-formatted dataset.
""
"If the input and output folders are the same, writes .txt files to the input folder,"
and neither moves nor modifies images.
""
"Currently ignores segmentation masks, and errors if an annotation has a"
segmentation polygon but no bbox
""
Has only been tested on a handful of COCO Camera Traps data sets; if you
"use it for more general COCO conversion, YMMV."
""
%% Imports and constants
%% Support functions
Validate input
Read input data
Parse annotations
i_ann = 0; ann = data['annotations'][0]
Make sure no annotations have *only* segmentation data
Re-map class IDs to make sure they run from 0...n-classes-1
""
"TODO: this allows unused categories in the output data set, which I *think* is OK,"
but I'm only 81% sure.
Process images (everything but I/O)
"List of dictionaries with keys 'source_image','dest_image','bboxes','dest_txt'"
i_image = 0; im = data['images'][i_image]
"assert os.path.isfile(source_image), 'Could not find image {}'.format(source_image)"
If this annotation has no bounding boxes...
"This is not entirely clear from the COCO spec, but it seems to be consensus"
"that if you want to specify an image with no objects, you don't include any"
annotations for that image.
We allow empty bbox lists in COCO camera traps; this is typically a negative
"example in a dataset that has bounding boxes, and 0 is typically the empty"
category.
...if this is an empty annotation
"COCO: [x_min, y_min, width, height] in absolute coordinates"
"YOLO: [class, x_center, y_center, width, height] in normalized coordinates"
Convert from COCO coordinates to YOLO coordinates
...for each annotation
...if this image has annotations
...for each image
Write output
Category IDs should range from 0..N-1
TODO: parallelize this loop
""
output_info = images_to_copy[0]
Only write an annotation file if there are bounding boxes.  Images with
"no .txt files are treated as hard negatives, at least by YOLOv5:"
""
https://github.com/ultralytics/yolov5/issues/3218
""
"I think this is also true for images with empty annotation files, but"
"I'm using the convention suggested on that issue, i.e. hard negatives"
are expressed as images without .txt files.
bbox = bboxes[0]
...for each image
...def coco_to_yolo()
%% Interactive driver
%% CCT data
"If preview_export is True, I'm exporting to preview these with BoundingBoxEditor:"
""
https://github.com/mfl28/BoundingBoxEditor
""
"This export will be compatible, other than the fact that you need to move"
"""object.data"" into the ""labels"" folder."
""
"Otherwise I'm exporting for training, in the YOLOv5 flat format."
%% Command-line driver
TODO
""
cct_to_wi.py
""
Converts COCO Camera Traps .json files to the Wildlife Insights
batch upload format
""
Also see:
""
https://github.com/ConservationInternational/Wildlife-Insights----Data-Migration
""
https://data.naturalsciences.org/wildlife-insights/taxonomy/search
""
%% Imports
%% Paths
A COCO camera traps file with information about this dataset
A .json dictionary mapping common names in this dataset to dictionaries with the
"WI taxonomy fields: common_name, wi_taxon_id, class, orer, family, genus, species"
%% Constants
%% Project information
%% Read templates
%% Compare dictionary to template lists
Write the header
Write values
%% Project file
%% Camera file
%% Deployment file
%% Images file
Read .json file with image information
Read taxonomy dictionary
Populate output information
df = pd.DataFrame(columns = images_fields)
annotation = annotations[0]
im = input_data['images'][0]
"We don't have counts, but we can differentiate between zero and 1"
This is the label mapping used for our incoming iMerit annotations
"Only used to parse the incoming annotations. In our database, the string name is used to avoid confusion"
MegaDetector outputs
""
add_bounding_boxes_to_megadb.py
""
"Given COCO-formatted JSONs containing manually labeled bounding box annotations, add them to"
"MegaDB sequence entries, which can then be ingested into MegaDB."
the category map that comes in the COCO JSONs for iMerit batch 12 - to check that each
JSON
"dataset name : (seq_id, frame_num) : [bbox, bbox]"
where bbox is a dict with str 'category' and list 'bbox'
iterate over image_id_to_image rather than image_id_to_annotations so we include
the confirmed empty images
The file_name field in the incoming json looks like
alka_squirrels.seq2020_05_07_25C.frame119221.jpg
there seems to be a bug in the annotations where sometimes there's a
non-empty label along with a label of category_id 5
ignore the empty label (they seem to be actually non-empty)
""
add_bounding_boxes_to_json.py
""
This script takes a image database in the COCO Camera Traps format and merges in a set of bounding
box annotations in the format that iMerit uses (a .json where actually only each row is a valid json).
""
"If you need to update an existing bbox database, please get all the original annotation files and"
re-generate from scratch
""
%% Imports
%% Configurations and paths
images database
output bboxes database
annotation files (pseudo json) obtained from our annotation vendor that contain annotations for this dataset
None or a string or tuple of strings that is the prefix to all file_name of interest / in this dataset in the annotation files
functions for mapping the image_id in the annotation files (pseudo jsons) to the image_id used in the image DB
our img_id doesn't contain frame info
"batch3 - ""file_name"":""ASG0000019_0_S1_B06_R1_PICT0007.JPG"""
"batch5 and 7 - ""file_name"":""datasetsnapshotserengeti.seqASG000002m-frame0.imgS1_B06_R1_PICT0056.JPG"""
sometimes - 'datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG'
"id in DB (old_token): 'S6/J01/J01_R1/S6_J01_R1_IMAG0001', 'S1/B05/B05_R1/S1_B05_R1_PICT0036'"
specify which one to use for your dataset here
%% Load the image database and fill in DB info for the output bbox database
load the images database
%% Find the height and width of images from the annotation files
""
...if they are not available in the images DB
each row in this pseudo-json is a COCO formatted entry for an image sequence
%% Other functions required by specific datasets
the IDFG image database does not include images from unlabeled folders that were annotated with bounding boxes
%% Create the bbox database from all annotation files pertaining to this dataset
"for the incoming annotations, look up by category name (common) and convert to the numerical id used in our databases"
"for each annotation pseudo-json, check that the image it refers to exists in the original database"
each row in this pseudo-json is a COCO formatted entry for an image sequence
check that entry is for this dataset
category map for this entry in the annotation file - usually the same across all entries but just in case
rspb_add_image_entry(img_id)
use the image length and width in the image DB
"[top left x, top left y, width, height] in relative coordinates"
"add all images that have been sent to annotation, some of which may be empty of bounding boxes"
rspb_add_image_entry(db_image_id)
%% Common queries
This query is used when preparing tfrecords for object detector training.
We do not want to get the whole seq obj where at least one image has bbox because
some images in that sequence will not be bbox labeled so will be confusing.
Include images with bbox length 0 - these are confirmed empty by bbox annotators.
"If frame_num is not available, it will not be a field in the result iterable."
"Note that the seq_id is the Cosmos DB assigned ID for that sequence, not the"
"seq_id field, which may contain ""/"" characters."
"For public datasets to be converted to the CCT format, we get the whole seq object because"
sequence level attributes need to be included too. megadb/converters/megadb_to_cct.py handles
the case of bbox-only JSONs with the flag exclude_species_class
Getting all sequences in a dataset - for updating or deleting entries which need the id field
%% Parameters
Use None if querying across all partitions
"The `sequences` table has the `dataset` as the partition key, so if only querying"
"entries from one dataset, set the dataset name here."
"e.g. {'name': '@top_n', 'value': 100} - see query_and_upsert_examples/query_for_data.ipynb"
Use False if do not want all results stored in a single JSON.
%% Script
execute the query
loop through and save the results
MODIFY HERE depending on the query
wiitigers Unicode issue - no good mapping from DB file names to file names in blob URL
build filename
if need to re-download a dataset's images in case of corruption
entries_to_download = {
"filename: entry for filename, entry in entries_to_download.items()"
if entry['dataset'] == DATASET
}
input validation
"existing files, with paths relative to <store_dir>"
parse JSON or TXT file
"create a new storage container client for this dataset,"
and cache it
schema already checks that the min possible value of frame_num is 0
"if there are more than one image item, each needs a frame_num"
checks across all sequence items
per sequence item checks
load the schema
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory
load the schema
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory
checks across all datasets items
check for expiry date of the SAS keys
update the sequences
"`id` is from the database, as well as all attributes starting with _"
"if valuable sequence information is available, add them to the image"
required fields for an image object
add seq-level class labels for this image
add other sequence-level properties to each image too
add other image-level properties
... for im in seq['images']
... for seq in mega_db
consolidate categories
some property names have changed in the new schema
a dummy sequence ID will be generated if the image entry does not have a seq_id field
seq_id only needs to be unique within this dataset; MegaDB does not rely on it as the _id field
"""annotations"" fields are opened and have its sub-field surfaced one level up"
set the `dataset` property on each sequence to the provided dataset_name
check that the location field is the same for all images in a sequence
check which fields in a CCT image entry are sequence-level
image-level properties that really should be sequence-level
need to add (misidentified) seq properties not present for each image in a sequence to img_level_properties
"(some properties act like flags - all have the same value, but not present on each img)"
add the sequence-level properties to the sequence objects
not every sequence have to have all the seq_level_properties
get the value of this sequence-level property from the first image entry
check which fields are really dataset-level and should be included in the dataset table instead.
delete sequence-level properties that should be dataset-level
make all `class` fields lower-case; cast `seq_id` to type string in case they're integers
turn all float NaN values into None so it gets converted to null when serialized
this was an issue in the Snapshot Safari datasets
%% validation
"at first a dict of image_id: image_obj with annotations embedded, then it becomes"
an array of image objects
%% integrate the image DB
takes in image entries and species and other annotations in the image DB
convert the species category to explicit string name
there may be other fields in the annotation object
these fields should already be gotten from the image object
%% integrate the bbox DB
add any images that are not in the image DB
also add any fields in the image object that are not present already
add bbox to the annotations field
for any newly added images
"'bbox_abs': bbox_anno['bbox'],"
not keeping height and width
""
jb_csv_to_json.py
""
Convert a particular .csv file to CCT format.  Images were not available at
"the time I wrote this script, so this is much shorter than other scripts"
in this folder.
""
%% Constants and environment
%% Read source data
%% Confirm filename uniqueness (this data set has one label per image)
"Build up a map from filenames to a list of rows, checking image existence as we go"
%% Create CCT dictionaries
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
imageName = imageFilenames[0]
"As per above, this is convenient and appears to be true; asserting to be safe"
"Filenames look like ""290716114012001a1116.jpg"""
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
%% Create info struct
%% Write output
%% Sanity-check
%% Imports and constants
%% Load data
"with open(image_json,'r') as f:"
data = json.load(f)
%% Sanity-check data
%% Label previews
%% Collect images to annotate
%% Sort by sequence and frame
%% Copy to a folder by GUID
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension
im = images_to_annotate[0]
...for each image
%% Write out the annotation list
%% Constants and imports
%% Enumerate files
edited_image_folder = edited_image_folders[0]
fn = edited_image_files[0]
%% Read metadata and capture location information
i_row = 0; row = df.iloc[i_row]
Sometimes '2017' was just '17' in the date column
%% Read the .json files and build output dictionaries
json_fn = json_files[0]
if 'partial' in json_fn:
continue
line = lines[0]
if image_fn == 'SD1_238_6_26_17_16_76.73.jpg':
asdfad
SD29_079_5_14_2018_17_52.85.jpg
Re-write two-digit years as four-digit years
Sometimes the year was written with two digits instead of 4
assert len(tokens[4]) == 4 and tokens[4].startswith('20')
Have we seen this location already?
"Not a typo, it's actually ""formateddata"""
An image shouldn't be annotated as both empty and non-empty
An image shouldn't be annotated as both empty and non-empty
box = formatteddata[0]
"open_file(os.path.join(base_folder,jpeg_image_folder,image_fn))"
...for each box
...if there are boxes on this image
...for each line
...with open()
...for each json file
%% Prepare the output .json
%% Check DB integrity
%% Print unique locations
SD12_202_6_23_2017_1_31.85.jpg
%% Preview some images
%% Statistics
""
"Import a Snapshot Safari project (one project, one season)"
""
Before running this script:
""
"* Mount the blob container where the images live, or copy the"
images to local storage
""
What this script does:
""
* Creates a .json file
* Creates zip archives of the season without humans.
* Copies animals and humans to separate folders
""
After running this script:
""
* Create or update LILA page
* Push zipfile and unzipped images to LILA
* Push unzipped humans to wildlifeblobssc
* Delete images from UMN uplaod storage
""
"Snapshot Serengeti is handled specially, because we're dealing with bounding"
boxes too.  See snapshot_serengeti_lila.py.
""
%% Imports
From ai4eutils
From CameraTraps
%% Constants
project_name = 'XXX'; season_name = 'S1'; project_friendly_name = 'Snapshot Unknown'
project_name = 'SER'; season_name = 'S1-11'; project_friendly_name = 'Snapshot Serengeti'
project_name = 'KRU'; season_name = 'S1'; project_friendly_name = 'Snapshot Kruger'
project_name = 'CDB'; season_name = 'S1'; project_friendly_name = 'Snapshot Camdeboo'
project_name = 'MTZ'; season_name = 'S1'; project_friendly_name = 'Snapshot Mountain Zebra'
project_name = 'ENO'; season_name = 'S1'; project_friendly_name = 'Snapshot Enonkishu'
project_name = 'KAR'; season_name = 'S1'; project_friendly_name = 'Snapshot Karoo'
project_name = 'KGA'; season_name = 'S1'; project_friendly_name = 'Snapshot Kgalagadi'
%% Folder/file creation
E.g. KRU_S1
E.g. Z:\KRU
E.g. Z:\KRU\KRU_S1
Contains annotations for each capture event (sequence)
Maps image IDs to filenames; each line looks like:
""
"KRU_S1#1#1#2,3,KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0004.JPG"
"Total number of each answer to each question, e.g. total number of times each species was identified"
""
Not used here
Create output folders
Images will be placed in a season-specific folder inside this (the source data includes
this in path names)
%% Load metadata files
%% Convert to dictionaries (prep)
%% Convert to dictionaries (loops)
"TODO: iterrows() is a terrible way to do this, but this is one of those days"
"where I want to get this done, not get better at Python."
irow = 0; row = image_table.iloc[0]
"Loaded as an int64, converting to int here"
...for each row in the image table
Make sure image IDs are what we think they are
...for each row in the annotation table
%% Take a look at categories (just sanity-checking)
print('\nCategories by species:')
pp.pprint(categories_by_species)
%% Fill in some image fields we didn't have when we created the image table
"width, height, corrupt, seq_num_frames, location, datetime"
Every annotation in this list should have the same sequence ID
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;"
there's no reason to do this redundantly for every image
Every image in this sequence should point back to the same equence
Every annotation in this list should have the same location
Every annotation in this list should have the same datetime
Is this image on disk?
iImage = 0; im = images[0]
...for each image
"images_processed = pool.map(process_image, images)"
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))"
%% Count missing/corrupted images
%% Print distribution of sequence lengths (sanity-check)
%% Replicate annotations across images
iAnn = 0; ann = annotations[iAnn]
%% See what files are on disk but not annotated
%% Sanity-check image and annotation uniqueness
%% Minor updates to fields
%% Write .json file
%% Create a list of human files
ann = annotations[0]
%% Create public archive and public/private folders
im = images[0]
E.g. KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0001.JPG
Copy to private output folder
Add to zipfile
Possibly start a new archive
Copy to public output folder
...for each image
%% Sanity-check .json file
"This will produce some validation errors, because this zipfile doesn't include humans"
%% Zip up .json and .csv files
%% When I skip to this part (using a pre-rendered .json file)
%%
%%
ann = annotations[0]
%% Summary prep for LILA
"%% Generate preview, sanity-check labels"
"viz_options.classes_to_include = ['jackalblackbacked','bustardkori']"
%% Scrap
%% Find annotations for a particular image
%% Write a list of missing images
""
save_the_elephants_survey_A.py
""
Convert the .csv file provided for the Save the Elephants Survey A data set to a
COCO-camera-traps .json file
""
%% Constants and environment
Handle all unstructured fields in the source data as extra fields in the annotations
"photo_type really should be an image property, but there are a few conflicts"
that forced me to handle it as an annotation proprerty
%% Read source data
"%% Map filenames to rows, verify image existence"
Maps relative paths to row indices in input_metadata
"Build up a map from filenames to a list of rows, checking image existence as we go"
Ignore directories
%% Make sure the multiple-annotation cases make sense
%%
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
Force the empty category to be ID 0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image]
Example filename:
""
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG'
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG'
i_row = row_indices[0]
There are a small handful of datetime mismatches across annotations
for the same image
assert im['datetime'] == timestamp
Special cases based on the 'photo type' field
Various spellings of 'community'
Have we seen this category before?
Create an annotation
fieldname = list(mapped_fields.keys())[0]
...for each row
...for each image
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
%% Scrap
%% Find unique photo types
##
""
timelapse_csv_set_to_json.py
""
Given a directory full of reasonably-consistent Timelapse-exported
".csvs, assemble a CCT .json."
""
"Assumes that you have a list of all files in the directory tree, including"
image and .csv files.
""
##
%% Constants and imports
Text file with relative paths to all files (images and .csv files)
"%% Read file list, make a list of all image files and all .csv files"
"%% Verify column consistency, create a giant array with all rows from all .csv files"
i_csv = 0; csv_filename = csv_files[0]
%% Prepare some data structures we'll need for mapping image rows in .csv files to actual image files
Enumerate all folders containing image files
"In this data set, a site folder looks like:"
""
Processed Images\\site_name
%% Map .csv files to candidate camera folders
fn = valid_csv_files[0]
"Some site folders appear as ""XXNNNN"", some appear as ""XXNNNN_complete"""
...for each .csv file
%% Map camera folders to candidate image folders
%% Main loop over labels (prep)
Force the empty category to be ID 0
Images that are marked empty and also have a species label
%% Main loop over labels (loop)
i_row = 0; row = input_metadata.iloc[i_row]
"for i_row,row in input_metadata.iterrows():"
"Usually this is just a single folder name, sometimes it's a full path,"
which we don't want
Check whether this file exists on disk
Retrieve image width and height
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each row in the big table of concatenated .csv files
%% Fix cases where an image was annotated as 'unlabeled' and as something else
This annotation is 'unlabeled'
Was there another category associated with this image?
%% Check for un-annnotated images
Enumerate all images
list(relative_path_to_image.keys())[0]
%% Create info struct
%% Write output
%% Sanity-check the database's integrity
%% Render a bunch of images to make sure the labels got carried along correctly
""
auckland_doc_to_json.py
""
Convert Auckland DOC data set to COCO camera traps format.  This was
for a training data set where class names were encoded in path names.
""
%% Constants and imports
Filenames will be stored in the output .json relative to this base dir
%% Enumerate files
%% Assemble dictionaries
Force the empty category to be ID 0
fn = image_files[0]; print(fn)
Typically y:\Maukahuka_Auckland_Island\1_Training\Winter_Trial_2019\cat\cat\eat\20190903_IDdY_34_E3_tmp_201908240051.JPG
"This data set has two top-level folders, ""1_Training"" (which has class names encoded"
"in paths) and ""2_Testing"" (which has no class information)."
...for each image
%% Write output .json
%% Write train/test .jsons
%% Validate .json files
%% Preview labels
checkpoint
""
awc_to_json.py
""
Convert a particular .csv file to CCT format.
""
%% Constants and environment
%% Read source data
%% Main loop over labels
Force the empty category to be ID 0
iRow = 0; row = input_metadata.iloc[iRow]
"ImageID,FileName,FilePath,SpeciesID,CommonName"
"This makes an assumption of one annotation per image, which happens to be"
true in this data set.
Retrieve image width and height
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
%% Check for images that aren't included in the metadata file
Enumerate all images
list(relativePathToImage.keys())[0]
%% Create info struct
%% Write output
%% Sanity-check the database's integrity
%% Render a bunch of images to make sure the labels got carried along correctly
""
idaho-camera-traps.py
""
Prepare the Idaho Camera Traps dataset for release on LILA.
""
%% Imports and constants
Multi-threading for .csv file comparison and image existence validation
"We are going to map the original filenames/locations to obfuscated strings, but once"
"we've done that, we will re-use the mappings every time we run this script."
This is the file to which mappings get saved
The maximum time (in seconds) between images within which two images are considered the
same sequence.
"This is a two-column file, where each line is [string in the original metadata],[category name we want to map it to]"
"The output file, using the original strings"
"The output file, using obfuscated strings for everything but filenamed"
"The output file, using obfuscated strings and obfuscated filenames"
"One time only, I ran MegaDetector on the whole dataset..."
...then set aside any images that *may* have contained humans that had not already been
annotated as such.  Those went in this folder...
...and the ones that *actually* had humans (identified via manual review) got
copied to this folder...
"...which was enumerated to this text file, which is a manually-curated list of"
images that were flagged as human.
Unopinionated .json conversion of the .csv metadata
%% List files (images + .csv)
Ignore .csv files in folders with multiple .csv files
...which would require some extra work to decipher.
fn = csv_files[0]
%% Parse each .csv file into sequences (function)
csv_file = csv_files[-1]
os.startfile(csv_file_absolute)
survey = csv_file.split('\\')[0]
Sample paths from which we need to derive locations:
""
St.Joe_elk\AM99\Trip 1\100RECNX\TimelapseData.csv
Beaverhead_elk\AM34\Trip 1\100RECNX\TimelapseData.csv
""
ClearCreek_mustelids\Winter2015-16\FS-001-P\FS-001-P.csv
ClearCreek_mustelids\Summer2015\FS-001\FS-001.csv
ClearCreek_mustelids\Summer2016\IDFG-016\IDFG-016.csv
""
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017b
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017a
Load .csv file
Validate the opstate column
# Create datetimes
print('Creating datetimes')
i_row = 0; row = df.iloc[i_row]
Make sure data are sorted chronologically
""
"In odd circumstances, they are not... so sort them first, but warn"
Debugging when I was trying to see what was up with the unsorted dates
# Parse into sequences
print('Creating sequences')
i_row = 0; row = df.iloc[i_row]
Start a new sequence if:
""
* This image has no timestamp
* This image has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each row
# Parse labels for each sequence
sequence_id = location_sequences[0]
Row indices in a sequence should be adjacent
sequence_df = df[df['seq_id']==sequence_id]
# Determine what's present
Be conservative; assume humans are present in all maintenance images
The presence columns are *almost* always identical for all images in a sequence
assert single_presence_value
print('Warning: presence value for {} is inconsistent for {}'.format(
"presence_column,sequence_id))"
...for each presence column
Tally up the standard (survey) species
"If no presence columns are marked, all counts should be zero"
count_column = count_columns[0]
Occasionally a count gets entered (correctly) without the presence column being marked
"assert len(values) == 1 and values[0] == 0, 'Non-zero counts with no presence"
columns marked for sequence {}'.format(sequence_id)
"Handle this by virtually checking the ""right"" box"
Make sure we found a match
Handle 'other' tags
column_name = otherpresent_columns[0]
print('Found non-survey counted species column: {}'.format(column_name))
...for each non-empty presence column
"Very rarely, the ""otherpresent"" column is checked, but no more detail is available"
...handling non-survey species
Build the sequence data
i_row = 0; row = sequence_df.iloc[i_row]
Only one folder used a single .csv file for two subfolders
...for each sequence
...def csv_to_sequences()
%% Parse each .csv file into sequences (loop)
%%
%%
i_file = -1; csv_file = csv_files[i_file]
%% Save sequence data
%% Load sequence data
%%
%% Validate file mapping (based on the existing enumeration)
sequences = sequences_by_file[0]
sequence = sequences[0]
"Actually, one folder has relative paths"
assert '\\' not in image_file_relative and '/' not in image_file_relative
os.startfile(csv_folder)
assert os.path.isfile(image_file_absolute)
found_file = os.path.isfile(image_file_absolute)
...for each image
...for each sequence
...for each .csv file
%% Load manual category mappings
The second column is blank when the first column already represents the category name
%% Convert to CCT .json (original strings)
Force the empty category to be ID 0
For each .csv file...
""
sequences = sequences_by_file[0]
For each sequence...
""
sequence = sequences[0]
Find categories for this image
"When 'unknown' is used in combination with another label, use that"
"label; the ""unknown"" here doesn't mean ""another unknown species"", it means"
there is some other unknown property about the main species.
category_name_string = species_present[0]
"This piece of text had a lot of complicated syntax in it, and it would have"
been too complicated to handle in a general way
print('Ignoring category {}'.format(category_name_string))
Don't process redundant labels
category_name = category_names[0]
If we've seen this category before...
If this is a new category...
print('Adding new category for {}'.format(category_name))
...for each category (inner)
...for each category (outer)
...if we do/don't have species in this sequence
"We should have at least one category assigned (which may be ""empty"" or ""unknown"")"
assert len(sequence_category_ids) > 0
Was any image in this sequence manually flagged as human?
print('Flagging sequence {} as human based on manual review'.format(sequence_id))
For each image in this sequence...
""
i_image = 0; im = images[i_image]
Create annotations for this image
...for each image in this sequence
...for each sequence
...for each .csv file
Verify that all images have annotations
ann = ict_data['annotations'][0]
For debugging only
%% Create output (original strings)
%% Validate .json file
%% Preview labels
%% Look for humans that were found by MegaDetector that haven't already been identified as human
This whole step only needed to get run once
%%
Load MD results
Get a list of filenames that MD tagged as human
im = md_results['images'][0]
...for each detection
...for each image
Map images to annotations in ICT
ann = ict_data['annotations'][0]
For every image
im = ict_data['images'][0]
Does this image already have a human annotation?
...for each annotation
...for each image
%% Copy images for review to a new folder
fn = missing_human_images[0]
%% Manual step...
Copy any images from that list that have humans in them to...
%% Create a list of the images we just manually flagged
fn = human_tagged_filenames[0]
E.g. '0000_Beaverhead_elk~AM174~Trip 1~100RECNX~IMG_1397.JPG'
"%% Translate location, image, sequence IDs"
Load mappings if available
Generate mappings
If we've seen this location before...
Otherwise assign a string-formatted int as the ID
If we've seen this sequence before...
Otherwise assign a string-formatted int as the ID
Assign an image ID
...for each image
Assign annotation mappings
Save mappings
"Back this file up, lest we should accidentally re-run this script"
with force_generate_mappings = True and overwrite the mappings we used.
...if we are/aren't re-generating mappings
%% Apply mappings
"%% Write new dictionaries (modified strings, original files)"
"%% Validate .json file (modified strings, original files)"
%% Preview labels (original files)
"viz_options.classes_to_exclude = ['empty','deer','elk']"
viz_options.classes_to_include = ['bobcat']
%% Copy images to final output folder (prep)
ann = d['annotations'][0]
Is this a public or private image?
Generate absolute path
Copy to output
Update the filename reference
...def process_image(im)
%% Copy images to final output folder (execution)
For each image
im = images[0]
Write output .json
%% Make sure the right number of images got there
%% Validate .json file (final filenames)
%% Preview labels (final filenames)
"viz_options.classes_to_exclude = ['empty','deer','elk']"
viz_options.classes_to_include = ['horse']
viz_options.classes_to_include = [viz_options.multiple_categories_tag]
"viz_options.classes_to_include = ['human','vehicle','domestic dog']"
%% Create zipfiles
%% List public files
%% Find the size of each file
fn = all_public_output_files[0]
%% Split into chunks of approximately-equal size
...for each file
%% Create a zipfile for each chunk
...for each filename
with ZipFile()
...def create_zipfile()
i_file_list = 0; file_list = file_lists[i_file_list]
"....if __name__ == ""__main__"""
""
wellington_to_json.py
""
Convert the .csv file provided for the Wellington data set to a
COCO-camera-traps .json file
""
%% Constants and environment
%% Read source data
"Filenames were provided as *.jpg, but images were *.JPG, converting here"
"%% Map filenames to rows, verify image existence"
"Takes ~30 seconds, since it's checking the existence of ~270k images"
"Build up a map from filenames to a list of rows, checking image existence as we go"
"I didn't expect this to be true a priori, but it appears to be true, and"
it saves us the trouble of checking consistency across multiple occurrences
of an image.
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
imageName = imageFilenames[0]
"As per above, this is convenient and appears to be true; asserting to be safe"
"Filenames look like ""290716114012001a1116.jpg"""
This gets imported as an int64
"These appear as ""image1"", ""image2"", etc."
"In the form ""001a"""
Can be in the form '111' or 's46'
"In the form ""7/29/2016 11:40"""
Check image height and width
"Use 'empty', to be consistent with other data on lila"
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
""
mcgill_to_json.py
""
Convert the .csv file provided for the McGill test data set to a
COCO-camera-traps .json file
""
%% Constants and environment
%% Read source data
"%% Map filenames to rows, verify image existence"
Create an additional column for concatenated filenames
Maps relative filenames to rows
"Build up a map from filenames to a list of rows, checking image existence as we go"
row = input_metadata.iloc[0]
"I didn't expect this to be true a priori, but it appears to be true, and"
it saves us the trouble of checking consistency across multiple occurrences
of an image.
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
row = input_metadata.iloc[0]
"Filenames look like ""290716114012001a1116.jpg"""
"In the form ""001a"""
Can be in the form '111' or 's46'
"In the form ""7/29/2016 11:40"""
Check image height and width
NaN is the only thing we should see that's not a string
NaN is the only thing we should see that's not a string
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
""
bellevue_to_json.py
""
"""Bellevue Camera Traps"" is the rather unremarkable camera trap data set"
used by one of the repo's maintainers for testing.  It's organized as:
""
approximate_date/[loose_camera_specifier/]/species
""
E.g.:
""
"""2018.03.30\coyote\DSCF0091.JPG"""
"""2018.07.18\oldcam\empty\DSCF0001.JPG"""
""
%% Constants and imports
from the ai4eutils repo
Filenames will be stored in the output .json relative to this base dir
%% Exif functions
"%% Enumerate files, create image/annotation/category info"
Force the empty category to be ID 0
Keep track of unique camera folders
Each element will be a dictionary with fields:
""
"relative_path, width, height, datetime"
fname = image_files[0]
Corrupt or not an image
Store file info
E.g. 2018.03.30/coyote/DSCF0091.JPG
...for each image file
%% Synthesize sequence information
Sort images by time within each folder
camera_path = camera_folders[0]
previous_datetime = sorted_images_this_camera[0]['datetime']
im = sorted_images_this_camera[1]
Start a new sequence if necessary
...for each image in this camera
...for each camera
Fill in seq_num_frames
%% A little cleanup
%% Write output .json
%% Sanity-check data
%% Label previews
""
snapshot_safar_importer_reprise.py
""
This is a 2023 update to snapshot_safari_importer.py.  We do a bunch of things now that
we didn't do the last time we imported Snapshot data (like updating the big taxonomy)
"file, and we skip a bunch of things now that we used to do (like generating massive"
"zipfiles).  So, new year, new importer."
""
%% Constants and imports
%% List files
"Do a one-time enumeration of the entire drive; this will take a long time,"
but will save a lot of hassle later.
%% Create derived lists
Takes about 60 seconds
CSV files are one of:
""
"_report_lila.csv (this is the one we want to use, with the species/count/etc. for each sequence)"
_report_lila_image_inventory.csv (maps captures to images)
_report_lila_overview.csv (distrubution of species)
%% List project folders
Project folders look like one of these:
""
APN
Snapshot Cameo/DEB
%% Map report and inventory files to codes
fn = csv_files[0]
%% Make sure that every report has a corresponding inventory file
%% Count species based on overview and report files
%% Print counts
%% Make sure that capture IDs in the reports/inventory files match
...and that all the images in the inventory tables are actually present on disk.
assert image_path_relative in all_files_relative_set
Make sure this isn't just a case issue
...for each report on this project
...for each project
"%% For all the files we have on disk, see which are and aren't in the inventory files"
"There aren't any capital-P .PNG files, but if I don't include that"
"in this list, I'll look at this in a year and wonder whether I forgot"
to include it.
fn = all_files_relative[0]
print('Skipping project {}'.format(project_code))
""
plot_wni_giraffes.py
""
Plot keypoints on a random sample of images from the wni-giraffes data set.
""
%% Constants and imports
%% Load and select data
%% Support functions
https://stackoverflow.com/questions/32504246/draw-ellipse-in-python-pil-with-line-thickness
Use a single channel image (mode='L') as mask.
The size of the mask can be increased relative to the imput image
to get smoother looking results.
draw outer shape in white (color) and inner shape in black (transparent)
downsample the mask using PIL.Image.LANCZOS
(a high-quality downsampling filter).
paste outline color to input image through the mask
%% Plot some images
ann = annotations_to_plot[0]
i_tool = 0; tool_name = short_tool_names[i_tool]
Don't plot tools that don't have a consensus annotation
...for each tool
...for each annotation
""
idfg_iwildcam_lila_prep.py
""
Adding class labels (from the private test .csv) to the iWildCam 2019 IDFG
"test set, in preparation for release on LILA."
""
This version works with the public iWildCam release images.
""
"%% ############ Take one, from iWildCam .json files ############"
%% Imports and constants
%% Read input files
Remove the header line
%% Parse annotations
Lines look like:
""
"b005e5b2-2c0b-11e9-bcad-06f1011196c4,1,Private"
%% Minor cleanup re: images
%% Create annotations
%% Prepare info
%% Minor adjustments to categories
Remove unused categories
Name adjustments
%% Create output
%% Write output
%% Validate .json file
%% Preview labels
viz_options.classes_to_exclude = ['test']
"%% ############ Take two, from pre-iWildCam .json files created from IDFG .csv files ############"
%% Imports and constants
%% One-time line break addition
%% Read input files
%% Prepare info
%% Minor adjustments to categories
%% Minor adjustments to annotations
%% Create output
%% Write output
%% Validate .json file
%% Preview labels
viz_options.classes_to_exclude = ['test']
""
missouri_to_json.py
""
Create .json files from the original source files for the Missouri Camera Traps
data set.  Metadata was provided here in two formats:
""
"1) In one subset of the data, folder names indicated species names.  In Set 1,"
there are no empty sequences.  Set 1 has a metadata file to indicate image-level
bounding boxes.
""
2) A subset of the data (overlapping with (1)) was annotated with bounding
"boxes, specified in a whitespace-delimited text file.  In set 2, there are"
"some sequences omitted from the metadata file, which implied emptiness."
""
"In the end, set 2 labels were not reliable enough to publish, so LILA includes only set 1."
""
%% Constants and imports
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Filenames will be stored in the output .json relative to this base dir
This will be a list of filenames that need re-annotation due to redundant boxes
"%% Enumerate files, read image sizes (both sets)"
"Takes a few minutes, since we're reading image sizes."
Each element will be a list of relative path/full path/width/height
"Only process leaf nodes corresponding to sequences, which look like:"
""
E:\wildlife_data\missouri_camera_traps\Set1\1.02-Agouti\SEQ75583
E:\wildlife_data\missouri_camera_traps\Set2\p1d101
""
assert len(files) <= 2
Read the image
Not an image...
Store file info
"...if we didn't hit the max file limit, keep going"
...for each file
%% Add sequence lengths (both sets)
%% Load the set 1 metadata file
"List of lists, length varies according to number of bounding boxes"
""
Preserves original ordering
iLine = 0; line = metadataSet1Lines[0]
"Lines should be filename, number of bounding boxes, boxes (four values per box)"
Make sure we have image info for this image
%% Print missing files from Set 1 metadata
Manual changes I made to the metadata file:
""
'IMG' --> 'IMG_'
Red_Brocket_Deer --> Red_Deer
European-Hare --> European_Hare
Wood-Mouse --> Wood_Mouse
Coiban-Agouti --> Coiban_Agouti
%% Load the set 2 metadata file
"This metadata file contains most (but not all) images, and a class label (person/animal/empty)"
"for each, plus bounding boxes."
"List of lists, length varies according to number of bounding boxes"
""
Preserves original ordering
"Create class IDs for each *sequence*, which we'll use to attach classes to"
images for which we don't have metadata
""
This only contains mappings for sequences that appear in the metadata.
iLine = 0; line = metadataSet2Lines[0]
"Lines should be filename, number of bounding boxes, labeled boxes (five values per box)"
""
Empty images look like filename\t0\t0
E.g. 'Set2\\p1d101\\p1d101s100i10.JPG'
Make sure we don't have mixed classes within an image
"Figure out what class this *sequence* is, so we know how to handle unlabeled"
images from this sequence
Can't un-do a mixed sequence
Previously-empty sequences get the image class label
"If the sequence has a non-empty class, possibly change it"
Make sure we have image info for this image
...for each line in the set 2 metadata file
%% What Set 2 images do I not have metadata for?
These are *mostly* empty images
iImage = 0; imageID = set2ImageIDs[iImage]
%% Create categories and annotations for set 1
"Though we have no empty sequences, we do have empty images in this set"
For each image
""
iImage = 0; imageID = set1ImageIDs[iImage]
E.g. Set1\\1.80-Coiban_Agouti\\SEQ83155\\SEQ83155_IMG_0010.JPG
Find the species name
This image may still be empty...
category['count'] = category['count'] + 1
"If we have bounding boxes, create image-level annotations"
"filename, number of bounding boxes, boxes (four values per box)"
"Make sure the relative filename matches, allowing for the fact that"
some of the filenames in the metadata aren't quite right
This image is non-empty
"Some redundant bounding boxes crept in, don't add them twice"
Check this bbox against previous bboxes
""
Inefficient?  Yes.  In an important way?  No.
"Bounding box values are in absolute coordinates, with the origin"
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1]."
""
"Convert to floats and to x/y/w/h, as per CCT standard"
...for each box
if we do/don't have boxes for this image
Else create a sequence-level annotation
...for each image
%% Write out the list of images with redundant boxes
%% Create categories and annotations for set 2
For each image
""
iImage = 0; imageID = set2ImageIDs[iImage]
E.g. 'Set2\\p1d100\\p1d100s10i1.JPG'
"Find the sequence ID, sanity check filename against what we stored"
"If we have bounding boxes or an explicit empty label, create image-level annotations"
"filename, number of bounding boxes, labeled boxes (five values per box)"
"Make sure the relative filename matches, allowing for the fact that"
some of the filenames in the metadata aren't quite right
"Bounding box values are in absolute coordinates, with the origin"
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1]."
""
Convert to floats and to x/y/w/h
...for each box
...if we do/don't have boxes for this image
Else create a sequence-level annotation
...for each image
"%% The 'count' field isn't really meaningful, delete it"
"It's really the count of image-level annotations, not total images assigned to a class"
%% Write output .json files
%% Sanity-check final set 1 .json file
"python sanity_check_json_db.py --bCheckImageSizes --baseDir ""E:\wildlife_data\missouri_camera_traps"" ""E:\wildlife_data\missouri_camera_traps\missouri_camera_traps_set1.json"""
%% Generate previews
""
filenames_to_json.py
""
Take a directory of images in which species labels are encoded by folder
"names, and produces a COCO-style .json file"
""
%% Constants and imports
from the ai4eutils repo
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Filenames will be stored in the output .json relative to this base dir
"rawClassListFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_classes.csv')"
"classMappingsFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_class_mapping.csv')"
"%% Enumerate files, read image sizes"
Each element will be a list of relative path/full path/width/height
Read the image
Corrupt or not an image
Store file info
Write to output file
...for each image file
...csv file output
...if the file list is/isn't available
%% Enumerate classes
Maps classes to counts
We like 'empty' to be class 0
%% Assemble dictionaries
...for each category
Each element is a list of relative path/full path/width/height/className
...for each image
%% External class mapping
%% Write raw class table
cat = categories[0]
%% Read the mapped class table
"id, source, count, target"
"%% Make classMappings contain *all* classes, not just remapped classes"
cat = categories[0]
%% Create new class list
"Start at 1, explicitly assign 0 to ""empty"""
One-off issue with character encoding
%% Re-map annotations
ann = annotations[0]
%% Write output .json
%% Utilities
%%
Find images with a particular tag
%% Randomly sample annotations
""
umn_to_json.py
""
Prepare images and metadata for the Orinoquía Camera Traps dataset.
""
%% Imports and constants
"For two deployments, we're only processing imagse in the ""detections"" subfolder"
%% Enumerate deployment folders
%% Load ground truth
i_row = 0; row = ground_truth_df.iloc[i_row]
%% Create relative paths for ground truth data
"Some deployment folders have no subfolders, e.g. this is a valid file name:"
""
M00/01010132.JPG
""
"But some deployment folders have subfolders, e.g. this is also a valid file name:"
""
N17/100EK113/07160020.JPG
""
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly"
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths."
deployment_name = list(deployment_folders)[0]
Enumerate all files in this folder
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))"
filename = files[100]
...for each file in this deployment
...for each deployment
%% Add relative paths to our ground truth table
i_row = 0; row = ground_truth_df.iloc[i_row]
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's"
"just the deployment ID and the image name, separated by a slash."
Find the relative path for this image
Make sure this image file exists
...for each row in the ground truth table
%% Take everything out of Pandas
%% Convert string timestamps to Python datetimes
im = ground_truth_dicts[0]
"Filenames look like, e.g., N36/100EK113/06040726.JPG"
%% Synthesize sequence information
Sort images by time within each location
i_location=0; location = locations[i_location]
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_camera[1]
"Timestamp for this image, may be None"
Start a new sequence if:
""
* This image has no timestamp
* This iamge has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each image in this location
...for each location
%% Create category dict and category IDs
im = images[0]
%% Imports and constants (.json generation)
%% Count frames in each sequence
%% Double check images with multiple annotations
im = images[0]
%% Assemble dictionaries
Force the empty category to be ID 0
input_im = images[0]
...for each image
%% Write output .json
%% Validate .json file
%% Map relative paths to annotation categories
ann = data['annotations'][0]
%% Copy images to output
EXCLUDE HUMAN AND MISSING
im = data['images'][0]
im = images[0]
%% Preview labels
viz_options.classes_to_exclude = ['test']
""
nacti_fieldname_adjustments.py
""
"NACTI metadata was posted with ""filename"" in images instead of ""file_name"", and"
"used string (rather than int) category IDs (in categories, but not in annotations)."
""
This script fixes those issues and rev's the version number.
""
%% Constants and environment
%% Read .json file
"%% Rev version number, update field names and types"
%% Write json file
""
cct_field_adjustments.py
""
CCT metadata was posted with int locations instead of strings.
""
This script fixes those issues and rev's the version number.
""
%% Constants and environment
%% Read .json file
"%% Rev version number, update field names and types"
%% Write json file
%% Check output data file
""
carrizo_shrubfree_2018.py
""
Convert the .csv file provided for the Carrizo Mojave data set to a
COCO-camera-traps .json file
""
%% Constants and environment
%% Read source data
Original .csv file had superfluous spaces in column names
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
58 missing files (of 17652)
%% Check for images that aren't included in the metadata file
3012 of 20606 files are not in metadata
%% Create CCT dictionaries
Map categories to integer IDs
""
The category '0' is reserved for 'empty'
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
Each filename should just match one row
Don't include images that don't exist on disk
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
""
snapshot_serengeti_lila.py
""
Create zipfiles of Snapshot Serengeti S1-S11.
""
"Create a metadata file for S1-S10, plus separate metadata files"
"for S1-S11.  At the time this code was written, S11 was under embargo."
""
Create zip archives of each season without humans.
""
Create a human zip archive.
""
%% Constants and imports
import sys; sys.path.append(r'c:\git\ai4eutils')
import sys; sys.path.append(r'c:\git\cameratraps')
assert(os.path.isdir(metadata_base))
"There are two redundant categories, and we re-map ""blank"" to ""empty"" as per CCT convention"
"%% Load metadata files, concatenate into a single table"
iSeason = 1
%% Convert to dictionaries (prep)
%% Convert to dictionaries (loops)
"TODO: iterrows() is a terrible way to do this, but this is one of those days"
"where I want to get this done, not get better at Python."
irow = 0; row = image_table.iloc[0]
"Loaded as an int64, converting to int here"
...for each row in the image table
Make sure image IDs are what we think they are
...for each row in the annotation table
%% Load previously-saved dictionaries when re-starting mid-script
%%
%% Take a look at categories (just sanity-checking)
%%
%% Fill in some image fields we didn't have when we created the image table
"width, height, corrupt, seq_num_frames, location, datetime"
Every annotation in this list should have the same sequence ID
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;"
there's no reason to do this redundantly for every image
Every image in this sequence should point back to the same equence
Every annotation in this list should have the same location
Every annotation in this list should have the same datetime
Is this image on disk?
iImage = 0; im = images[0]
...for each image
"images_processed = pool.map(process_image, images)"
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))"
%%
%% Count missing/corrupted images
%% Print distribution of sequence lengths (sanity-check)
%% Replicate annotations across images
iAnn = 0; ann = annotations[iAnn]
%% See what files are on disk but not annotated (~15 mins)
247370 files not in the database (of 7425810)
%% Load old image database
%% Look for old images not in the new DB and vice-versa
"At the time this was written, ""old"" was S1-S6"
old_im = cct_old['images'][0]
new_im = images[0]
4 old images not in new db
12 new images not in old db
%% Save our work
%% Load our work
%%
%% Examine size mismatches
i_mismatch = -1; old_im = size_mismatches[i_mismatch]
%% Sanity-check image and annotation uniqueness
"%% Split data by seasons, create master list for public seasons"
ann = annotations[0]
%% Minor updates to fields
"%% Write master .json out for S1-10, write individual season .jsons (including S11)"
"Loop over all seasons, plus one iteration for the ""all public data"" iteration, and"
"one for the ""all data"" iteration"
%% Find categories that only exist in S11
List of categories in each season
Category 55 (fire) only in S11
Category 56 (hyenabrown) only in S11
Category 57 (wilddog) only in S11
Category 58 (kudu) only in S11
Category 59 (pangolin) only in S11
Category 60 (lioncub) only in S11
%% Prepare season-specific .csv files
iSeason = 1
%% Create a list of human files
ann = annotations[0]
%% Save our work
%% Load our work
%%
"%% Create archives (human, per-season) (prep)"
im = images[0]
im = images[0]
Don't include humans
Only include files from this season
Possibly start a new archive
...for each image
i_season = 0
"for i_season in range(0,nSeasons):"
create_season_archive(i_season)
%% Create archives (loop)
pool = ThreadPool(nSeasons+1)
"n_images = pool.map(create_archive, range(-1,nSeasons))"
"seasons_to_zip = range(-1,nSeasons)"
...for each season
%% Sanity-check .json files
%logstart -o r'E:\snapshot_temp\python.txt'
%% Zip up .json and .csv files
pool = ThreadPool(len(files_to_zip))
"pool.map(zip_single_file, files_to_zip)"
%% Super-sanity-check that S11 info isn't leaking
im = data_public['images'][0]
ann = data_public['annotations'][0]
iRow = 0; row = annotation_df.iloc[iRow]
iRow = 0; row = image_df.iloc[iRow]
%% Create bounding box archive
i_image = 0; im = data['images'][0]
i_box = 0; boxann = bbox_data['annotations'][0]
%% Sanity-check a few files to make sure bounding boxes are still sensible
import sys; sys.path.append(r'C:\git\CameraTraps')
%% Check categories
%% Summary prep for LILA
""
wi_to_json
""
Prepares CCT-formatted metadata based on a Wildlife Insights data export.
""
"Mostly assumes you have the images also, for validation/QA."
""
%% Imports and constants
"Because WI filenames are GUIDs, it's not practical to page through sequences in an"
image viewer.  So we're going to (optionally) create a copy of the data set where
images are ordered.
%% Load ground truth
%% Take everything out of Pandas
%% Synthesize common names when they're not available
"Blank rows should always have ""Blank"" as the common name"
"print('Warning: missing common name for row {} ({})'.format(i_row,row['filename']))"
%% Convert string timestamps to Python datetimes
im = ground_truth_dicts[0]
"The field called ""location"" in the WI .csv file is a URL, we want to reclaim"
"the ""location"" keyword for CCT output"
"Filenames look like, e.g., N36/100EK113/06040726.JPG"
%% Synthesize sequence information
Sort images by time within each location
i_location=0; location = locations[i_location]
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_camera[1]
"Timestamp for this image, may be None"
Start a new sequence if:
""
* This image has no timestamp
* This iamge has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each image in this location
...for each location
%% Create category dict and category IDs
im = images[0]
%% Count frames in each sequence
%% Build relative paths
im = images[0]
Sample URL:
""
gs://project-asfasdfd/deployment/21444549/asdfasdfd-616a-4d10-a921-45ac456c568a.jpg'
%% Double check images with multiple annotations
im = images[0]
%% Assemble dictionaries
Force the empty category to be ID 0
input_im = images[0]
"print('Warning: image ID {} ({}) has multiple annotations'.format(im['id'],im['id'].replace('_','/')))"
...for each image
%% Write output .json
%% Validate .json file
%% Preview labels
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))"
%%
%% Create ordered dataset
"Because WI filenames are GUIDs, it's not practical to page through sequences in an"
image viewer.  So we're going to create a copy of the data set where images are
ordered.
im = images_out[0]; im
%% Create ordered .json
%% Copy files to their new locations
im = ordered_images[0]
im = data_ordered['images'][0]
%% Preview labels in the ordered dataset
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))"
%% Open an ordered filename from the unordered filename
""
rspb_to_json.py
""
Convert the .csv file provided for the RSPB data set to a
COCO-camera-traps .json file
""
%% Constants and environment
[location] is an obfuscation
%% Create info struct
%% Read source data
metadataTable.columns.values
""
"array(['Project', 'inDir', 'FileName', 'Station', 'Camera',"
"'StationCameraFileName', 'DateTimeOriginal', 'DateReadable',"
"'outDir', 'filename_new', 'fileExistsAlready', 'CopyStatus',"
"'Species'], dtype=object)"
We'll populate these later
keys should be lowercase
"%% Enumerate images, confirm filename uniqueness"
"%% Update metadata filenames to include site and camera folders, check existence"
""
Takes ~1min
iRow = 0; row = metadataTable.iloc[iRow]
There's a bug in the metadata; the 'camera' column isn't correct.
camera = row['Camera']
"These appear as, e.g., '3.22e12'"
camera = str(int(float(camera)))
Let's pull this out of the file name instead
""
Filenames look like one of the following:
""
A1__03224850850507__2015-11-28__10-45-04(1).JPG
Bayama2PH__C05__NA(NA).JPG
assert(os.path.isfile(fullPath))
metadataTable.iloc[iRow] = row
Re-assemble into an updated table
%% Check for images that aren't included in the metadata file
Enumerate all images
Write to a text file
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
iRow = 0; row = metadataTable.iloc[iRow]
A1__03224850850507__2015-11-28__10-45-04(1).JPG
'A1\\03224850850507\\A1__03224850850507__2015-11-28__10-45-04(1).JPG'
Not currently populated
"Often -1, sometimes a semi-meaningful int"
A1
03224850850507
"In variable form, but sometimes '28/11/2015 10:45'"
Check image height and width
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Write output
%% Check database integrity
%% Preview a few images to make sure labels were passed along sensibly
%% One-time processing step: copy images to a flat directory for annotation
%%
""
pc_to_json.py
""
Convert a particular collection of .csv files to CCT format.
""
%% Constants and environment
%% Read and concatenate source data
List files
"List of dataframes, one per .csv file; we'll concatenate later"
i_file = 87; fn = input_files[i_file]
Concatenate into a giant data frame
%% List files
%% Main loop over labels (prep)
Force the empty category to be ID 0
%% Main loop over labels (loop)
iRow = 0; row = input_metadata.iloc[iRow]
"ImageID,FileName,FilePath,SpeciesID,CommonName"
assert os.path.isfile(full_path)
Retrieve image width and height
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
%% See what's up with missing files
s = list(image_relative_paths)[0]
s = missing_files[0]
%% Check for images that aren't included in the metadata file
%% Sample the database
%%
Collect the images we want
%% Create info struct
%% Write output
%% Sanity-check the database's integrity
%% Render a bunch of images to make sure the labels got carried along correctly
options.classes_to_exclude = ['unlabeled']
%% Write out a list of files to annotate
""
save_the_elephants_survey_B.py
""
Convert the .csv file provided for the Save the Elephants Survey B data set to a
COCO-camera-traps .json file
""
%% Constants and environment
input_base = r'/mnt/blobfuse/wildlifeblobssc/ste_2019_08_drop'
output_base = r'/home/gramener/survey_b'
Handle all unstructured fields in the source data as extra fields in the annotations
"photo_type really should be an image property, but there are a few conflicts"
that forced me to handle it as an annotation proprerty
%% Read source data
"%% Map filenames to rows, verify image existence"
"%% Map filenames to rows, verify image existence"
Maps relative paths to row indices in input_metadata
"Build up a map from filenames to a list of rows, checking image existence as we go"
Ignore directories
%% Make sure the multiple-annotation cases make sense
%%
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
Force the empty category to be ID 0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image]
Example filename:
""
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG'
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG'
i_row = row_indices[0]
timestamp = row['Date']
There are a small handful of datetime mismatches across annotations
for the same image
assert im['datetime'] == timestamp
Special cases based on the 'photo type' field
Various spellings of 'community'
Have we seen this category before?
Create an annotation
fieldname = list(mapped_fields.keys())[0]
...for each row
...for each image
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
%% Scrap
%% Find unique photo types
""
ubc_to_json.py
""
Convert the .csv file provided for the UBC data set to a
COCO-camera-traps .json file
""
"Images were provided in eight folders, each of which contained a .csv"
file with annotations.  Those annotations came in two slightly different
"formats, the two formats corresponding to folders starting with ""SC_"" and"
otherwise.
""
%% Constants and environment
Map Excel column names - which vary a little across spreadsheets - to a common set of names
%% Enumerate images
Load from file if we've already enumerated
%% Create CCT dictionaries
Force the empty category to be ID 0
To simplify debugging of the loop below
#%% Create CCT dictionaries (loop)
#%%
Read source data for this folder
Rename columns
Folder name is the first two characters of the filename
""
Create relative path names from the filename itself
Folder name is the camera name
""
Create relative path names from camera name and filename
Which of our images are in the spreadsheet?
i_row = 0; fn = input_metadata['image_relative_path'][i_row]
#%% Check for images that aren't included in the metadata file
Find all the images in this folder
Which of these aren't in the spreadsheet?
#%% Create entries in CCT dictionaries
Only process images we have on disk
"This is redundant, but doing this for clarity, at basically no performance"
cost since we need to *read* the images below to check validity.
i_row = row_indices[0]
"These generally represent zero-byte images in this data set, don't try"
to find the very small handful that might be other kinds of failures we
might want to keep around.
print('Error opening image {}'.format(image_relative_path))
If we've seen this category before...
...make sure it used the same latin --> common mapping
""
"If the previous instance had no mapping, use the new one."
assert common_name == category['common_name']
Create an annotation
...for each annotation we found for this image
...for each image
...for each dataset
Print all of our species mappings
"%% Copy images for which we actually have annotations to a new folder, lowercase everything"
im = images[0]
%% Create info struct
"%% Convert image IDs to lowercase in annotations, tag as sequence level"
"While there isn't any sequence information, the nature of false positives"
"here leads me to believe the images were labeled at the sequence level, so"
we should trust labels more when positives are verified.  Overall false
positive rate looks to be between 1% and 5%.
%% Write output
%% Validate output
%% Preview labels
""
helena_to_cct.py
""
Convert the Helena Detections data set to a COCO-camera-traps .json file
""
%% Constants and environment
This is one time process
%% Create Filenames and timestamps mapping CSV
import pdb;pdb.set_trace()
%% To create CCT JSON for RSPB dataset
%% Read source data
Original Excel file had timestamp in different columns
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
%% Skipping this check because one image has multiple species
assert len(duplicate_rows) == 0
%% Check for images that aren't included in the metadata file
%% Create CCT dictionaries
Map categories to integer IDs
""
The category '0' is reserved for 'empty'
For each image
Don't include images that don't exist on disk
Some filenames will match to multiple rows
assert(len(rows) == 1)
iRow = rows[0]
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
ann['datetime'] = row['datetime']
ann['site'] = row['site']
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
%% Imports and constants
from github.com/microsoft/ai4eutils
from github.com/ecologize/CameraTraps
A list of files in the lilablobssc container for this data set
The raw detection files provided by NOAA
A version of the above with filename columns added
%% Read input .csv
%% Read list of files
%% Convert paths to full paths
i_row = 0; row = df.iloc[i_row]
assert ir_image_path in all_files
...for each row
%% Write results
"%% Load output file, just to be sure"
%% Render annotations on an image
i_image = 2004
%% Download the image
%% Find all the rows (detections) associated with this image
"as l,r,t,b"
%% Render the detections on the image(s)
In pixel coordinates
In pixel coordinates
%% Save images
%% Clean up
""
carrizo_trail_cam_2017.py
""
"Convert the .csv files provided for the ""Trail Cam Carrizo"" 2017 data set to"
a COCO-camera-traps .json file.
""
%% Constants and environment
%% Read source data
Removing the empty records
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
908 missing files (of 60562)
%% Check for images that aren't included in the metadata file
105329 of 164983 files are not in metadata
%% Create CCT dictionaries
Map categories to integer IDs
""
The category '0' is reserved for 'empty'
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
Each filename should just match one row
Don't include images that don't exist on disk
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
""
channel_islands_to_cct.py
""
Convert the Channel Islands data set to a COCO-camera-traps .json file
""
"Uses the command-line tool ExifTool (exiftool.org) to pull EXIF tags from images,"
"because every Python package we tried failed to pull the ""Maker Notes"" field properly."
""
"%% Imports, constants, paths"
# Imports ##
# Constants ##
# Paths ##
Confirm that exiftool is available
"assert which(exiftool_command_name) is not None, 'Could not locate the ExifTool executable'"
%% Load information from every .json file
"Ignore the sample file... actually, first make sure there is a sample file"
...and now ignore that sample file.
json_file = json_files[0]
ann = annotations[0]
...for each annotation in this file
...for each .json file
"%% Confirm URL uniqueness, handle redundant tags"
Have we already added this image?
"One .json file was basically duplicated, but as:"
""
Ellie_2016-2017 SC12.json
Ellie_2016-2017-SC12.json
"If the new image has no output, just leave the old one there"
"If the old image has no output, and the new one has output, default to the one with output"
Don't worry about the cases where someone tagged 'fox' and someone tagged 'fox_partial'
...for each image we've already added
...if this URL is/isn't in the list of URLs we've already processed
...for each image
%% Save progress
%%
%%
%% Download files (functions)
https://www.quickprogrammingtips.com/python/how-to-download-multiple-files-concurrently-in-python.html
"This is returned with a leading slash, remove it"
%% Download files (execution)
%% Read required fields from EXIF data (functions)
"-G means ""Print group name for each tag"", e.g. print:"
""
[File]          Bits Per Sample                 : 8
""
...instead of:
""
Bits Per Sample                 : 8
"If we don't get any EXIF information, this probably isn't an image"
line_raw = exif_lines[0]
"Split on the first occurrence of "":"""
Typically:
""
"'[MakerNotes]    Sequence                        ', '1 of 3']"
Not a typo; we are using serial number as a location
"If there are multiple timestamps, make sure they're *almost* the same"
"If there are multiple timestamps, make sure they're *almost* the same"
...for each line in the exiftool output
"This isn't directly related to the lack of maker notes, but it happens that files that are missing"
maker notes also happen to be missing EXIF date information
...process_exif()
"This is returned with a leading slash, remove it"
Ignore non-image files
%% Read EXIF data (execution)
ann = images[0]
%% Save progress
Use default=str to handle datetime objects
%%
%%
"Not deserializing datetimes yet, will do this if I actually need to run this"
%% Check for EXIF read errors
%% Remove junk
Ignore non-image files
%% Fill in some None values
"...so we can sort by datetime later, and let None's be sorted arbitrarily"
%% Find unique locations
%% Synthesize sequence information
Sort images by time within each location
i_location=0; location = locations[i_location]
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_camera[1]
"Timestamp for this image, may be None"
Start a new sequence if:
""
* This image has no timestamp
* This image has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each image in this location
...for each location
%% Count frames in each sequence
images_this_sequence = [im for im in images if im['seq_id'] == seq_id]
"%% Create output filenames for each image, store original filenames"
i_location = 0; location = locations[i_location]
i_image = 0; im = sorted_images_this_location[i_image]
%% Save progress
Use default=str to handle datetime objects
%%
%%
%% Copy images to their output files (functions)
%% Copy images to output files (execution)
%% Rename the main image list for consistency with other scripts
%% Create CCT dictionaries
Make sure this is really a box
Transform to CCT format
Force the empty category to be ID 0
i_image = 0; input_im = all_image_info[0]
"This issue only impacted one image that wasn't a real image, it was just a screenshot"
"showing ""no images available for this camera"""
Convert datetime if necessary
Process temperature if available
Read width and height if necessary
I don't know what this field is; confirming that it's always None
Process object and bbox
os.startfile(output_image_full_path)
"Zero is hard-coded as the empty category, but check to be safe"
"I can't figure out the 'index' field, but I'm not losing sleep about it"
assert input_annotation['index'] == 1+i_ann
"Some annotators (but not all) included ""_partial"" when animals were partially obscured"
"Annotators *mostly* used 'none', but sometimes 'empty'.  'empty' is CCT-correct."
If we've seen this category before...
If this is a new category...
...if this is an empty/non-empty annotation
Create an annotation
...for each annotation on this image
...for each image
%% Change *two* annotations on images that I discovered contains a human after running MDv4
%% Move human images
ann = annotations[0]
%% Count images by location
%% Write output
%% Validate output
%% Preview labels
viz_options.classes_to_exclude = [0]
""
ena24_to_json_2017.py
""
Convert the ENA24 data set to a COCO-camera-traps .json file
""
%% Constants and environment
Temporary folders for human and non-human images
Clean existing output folders/zipfiles
%% Support functions
%% Read source data
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
Check image height and width
"Each row is category, [box coordinates]"
"If there's just one row, loadtxt reads it as a 1d array; make it a 2d array"
with one row
Each row is a bounding box
Have we seen this category before?
Create an annotation
...for each bounding box
"This was here for debugging; nearly every instance is Human+Horse, Human+Vehicle,"
"or Human+Dog, but there is one Rabbit+Opossium, and a few Deer+Chicken!"
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Create ZIP files for human and non human
%% Validate output
%% Preview labels
%% Imports and constants
configurations and paths
%% Helper functions
"dest_path = copy(source_path, dest_folder)"
num_workers = multiprocessing.cpu_count()
pool = ThreadPool(num_workers)
"results = pool.starmap(_copy_unzip, zip(sources, itertools.repeat(dest_folder)))"
""
print('Waiting for processes to finish...')
pool.close()
pool.join()
sequential
%% Command-line driver
if the blob container is already mounted on the VM
or you can download them using the storage Python SDK
store storage account key in environment variable AZ_STORAGE_KEY
""
eMammal_helpers.py
""
Support functions for processing eMammal metadata
""
%% Constants and imports
%% Support functions
"pad to a total of 3 digits if < 1000, or 4 digits otherwise"
img_frame is a string from the xml tree
"length 4 frame order is returned as is, others are left padded to be 3 digit long"
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels
pad to a total of 4 digits
img_frame is a string from the xml tree
"length 4 frame order is returned as is, others are left padded to be 3 digit long"
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels
others column
summer day hours: 6am - 7pm
others day hours: 7am - 6pm
""
make_eMammal_json.py
""
"Produces the COCO-formatted json database for an eMammal dataset, i.e. a"
"collection of folders, each of which contains a deployment_manifest.xml file."
""
"In this process, each image needs to be loaded to size it."
""
"To add bounding box annotations to the resulting database, use"
add_annotations_to_eMammal_json.py.
""
%% Constants and imports
"Either add the eMammal directory to your path, or run from there"
os.chdir(r'd:\git\CameraTraps\database_tools\eMammal')
import warnings
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
warnings.filterwarnings('ignore')
Should we run the image size retrieval in parallel?
%% Support functions
%% Main loop (metadata processing; image sizes are retrieved later)
deployment = folders[0]
sequence = image_sequences[0]
get species info for this sequence
add each image's info to database
img = images[0]
"some manifests don't have the ImageOrder info, but the info is in the file name"
full_img_id has no frame info
""
frame number only used in requests to iMerit for ordering
...for each image
...for each sequence
...for each deployment
%% Get image sizes
"'tasks' is currently a list of 2-tuples, with each entry as [image dictionary,path]."
""
"Go through that and copy just the image dictionaries to 'db_images', adding size"
information to each entry.  Takes a couple hours.
opening each image seems too fast for this multi-threaded version to be faster than sequential code.
%% Assemble top-level dictionaries
%% Write out .json
""
make_full_SS_json.py
""
Create a COCO-camera-traps .json file for Snapshot Serengeti data from
the original .csv files provided on Dryad.
""
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no"
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated
Snapshot Safari preparation process.
""
%% Imports and constants
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays"
Count the number of images with multiple species
"%% Read image .csv file, format into a dictionary mapping images to capture events"
%% Create CCT-style .json
still need image width and height
...for each image
%% Write output files
""
make_per_season_SS_json.py
""
Create a COCO-camera-traps .json file for each Snapshot Serengeti season from
the original .csv files provided on Dryad.
""
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no"
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated
Snapshot Safari preparation process.
""
%% Imports and constants
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays"
"%% Read image .csv file, format into a dictionary mapping images to capture events"
%% Create CCT-style .json
still need image width and height
...for each image ID
%% Write output files
...for each season
""
generate_lila_per_image_labels.py
""
"Generate a .csv file with one row per annotation, containing full URLs to every"
"camera trap image on LILA, with taxonomically expanded labels."
""
"Typically there will be one row per image, though images with multiple annotations"
will have multiple rows.
""
"Some images may not physically exist, particularly images that are labeled as ""human""."
This script does not validate image URLs.
""
Does not include bounding box annotations.
""
%% Constants and imports
"We'll write images, metadata downloads, and temporary files here"
"Some datasets don't have ""sequence_level_annotation"" fields populated, but we know their"
annotation level
%% Download and parse the metadata file
To select an individual data set for debugging
%% Download and extract metadata for the datasets we're interested in
%% Load taxonomy data
"%% Build a dictionary that maps each [dataset,query] pair to the full taxonomic label set"
i_row = 0; row = taxonomy_df.iloc[i_row]
%% Process annotations for each dataset
ds_name = list(metadata_table.keys())[0]
"Go through annotations, marking each image with the categories that are present"
""
ann = annotations[0]
im = images[10]
This field name was only used for Caltech Camera Traps
raise ValueError('Suspicious date parsing result')
Special case we don't want to print a warning about
"Location, sequence, and image IDs are only guaranteed to be unique within"
"a dataset, so for the output .csv file, include both"
category_name = list(categories_this_image)[0]
Only print a warning the first time we see an unmapped label
...for each category that was applied at least once to this image
...for each image in this dataset
print('Warning: no date information available for this dataset')
print('Warning: no location information available for this dataset')
...for each dataset
...with open()
%% Read the .csv back
%% Do some post-hoc integrity checking
"-1 isn't *really* valid, but we use it sometimes for sequences of unknown length"
i_row = 0; row = df.iloc[i_row]
%% Preview constants
%% Choose images to download
ds_name = list(metadata_table.keys())[2]
Find all rows for this dataset
...for each dataset
%% Download images
i_image = 0; image = images_to_download[i_image]
%% Write preview HTML
im = images_to_download[0]
""
Common constants and functions related to LILA data management/retrieval.
""
%% Imports and constants
LILA camera trap master metadata file
"wildlife_insights_taxonomy_url = 'https://api.wildlifeinsights.org/api/v1/taxonomy?fields=class,order,family,genus,species,authority,taxonomyType,uniqueIdentifier,commonNameEnglish&page[size]={}'.format(wildlife_insights_page_size)"
from ai4eutils
%% Common functions
"We haven't implemented paging, make sure that's not an issue"
d['data'] is a list of items that look like:
Put the master metadata file in the same folder where we're putting images
Read lines from the master metadata file
Parse those lines into a table
Each line in this file is name/sas_url/json_url/[bbox_json_url]
Create a separate entry for bounding boxes if they exist
Unzip if necessary
""
get_lila_category_list.py
""
Generates a .json-formatted dictionary mapping each LILA dataset to all categories
"that exist for that dataset, with counts for the number of occurrences of each category"
"(the number of *annotations* for each category, not the number of *images*)."
""
"Also loads the taxonomy mapping file, to include scientific names for each category."
""
get_lila_category_counts counts the number of *images* for each category in each dataset.
""
%% Constants and imports
array to fill for output
"We'll write images, metadata downloads, and temporary files here"
Created by get_lila_category_list.py... contains counts for each category
%% Load category and taxonomy files
%% Map dataset names and category names to scientific names
i_row = 1; row = taxonomy_df.iloc[i_row]; row
%% Download and parse the metadata file
%% Download and extract metadata for the datasets we're interested in
%% Get category names and counts for each dataset
ds_name = 'NACTI'
Open the metadata file
Collect list of categories and mappings to category name
ann = annotations[0]
c = categories[0]
"Don't do taxonomy mapping for bbox data sets, which are sometimes just binary and are"
always redundant with the class-level data sets.
"As of right now, this is the only quirky case"
...for each dataset
%% Save dict
%% Print the results
ds_name = list(dataset_to_categories.keys())[0]
...for each dataset
""
download_lila_subset.py
""
"Example of how to download a list of files from LILA, e.g. all the files"
in a data set corresponding to a particular species.
""
%% Constants and imports
LILA camera trap master metadata file
"In this example, we're using the Missouri Camera Traps data set and the Caltech Camera Traps dataset"
All lower-case; we'll convert category names to lower-case when comparing
"We'll write images, metadata downloads, and temporary files here"
"We will demonstrate two approaches to downloading, one that loops over files"
"and downloads directly in Python, another that uses AzCopy."
""
AzCopy will generally be more performant and supports resuming if the
transfers are interrupted.  This script assumes that azcopy is on the system path.
Number of concurrent download threads (when not using AzCopy) (AzCopy does its
own magical parallelism)
%% Support functions
remove the leading '/'
%% Download and parse the metadata file
Put the master metadata file in the same folder where we're putting images
Read lines from the master metadata file
Parse those lines into a table
Each line in this file is name/base_url/json_url/[box_url]
%% Download and extract metadata for the datasets we're interested in
Unzip if necessary
...for each dataset of interest
%% List of files we're going to download (for all data sets)
"Flat list or URLS, for use with direct Python downloads"
For use with azcopy
This may or may not be a SAS URL
# Open the metadata file
# Build a list of image files (relative path names) that match the target species
Retrieve all the images that match that category
Retrieve image file names
Convert to URLs
...for each dataset
%% Download those image files
ds_name = 'Caltech Camera Traps'
ds_name = 'SWG Camera Traps'
We want to use the whole relative path for this script (relative to the base of the container)
"to build the output filename, to make sure that different data sets end up in different folders."
This may or may not be a SAS URL
For example:
""
caltech-unzipped/cct_images
swg-camera-traps
Check whether the URL includes a folder
E.g. caltech-unzipped
E.g. cct_images
E.g. swg-camera-traps
"Write out a list of files, and use the azcopy ""list-of-files"" option to download those files."
""
This azcopy feature is unofficially documented at:
""
https://github.com/Azure/azure-storage-azcopy/wiki/Listing-specific-files-to-transfer
""
import clipboard; clipboard.copy(cmd)
Loop over files
""
get_lila_category_counts.py
""
Count the number of images and bounding boxes with each label in one or more LILA datasets.
""
"This script doesn't write these counts out anywhere other than the console, it's just intended"
as a template for doing operations like this on LILA data.  get_lila_category_list.py writes
"information out to a .json file, but it counts *annotations*, not *images*, for each category."
""
%% Constants and imports
"If None, will use all datasets"
"We'll write images, metadata downloads, and temporary files here"
%% Download and parse the metadata file
%% Download and extract metadata for the datasets we're interested in
%% Count categories
ds_name = datasets_of_interest[0]
"Go through annotations, marking each image with the categories that are present"
""
ann = annotations[0]
Now go through images and count categories
im = images[0]
...for each dataset
%% Print the results
...for each dataset
""
add_url_to_database.py
""
"Adds a ""url"" field to a coco-camera-traps .json database, specifically to allow the db to"
be reviewed in the Visipedia annotation tool.
""
""
remove_corrupted_images_from_database.py
""
"Given a coco-camera-traps .json file, checks all images for TF-friendliness and generates"
a new .json file that only contains the non-corrupted images.
""
%% Imports and constants
import multiprocessing
"I leave this at an annoying low number, since by definition weird stuff will"
"be happening in the TF kernel, and it's useful to keep having content in the console."
%% Function definitions
"I sometimes pass in a list of images, sometimes a dict with a single"
element mapping a job ID to the list of images
"We're about to start a lot of TF sessions, and we don't want gobs"
of debugging information printing out for every session.
At some point we were creating a single session and looping over images
"within that session, but the only way I found to reliably not run out"
of GPU memory was to create a session per image and gc.collect() after
each session.
Map Image IDs to boolean (should I keep this image?)
"Convert to lists, append job numbers to the image lists"
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)"
Merge results
%% Interactive driver
%%
base_dir = r'D:\temp\snapshot_serengeti_tfrecord_generation'
Load annotations
Check for corruption
Write out only the uncorrupted data
%% Command-line driver
#######
""
integrity_check_json_db.py
""
"Does some integrity-checking and computes basic statistics on a db, specifically:"
""
* Verifies that required fields are present and have the right types
* Verifies that annotations refer to valid images
* Verifies that annotations refer to valid categories
"* Verifies that image, category, and annotation IDs are unique"
""
* Optionally checks file existence
""
* Finds un-annotated images
* Finds unused categories
""
* Prints a list of categories sorted by count
""
#######
%% Constants and environment
%% Functions
"If baseDir is non-empty, checks image existence"
This is used in a medium-hacky way to share modified options across threads
print('Image path {} does not exist'.format(filePath))
"#%% Read .json file if necessary, integrity-check fields"
info = data['info']
"#%% Build dictionaries, checking ID uniqueness and internal validity as we go"
Confirm that required fields are present
Confirm ID uniqueness
...for each category
image = images[0]
Confirm that required fields are present
Confirm ID uniqueness
We previously supported ints here; this should be strings now
"assert isinstance(image['location'], str) or isinstance(image['location'], int), \"
'Illegal image location type'
Are we checking for unused images?
Recursively enumerate images
print('Image {} is unused'.format(p))
Are we checking file existence and/or image size?
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)"
...for each image
Confirm that required fields are present
Confirm ID uniqueness
Confirm validity
...for each annotation
#%% Print statistics
Find un-annotated images and multi-annotation images
Find unused categories
Prints a list of categories sorted by count
""
https://stackoverflow.com/questions/72899/how-do-i-sort-a-list-of-dictionaries-by-a-value-of-the-dictionary
...def integrity_check_json_db()
%% Command-line driver
%% Interactive driver(s)
%%
Integrity-check .json files for LILA
options.iMaxNumImages = 10
""
combine_two_json_files.py
""
"Merges two coco-camera-traps .json files. In particular, categories are combined and re-numbered."
""
Combined Info
Combined Images
Combined Categories
## categories to merge
Combined Annotations
""
make_detection_db_for_viewing.py
""
"Given a .json file with ground truth bounding boxes, and a .p file containing detections for the same images,"
"creates a new .json file with separate classes for ground truth and detection, suitable for viewing in the Visipedia"
annotation tool.
""
%% Imports and constants
%% Main function
im_id_to_im = {im['id']:im for im in images}
make new categories to distinguish between ground truth and detections
"update all gt annotations to be class ""gt"""
collect all detections by image
keep any detection with score above det_thresh
"need to convert bbox from [x1,y1,x2,y2] to [x,y,w,h]"
"add ""info"" and ""licenses"" for annotation tools to function"
create new db
%% Command-line handling
""
analyze_json_database.py
""
Plots location/class/etc. distributions for classes in a coco-camera-traps .json file.
""
Currently includes some one-off code for specific species.
""
%% Constants and imports
%% Path configuration
%% Load source data
%% Build image/category dictionaries
%% Make plot of category distribution
%% make plots of location distribution
"plt.title('Number of images per location, by category')"
"plt.tight_layout(rect=[0,0,1,0.9])"
#make plot of images per season
%% Make plot of lions per location
%% Make plot of elephants per location
for loc in sorted_by_total[:25]:
"print('Location:' + loc[0] +', Lions: ' + str(len(loc_to_lion_ims[loc[0]])) + ', Elephants: ' + str(len(loc_to_elephant_ims[loc[0]])) + ', Total ims: ' + str(len(loc_to_ims[loc[0]])))"
""
subset_json_db.py
""
Select a subset of images (and associated annotations) from a .json file
in COCO Camera Traps format.
""
"To subset the .json files produced by our batch processing API, see"
subset_json_detector_output.py
""
Sample invocation:
""
"python subset_json_db.py ""E:\Statewide_wolf_container\idfg_20190409.json"" ""E:\Statewide_wolf_container\idfg_20190409_clearcreek.json"" ""clearcreek"" --ignore_case"
""
%% Constants and imports
%% Functions
Load the input file if necessary
Find images matching the query
Find annotations referring to those images
Write the output file if requested
%% Interactive driver
%%
%% Command-line driver
%% Constants and imports
%% Merge functions
i_input_dict = 0; input_dict = input_dicts[i_input_dict]
We will prepend an index to every ID to guarantee uniqueness
Map detection categories from the original data set into the merged data set
...for each category
Merge original image list into the merged data set
Create a unique ID
...for each image
Same for annotations
...for each annotation
"Merge info dicts, don't check completion time fields"
...for each dictionary
Convert merged image dictionaries to a sorted list
%% Driver
""
add_width_and_height_to_database.py
""
Grabs width and height from actual image files for a .json database that is missing w/h.
""
Originally used when we created a .json file for snapshot serengeti from .csv.
""
"This file converts the JSON output of the batch processing API to a pickle file, which can be used by the"
script ./make_classification_dataset.py
"Convert boxes from JSON   [x_min, y_min, width_of_box, height_of_box]"
"to PICKLE [ymin,  xmin,  ymax,         xmax]"
Write detections to file with pickle
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the"
script ./make_classification_dataset.py
Write detections to file with pickle
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the"
script ./make_classification_dataset.py
Assumes that the root of the CameraTrap repo is in the PYTHONPATH
Minimum threshold to put a detection into the output JSON file
Parameter check
Load detections from input
Load COCO style annotations
Build output JSON in format version 1.0
Adding the only known metadata info
"The pickle file does not contain category information, so we assume the default"
For each image with detections
for each detection
"Convert boxes from [ymin, xmin, ymax, xmax] format to"
"[x_min, y_min, width_of_box, height_of_box]"
Write output json
global flag for whether or not we encounter missing images
"- will only print ""missing image"" warning once"
TFRecords variables
1.3 for the cropping during test time and 1.3 for the context that the
CNN requires in the left-over image
Create output directories
Load COCO style annotations from the input dataset
"Get all categories, their names, and create updated ID for the json file"
Prepare the coco-style json files
Split the dataset by locations
Load detections
Write out COCO-style json files to the output directory
Write detections to file with pickle
## Preparations: get all the output tensors
For all images listed in the annotations file
Skip the image if it is annotated with more than one category
"Get ""old"" and ""new"" category ID and category name for this image."
Skip if in excluded categories.
get path to image
"If we already have detection results, we can use them"
Otherwise run detector and add detections to the collection
Only select detections with confidence larger than DETECTION_THRESHOLD
Skip image if no detection selected
whether it belongs to a training or testing location
Skip images that we do not have available right now
- this is useful for processing parts of large datasets
Load image
Run inference
"remove batch dimension, and convert from float32 to appropriate type"
convert normalized bbox coordinates to pixel coordinates
Pad the detected animal to a square box and additionally by
"PADDING_FACTOR. The result will be in crop_boxes. However, we need to make"
sure that its box coordinates are still within the image.
"for each bounding box, crop the image to the padded box and save it"
"Create the file path as it will appear in the annotation json,"
adding the box number if there are multiple boxes
"if the cropped file already exists, verify its size"
Add annotations to the appropriate json
js_keys = ['/'.join(im['file_name'].split('/')[1:])[:-4] for im in js['images']]
for tk in js_keys:
"assert np.isclose(1, np.sum(detections[tk]['detection_scores'] > 0.5))"
""
separate_detections_by_size
""
Not-super-well-maintained script to break a list of API output files up
based on bounding box size.
""
%% Imports and constants
Folder with one or more .json files in it that we want to split up
Enumerate .json files
Define size thresholds and confidence thresholds
"Not used directly in this script, but useful if we want to generate previews"
%% Split by size
For each size threshold...
For each file...
fn = input_files[0]
Just double-checking; we already filtered this out above
Don't reprocess .json files we generated with this script
Load the input file
For each image...
1.1 is the same as infinity here; no box can be bigger than a whole image
What's the smallest detection above threshold?
"[x_min, y_min, width_of_box, height_of_box]"
""
size = w * h
...for each detection
Which list do we put this image on?
...for each image in this file
Make sure the number of images adds up
Write out all files
...for each size threshold
...for each file
""
tile_images.py
""
Split a folder of images into tiles.  Preserves relative folder structure in a
"new output folder, with a/b/c/d.jpg becoming, e.g.:"
""
a/b/c/d_row_0_col_0.jpg
a/b/c/d_row_0_col_1.jpg
""
%% Imports and constants
from ai4eutils
%% Main function
TODO: parallelization
""
i_fn = 2; relative_fn = image_relative_paths[i_fn]
Can we skip this image because we've already generated all the tiles?
TODO: super-sloppy that I'm pasting this code from below
From:
""
https://github.com/whiplashoo/split-image/blob/main/src/split_image/split.py
i_col = 0; i_row = 1
left/top/right/bottom
...for each row
...for each column
...for each image
%% Interactive driver
%%
%%
""
rde_debug.py
""
Some useful cells for comparing the outputs of the repeat detection
"elimination process, specifically to make sure that after optimizations,"
results are the same up to ordering.
""
%% Compare two RDE files
i_dir = 0
break
"Regardless of ordering within a directory, we should have the same"
number of unique detections
Re-sort
Make sure that we have the same number of instances for each detection
Make sure the box values match
""
aggregate_video.py
""
Aggregate results and render output video for a video that's already been run through MD
""
%% Constants
%% Processing
im = d['images'][0]
...for each detection
This is no longer included in output files by default
# Split into frames
# Render output video
## Render detections to images
## Combine into a video
""
ocr_sandbox.py
""
sandbox for experimenting with using OCR to pull metadata from camera trap images
""
The general approach is:
""
"* Crop a fixed percentage from the top and bottom of an image, slightly larger"
than the largest examples we've seen of how much space is used for metadata.
""
"* Refine that crop by blurring a little, then looking for huge peaks in the"
"color histogram suggesting a solid background, then finding rows that are"
mostly that color.
""
"* Crop to the refined crop, then run pytesseract to extract text"
""
"* Use regular expressions to find time and date, in the future can add, e.g.,"
"temperature (which is often present *only* in the images, unlike time/date which"
are also usually in EXIF but often wrong or lost in processing)
""
"The metadata extraction (EXIF, IPTC) here is just sample code that seemed to"
belong in this file.
""
Contact: Dan Morris (cameratraps@lila.science)
""
%% Constants and imports
pip install pytesseract
""
"Also intall tesseract from: https://github.com/UB-Mannheim/tesseract/wiki, and add"
"the installation dir to your path (on Windows, typically C:\Program Files (x86)\Tesseract-OCR)"
pip install IPTCInfo3
from the ai4eutils repo: https://github.com/Microsoft/ai4eutils
""
"Only used for writing out a summary, not important for the core metadata extraction"
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Using a semi-arbitrary metric of how much it feels like we found the
"text-containing region, discard regions that appear to be extraction failures"
Pad each crop with a few pixels to make tesseract happy
Discard text from the top
"When we're looking for pixels that match the background color, allow some"
tolerance around the dominant color
We need to see a consistent color in at least this fraction of pixels in our rough
crop to believe that we actually found a candidate metadata region.
"What fraction of the [top,bottom] of the image should we use for our rough crop?"
A row is considered a probable metadata row if it contains at least this fraction
"of the background color.  This is used only to find the top and bottom of the crop area,"
"so it's not that *every* row needs to hit this criteria, only the rows that are generally"
above and below the text.
%% Support functions
"%% Load some images, pull EXIF and IPTC data for fun"
%% Rough crop
"This will be an nImages x 1 list of 2 x 1 lists (image top, image bottom)"
image = images[0]
"l,t,r,b"
""
"0,0 is upper-left"
"%% Close-crop around the text, return a revised image and success metric"
Did we find a sensible mode that looks like a background value?
"This looks very scientific, right?  Definitely a probability?"
"print('Failed min background fraction test: {} of {}'.format(pBackGroundValue,minBackgroundFraction))"
"Notes to self, things I tried that didn't really go anywhere..."
"analysisImage = cv2.blur(analysisImage, (3,3))"
"analysisImage = cv2.medianBlur(analysisImage,5)"
"analysisImage = cv2.Canny(analysisImage,100,100)"
imagePil = Image.fromarray(analysisImage); imagePil
Use row heuristics to refine the crop
""
This egregious block of code makes me miss my fluency in Matlab.
"print('Cropping to {},{},{},{}'.format(x,y,w,h))"
Crop the image
"For some reason, tesseract doesn't like characters really close to the edge"
imagePil = Image.fromarray(croppedImage); imagePil
%% Go to OCR-town
"An nImages x 2 list of strings, extracted from the top and bottom of each image"
An nImages x 2 list of cropped images
iImage = 0; iRegion = 1; regionSet = imageRegions[iImage]; region = regionSet[iRegion]
text = pytesseract.image_to_string(region)
pil --> cv2
"image = cv2.medianBlur(image, 3)"
"image = cv2.erode(image, None, iterations=2)"
"image = cv2.dilate(image, None, iterations=4)"
"image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]"
"image = cv2.blur(image, (3,3))"
"image = cv2.copyMakeBorder(image,10,10,10,10,cv2.BORDER_CONSTANT,value=[0,0,0])"
"text = pytesseract.image_to_string(imagePil, lang='eng')"
https://github.com/tesseract-ocr/tesseract/wiki/Command-Line-Usage
"psm 6: ""assume a single uniform block of text"""
""
...for each cropped region
...for each image
%% Extract dates and times
s = '1:22 pm'
s = '1:23:44 pm'
%% Write results to a handy html file
Add image name and resized image
Add results and individual region images
"textStyle = ""font-family:calibri,verdana,arial;font-weight:bold;font-size:150%;text-align:left;margin-left:50px;"""
%% Scrap
Alternative approaches to finding the text/background  region
Using findCountours()
imagePil = Image.fromarray(analysisImage); imagePil
"analysisImage = cv2.erode(analysisImage, None, iterations=3)"
"analysisImage = cv2.dilate(analysisImage, None, iterations=3)"
"analysisImage = cv2.threshold(analysisImage, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]"
Find object with the biggest bounding box
Using connectedComponents()
analysisImage = image
print('Found {} components'.format(nb_components))
We just want the *background* image
""
"Takes a .json file with MD results for an individual video, and converts to a .csv that includes"
frame times.  This is very bespoke to animal detection and does not include other classes.
""
%% Imports and constants
Only necessary if you want to extract the sample rate from the video
%% Extract the sample rate if necessary
%% Load results
%% Convert to .csv
i_image = 0; im = results['images'][i_image]
""
umn-pr-analysis.py
""
Precision/recall analysis for UMN data
""
%% Imports and constants
results_file = results_file_filtered
"For two deployments, we're only processing imagse in the ""detections"" subfolder"
String to remove from MegaDetector results
%% Enumerate deployment folders
%% Load MD results
im = md_results['images'][0]
%% Load ground truth
i_row = 0; row = ground_truth_df.iloc[i_row]
%% Create relative paths for ground truth data
"Some deployment folders have no subfolders, e.g. this is a valid file name:"
""
M00/01010132.JPG
""
"But some deployment folders have subfolders, e.g. this is also a valid file name:"
""
N17/100EK113/07160020.JPG
""
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly"
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths."
deployment_name = list(deployment_folders)[0]
Enumerate all files in this folder
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))"
filename = files[100]
...for each file in this deployment
...for each deployment
%% Map relative paths to MD results
%% Add relative paths to our ground truth table
i_row = 0; row = ground_truth_df.iloc[i_row]
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's"
"just the deployment ID and the image name, separated by a slash."
Find the relative path for this image
Make sure we have MegaDetector results for this file
Make sure this image file exists
...for each row in the ground truth table
%% Take everything out of Pandas
%% Some additional error-checking of the ground truth
An early version of the data required consistency between common_name and is_blank
%% Combine MD and ground truth results
d = ground_truth_dicts[0]
Find the maximum confidence for each category
...for each detection
...for each image
%% Precision/recall analysis
...for each image
Thresholds go up throughout precisions/recalls/thresholds; find the last
value where recall is at or above target.  That's our precision @ target recall.
Flatten the confusion matrix
Write precision/recall plot to .png file in output directory
pr_figure_relative_filename = 'prec_recall.png'
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)"
plt.show(block=False)
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')"
%% Find and manually review all images of humans
%%
"...if this image is annotated as ""human"""
...for each image
%% Find and manually review all MegaDetector animal misses
%%
im = merged_images[0]
GT says this is not an animal
GT says this is an animal
%% Convert .json to .csv
%%
""
kga-pr-analysis.py
""
Precision/recall analysis for KGA data
""
%% Imports and constants
%% Load and filter MD results
%% Load and filter ground truth
%% Map images to image-level results
%% Map sequence IDs to images and annotations to images
Verify consistency of annotation within a sequence
TODO
%% Find max confidence values for each category for each sequence
seq_id = list(sequence_id_to_images.keys())[1000]
im = images_this_sequence[0]
det = md_result['detections'][]
...for each detection
...for each image in this sequence
...for each sequence
%% Prepare for precision/recall analysis
seq_id = list(sequence_id_to_images.keys())[1000]
cat_id = list(category_ids_this_sequence)[0]
...for each category in this sequence
...for each sequence
%% Precision/recall analysis
"Confirm that thresholds are increasing, recall is decreasing"
This is not necessarily true
assert np.all(precisions[:-1] <= precisions[1:])
Thresholds go up throughout precisions/recalls/thresholds; find the max
value where recall is at or above target.  That's our precision @ target recall.
"This is very slightly optimistic in its handling of non-monotonic recall curves,"
but is an easy scheme to deal with.
Flatten the confusion matrix
Write precision/recall plot to .png file in output directory
pr_figure_relative_filename = 'prec_recall.png'
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)"
plt.show(block=False)
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')"
%% Scrap
%% Find and manually review all sequence-level MegaDetector animal misses
input_base = '/media/user/lila-01/lila/snapshot-safari/KGA/KGA_public'
i_sequence = 0; seq_id = pr_sequence_ids[i_sequence]
i_seq = 0; seq_id = false_negative_sequences[i_seq]
"sequence_folder = os.path.join(sequence_preview_dir,'seq_{}'.format(str(i_seq).zfill(3)))"
fn = image_files[0]
"print('Copying {} to {}'.format(input_path,output_path))"
...for each file in this sequence.
...for each sequence
%% Image-level postprocessing
parse arguments
check if a GPU is available
load a pretrained embedding model
setup experiment
load the embedding model
setup the target dataset
setup finetuning criterion
setup an active learning environment
create a classifier
the main active learning loop
Active Learning
finetune the embedding model and load new embedding values
gather labeled pool and train the classifier
save a snapshot
Load a checkpoint if necessary
setup the training dataset and the validation dataset
setup data loaders
check if a GPU is available
create a model
setup loss criterion
define optimizer
load a checkpoint if provided
setup a deep learning engine and start running
train the model
train for one epoch
evaluate on validation set
save a checkpoint
open the file
read it
do the substitution
matplotlib.use('Agg')
from UIComponents.DBObjects import *
Initialize Database
# database connection credentials
# try to connect as USER to database DB_NAME through peewee
Load the saved embedding model
dataset_query = Detection.select().limit(5)
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})"
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getalllabels()) , dataset.getallpaths(), {})"
Random examples to start
"random_ids = np.random.choice(dataset.current_set, 1000, replace=False).tolist()"
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)"
print(random_ids)
Move Records
"moveRecords(dataset, DetectionKind.ModelDetection.value, DetectionKind.UserDetection.value, random_ids)"
#print([len(x) for x in dataset.set_indices])
# Finetune the embedding model
#dataset.set_kind(DetectionKind.UserDetection.value)
#dataset.train()
"#train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)"
"#finetune_embedding(model, checkpoint['loss_type'], dataset, 32, 4, 100)"
#save_checkpoint({
"#        'arch': model.arch,"
"#        'state_dict': model.state_dict(),"
"#        'optimizer' : optimizer.state_dict(),"
"#        'loss_type' : loss_type,"
"#        }, False, ""%s%s_%s_%04d.tar""%('finetuned', loss_type, model.arch, len(dataset.set_indices[DetectionKind.UserDetection.value])))"
Get indices of samples to get user to label
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
"kwargs[""already_selected""].extend(indices)"
Train on samples that have been labeled so far
Test on the samples that have not been labeled
"'optimizer' : optimizer.state_dict(),"
num_classes= len(train_dataset.getClassesInfo()[0])
"print(""Num Classes= ""+str(num_classes))"
define loss function (criterion) and optimizer
"optimizer = torch.optim.SGD(params, momentum = 0.9, lr = args.lr, weight_decay = args.weight_decay)"
optimizer.load_state_dict(checkpoint['optimizer'])
train for one epoch
"adjust_lr(optimizer,epoch)"
if epoch % 1 == 0 and epoch > 0:
"a, b, c = e.predict(train_embd_loader, load_info = True, dim = args.feat_dim)"
"plot_embedding(reduce_dimensionality(a), b, c, {})"
evaluate on validation set
--------some stuff needed to get AJAX to work with bottle?--------#
-------------------------------------------------------------------------------- #
PREPARE TO QUEUE IMAGES FOR LABELING
-------------------------------------------------------------------------------- #
# Connect as USER to database DB_NAME through peewee and initialize database proxy
# Load embedding model
---------------------------------------------------------------------- #
CREATE QUEUE OF IMAGES TO LABEL
---------------------------------------------------------------------- #
Use classifier to generate predictions
# Update model predicted class in PostgreSQL database
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
-------------------------------------------------------------------------------- #
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI
-------------------------------------------------------------------------------- #
"# static routes (to serve CSS, etc.)"
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps"
"return bottle.static_file(filename, root='../../../../../../../../../../../.')"
# dynamic routes
if data['display_grayscale']:
indices_to_exclude.update(set(color_indices))
elif not data['display_grayscale']:
indices_to_exclude.update(set(grayscale_indices))
data['display_images'] = {}
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices]
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices]
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices]
data['display_images']['detection_categories'] = []
for i in indices:
if str(dataset.samples[i][1]) == 'None':
data['display_images']['detection_categories'].append('None')
else:
existing_category_entries = {cat.id: cat.name for cat in Category.select()}
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()"
data['display_images']['detection_categories'].append(cat_name)
Use image ids in images_to_label to get the corresponding dataset indices
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
print(set(dataset.set_indices[4]).update(set(indices_to_label)))
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Use image id images_to_label to get the corresponding dataset index
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Train on samples that have been labeled so far
dataset.set_kind(DetectionKind.UserDetection.value)
print(y_train)
Predict on the samples that have not been labeled
print(y_pred)
Update model predicted class in PostgreSQL database
timer = time.time()
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
print('Updating the database took %0.2f seconds'%(time.time() - timer))
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
"once the classifier has been trained the first time, switch to AL sampling"
# sourceMappingURL=bootstrap.bundle.min.js.map
"print(ap_distances.size(),an_distances.size())"
losses = -(((-ap_distances)/128)+1+1e-16).log() - (((-(128-an_distances))/128)+1+1e-16).log()
import pdb
pdb.set_trace()
losses = ap_distances - an_distances + self.margin
print(losses.size())
from UIComponents.DBObjects import *
TODO: should this also change self.kind?
get the embedding representations for all samples (i.e. set current_set to all indices)
"return PILImage.open(os.path.join(self.img_base,path)).convert('RGB')"
"print(self.labels_set, self.n_classes)"
from sklearn.manifold import TSNE
embedding= TSNE(n_components=2).fit_transform(X)
embedding= PCA(n_components=2).fit_transform(X)
return X
"print(dir(event), type(sc))"
"print(label,bgcolor)"
"patches.append(mpatches.Patch(color=indexcolors[i], label=label))"
"plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,"
"ncol=12, mode=""expand"", borderaxespad=0., handles=patches)"
plt.legend(handles=patches)
"plt.xlabel('Dim 1', fontsize=12)"
"plt.ylabel('Dim 2', fontsize=12)"
plt.grid(True)
"plt.xlabel('Dim 1', fontsize=12)"
"plt.ylabel('Dim 2', fontsize=12)"
plt.grid(True)
print(thumb)
"img.thumbnail((16, 12), PILImage.ANTIALIAS)"
plt.show()
Add all negatives for all positive pairs
print(triplets.shape[0])
compute output
measure accuracy
compute loss on this batch
"train on a batch, record loss, and measure accuracy (if calc_accuracy)"
compute output
measure accuracy and record loss
switch to evaluate mode
compute output
switch to evaluate mode
"self.fc13 = nn.Linear(128, 64)"
self.bn2 = nn.BatchNorm1d(64)
x = F.relu(self.fc12(x))
x = F.relu(self.bn1(self.fc13(x)))
x = F.relu(self.fc13(x))
"x = F.dropout(x, training=self.training)"
save features last FC layer
x = F.relu(x)
save features last FC layer
import matplotlib.pyplot as plt
plt.switch_backend('agg')
class EmbeddingNet(nn.Module):
"def __init__(self, architecture, feat_dim, use_pretrained=False):"
"super(EmbeddingNet, self).__init__()"
self.feat_dim= feat_dim
self.inner_model = models.__dict__[architecture](pretrained=use_pretrained)
if architecture.startswith('resnet'):
in_feats= self.inner_model.fc.in_features
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)"
elif architecture.startswith('inception'):
in_feats= self.inner_model.fc.in_features
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)"
if architecture.startswith('densenet'):
in_feats= self.inner_model.classifier.in_features
"self.inner_model.classifier = nn.Linear(in_feats, feat_dim)"
if architecture.startswith('vgg'):
in_feats= self.inner_model.classifier._modules['6'].in_features
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)"
if architecture.startswith('alexnet'):
in_feats= self.inner_model.classifier._modules['6'].in_features
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)"
"def forward(self, x):"
return self.inner_model.forward(x)
class NormalizedEmbeddingNet(EmbeddingNet):
"def __init__(self, architecture, feat_dim, use_pretrained=False):"
"EmbeddingNet.__init__(self, architecture, feat_dim, use_pretrained = use_pretrained)"
"def forward(self, x):"
embedding =  F.normalize(self.inner_model.forward(x))*10.0
"return embedding, embedding"
"def get_random_images(num, image_dir, test_transforms):"
"data = datasets.ImageFolder(image_dir, transform=test_transforms) # slight abuse; this expects subfolders corresponding to classes but we have no classes here"
indices = list(range(len(data)))
np.random.shuffle(indices)
idx = indices[:num]
from torch.utils.data.sampler import SubsetRandomSampler
sampler = SubsetRandomSampler(idx)
"loader = torch.utils.data.DataLoader(data,"
"sampler=sampler, batch_size=num)"
dataiter = iter(loader)
"images, labels = dataiter.next()"
"return images, labels"
"def predict_image(image, model, test_transforms):"
"device = torch.device(""cuda"" if torch.cuda.is_available()"
"else ""cpu"")"
image_tensor = test_transforms(image).float()
image_tensor = image_tensor.unsqueeze_(0)
input = Variable(image_tensor)
input = input.to(device)
output = model(input)[0]
return output.data.cpu().numpy()
Connect to database and initialize db_proxy
# database connection credentials
# load the dataset
Load the saved embedding model from the checkpoint
# update the dataset embedding
# Create a folder for saving embedding visualizations with this model checkpoint
model_emb_dirname = os.path.basename(args.base_model).split('.')[0]
"os.makedirs(model_emb_dirname, exist_ok=True)"
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')"
dataset.embedding_mode()
"assert 2==3, 'break'"
datasetindices = list(range(len(dataset)))
np.random.shuffle(datasetindices)
random_indices = datasetindices[:args.num]
print(random_indices)
"selected_sample_features = np.array([]).reshape(0, 256)"
selected_sample_labels = []
for idx in random_indices:
"selected_sample_features = np.vstack([selected_sample_features, X_train[idx]])"
selected_sample_labels.append(y_train[idx])
img_path = imagepaths[idx].split('.JPG')[0]
image = dataset.loader(img_path)
selected_sample_images.append(image)
# TRY NEAREST NEIGHBORS WALK THROUGH EMBEDDING
nbrs = NearestNeighbors(n_neighbors=args.num).fit(selected_sample_features)
"distances, indices = nbrs.kneighbors(selected_sample_features)"
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')"
"idx_w_closest_nbr = np.where(distances[:,1] == min(distances[:,1]))[0][0]"
order = [idx_w_closest_nbr]
for ii in range(len(distances)):
"distances[ii, 0] = np.inf"
while len(order)<args.num:
curr_idx = order[-1]
curr_neighbors = indices[curr_idx]
curr_dists = list(distances[curr_idx])
# print(min(curr_dists))
next_closest_pos = curr_dists.index(min(curr_dists))
next_closest = curr_neighbors[next_closest_pos]
order.append(next_closest)
# make sure you can't revisit past nodes
for vi in order:
vi_pos = list(indices[next_closest]).index(vi)
"distances[next_closest, vi_pos] = np.inf"
for ii in range(len(order)):
imgidx = order[ii]
image = selected_sample_images[imgidx]
"image.save(model_emb_dirname+""/img""+str(ii)+""_""+str(selected_sample_labels[imgidx])+"".png"")"
# Specify the transformations on the input images before inference
"# test_transforms = transforms.Compose([transforms.Resize([224, 224]), transforms.ToTensor()])"
"test_transforms = transforms.Compose([transforms.Resize([256, 256]), transforms.RandomCrop([224, 224]), transforms.RandomHorizontalFlip(), transforms.ColorJitter(), transforms.ToTensor(), transforms.Normalize([0.407328, 0.407328, 0.407328], [0.118641, 0.118641, 0.118641])])"
"images, labels = get_random_images(num, image_dir, test_transforms)"
"all_features = np.array([]).reshape(0, 256)"
for ii in range(len(images)):
image = to_pil(images[ii])
"features = predict_image(image, model, test_transforms)"
"all_features = np.vstack([all_features, features])"
# for ii in range(len(images)):
#     image = to_pil(images[ii])
"#     image.save(""img""+str(ii)+"".png"")"
# TRY CLUSTERING
kmeans1 = KMeans(n_clusters=5).fit(StandardScaler().fit_transform(all_features))
print(kmeans1.labels_)
for ii in range(len(images)):
image = to_pil(images[ii])
"filename = str(kmeans1.labels_[ii])+""/img""+str(ii)+"".png"""
if not os.path.exists(os.path.dirname(filename)):
os.makedirs(os.path.dirname(filename))
image.save(filename)
class SaveFeatures():
"def __init__(self, module):"
self.hook = module.register_forward_hook(self.hook_fn)
"def hook_fn(self, module, input, output):"
"self.features = torch.tensor(output, requires_grad=True).cuda()"
def close(self):
self.hook.remove()
Load the saved embedding model from the checkpoint
"Get a sample from the database, with eval transforms applied, etc."
Connect to database and sample a dataset
output = model.forward(sample_image.unsqueeze(0))
print(output)
with torch.no_grad():
sample_image_input = sample_image.cuda(non_blocking=True)
"_, output = model(sample_image_input) # compute output"
print(output)
sample_image = PILImage.open(sample_image_path).convert('RGB')
"sample_image = transforms.Compose([Resize([256, 256]), CenterCrop(([[224,224]])), ToTensor(), Normalize([0.369875, 0.388726, 0.347536], [0.136821, 0.143952, 0.145229])])(sample_image)"
print(list(model_inner_resnet.children()))
print(model_inner_resnet.fc)
print(model_inner_resnet.fc0)
# print(model_inner_resnet.layer4[0].conv2)
# print(type(model))
# print(len(list(model_inner_resnet.children())))
# print(list(model.children()))
# print(list(list(model.children())[0].children()))
"img = np.uint8(np.random.uniform(150, 180, (56, 56, 3)))/255"
"img_tensor = torch.unsqueeze(torch.from_numpy(img), 0)"
full_out = model_inner_resnet.forward(img_tensor)
print(full_out)
model(img_tensor)
activations = SaveFeatures(model_inner_resnet.layer4[0].conv2)
print(activations.features)
print(type(activations.features))
activations.close()
Connect to database and sample a dataset
Load the saved embedding model from the checkpoint
Update the dataset embedding
Get a random query image
# # # IMAGES IN THE SAME SEQUENCE # # # #
"assert 2==3, 'break'"
# # # CLOSEST IN (EMBEDDING) FEATURE SPACE # # # #
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Variables for the hierarchical cluster
Variables for the AL algorithm
Data variables
"connectivity = kneighbors_graph(self.transformed_X,max_features)"
Fit cluster and update cluster variables
The sklearn hierarchical clustering algo numbers leaves which correspond
to actual datapoints 0 to n_points - 1 and all internal nodes have
ids greater than n_points - 1 with the root having the highest node id
"If no labels have been observed, simply return uniform distribution"
"If no observations, return worst possible upper lower bounds"
Loop through generations from bottom to top
Update admissible labels for node
Calculate score
Determine if node should be split
Make sure label set for node so that we can flow to children
if necessary
Only split if all ancestors are admissible nodes
This is part  of definition of admissible pruning
Check that pruning covers all leave nodes
Fill in labels
Observe labels for previously recommended batches
TODO(lishal): implement multiple selection methods
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
A list of initialized samplers is allowed as an input because
"for AL_methods that search over different mixtures, may want mixtures to"
have shared AL_methods so that initialization is only performed once for
computation intensive methods like HierarchicalClusteringAL and
states are shared between mixtures.
"If initialized samplers are not provided, initialize them ourselves."
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copy these objects to make sure they are not modified while simulating
trajectories as they are used later by the main run_experiment script.
Assumes that model has already by fit using all labeled data so
the probabilities can be used immediately to hallucinate labels
All models need to have predict_proba method
Hallucinate labels for selected datapoints to be label
using class probabilities from model
"Not saving already_selected here, if saving then should sort"
only for the input to fit but preserve ordering of indices in
already_selected
Useful to know how accuracy compares for model trained on hallucinated
labels vs trained on true labels.  But can remove this train to speed
up simulations.  Won't speed up significantly since many more models
are being trained inside the loop above.
Save trajectory for reference
Delete created copies
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER
Should check model but kernel_svm does not have coef_ so need better
handling here
Make sure that model object fed in did not change during simulations
Return indices based on return type specified
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Set gamma for gaussian kernel to be equal to 1/n_features
kneighbors graph is constructed using k=10
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of"
"another point, make it vice versa"
Graph edges are weighted by applying gaussian kernel to manhattan dist.
"By default, gamma for rbf kernel is equal to 1/n_features but may"
get better results if gamma is tuned.
Define graph density for an observation to be sum of weights for all
edges to the node representing the datapoint.  Normalize sum weights
by total number of neighbors.
"If a neighbor has already been sampled, reduce the graph density"
for its direct neighbors to promote diversity.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update observed reward and arm probabilities
Sample an arm
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
y only used for determining how many clusters there should be
probably not practical to assume we know # of classes before hand
should also probably scale with dimensionality of data
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
TODO(lishal): have MarginSampler and this share margin function
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update min_distances for all examples given new cluster center.
Assumes that the transform function takes in original data and not
flattened data.
Initialize centers with a randomly selected datapoint
New examples should not be in already selected since those points
should have min_distance of zero to a cluster center.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
This is uniform given the remaining pool but biased wrt the entire pool.
sample = [i for i in range(self.X.shape[0]) if i not in already_selected]
return sample[0:N]
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Fields for hierarchical clustering AL
Setting parent and storing nodes in dict for fast access
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Connect to database DB_NAME as USER and initialize tables
Populate Info table
Populate Category table
"# For now, we have a predefined list of species we expect to see in the camera trap database (e.g. maybe from a quick look through the images)"
# TODO: allow user to update the class list through the labeling tool UI as they see different species
Populate Image and Detection tables
Image entry data
Detection entry data
Connect to database DB_NAME as USER and initialize tables
Populate Info table
Populate Category table
# Create category records from species present in the COCO camera trap classification dataset (COCO classes minus any excluded classes)
# TODO: allow user to update the class list through the labeling tool UI as they see different species
Populate Image and Detection tables
"with open(os.path.join(args.crop_dir,'crops.json'), 'r') as infile:"
crops_json = json.load(infile)
counter = 0
timer = time.time()
num_detections = len(crops_json)
for detectionid in crops_json:
counter += 1
detection_data = crops_json[detectionid]
# Image entry data
existing_image_entries = Image.select().where((Image.file_name == detection_data['file_name']))
try:
existing_image_entry = existing_image_entries.get()
except:
"image_entry = Image.create(id=detectionid, file_name=detection_data['file_name'], width=detection_data['width'], height=detection_data['height'], grayscale=detection_data['grayscale'],"
"source_file_name=detection_data['source_file_name'], relative_size=detection_data['relative_size'],"
"seq_id=detection_data['seq_id'], seq_num_frames=detection_data['seq_num_frames'], frame_num=detection_data['frame_num'])"
image_entry.save()
# Detection entry data
"detection_entry = Detection.create(id=detectionid, image=detectionid, bbox_confidence=detection_data['bbox_confidence'],"
"bbox_X1=detection_data['bbox_X1'], bbox_Y1=detection_data['bbox_Y1'], bbox_X2=detection_data['bbox_X2'], bbox_Y2=detection_data['bbox_Y2'],"
kind=DetectionKind.ModelDetection.value)
detection_entry.save()
if counter%100 == 0:
"print('Updated database with Image and Detection table entries for %d out of %d crops in %0.2f seconds'%(counter, num_detections, time.time() - timer))"
# data related to original image
Get class names from .txt list
Initialize Oracle table
Map filenames to classes (NOTE: we assume a single image does not contain more than one class)
"For each detection, use source image path to get class"
TODO update: Assumes that crops have already
been generated for the images using make_active_learning_classification_dataset.py. The created DB contains tables:
- info: information about the dataset
- image: images present in the dataset
- detections: crops of images with detections with confidence greater than a specified threshold
Initialize Database
# database connection credentials
HOST = 'localhost'
PORT = 5432
"# first, make sure the (user, password) has been created"
"# sudo -u postgres psql -c ""CREATE USER <db_user> WITH PASSWORD <db_password>;"""
"# sudo -u postgres psql -c ""CREATE DATABASE <db_name> WITH OWNER <db_user> CONNECTION LIMIT -1;"""
"# sudo -u postgres psql -c ""GRANT CONNECT ON DATABASE <db_name> TO <db_user>;"""
"# sudo -u postgres psql -d <db_name> -c ""CREATE EXTENSION IF NOT EXISTS \""uuid-ossp\"";"""
# Try to connect as USER to database DB_NAME through peewee
Populate Tables
# create Info table
# get class names for Category table
Faster anD available in Python 3.5 and above
# iterate through images in each class folder
killing this process after over 38 hours adding over 500k white-tailed deer crops from emammal
resuming for remaining classes
# get cropped image data for Image table
"if mean of each channel is about the same, image is likely grayscale"
# still have no info on these:
seq_id = CharField(null= True)                # sequence identifier for the original image
seq_num_frames = IntegerField(null = True)    # number of frames in sequence
frame_num = IntegerField(null = True)         # which frame number in sequence
location = CharField(null = True)             # location of camera trap
datetime = DateTimeField(null = True)
# store info about the detection corresponding to this image
# store info about the true labels for the detection
#  - for pretrain dataset this is the same as the detection_category if the detection categories
print(classes)
Connect to database and sample a dataset
Load the saved embedding model from the checkpoint
Update the dataset embedding
save the images
save the features
"with open(os.path.join(args.output_dir, 'lastlayer_features.mat'), 'wb') as f:"
"pickle.dump(sample_features, f)"
"with open(os.path.join(args.output_dir, 'labels.mat'), 'wb') as f:"
"pickle.dump(sample_labels, f)"
"parser.add_argument('--db_name', default='missouricameratraps', type=str, help='Name of the training (target) data Postgres DB.')"
"parser.add_argument('--db_user', default='user', type=str, help='Name of the user accessing the Postgres DB.')"
"parser.add_argument('--db_password', default='password', type=str, help='Password of the user accessing the Postgres DB.')"
"parser.add_argument('--base_model', type=str, help='Path to latest embedding model checkpoint.')"
"parser.add_argument('--output_dir', type=str, help='Output directory for subset of crops')"
Add json entry for this crop
Copy file for this crop to subset dataset crop dir
Copy file for its full-size source image to subset dataset image dir
Write crops.json to subset dataset crop dir
store info about the crops produced in a JSON file
------------------------------------------------------------------------------------------------------------#
COMMENT OUT IF NOT USING A SPECIFIC PROJECT WITHIN ROBERT LONG EMAMMAL DATASET
------------------------------------------------------------------------------------------------------------#
get some information about the source image
------------------------------------------------------------------------------------------------------------#
NOTE: EDIT THIS SECTION BASED ON DATASET SOURCE
get info about sequence the source image belongs to from path and directory
# missouricameratraps:
imgframenum = int(os.path.basename(imgfile).split('.JPG')[0].split('_')[-1])
imgseqid = int(os.path.split(os.path.dirname(imgfile))[-1])
"imgseqnumframes = len([name for name in os.listdir(os.path.dirname(imgfile)) if os.path.isfile(os.path.join(os.path.dirname(imgfile), name))])"
# emammal:
------------------------------------------------------------------------------------------------------------#
utility function
compute output
measure accuracy and record loss
switch to train mode
measure accuracy and record loss
compute output
measure accuracy and record loss
switch to evaluate mode
compute output
switch to evaluate mode
compute output
switch to evaluate mode
"print(self.labels_set, self.n_classes)"
Add all negatives for all positive pairs
print(triplets.shape[0])
constructor
update embedding values after a finetuning
select either the default or active pools
gather test set
gather train set
finetune the embedding model over the labeled pool
a utility function for saving the snapshot
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Variables for the hierarchical cluster
Variables for the AL algorithm
Data variables
"connectivity = kneighbors_graph(self.transformed_X,max_features)"
Fit cluster and update cluster variables
The sklearn hierarchical clustering algo numbers leaves which correspond
to actual datapoints 0 to n_points - 1 and all internal nodes have
ids greater than n_points - 1 with the root having the highest node id
"If no labels have been observed, simply return uniform distribution"
"If no observations, return worst possible upper lower bounds"
Loop through generations from bottom to top
Update admissible labels for node
Calculate score
Determine if node should be split
Make sure label set for node so that we can flow to children
if necessary
Only split if all ancestors are admissible nodes
This is part  of definition of admissible pruning
Check that pruning covers all leave nodes
Fill in labels
Observe labels for previously recommended batches
TODO(lishal): implement multiple selection methods
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
A list of initialized samplers is allowed as an input because
"for AL_methods that search over different mixtures, may want mixtures to"
have shared AL_methods so that initialization is only performed once for
computation intensive methods like HierarchicalClusteringAL and
states are shared between mixtures.
"If initialized samplers are not provided, initialize them ourselves."
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copy these objects to make sure they are not modified while simulating
trajectories as they are used later by the main run_experiment script.
Assumes that model has already by fit using all labeled data so
the probabilities can be used immediately to hallucinate labels
All models need to have predict_proba method
Hallucinate labels for selected datapoints to be label
using class probabilities from model
"Not saving already_selected here, if saving then should sort"
only for the input to fit but preserve ordering of indices in
already_selected
Useful to know how accuracy compares for model trained on hallucinated
labels vs trained on true labels.  But can remove this train to speed
up simulations.  Won't speed up significantly since many more models
are being trained inside the loop above.
Save trajectory for reference
Delete created copies
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER
Should check model but kernel_svm does not have coef_ so need better
handling here
Make sure that model object fed in did not change during simulations
Return indices based on return type specified
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Set gamma for gaussian kernel to be equal to 1/n_features
kneighbors graph is constructed using k=10
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of"
"another point, make it vice versa"
Graph edges are weighted by applying gaussian kernel to manhattan dist.
"By default, gamma for rbf kernel is equal to 1/n_features but may"
get better results if gamma is tuned.
Define graph density for an observation to be sum of weights for all
edges to the node representing the datapoint.  Normalize sum weights
by total number of neighbors.
"If a neighbor has already been sampled, reduce the graph density"
for its direct neighbors to promote diversity.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update observed reward and arm probabilities
Sample an arm
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
y only used for determining how many clusters there should be
probably not practical to assume we know # of classes before hand
should also probably scale with dimensionality of data
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
TODO(lishal): have MarginSampler and this share margin function
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update min_distances for all examples given new cluster center.
Assumes that the transform function takes in original data and not
flattened data.
Initialize centers with a randomly selected datapoint
New examples should not be in already selected since those points
should have min_distance of zero to a cluster center.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
This is uniform given the remaining pool but biased wrt the entire pool.
sample = [i for i in range(self.X.shape[0]) if i not in already_selected]
return sample[0:N]
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Fields for hierarchical clustering AL
Setting parent and storing nodes in dict for fast access
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Faster and available in Python 3.5 and above
transform_list.append(CenterCrop((processed_size)))
transform_list.append(Lambda(lambda X: normalize(X)))
"print(self.labels_set, self.n_classes)"
"from PyQt5 import QtCore, QtWidgets,QtGui"
from collections import deque
from peewee import *
from UIComponents.Tag import Tag
policy.setHeightForWidth(True)
"print(self.tab1.parentWidget(),self)"
self.tab4.add.clicked.connect(self.addSpecies)
self.tab4.update.clicked.connect(self.updateSpecies)
checkpoint= load_checkpoint('../merge/triplet_model_0054.tar')
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))"
"embedding_net = EmbeddingNet(checkpoint['arch'], checkpoint['feat_dim'])"
if checkpoint['loss_type'].lower()=='center':
"model = torch.nn.DataParallel(ClassificationNet(embedding_net, n_classes=14)).cuda()"
else:
model= torch.nn.DataParallel(embedding_net).cuda()
model.load_state_dict(checkpoint['state_dict'])
self.parentWidget().progressBar.setMaximum(len(run_dataset)//2048)
"e=Engine(model,None,None, verbose=True,progressBar= self.parentWidget().progressBar)"
label = [ x[1] for x in run_dataset.samples]
"print(indices,selected_set)"
print(query.sql())
src.delete().where(src.image_id<<rList))
"det= UserDetection.create(category_id=0, id=str(index+label[1][2]),image_id=final[0], bbox_X=label[1][0], bbox_Y=label[1][1], bbox_W=label[1][2], bbox_H=label[1][3])"
for x in self.tab1.grid.tags:
x.delete_instance()
db.create_tables([Detection])
This is simply to show the bar
"p = Process(target=ex.active, args=())"
p.start()
p.join()
ex.active()
ex.centralWidget().setCurrentIndex(1)
main()
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))"
print(row)
"unq_id= ""crops_""+str(uuid.uuid1())"
"print(line,imageWidth,imageHeight)"
"print(""%s,%s,%s,%0.6f,%0.6f,%0.6f,%0.6f,%0.6f""%(line[0], line[1],line[2],float(line[3]),topRel,leftRel,bottomRel,rightRel))"
if not os.path.exists(dest):
os.mkdir(dest)
raise
out.close()
"print length,(i-1)*length,i*length"
matplotlib.use('Agg')
get the inputs
zero the parameter gradients
forward + backward + optimize
print statistics
get the inputs
forward + backward + optimize
print statistics
get the inputs
zero the parameter gradients
forward + backward + optimize
get the inputs
forward + backward + optimize
print statistics
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
"conf.reset()"""""""
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
conf.reset()
for clf in classifiers:
"clf.fit(X_train, y_train)"
preds= clf.predict_proba(X)
uncertainty+= preds.max(axis=1)
print(uncertainty[ind])
print(uncertainty[ind])
true_labels = y[unlabeled_indices]
select up to 5 digit examples that the classifier is most uncertain about
remember best acc@1 and save checkpoint
"completeClassificationLoop(run_dataset, model,num_classes)"
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)"
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd)
labels = db.labels_
"mapp=(find_probablemap(label,labels, K=args.K))"
"print(""Clusters"")"
"for i,x in enumerate(labels):"
labels[i]= mapp[x]
print(np.sum(labels == label)/labels.size)
"print(""Confidence Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning(embd, label, idx)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(""Entropy Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning_entropy(embd, label, idx)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(idx,idx.shape)"
for i in idx:
print(paths[i])
"print(""Silohette active learning"")"
"idx= active_learning2(embd, 1000, args.num_clusters)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
embd= reduce_dimensionality(embd)#[0:10000])
labels= labels[0:10000]
label= label[0:10000]
paths= paths[0:10000]
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])"
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])"
plt.show()
"np.save(args.name_prefix+""_embeddings.npy"",embd)"
"np.save(args.name_prefix+""_labels.npy"",label)"
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")"
del output
define loss function (criterion) and optimizer
"optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay= args.weight_decay)"
optimizer.load_state_dict(checkpoint['optimizer'])
train for one epoch
evaluate on validation set
matplotlib.use('Agg')
selected_set.add(rand_ind[i])
"embedding_net = EmbeddingNet('resnet50', 256, True)"
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)"
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)"
print('Embedding Done')
sys.stdout.flush()
Random examples to start
"random_ids = noveltySamples(unlabeled_dataset.em, unlabeled_dataset.getIDs(), 1000)"
print(random_ids)
Move Records
Finetune the embedding model
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)"
unlabeled_dataset.updateEmbedding(model)
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})"
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()"
matplotlib.use('Agg')
"uncertainty= np.apply_along_axis(stats.entropy,1,probs) * (1 - probs.max(axis=1))"
"copy(paths[srt[i]], ""active"")"
"plot_together( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()), preds, base_ind, dataset.getpaths(), {})"
"return np.random.choice(range(0,prob_list[0].shape[0]), 100, replace=False).tolist()"
selected_set.add(rand_ind[i])
"query= Detection.update(kind = destKind.value).where(Detection.id.in_(rList), Detection.kind == srcKind.value)"
query.execute()
"embedding_net = EmbeddingNet('resnet50', 256, True)"
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)"
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)"
print('Embedding Done')
sys.stdout.flush()
"plot_embedding(dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})"
Random examples to start
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)"
print(random_ids)
Move Records
Finetune the embedding model
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)"
unlabeled_dataset.updateEmbedding(model)
"print(dataset.em[dataset.current_set].shape, np.asarray(dataset.getlabels()).shape, len(dataset.getpaths()))"
"plot_embedding( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})"
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})"
train_eval_classifier()
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()"
"names = [""Linear SVM"", ""RBF SVM"", ""Random Forest"", ""Neural Net"", ""Naive Bayes""]"
"classifiers = [SVC(kernel=""linear"", C=0.025, probability= True, class_weight='balanced'),"
"SVC(gamma=2, C=1, probability= True, class_weight='balanced'),"
"RandomForestClassifier(max_depth=None, n_estimators=100, class_weight='balanced'),"
"MLPClassifier(alpha=1),"
GaussianNB()]
estimators= []
"for name, clf in zip(names, classifiers):"
"estimators.append((name, clf))"
"eclf1 = VotingClassifier(estimators= estimators, voting='hard')"
"eclf2 = VotingClassifier(estimators= estimators, voting='soft')"
"names.append(""ensemble hard"")"
classifiers.append(eclf1)
"names.append(""ensemble soft"")"
classifiers.append(eclf2)
dataset.image_mode()
dataset.updateEmbedding(model)
y_pred= clf.predict(X_test)
"print(confusion_matrix(y_test, y_pred))"
paths= dataset.getpaths()
"for i, (yp, yt) in enumerate(zip(y_pred, y_test)):"
if yp != yt:
"copy(paths[i],""mistakes"")"
"print(yt, yp, paths[i],i)"
"clf_output= clf_e.embedding(eval_loader, dim=48)"
"self.train_transform = transforms.Compose([Resize(raw_size), CenterCrop((processed_size)), ToTensor(), Normalize(mean, std)])"
"print(self.labels_set, self.n_classes)"
"print(line,imageWidth,imageHeight)"
"print length,(i-1)*length,i*length"
#########################################################
## Configuration
#########################################################
## The actual code
Check arguments
Create output directories
Padding around the detected objects when cropping
1.3 for the cropping during test time and 1.3 for
the context that the CNN requires in the left-over
image
Load a (frozen) Tensorflow model into memory.
## Preparations: get all the output tensors
The following processing is only for single image
Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
Follow the convention by adding back the batch dimension
For all images in the image directoryig
Load image
Run inference
"all outputs are float32 numpy arrays, so convert types as appropriate"
Add detections to the collection
Get info about the image
Select detections with a confidence larger than DETECTION_CONFIDENCE
Get these boxes and convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes"
"However, we need to make sure that it box coordinates are still within the image"
"For each detected bounding box with high confidence, we will"
crop the image to the padded box and save it
generate a unique identifier for the detection
"bbox is the detected box, crop_box the padded / enlarged box"
Add numbering to the original file name if there are multiple boxes
The absolute file path where we will store the image
"if COCO_OUTPUT_DIR is set, then we will only use the shape"
of cropped_img in the following code. So instead of reading
cropped_img = np.array(Image.open(out_file))
we can speed everything up by reading only the size of the image
matplotlib.use('Agg')
conf= ConfusionMatrix(24)
get the inputs
zero the parameter gradients
forward + backward + optimize
print statistics
get the inputs
forward + backward + optimize
print statistics
get the inputs
zero the parameter gradients
forward + backward + optimize
get the inputs
forward + backward + optimize
print statistics
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
"conf.reset()"""""""
define loss function (criterion) and optimizer
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
conf.reset()
for clf in classifiers:
"clf.fit(X_train, y_train)"
preds= clf.predict_proba(X)
uncertainty+= preds.max(axis=1)
print(uncertainty[ind])
print(uncertainty[ind])
true_labels = y[unlabeled_indices]
select up to 5 digit examples that the classifier is most uncertain about
"print(indices,selected_set)"
print(query.sql())
remember best acc@1 and save checkpoint
"completeClassificationLoop(run_dataset, model,num_classes)"
"embd, label, paths = extract_embeddings(run_loader, model)"
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)"
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd)
labels = db.labels_
"mapp=(find_probablemap(label,labels, K=args.K))"
"print(""Clusters"")"
"for i,x in enumerate(labels):"
labels[i]= mapp[x]
print(np.sum(labels == label)/labels.size)
"print(""Confidence Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning(embd, label, idx)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(""Entropy Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning_entropy(embd, label, idx)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
"new_selected= selectSamples(embd,paths,3000)"
"print(idx,idx.shape)"
for i in idx:
print(paths[i])
"print(""Silohette active learning"")"
"idx= active_learning2(embd, 1000, args.num_clusters)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(""Random"")"
"idx = np.random.choice(np.arange(len(paths)), 1000, replace=False)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
embd= reduce_dimensionality(embd)#[0:10000])
labels= labels[0:10000]
label= label[0:10000]
paths= paths[0:10000]
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])"
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])"
plt.show()
"np.save(args.name_prefix+""_embeddings.npy"",embd)"
"np.save(args.name_prefix+""_labels.npy"",label)"
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")"
combo.setEnabled(not finalized)
print(self.bbox)
self.addWidget(self.child)
Moving container with arrows
Left - Bottom
Right-Bottom
Left-Top
Right-Top
Left - Bottom
Right - Bottom
Left - Top
Right - Top
check cursor horizontal position
check cursor vertical position
self.resizeEvent=self.onResize
"print(""Parent"", parent, parent.width(), parent.height())"
"self.setGeometry(0,0,410,307)"
"print(w,h,""w,h"")"
"print(""final"",tag.getFinal())"
pass
"self.tags.append(TContainer(self,Category.get(-1),[0,0,0.1,0.1],True, Qt.red))"
label= CharField()
fullname=str(self.model)
"self.name= (fullname[fullname.find("":"")+2:fullname.find("">"")].strip()+'_set').lower()"
"print(""Parent"", self.parentWidget().width(), self.parentWidget().height())"
"print(self.model,self.name,query.sql())"
self.tab4.speciesList.setModel(species)
"self.tab4.speciesList.setRowHidden(len(species.stringList())-1, True)"
self.speciesList.itemChanged.connect(self.itemChanged)
#####
""
video_utils.py
""
"Utilities for splitting, rendering, and assembling videos."
""
#####
"%% Constants, imports, environment"
from ai4eutils
%% Path utilities
%% Function for rendering frames to video and vice-versa
http://tsaith.github.io/combine-images-into-a-video-with-python-3-and-opencv-3.html
Determine the width and height from the first image
Define the codec and create VideoWriter object
"If we're not over-writing, check whether all frame images already exist"
"OpenCV seems to over-report the number of frames by 1 in some cases, or fails"
"to read the last frame; either way, I'm allowing one missing frame."
"print(""Rendering video {}, couldn't find frame {}"".format("
"input_video_file,missing_frame_number))"
...if we need to check whether to skip this video entirely
"for frame_number in tqdm(range(0,n_frames)):"
print('Skipping frame {}'.format(frame_filename))
Recursively enumerate video files
Create the target output folder
Render frames
input_video_file = input_fn_absolute; output_folder = output_folder_video
For each video
""
input_fn_relative = input_files_relative_paths[0]
"process_detection_with_options = partial(process_detection, options=options)"
zero-indexed
Load results
# Break into videos
im = images[0]
# For each video...
video_name = list(video_to_frames.keys())[0]
frame = frames[0]
At most one detection for each category for the whole video
category_id = list(detection_categories.keys())[0]
Find the nth-highest-confidence video to choose a confidence value
Prepare the output representation for this video
'max_detection_conf' is no longer included in output files by default
...for each video
Write the output file
%% Test driver
%% Constants
%% Split videos into frames
"%% List image files, break into folders"
Find unique folders
fn = frame_files[0]
%% Load detector output
%% Render detector frames
folder = list(folders)[0]
d = detection_results_this_folder[0]
...for each file in this folder
...for each folder
%% Render output videos
folder = list(folders)[0]
...for each video
""
run_inference_with_yolov5_val.py
""
Runs a folder of images through MegaDetector (or another YOLOv5 model) with YOLOv5's
"val.py, converting the output to the standard MD format.  The main goal is to leverage"
YOLO's test-time augmentation tools.
""
"YOLOv5's val.py uses each file's base name as a unique identifier, which doesn't work"
when you have typical camera trap images like:
""
a/b/c/RECONYX0001.JPG
d/e/f/RECONYX0001.JPG
""
...so this script jumps through a bunch of hoops to put a symlinks in a flat
"folder, run YOLOv5 on that folder, and map the results back to the real files."
""
"Currently requires the user to supply the path where a working YOLOv5 install lives,"
and assumes that the current conda environment is all set up for YOLOv5.
""
TODO:
""
* Figure out what happens when images are corrupted... right now this is the #1
"reason not to use this script, it may be the case that corrupted images look the"
same as empty images.
""
* Multiple GPU support
""
* Checkpointing
""
* Windows support (I have no idea what all the symlink operations will do on Windows)
""
"* Support alternative class names at the command line (currently defaults to MD classes,"
though other class names can be supplied programmatically)
""
%% Imports
%% Options class
# Required ##
# Optional ##
%% Main function
#%% Path handling
#%% Enumerate images
#%% Create symlinks to give a unique ID to each image
i_image = 0; image_fn = image_files_absolute[i_image]
...for each image
#%% Create the dataset file
Category IDs need to be continuous integers starting at 0
#%% Prepare YOLOv5 command
#%% Run YOLOv5 command
#%% Convert results to MD format
"We'll use the absolute path as a relative path, and pass '/'"
as the base path in this case.
#%% Clean up
...def run_inference_with_yolo_val()
%% Command-line driver
%% Scrap
%% Test driver (folder)
%% Test driver (file)
%% Preview results
options.sample_seed = 0
...for each prediction file
%% Compare results
Choose all pairwise combinations of the files in [filenames]
"%% Constants, imports, environment"
from multiprocessing.pool import ThreadPool as workerpool
Number of images to pre-fetch
Useful hack to force CPU inference.
""
"Need to do this before any PT/TF imports, which happen when we import"
from run_detector.
Numpy FutureWarnings from tensorflow import
%% Support functions for multiprocessing
TODO
""
The queue system is a little more elegant if we start one thread for reading and one
"for processing, and this works fine on Windows, but because we import TF at module load,"
"CUDA will only work in the main process, so currently the consumer function runs here."
""
"To enable proper multi-GPU support, we may need to move the TF import to a separate module"
that isn't loaded until very close to where inference actually happens.
%% Other support funtions
%% Image processing functions
%% Main function
Handle the case where image_file_names is not yet actually a list
"Find the images to score; images can be a directory, may need to recurse"
A json list of image paths
A single image file
Load the detector
Does not count those already processed
Will not add additional entries not in the starter checkpoint
Write a checkpoint if necessary
"Back up any previous checkpoints, to protect against crashes while we're writing"
the checkpoint file.
Write the new checkpoint
Remove the backup checkpoint if it exists
...if it's time to make a checkpoint
"When using multiprocessing, let the workers load the model"
"Results may have been modified in place, but we also return it for"
backwards-compatibility.
The typical case: we need to build the 'info' struct
"If the caller supplied the entire ""info"" struct"
"The 'max_detection_conf' field used to be included by default, and it caused all kinds"
"of headaches, so it's no longer included unless the user explicitly requests it."
%% Interactive driver
%%
%% Command-line driver
This is an experimental hack to allow the use of non-MD YOLOv5 models through
the same infrastructure; it disables the code that enforces MDv5-like class lists.
Load the checkpoint if available
""
Relative file names are only output at the end; all file paths in the checkpoint are
still full paths.
"Find the images to score; images can be a directory, may need to recurse"
A json list of image paths
A single image file
Test that we can write to the output_file's dir if checkpointing requested
"Don't overwrite existing checkpoint files, this is a sure-fire way to eventually"
erase someone's checkpoint.
"Commenting this out for now... the scenario where we are resuming from a checkpoint,"
then immediately overwrite that checkpoint with empty data is higher-risk than the
annoyance of crashing a few minutes after starting a job.
Confirm that we can write to the checkpoint path; this avoids issues where
we crash after several thousand images.
%% Imports
import pre- and post-processing functions from the YOLOv5 repo
scale_coords() became scale_boxes() in later YOLOv5 versions
%% Classes
padded resize
"Image size can be an int (which translates to a square target size) or (h,w)"
...if the caller has specified an image size
NMS
"As of v1.13.0.dev20220824, nms is not implemented for MPS."
""
Send predication back to the CPU to fix.
format detections/bounding boxes
"This is a loop over detection batches, which will always be length 1 in our case,"
since we're not doing batch inference.
Rescale boxes from img_size to im0 size
"normalized center-x, center-y, width and height"
"MegaDetector output format's categories start at 1, but the MD"
model's categories start at 0.
...for each detection in this batch
...if this is a non-empty batch
...for each detection batch
...try
for testing
"MegaDetector was trained with batch size of 1, and the resizing function is a part"
of the inference graph
"change from [y1, x1, y2, x2] to [x1, y1, width, height]"
convert numpy floats to Python floats
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size
"np_images = [np.asarray(image, np.uint8) for image in images]"
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)"
performs inference
our batch size is 1; need to loop the batch dim if supporting batch size > 1
"%% Constants, imports, environment"
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Numpy FutureWarnings from tensorflow import
Useful hack to force CPU inference
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
An enumeration of failure reasons
Number of decimal places to round to for confidence and bbox coordinates
Label mapping for MegaDetector
Should we allow classes that don't look anything like the MegaDetector classes?
"Each version of the detector is associated with some ""typical"" values"
"that are included in output files, so that downstream applications can"
use them as defaults.
%% Classes
Stick this into filenames before the extension for the rendered result
%% Utility functions
mps backend only available in torch >= 1.12.0
%% Main function
Dictionary mapping output file names to a collision-avoidance count.
""
"Since we'll be writing a bunch of files to the same folder, we rename"
as necessary to avoid collisions.
...def input_file_to_detection_file()
Image is modified in place
...for each image
...def load_and_run_detector()
%% Command-line driver
Must specify either an image file or a directory
"but for a single image, args.image_dir is also None"
%% Interactive driver
%%
#####
""
process_video.py
""
"Split a video (or folder of videos) into frames, run the frames through run_detector_batch.py,"
and optionally stitch together results into a new video with detection boxes.
""
TODO: allow video rendering when processing a whole folder
TODO: allow video rendering from existing results
""
#####
"%% Constants, imports, environment"
Only relevant if render_output_video is True
Folder to use for extracted frames
Folder to use for rendered frames (if rendering output video)
Should we render a video with detection boxes?
""
"Only supported when processing a single video, not a folder."
"If we are rendering boxes to a new video, should we keep the temporary"
rendered frames?
Should we keep the extracted frames?
%% Main functions
"TODO: keep track of whether we created this folder, delete if we're deleting the extracted"
"frames and we created the folder, and the output files aren't in the same folder.  For now,"
we're just deleting the extracted frames and leaving the empty folder around in this case.
Render detections to images
"TODO: keep track of whether we created this folder, delete if we're deleting the rendered"
"frames and we created the folder, and the output files aren't in the same folder.  For now,"
we're just deleting the rendered frames and leaving the empty folder around in this case.
Combine into a video
Delete the temporary directory we used for detection images
shutil.rmtree(rendering_output_dir)
(Optionally) delete the frames on which we ran MegaDetector
shutil.rmtree(frame_output_folder)
...process_video()
# Validate options
# Split every video into frames
# Run MegaDetector on the extracted frames
# (Optionally) delete the frames on which we ran MegaDetector
shutil.rmtree(frame_output_folder)
# Convert frame-level results to video-level results
...process_video_folder()
%% Interactive driver
%% Process a folder of videos
import clipboard; clipboard.copy(cmd)
%% Process a single video
import clipboard; clipboard.copy(cmd)
"%% Render a folder of videos, one file at a time"
import clipboard; clipboard.copy(s)
%% Command-line driver
Lint as: python3
Copyright 2020 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TPU is automatically inferred if tpu_name is None and
we are running under cloud ai-platform.
""
copy_checkpoints.py
""
Run this script with specified source_dir and target_dir while the model is training to make a copy
of every checkpoint (checkpoints are kept once an hour by default and is difficult to adjust)
""
do not copy event or evaluation results
input validation
plot the images
adjust the figure
"read in dataset CSV and create merged (dataset, location) col"
map label to label_index
load the splits
only weight the training set by detection confidence
TODO: consider weighting val and test set as well
isotonic regression calibration of MegaDetector confidence
treat each split separately
new_weight[i] = confidence[i] * (n / c) / total_confidence(i's label)
- n = # examples in split (weighted by confidence); c = # labels
- weight allocated to each label is n/c
"- within each label, weigh each example proportional to confidence"
- new weights sum to n
error checking
"maps output label name to set of (dataset, dataset_label) tuples"
find which other label (label_b) has intersection
input validation
create label index JSON
look into sklearn.preprocessing.MultiLabelBinarizer
Note: JSON always saves keys as strings!
"True for ground truth, False for MegaDetector"
always save as .jpg for consistency
get bounding boxes
convert from category ID to category name
"check if crops are already downloaded, and ignore bboxes below the"
confidence threshold
assign all images without location info to 'unknown_location'
remove images from labels that have fewer than min_locs locations
merge dataset and location into a single string '<dataset>/<location>'
"create DataFrame of counts. rows = locations, columns = labels"
label_count: label => number of examples
loc_count: label => number of locs containing that label
generate a new split
score the split
SSE for # of images per label (with 2x weight)
SSE for # of locs per label
label => list of datasets to prioritize for test and validation sets
"merge dataset and location into a tuple (dataset, location)"
sorted smallest to largest
greedily add to test set until it has >= 15% of images
sort the resulting locs
"modify loc_to_size in place, so copy its keys before iterating"
arguments relevant to both creating the dataset CSV and splits.json
arguments only relevant for creating the dataset CSV
arguments only relevant for creating the splits JSON
comment lines starting with '#' are allowed
""
prepare_classification_script.py
""
Notebook-y script used to prepare a series of shell commands to run a classifier
(other than MegaClassifier) on a MegaDetector result set.
""
Differs from prepare_classification_script_mc.py only in the final class mapping step.
""
%% Job options
%% Constants
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
%% Set up environment
commands.append('cd CameraTraps/classification\n')
commands.append('conda activate cameratraps-classifier\n')
%% Crop images
fn = input_files[0]
%% Run classifier
fn = input_files[0]
%% Merge classification and detection outputs
fn = input_files[0]
%% Write everything out
input validation
"Evaluating with accimage is much faster than Pillow or Pillow-SIMD, but accimage"
is Linux-only.
create output directory
override saved params with kwargs
"For now, we don't weight crops by detection confidence during"
evaluation. But consider changing this.
"create model, compile with TorchScript if given checkpoint is not compiled"
"verify that target names matches original ""label names"" from dataset"
"if the dataset does not already have a 'other' category, then the"
'other' category must come last in label_names to avoid conflicting
with an existing label_id
define loss function (criterion)
"this file ends up being huge, so we GZIP compress it"
double check that the accuracy metrics are computed properly
save the confusion matrices to .npz
save per-label statistics
set dropout and BN layers to eval mode
"even if batch contains sample weights, don't use them"
Do target mapping on the outputs (unnormalized logits) instead of
"the normalized (softmax) probabilities, because the loss function"
uses unnormalized logits. Summing probabilities is equivalent to
log-sum-exp of unnormalized logits.
"a confusion matrix C is such that C[i,j] is the # of observations known to"
be in group i and predicted to be in group j.
match pytorch EfficientNet model names
images dataset
"for smaller disk / memory usage, we cache the raw JPEG bytes instead"
of the decoded Tensor
convert JPEG bytes to a 3D uint8 Tensor
"keras EfficientNet already includes normalization from [0, 255] to [0, 1],"
so we don't need to do that here
labels dataset
img_files dataset
weights dataset
define the transforms
efficientnet data preprocessing:
- train:
"1) random crop: aspect_ratio_range=(0.75, 1.33), area_range=(0.08, 1.0)"
2) bicubic resize to img_size
3) random horizontal flip
- test:
1) center crop
2) bicubic resize to img_size
weights sums to:
- if weight_by_detection_conf: (# images in split - conf delta)
- otherwise: (# images in split)
"freeze the base model's weights, including BatchNorm statistics"
https://www.tensorflow.org/guide/keras/transfer_learning#fine-tuning
rebuild output
input validation
set seed
create logdir and save params
create dataloaders and log the index_to_label mapping
define loss function and optimizer
using EfficientNet training defaults
- batch norm momentum: 0.99
"- optimizer: RMSProp, decay 0.9 and momentum 0.9"
- epochs: 350
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs"
- weight decay: 1e-5
TODO: change weighted to False if oversampling minority classes
stop training after 8 epochs without improvement
log metrics
log confusion matrix
log tp/fp/fn images
"tf.summary.image requires input of shape [N, H, W, C]"
false positive for top3_pred[0]
false negative for label
"if evaluating or finetuning, set dropout & BN layers to eval mode"
"for each label, track 5 most-confident and least-confident examples"
"even if batch contains sample weights, don't use them"
we do not track L2-regularization loss in the loss metric
This dictionary will get written out at the end of this process; store
diagnostic variables here
error checking
refresh detection cache
save log of bad images
cache of Detector outputs: dataset name => {img_path => detection_dict}
img_path: <dataset-name>/<img-filename>
get SAS URL for images container
strip image paths of dataset name
save list of dataset names and task IDs for resuming
complete task name: 'detect_for_classifier_caltech_20200722_110816_task01'
HACK! Sleep for 10s between task submissions in the hopes that it
"decreases the chance of backend JSON ""database"" corruption"
task still running => continue
"task finished successfully, save response to disk"
error checking before we download and crop any images
convert from category ID to category name
we need the datasets table for getting SAS keys
"we already did all error checking above, so we don't do any here"
get ContainerClient
get bounding boxes
we must include the dataset <ds> in <crop_path_template> because
'{img_path}' actually gets populated with <img_file> in
load_and_crop()
"get the image, either from disk or from Blob Storage"
inelegant way to close the container_clients
""
prepare_classification_script_mc.py
""
Notebook-y script used to prepare a series of shell commands to run MegaClassifier
on a MegaDetector result set.
""
Differs from prepare_classification_script.py only in the final class mapping step.
""
%% Job options
%% Constants
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
%% Set up environment
commands.append('cd CameraTraps/classification\n')
commands.append('conda activate cameratraps-classifier\n')
%% Crop images
fn = input_files[0]
%% Run classifier
fn = input_files[0]
%% Remap classifier outputs
fn = input_files[0]
%% Merge classification and detection outputs
fn = input_files[0]
%% Write everything out
mean/std values from https://pytorch.org/docs/stable/torchvision/models.html
define the transforms
resizes smaller edge to img_size
weights sums to:
- if weight_by_detection_conf: (# images in split - conf delta)
- otherwise: # images in split
for normal (non-weighted) shuffling
set all parameters to not require gradients except final FC layer
replace final fully-connected layer (which has 1000 ImageNet classes)
"detect GPU, use all if available"
input validation
set seed
create logdir and save params
create dataloaders and log the index_to_label mapping
create model
define loss function and optimizer
using EfficientNet training defaults
- batch norm momentum: 0.99
"- optimizer: RMSProp, decay 0.9 and momentum 0.9"
- epochs: 350
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs"
- weight decay: 1e-5
stop training after 8 epochs without improvement
do a complete evaluation run
log metrics
log confusion matrix
log tp/fp/fn images
"for every image: undo normalization, clamp to [0, 1], CHW -> HWC"
"- cannot be in-place, because the HeapItem might be in multiple heaps"
writer.add_figure() has issues => using add_image() instead
"writer.add_figure(f'{label_name}/{tag}', fig, global_step=epoch)"
false positive for top3_pred[0]
false negative for label
"preds and labels both have shape [N, k]"
"if evaluating or finetuning, set dropout and BN layers to eval mode"
"for each label, track k_extreme most-confident and least-confident images"
"even if batch contains sample weights, don't use them"
lock before changing ImageFile.LOAD_TRUNCATED_IMAGES
input validation
use MegaDB to generate list of images
only keep images that:
"1) end in a supported file extension, and"
2) actually exist in Azure Blob Storage
3) belong to a label with at least min_locs locations
write out log of images / labels that were removed
"save label counts, pre-subsampling"
"save label counts, post-subsampling"
spec_dict['taxa']: list of dict
[
"{'level': 'family', 'name': 'cervidae', 'datasets': ['idfg']},"
"{'level': 'genus',  'name': 'meleagris'}"
]
"spec_dict['dataset_labels']: dict, dataset => list of dataset_label"
{
"""idfg"": [""deer"", ""elk"", ""prong""],"
"""idfg_swwlf_2019"": [""elk"", ""muledeer"", ""whitetaileddeer""]"
}
"maps output label name to set of (dataset, dataset_label) tuples"
"because MegaDB is organized by dataset, we do the same"
ds_to_labels = {
'dataset_name': {
"'dataset_label': [output_label1, output_label2]"
}
}
we need the datasets table for getting full image paths
The line
"[img.class[0], seq.class[0]][0] as class"
selects the image-level class label if available. Otherwise it selects the
"sequence-level class label. This line assumes the following conditions,"
expressed in the WHERE clause:
- at least one of the image or sequence class label is given
- the image and sequence class labels are arrays with length at most 1
- the image class label takes priority over the sequence class label
""
"In Azure Cosmos DB, if a field is not defined, then it is simply excluded"
"from the result. For example, on the following JSON object,"
{
"""dataset"": ""camera_traps"","
"""seq_id"": ""1234"","
"""location"": ""A1"","
"""images"": [{""file"": ""abcd.jpeg""}],"
"""class"": [""deer""],"
}
"the array [img.class[0], seq.class[0]] just gives ['deer'] because"
img.class is undefined and therefore excluded.
"if no path prefix, set it to the empty string '', because"
"os.path.join('', x, '') = '{x}/'"
result keys
"- already has: ['dataset', 'location', 'file', 'class', 'bbox']"
"- add ['label'], remove ['file']"
"if img is mislabeled, but we don't know the correct class, skip it"
"otherwise, update the img with the correct class, but skip the"
img if the correct class is not one we queried for
sort keys for determinism
we need the datasets table for getting SAS keys
strip leading '?' from SAS token
only check Azure Blob Storage
check local directory first before checking Azure Blob Storage
1st pass: populate label_to_locs
"label (tuple of str) => set of (dataset, location)"
2nd pass: eliminate bad images
prioritize is a list of prioritization levels
number of already matching images
main(
"label_spec_json_path='idfg_classes.json',"
"taxonomy_csv_path='../../camera-traps-private/camera_trap_taxonomy_mapping.csv',"
"output_dir='run_idfg',"
json_indent=4)
recursively find all files in cropped_images_dir
only find crops of images from detections JSON
resizes smaller edge to img_size
"Evaluating with accimage is much faster than Pillow or Pillow-SIMD, but accimage"
is Linux-only.
create dataset
create model
set dropout and BN layers to eval mode
load files
dataset => set of img_file
crop_path: <dataset>/<img_file>___cropXX_mdvY.Y.jpg
[----<img_path>----]
error checking
any row with 'correct_class' should be marked 'mislabeled'
filter to only the mislabeled rows
convert '\' to '/'
verify that overlapping indices are the same
"""add"" any new mislabelings"
write out results
error checking
load detections JSON
get detector version
convert from category ID to category name
copy keys to modify dict in-place
This will be removed later when we filter for animals
save log of bad images
"True for ground truth, False for MegaDetector"
always save as .jpg for consistency
"we already did all error checking above, so we don't do any here"
"get the image, either from disk or from Blob Storage"
inelegant way to close the container_client
"crop_path => normalized bbox coordinates [xmin, ymin, width, height]"
"only ground-truth bboxes do not have a ""confidence"" value"
try loading image from local directory
try to download image from Blob Storage
crop the image
"expand box width or height to be square, but limit to img size"
"Image.crop() takes box=[left, upper, right, lower]"
pad to square using 0s
Author: lukemelas (github username)
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch
With adjustments and added comments by workingcoder (github username).
Support the construction of 'efficientnet-l2' without pretrained weights
Expansion phase (Inverted Bottleneck)
"image_size = calculate_output_image_size(image_size, 1) <-- this wouldn't modify image_size"
Depthwise convolution phase
"Squeeze and Excitation layer, if desired"
Pointwise convolution phase
Expansion and Depthwise Convolution
Squeeze and Excitation
Pointwise Convolution
Skip connection and drop connect
The combination of skip connection and drop connect brings about stochastic depth.
Batch norm parameters
Get stem static or dynamic convolution depending on image size
Stem
Build blocks
Update block input and output filters based on depth multiplier.
The first block needs to take care of stride and filter size increase.
"image_size = calculate_output_image_size(image_size, block_args.stride)  # stride = 1"
Head
Final linear layer
Stem
Blocks
Head
Stem
Blocks
Head
Convolution layers
Pooling and final linear layer
Author: lukemelas (github username)
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch
With adjustments and added comments by workingcoder (github username).
###############################################################################
## Help functions for model architecture
###############################################################################
GlobalParams and BlockArgs: Two namedtuples
Swish and MemoryEfficientSwish: Two implementations of the method
round_filters and round_repeats:
Functions to calculate params for scaling model width and depth ! ! !
get_width_and_height_from_size and calculate_output_image_size
drop_connect: A structural design
get_same_padding_conv2d:
Conv2dDynamicSamePadding
Conv2dStaticSamePadding
get_same_padding_maxPool2d:
MaxPool2dDynamicSamePadding
MaxPool2dStaticSamePadding
"It's an additional function, not used in EfficientNet,"
but can be used in other model (such as EfficientDet).
"Parameters for the entire model (stem, all blocks, and head)"
Parameters for an individual model block
Set GlobalParams and BlockArgs's defaults
An ordinary implementation of Swish function
A memory-efficient implementation of Swish function
TODO: modify the params names.
"maybe the names (width_divisor,min_width)"
"are more suitable than (depth_divisor,min_depth)."
follow the formula transferred from official TensorFlow implementation
follow the formula transferred from official TensorFlow implementation
"generate binary_tensor mask according to probability (p for 0, 1-p for 1)"
Note:
The following 'SamePadding' functions make output size equal ceil(input size/stride).
"Only when stride equals 1, can the output size be the same as input size."
Don't be confused by their function names ! ! !
Tips for 'SAME' mode padding.
Given the following:
i: width or height
s: stride
k: kernel size
d: dilation
p: padding
Output after Conv2d:
o = floor((i+p-((k-1)*d+1))/s+1)
"If o equals i, i = floor((i+p-((k-1)*d+1))/s+1),"
=> p = (i-1)*s+((k-1)*d+1)-i
With the same calculation as Conv2dDynamicSamePadding
Calculate padding based on image size and save it
Calculate padding based on image size and save it
###############################################################################
## Helper functions for loading model params
###############################################################################
BlockDecoder: A Class for encoding and decoding BlockArgs
efficientnet_params: A function to query compound coefficient
get_model_params and efficientnet:
Functions to get BlockArgs and GlobalParams for efficientnet
url_map and url_map_advprop: Dicts of url_map for pretrained weights
load_pretrained_weights: A function to load pretrained weights
Check stride
"Coefficients:   width,depth,res,dropout"
Blocks args for the whole model(efficientnet-b0 by default)
It will be modified in the construction of EfficientNet Class according to model
note: all models have drop connect rate = 0.2
ValueError will be raised here if override_params has fields not included in global_params.
train with Standard methods
check more details in paper(EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks)
train with Adversarial Examples(AdvProp)
check more details in paper(Adversarial Examples Improve Image Recognition)
TODO: add the petrained weights url map of 'efficientnet-l2'
AutoAugment or Advprop (different preprocessing)
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils)
from data_management.megadb.schema import sequences_schema_check
resize is for displaying them more quickly
dataset and seq_id are required fields
sort the images in the sequence
"if no class label on the image, show class label on the sequence"
pool = ThreadPool()
"print('len of rendering_info', len(rendering_info))"
"tqdm(pool.imap_unordered(render_image_info_partial, rendering_info), total=len(rendering_info))"
options = write_html_image_list()
options['headerHtml']
print('Checking that the MegaDB entries conform to the schema...')
sequences_schema_check.sequences_schema_check(sequences)
#######
""
visualize_db.py
""
Outputs an HTML page visualizing annotations (class labels and/or bounding boxes)
on a sample of images in a database in the COCO Camera Traps format
""
#######
%% Imports
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils)
Assumes the cameratraps repo root is on the path
%% Settings
Set to None to visualize all images
Target size for rendering; set either dimension to -1 to preserve aspect ratio
"These are mutually exclusive; both are category names, not IDs"
"Special tag used to say ""show me all images with multiple categories"""
We sometimes flatten image directories by replacing a path separator with
another character.  Leave blank for the typical case where this isn't necessary.
Control rendering parallelization
Process-based parallelization in this function is currently unsupported
"due to pickling issues I didn't care to look at, but I'm going to just"
"flip this with a warning, since I intend to support it in the future."
%% Helper functions
"Translate the file name in an image entry in the json database to a path, possibly doing"
some manipulation of path separators
%% Core functions
"Optionally remove all images without bounding boxes, *before* sampling"
"Optionally include/remove images with specific labels, *before* sampling"
Put the annotations in a dataframe so we can select all annotations for a given image
Construct label map
Take a sample of images
Set of dicts representing inputs to render_db_bounding_boxes:
""
"bboxes, boxClasses, image_path"
All the class labels we've seen for this image (with out without bboxes)
Iterate over annotations for this image
iAnn = 0; anno = annos_i.iloc[iAnn]
...for each of this image's annotations
"We're adding html for an image before we render it, so it's possible this image will"
fail to render.  For applications where this script is being used to debua a database
"(the common case?), this is useful behavior, for other applications, this is annoying."
...for each image
...def render_image_info
def process_images(...)
%% Command-line driver
"Copy all fields from a Namespace (i.e., the output from parse_args) to an object."
""
Skips fields starting with _.  Does not check existence in the target object.
Convert to an options object
%% Interactive driver(s)
%%
os.startfile(htmlOutputFile)
%% Constants and imports
convert category ID from int to str
Retry on blob storage read failures
%% Functions
PIL.Image.convert() returns a converted copy of this image
Alter orientation as needed according to EXIF tag 0x112 (274) for Orientation
""
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28
https://www.media.mit.edu/pia/Research/deepview/exif.html
""
Null operation
Aspect ratio as width over height
ar = w / h
h = w / ar
w = ar * h
The following three functions are modified versions of those at:
""
https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py
Convert to pixels so we can use the PIL crop() function
PIL's crop() does surprising things if you provide values outside of
"the image, clip inputs"
...if this detection is above threshold
...for each detection
"list of lists, one list of strings for each bounding box (to accommodate multiple labels)"
for color selection
"Always render objects with a confidence of ""None"", this is typically used"
for ground truth data.
"{} is the default, which means ""show labels with no mapping"", so don't use ""if label_map"" here"
if label_map:
"To avoid duplicate colors with detection-only visualization, offset"
the classification class index by the number of detection classes
...for each classification
...if we have classification results
...if the confidence of this detection is above threshold
...for each detection
"print('Input must be of size [N, 4], but is ' + str(boxes_shape))"
Deliberately trimming to the width of the image only in the case where
"box expansion is turned on.  There's not an obvious correct behavior here,"
but the thinking is that if the caller provided an out-of-range bounding
"box, they meant to do that, but at least in the eyes of the person writing"
"this comment, if you expand a box for visualization reasons, you don't want"
to end up with part of a box.
""
A slightly more sophisticated might check whether it was in fact the expansion
"that made this box larger than the image, but this is the case 99.999% of the time"
"here, so that doesn't seem necessary."
...if we need to expand boxes
If the total height of the display strings added to the top of the bounding
"box exceeds the top of the image, stack the strings below the bounding box"
instead of above.
Each display_str has a top and bottom margin of 0.05x.
Reverse list and print from bottom to top.
Skip empty strings
"list of lists, one list of strings for each bounding box (to accommodate multiple labels)"
need to be a string here because PIL needs to iterate through chars
""
stacked bar charts are made with each segment starting from a y position
"To fit the legend in, shrink current axis by 20%"
Put a legend to the right of the current axis
####
""
visualize_incoming_annotations.py
""
Spot-check the annotations received from iMerit by visualizing annotated bounding
boxes on a sample of images and display them in HTML.
""
"Modified in 2021 March to use the new format (iMerit batch 12 onwards), which is a"
COCO formatted JSON with relative coordinates for the bbox.
""
from data_management.megadb.schema import sequences_schema_check
we used frame_num of 1 when sending out images to annotators when it is not explicitly stored (wcs esp.)
The file_name field in the incoming json looks like
alka_squirrels.seq2020_05_07_25C.frame119221.jpg
"we need to use the dataset, sequence and frame info to find the actual path in blob storage"
using the sequences
category_id 5 is No Object Visible
download the image
Write to HTML
allow forward references in typing annotations
class variables
instance variables
get path to root
taxonomy CSV rows without 'taxonomy_string' entries are excluded
"from the taxonomy graph, but can be included in a classification"
label specification JSON via the 'dataset_labels' key
use the lower parent
special cases
%% Imports
%% Taxnomy checking
taxonomy CSV rows without 'taxonomy_string' entries are excluded
"from the taxonomy graph, but can be included in a classification"
label specification JSON via the 'dataset_labels' key
...for each row in the taxnomy file
%% Command-line driver
%% Interactive driver
%%
which datasets are already processed?
"sequence-level query should be fairly fast, ~1 sec"
cases when the class field is on the image level (images in a sequence
"that had different class labels, 'caltech' dataset is like this)"
"this query may take a long time, >1hr"
"this query should be fairly fast, ~1 sec"
read species presence info from the JSON files for each dataset
has this class name appeared in a previous dataset?
columns to populate the spreadsheet
sort by descending species count
make the spreadsheet
hyperlink Bing search URLs
hyperlink example image SAS URLs
TODO hardcoded columns: change if # of examples or col_order changes
""
map_lila_taxonomy_to_wi_taxonomy.py
""
Loads the LILA category mapping (in which taxonomy information comes from an iNat taxonomy snapshot)
and tries to map each class to the Wildlife Insights taxonomy.
""
%% Constants and imports
Created by get_lila_category_list.py... contains counts for each category
This is a manually-curated file used to store mappings that had to be made manually
This is the main output file from this whole process
%% Load category and taxonomy files
%% Pull everything out of pandas
%% Cache WI taxonomy lookups
This is just a handy lookup table that we'll use to debug mismatches
taxon = wi_taxonomy[21653]; print(taxon)
Look for keywords that don't refer to specific taxa: blank/animal/unknown
Do we have a species name?
"If 'species' is populated, 'genus' should always be populated; one item currently breaks"
this rule.
...for each taxon
%% Find redundant taxa
%% Manual review of redundant taxa
%% Clean up redundant taxa
taxon_name = list(taxon_name_to_preferred_taxon_id.keys())[0]
"If we've gotten this far, we should be choosing from multiple taxa."
""
"This will become untrue if any of these are resolved later, at which point we shoudl"
remove them from taxon_name_to_preferred_id
Choose the preferred taxa
%% Read supplementary mappings
"Each line is [lila query],[WI taxon name],[notes]"
%% Map LILA categories to WI categories
Must be ordered from kingdom --> species
TODO:
"['subspecies','variety']"
i_taxon = 0; taxon = lila_taxonomy[i_taxon]; print(taxon)
"Go from kingdom --> species, choosing the lowest-level description as the query"
"E.g., 'car'"
"print('Made a supplementary mapping from {} to {}'.format(query,wi_taxon['taxon_name']))"
print('No match for {}'.format(query))
...for each LILA taxon
%% Manual mapping
%% Build a dictionary from LILA dataset names and categories to LILA taxa
i_d = 0; d = lila_taxonomy[i_d]
"%% Map LILA datasets to WI taxa, and count the number of each taxon available in each dataset"
dataset_name = list(lila_dataset_to_categories.keys())[0]
dataset_category = dataset_categories[0]
"Write out the dataset name, category name, WI GUID, WI scientific name, WI common name,"
and count
...for each category in this dataset
...for each dataset
...with open()
""
retrieve_sample_image.py
""
"Downloader that retrieves images from Google images, used for verifying taxonomy"
"lookups and looking for egregious mismappings (e.g., ""snake"" being mapped to a fish called"
"""snake"")."
""
"Simple wrapper around simple_image_download, but I've had to swap in and out the underlying"
downloader a few times.
""
%% Imports and environment
%%
%% Main entry point
%% Test driver
%%
""
"Using the taxonomy .csv file, map all LILA datasets to the standard taxonomy"
""
Does not currently produce results; this is just used to confirm that all category names
have mappings in the taxonomy file.
""
%% Constants and imports
Created by get_lila_category_list.py... contains counts for each category
%% Load category and taxonomy files
%% Map dataset names and category names to scientific names
i_row = 1; row = taxonomy_df.iloc[i_row]; row
"%% For each dataset, make sure we can map every category to the taxonomy"
dataset_name = list(lila_dataset_to_categories.keys())[0]
c = categories[0]
""
"Takes the megadb taxonomy mapping, extracts the rows that are relevant to"
"LILA, and does some cleanup."
""
%% Constants and imports
This is a partially-completed taxonomy file that was created from a different set of
"scripts, but covers *most* of LILA as of June 2022"
Created by get_lila_category_list.py
%% Read the input files
Get everything out of pandas
"%% Find all unique dataset names in the input list, compare them with data names from LILA"
d = input_taxonomy_rows[0]
The script that generates this dictionary creates a separate entry for bounding box
"metadata files, but those don't represent new dataset names"
%% Map input columns to output datasets
Make sure all of those datasets actually correspond to datasets on LILA
%% Re-write the input taxonomy file to refer to LILA datasets
Map the string datasetname:token to a taxonomic tree json
mapping = input_taxonomy_rows[0]
Make sure that all occurrences of this mapping_string give us the same output
assert taxonomy_string == taxonomy_mappings[mapping_string]
%% Re-write the input file in the target format
mapping_string = list(taxonomy_mappings.keys())[0]
""
prepare_lila_taxonomy_release.py
""
"Given the private intermediate taxonomy mapping, prepare the public (release)"
taxonomy mapping file.
""
%% Imports and constants
Created by get_lila_category_list.py... contains counts for each category
%% Find out which categories are actually used
dataset_name = datasets_to_map[0]
i_row = 0; row = df.iloc[i_row]; row
%% Generate the final output file
i_row = 0; row = df.iloc[i_row]; row
match_at_level = taxonomic_match[0]
i_row = 0; row = df.iloc[i_row]; row
"E.g.: (43117, 'genus', 'lepus', ['hares and jackrabbits']"
###############
---> CONSTANTS
###############
max_progressbar = count * (list(range(limit+1))[-1]+1)
"bar = progressbar.ProgressBar(maxval=max_progressbar,"
"widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()]).start()"
bar.update(bar.currval + 1)
bar.finish()
""
"Given a subset of LILA datasets, find all the categories, and start the taxonomy"
mapping process.
""
%% Constants and imports
Created by get_lila_category_list.py
'NACTI'
'Channel Islands Camera Traps'
%% Read the list of datasets
The script that generates this dictionary creates a separate entry for bounding box
"metadata files, but those don't represent new dataset names"
%% Find all categories
dataset_name = datasets_to_map[0]
%% Initialize taxonomic lookup
from taxonomy_mapping.species_lookup import (
"get_taxonomic_info, print_taxonomy_matche)"
%% Manual lookup
%%
q = 'white-throated monkey'
raise ValueError('')
%% Match every query against our taxonomies
mapping_string = category_mappings[1]; print(mapping_string)
...for each mapping
%% Write output rows
""
"Does some consistency-checking on the LILA taxonomy file, and generates"
an HTML preview page that we can use to determine whether the mappings
make sense.
""
%% Imports and constants
"lila_taxonomy_file = r""G:\git\agentmorrisprivate\lila-taxonomy\lila-taxonomy-mapping.csv"""
"lila_taxonomy_file = r""G:\temp\lila\lila_additions_2022.06.29.csv"""
%% Support functions
%% Read the taxonomy mapping file
%% Prepare taxonomy lookup
from taxonomy_mapping.species_lookup import (
"get_taxonomic_info, print_taxonomy_matche)"
%% Optionally remap all gbif-based mappings to inat (or vice-versa)
%%
i_row = 1; row = df.iloc[i_row]; row
This should be zero for the release .csv
%%
%% Check for mappings that disagree with the taxonomy string
Look for internal inconsistency
Look for outdated mappings
i_row = 0; row = df.iloc[i_row]
%% List null mappings
""
"These should all be things like ""unidentified"" and ""fire"""
""
i_row = 0; row = df.iloc[i_row]
%% List mappings with scientific names but no common names
%% List mappings that map to different things in different data sets
x = suppress_multiple_matches[-1]
...for each row where we saw this query
...for each row
"%% Verify that nothing ""unidentified"" maps to a species or subspecies"
"E.g., ""unidentified skunk"" should never map to a specific species of skunk"
%% Make sure there are valid source and level values for everything with a mapping
%% Find WCS mappings that aren't species or aren't the same as the input
"WCS used scientific names, so these remappings are slightly more controversial"
then the standard remappings.
row = df.iloc[-500]
"Anything marked ""species"" or ""unknown"" by definition doesn't map to a species,"
so ignore these.
print('WCS query {} ({}) remapped to {} ({})'.format(
"query,common_name,scientific_name,common_names_from_taxonomy))"
%% Download sample images for all scientific names
i_row = 0; row = df.iloc[i_row]
if s != 'mirafra':
continue
Check whether we already have enough images for this query
"print('Skipping query {}, already have {} images'.format(s,len(sizes_above_threshold)))"
Check whether we've already run this query for a previous row
...for each row in the mapping table
%% Rename .jpeg to .jpg
"print('Renaming {} to {}'.format(fn,new_fn))"
%% Choose representative images for each scientific name
s = list(scientific_name_to_paths.keys())[0]
Be suspicious of duplicate sizes
...for each scientific name
%% Delete unused images
%% Produce HTML preview
i_row = 2; row = df.iloc[i_row]
"image_paths = [os.path.relpath(p, output_base) for p in image_paths]"
...for each row
%% Open HTML preview
######
""
species_lookup.py
""
Look up species names (common or scientific) in the GBIF and iNaturalist
taxonomies.
""
Run initialize_taxonomy_lookup() before calling any other function.
""
######
%% Constants and imports
As of 2020.05.12:
""
"GBIF: ~777MB zipped, ~1.6GB taxonomy"
"iNat: ~2.2GB zipped, ~51MB taxonomy"
These are un-initialized globals that must be initialized by
the initialize_taxonomy_lookup() function below.
%% Functions
Initialization function
# Load serialized taxonomy info if we've already saved it
"# If we don't have serialized taxonomy info, create it from scratch."
Download and unzip taxonomy files
taxonomy_name = list(taxonomy_urls.items())[0][0]; zip_url = list(taxonomy_urls.items())[0][1]
Don't download the zipfile if we've already unzipped what we need
Bypasses download if the file exists already
Unzip the files we need
...for each file that we need from this zipfile
Remove the zipfile
os.remove(zipfile_path)
...for each taxonomy
"Create dataframes from each of the taxonomy files, and the GBIF common"
name file
Load iNat taxonomy
Load GBIF taxonomy
Remove questionable rows from the GBIF taxonomy
Load GBIF vernacular name mapping
Only keep English mappings
Convert everything to lowercase
"For each taxonomy table, create a mapping from taxon IDs to rows"
Create name mapping dictionaries
Build iNat dictionaries
row = inat_taxonomy.iloc[0]
Build GBIF dictionaries
"The canonical name is the Latin name; the ""scientific name"""
include the taxonomy name.
""
http://globalnames.org/docs/glossary/
This only seems to happen for really esoteric species that aren't
"likely to apply to our problems, but doing this for completeness."
Don't include taxon IDs that were removed from the master table
Save everything to file
...def initialize_taxonomy_lookup()
"list of dicts: {'source': source_name, 'taxonomy': match_details}"
i_match = 0
"list of (taxon_id, taxonRank, scientific name, [vernacular names])"
corresponding to an exact match and its parents
Walk taxonomy hierarchy
This can happen because we remove questionable rows from the
GBIF taxonomy
"print(f'Warning: no row exists for parent_taxon_id {parent_taxon_id},' + \"
"f'child taxon_id: {taxon_id}, query: {query}')"
The GBIF taxonomy contains unranked entries
...while there is taxonomy left to walk
...for each match
Remove redundant matches
i_tree_a = 0; tree_a = matching_trees[i_tree_a]
i_tree_b = 1; tree_b = matching_trees[i_tree_b]
"If tree a's primary taxon ID is inside tree b, discard tree a"
""
taxonomy_level_b = tree_b['taxonomy'][0]
...for each level in taxonomy B
...for each tree (inner)
...for each tree (outer)
...def traverse_taxonomy()
"print(""Finding taxonomy information for: {0}"".format(query))"
"In GBIF, some queries hit for both common and scientific, make sure we end"
up with unique inputs
"If the species is not found in either taxonomy, return None"
Both GBIF and iNat have a 1-to-1 mapping between taxon_id and row number
Walk both taxonomies
...def get_taxonomic_info()
m = matches[0]
"For example: [(9761484, 'species', 'anas platyrhynchos')]"
...for each taxonomy level
...for each match
...def print_taxonomy_matches()
%% Taxonomy functions that make subjective judgements
query = 'person'
Do we have an iNat match?
"print_taxonomy_matches(inat_matches, verbose=True)"
"print_taxonomy_matches(gbif_matches, verbose=True)"
print('Warning: multiple iNat matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple iNat common names for {query}')
Default to returning the query
"print(f'Matched iNat {query} to {scientific_name},{common_name}')"
...if we had iNat matches
If we either prefer GBIF or didn't have iNat matches
""
Code is deliberately redundant here; I'm expecting some subtleties in how
handle GBIF and iNat.
print('Warning: multiple GBIF matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple GBIF common names for {query}')
Default to returning the query
...if we needed to look in the GBIF taxonomy
...def _get_preferred_taxonomic_match()
%% Interactive drivers and debug
%% Initialization
%% Taxonomic lookup
query = 'lion'
print(matches)
Print the taxonomy in the taxonomy spreadsheet format
%% Directly access the taxonomy tables
%% Command-line driver
Read command line inputs (absolute path)
Read the tokens from the input text file
Loop through each token and get scientific name
""
process_species_by_dataset
""
We generated a list of all the annotations in our universe; this script is
used to (interactively) map them onto the GBIF and iNat taxonomies.  Don't
"try to run this script from top to bottom; it's used like a notebook, not like"
"a script, since manual review steps are required."
""
%% Imports
%autoreload 0
%autoreload -species_lookup
%% Constants
Input file
Output file after automatic remapping
File to which we manually copy that file and do all the manual review; this
should never be programmatically written to
The final output spreadsheet
HTML file generated to facilitate the identificaiton of egregious mismappings
%% Functions
Prefer iNat matches over GBIF matches
query = 'person'
Do we have an iNat match?
"print_taxonomy_matches(inat_matches, verbose=True)"
"print_taxonomy_matches(gbif_matches, verbose=True)"
print('Warning: multiple iNat matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple iNat common names for {query}')
Default to returning the query
"print(f'Matched iNat {query} to {scientific_name},{common_name}')"
...if we had iNat matches
If we either prefer GBIF or didn't have iNat matches
""
Code is deliberately redundant here; I'm expecting some subtleties in how
handle GBIF and iNat.
print('Warning: multiple GBIF matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple GBIF common names for {query}')
Default to returning the query
...if we needed to look in the GBIF taxonomy
...def get_preferred_taxonomic_match()
%% Initialization
%% Test single-query lookup
%%
%%
"q = ""grevy's zebra"""
%% Read the input data
%% Run all our taxonomic lookups
i_row = 0; row = df.iloc[i_row]
query = 'lion'
...for each query
Write to the excel file that we'll use for manual review
%% Download preview images for everything we successfully mapped
uncomment this to load saved output_file
"output_df = pd.read_excel(output_file, keep_default_na=False)"
i_row = 0; row = output_df.iloc[i_row]
...for each query
%% Write HTML file with representative images to scan for obvious mis-mappings
i_row = 0; row = output_df.iloc[i_row]
...for each row
%% Look for redundancy with the master table
Note: `master_table_file` is a CSV file that is the concatenation of the
"manually-remapped files (""manual_remapped.xlsx""), which are the output of"
this script run across from different groups of datasets. The concatenation
"should be done manually. If `master_table_file` doesn't exist yet, skip this"
"code cell. Then, after going through the manual steps below, set the final"
manually-remapped version to be the `master_table_file`.
%% Manual review
Copy the spreadsheet to another file; you're about to do a ton of manual
review work and you don't want that programmatically overwrriten.
""
See manual_review_xlsx above
%% Read back the results of the manual review process
%% Look for manual mapping errors
Manually inspect df for typos in 'dataset_names' and 'taxonomy_level' columns
Identify rows where:
""
- 'taxonomy_level' does not match level of 1st element in 'taxonomy_string'
- 'scientific_name' does not match name of 1st element in 'taxonomy_string'
""
...both of which typically represent manual mapping errors.
i_row = 0; row = df.iloc[i_row]
"I'm not sure why both of these checks are necessary, best guess is that"
the Excel parser was reading blanks as na on one OS/Excel version and as ''
on another.
The taxonomy_string column is a .json-formatted string; expand it into
an object via eval()
"%% Find scientific names that were added manually, and match them to taxonomies"
i_row = 0; row = df.iloc[i_row]
...for each query
%% Write out final version
""
"Generate read-only SAS URLs for all LILA containers, to facilitate partial downloads."
""
The results of this script end up here:
""
http://lila.science/wp-content/uploads/2020/03/lila_sas_urls.txt
""
"Update: that file is manually maintained now, it can't be programmatically generated"
""
%% Imports
Read-only
%% Enumerate containers
%% Generate SAS tokens
%% Generate SAS URLs
%% Write to output file
--------some stuff needed to get AJAX to work with bottle?--------#
-------------------------------------------------------------------------------- #
PREPARE TO QUEUE IMAGES FOR LABELING
-------------------------------------------------------------------------------- #
# Connect as USER to database DB_NAME through peewee and initialize database proxy
# Load embedding model
---------------------------------------------------------------------- #
CREATE QUEUE OF IMAGES TO LABEL
---------------------------------------------------------------------- #
Use classifier to generate predictions
# Update model predicted class in PostgreSQL database
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
-------------------------------------------------------------------------------- #
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI
-------------------------------------------------------------------------------- #
"# static routes (to serve CSS, etc.)"
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps"
"return bottle.static_file(filename, root='../../../../../../../../../../../.')"
# dynamic routes
if data['display_grayscale']:
indices_to_exclude.update(set(color_indices))
elif not data['display_grayscale']:
indices_to_exclude.update(set(grayscale_indices))
data['display_images'] = {}
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices]
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices]
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices]
data['display_images']['detection_categories'] = []
for i in indices:
if str(dataset.samples[i][1]) == 'None':
data['display_images']['detection_categories'].append('None')
else:
existing_category_entries = {cat.id: cat.name for cat in Category.select()}
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()"
data['display_images']['detection_categories'].append(cat_name)
Use image ids in images_to_label to get the corresponding dataset indices
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
print(set(dataset.set_indices[4]).update(set(indices_to_label)))
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Use image id images_to_label to get the corresponding dataset index
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Train on samples that have been labeled so far
dataset.set_kind(DetectionKind.UserDetection.value)
print(y_train)
Predict on the samples that have not been labeled
print(y_pred)
Update model predicted class in PostgreSQL database
timer = time.time()
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
print('Updating the database took %0.2f seconds'%(time.time() - timer))
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
"once the classifier has been trained the first time, switch to AL sampling"
""
make_oneclass_json.py
""
"Takes a coco-camera-traps .json database and collapses species classes to binary,"
optionally removing labels from empty images (to be detector-friendly) (depending on
"""experiment_type"")."
""
"Assumes that empty images are labeled as ""empty""."
""
%% Imports and environment
%% Core conversion function
"We're removing empty images from the annotation list, but not from"
"the ""images"" list; they'll still get used in detector training."
print('Ignoring empty annotation')
%% Interactive driver
%%
Load annotations
Convert from multi-class to one-class
Write out the one-class data
%% Command-line driver
""
plot_bounding_boxes.py
""
Takes a .json database containing bounding boxes and renders those boxes on the
source images.
""
"This assumes annotations in coco-camera-traps format, with absolute bbox"
coordinates.
""
%% Imports and environment
How many images should we process?  Set to -1 to process all images.
Should we randomize the image order?
%%  Read database and build up convenience mappings
Category ID to category name
Image ID to image info
Image ID to image path
Image ID to all annotations referring to this image
"%% Iterate over images, draw bounding boxes, write to file"
For each image
image = images[0]
Build up a list of bounding boxes to draw on this image
Load the image
Create figure and axes
Display the image
ann = imageAnnotations[0]
"For each annotation associated with this image, render bounding box and label"
"In the Rectangle() function, the first argument (""location"") is the bottom-left"
of the rectangle.
""
Origin is the upper-left of the image.
Add the patch to the Axes
Add a class label
This is magic goop that removes whitespace around image plots (sort of)
Write the output image
...for each image
""
plot_imerit_annotations.py
""
Takes a .json file full of bounding box annotations and renders those boxes on the
source images.
""
"This assumes annotations in the format we receive them, specifically:"
""
1) Relative bbox coordinates
"2) A list of .json objects, not a well-formatted .json file"
""
"I.e., don't use this on a COCO-style .json file.  See plot_bounding_boxes.py"
for the same operation performed on a proper COCO-camera-traps database.
""
%% Imports and environment
"os.makedirs(outputBase, exist_ok=True)"
%%  Read all source images and build up a hash table from image name to full path
"This spans training and validation directories, so it's not the same as"
just joining the image name to a base path
"%% Iterate over annotations, draw bounding boxes, write to file"
annData has keys:
""
"annotations, categories, images"
""
Each of these are lists of dictionaries
%% Render all annotations on each image in the sequence
%% Pull out image metadata
Build up a list of bounding boxes to draw on this image
Pull out just the image name from the filename
""
File names look like:
""
seq6efffac2-5567-11e8-b3fe-dca9047ef277.frame1.img59a94e52-23d2-11e8-a6a3-ec086b02610b.jpg
"m = re.findall(r'img(.*\.jpg)$', imgFileName, re.M|re.I)"
print(m)
assert(len(m) == 1)
queryFileName = m[0]
Map this image back to the original directory
"%% Loop over annotations, find annotations that match this image"
%%
"x,y,w,h"
""
"x,y is the bottom-left of the rectangle"
""
"x,y origin is the upper-left"
...for each annotation
%% Render with PIL (scrap)
%% Render with Matplotlib
Create figure and axes
Display the image
Location is the bottom-left of the rect
""
Origin is the upper-left
Add the patch to the Axes
This is magic goop that removes whitespace around image plots (sort of)
os.startfile(outputFileName)
%% Showing figures on-screen during debugging
plt.show()
Various (mostly unsuccessful) approaches to getting the plot window to show up
"in the foreground, which is a backend-specific operation..."
""
fig.canvas.manager.window.activateWindow()
fig.canvas.manager.window.raise_()
fm = plt.get_current_fig_manager()
"fm.window.attributes('-topmost', 1)"
"fm.window.attributes('-topmost', 0)"
""
# This is the one that I found to be most robust... at like 80% robust.
plt.get_current_fig_manager().window.raise_()
%%
...for each image
...for each file
""
convert_imerit_json_to_coco_json.py
""
"Takes a .json file with bounding boxes but no class labels, and a .json file containing the"
"class labels for those images, and creates a new json file with class labels and bounding"
boxes.
""
"The bounding box .json file is in the format returned by our annotators, which is not"
actually a fully-formed .json file; rather it's a series of .json objects
""
"Leaves filenames intact.  Stores imerit ""category IDs"" (empty/human/group/animal) in a new"
"field called ""annotation_type""."
""
%% Imports and constants
%% Configure files and paths
"For Snapshot Serengeti, we stored image IDs in our annotation files as:"
""
S1_B06_R1_PICT0008
""
...but the corresponding ID in the master database is actually:
""
S1\B06\R1\S1_B06_R1_PICT0008
""
"If this is ""True"", we'll expand the former to the latter"
Handling a one-off issue in which .'s were mysteriously replaced with -'s
"in our annotations.  This will be set dynamically, but I keep it here as"
a constant to remind me to remove this code when we clean this issue up.
Used in the (rare) case where a bounding box was added to an image that was originally
annotated as empty
Used in the (rare) case where we added bounding boxes to an image with multiple species
"Should we include ambiguous bounding boxes (with the ""ambiguous"" category label)"
in the output file?  Ambiguous boxes are boxes drawn on images with multiple species.
"%%  Read metadata from the master database, bounding boxes from the annotations file"
"The bounding box .json file is in the format returned by our annotators, which is not"
actually a fully-formed .json file; rather it's a series of .json objects
"Each element of annData is a dictionary corresponding to a single sequence, with keys:"
""
"annotations, categories, images"
sequence = annData[0]
%% Build convenience mappings
Image ID to images
Category ID to categories (referring to the database categories)
"Image ID to categories (i.e., species labels)"
Utility function we'll use to create annotations for images in empty
sequences (empty images in non-empty sequences already have annotations)
"%% Reformat annotations, grabbing category IDs from the master database (prep)"
iSequence = 0; sequence = annData[0]
"%% Reformat annotations, grabbing category IDs from the master database (loop)"
Make a copy here; we're going to manipulate the sequence annotations
when we need to add synthetic annotations for empty images
im = sequenceImages[0]
Are there any annotations in this sequence?
Which images in this sequence have annotations?
For each image in this sequence...
imeritImageID = im['id']
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG
Confirm that the file exists
Hande a one-off issue with our annotations
datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG
""
...had become:
""
datasetsnapshotserengeti.seqASG000001a-frame0.imgS1_B06_R1_PICT0008.JPG
Does it look like we encountered this issue?
Convert:
""
S1_B06_R1_PICT0008
""
...to:
""
S1/B06/B06_R1/S1_B06_R1_PICT0008
Make sure we only see each image once
Create empty annotations for empty images
""
Here we use the *unmodified* file name
Create an empty annotation for this image
Annotations still use the annotation filename (not database ID) at this point;
these will get converted to database IDs below when we process the
whole sequence.
Sanity-check image size
"print('Warning: img {} was listed in DB as {}x{}, annotated as {}x{}, actual size{}x{}'.format("
"old_id,new_im['width'],new_im['height'],im['width'],im['height'],imgObj.width,imgObj.height))"
...for each image in this sequence
For each annotation in this sequence...
ann = sequenceAnnotations[0]
Prepare an annotation using the category ID from the database and
the bounding box from the annotations file
Maintain iMerit's annotation category
Generate an (arbitrary) ID for this annotation; the COCO format has a concept
"of annotation ID, but our annotation files don't"
This was a one-off quirk with our file naming
We'll do special handling of images with multiple categories later
Store the annotation type (group/human/animal/empty)
This annotation has no bounding box but the image wasn't originally
annotated as empty
This annotation has a bounding box but the image was originally
annotated as empty
unnormalize the bbox
... for each annotation in this sequence
... for each sequence
%% Post-processing
Count empty images
...for each file
%% Sanity-check empty images
""
make_ss_annotation_image_folder.py
""
Take a directory full of images with the very long filenames we give annotators:
""
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension
""
"...along with a COCO-camera-traps database referring to those files, and:"
""
1) Creates a new COCO-camera-traps database with the original filenames in them
(copying the annotations)
""
2) Optionally creates a new directory with those images named according to the
"Snapshot Serengeti naming convention, including complete relative paths."
""
See convert_imerit_json_to_coco_json to see how we get from the original annotation
.json to a COCO-camera-traps database.
""
%% Constants and imports
%% Configure files/paths
%% Read the annotations (referring to the old filenames)
"%% Update filenames, optionally copying files"
im = data['images'][0]
For each image...
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG
"Find the image name, e.g. S1_B06_R1_PICT0008"
Convert:
""
S1_B06_R1_PICT0008.JPG
""
...to:
""
S1/B06/B06_R1/S1_B06_R1_PICT0008.JPG
...for each image
%% Write the revised database
""
get_annotation_tool_link.py
""
"Takes a COCO-camera-traps-style .json file with URLs already embedded, and prepares"
a link to the visipedia annotation tool that reviews a subset of those images.
""
""
create_new_annotation_json.py
""
"Creates a subset of a larger .json database, in this case specifically to pick some images"
from Snapshot Serengeti.
""
from utils import get_db_dicts
for seq in already_annotated:
seq_to_ims.pop(seq)
remove already annotated images
add lion images
lion_seqs = cat_to_seqs[cat_to_id['lionMale']] + cat_to_seqs[cat_to_id['lionFemale']]
#print(len(lion_seqs))
lion_seqs = [seq for seq in lion_seqs if seq not in already_annotated]
#print(len(lion_seqs))
"lion_seqs_to_annotate.extend(random.sample(lion_seqs, min(len(lion_seqs),num_lions)))"
print(len(lion_seqs_to_annotate))
num_elephants = 1000
elephant_seqs = cat_to_seqs[cat_to_id['elephant']]
#print(len(lion_seqs))
elephant_seqs = [seq for seq in elephant_seqs if seq not in already_annotated]
#print(len(lion_seqs))
"elephant_seqs_to_annotate = random.sample(elephant_seqs, num_elephants)"
num_empty = 10
empty_seqs_to_annotate = []
for loc in loc_to_seqs:
empty_seqs = cats_per_location[loc][cat_to_id['empty']]
empty_seqs = [seq for seq in empty_seqs if seq not in already_annotated]
empty_seqs = [seq for seq in empty_seqs if seq_to_season[seq] in seasons_to_keep]
"empty_seqs_to_annotate.extend(random.sample(empty_seqs, min(len(empty_seqs),num_empty)))"
ims_to_annotate.extend(empty_ims_to_annotate)
""
filter_database.py
""
"Look through a COCO-ct database and find images matching some crtieria, writing"
a subset of images and annotations to a new file.
""
%% Constants and imports
%% Configuration
%%  Read database and build up convenience mappings
Category ID to category name
Image ID to image info
Image ID to image path
%% Filter
ann = annotations[0]
Is this a tiny box or a group annotation?
"x,y,w,h"
All empty annotations should be classified as either empty or ambiguous
""
"The ambiguous cases are basically minor misses on the annotators' part,"
where two different small animals were present somewhere.
%% Write output file
""
find_images_for_classes.py
""
"Given a .json database, find images that are associated with one or more"
classes.
""
%% Constants and imports
%% Configuration
%%  Read database and build up convenience mappings
Category ID to category name
Image ID to image info
Image ID to image path
%% Look for target-class annotations
ann = annotations[0]
""
create_tfrecords_format.py
""
This script creates a tfrecords file from a dataset in VOTT format.
%% Imports and environment
set up the filenames and annotations
This loop reads the bboxes and corresponding labels and assigns them
the correct image. Kind of slow at the moment...
If needed: merging all classes
bbox_labels = ['Animal' for _ in bbox_labels]
BBox coords are stored in the format
"x_min (of width axis) y_min (of height axis), x_max, y_max"
Coordinate system starts in top left corner
"In this framework, we need ('ymin', 'xmin', 'ymax', 'xmax') format"
print out some stats
To make sure we loaded the bboxes correctly:
self.validate_bboxes()
For each image in the data set...
Make sure all are greater equal 0
%% Main tfrecord generation function
Propagate optional metadata to tfrecords
endfor each annotation for the current image
endfor each image
"Calculate number of shards to get the desired number of images per record,"
ensure it is evenly divisible by the number of threads
%% Command-line driver
%% Driver
""
make_tfrecords_cis_trans.py
""
Given a .json file that contains a three-element list (train/val/test) of image IDs and a .json database that contains
"those image IDs, generates tfrecords whose filenames include ""train""/""val""/""test"""
""
"dataset = json.load(open('/ai4efs/databases/snapshotserengeti/oneclass/SnapshotSerengeti_Seasons_1_to_4_tfrecord_format_valid_ims.json','r'))"
print('Creating trans_val tfrecords')
dataset = [im_id_to_im[idx] for idx in trans_val]
""
create_classification_tfrecords_from_json.py
""
Called from make_tfrecords_cis_trans.py if you're running a classification experiment.
""
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/'
datafile = 'eccv_18_annotation_files_oneclass/CaltechCameraTrapsECCV18'
image_file_root = datafolder+'eccv_18_all_images/'
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/'
datafolder = '/data/iwildcam/'
datafile = 'combined_iwildcam_annotations_oneclass/eccv_train_and_imerit_2'
need consecutive category ids
old_cat_id_to_new_cat_id = {categories[idx]['id']:idx+1 for idx in range(len(categories))}
print(old_cat_id_to_new_cat_id)
remove multiclass images
print(images[0])
print(vis_data[0])
""
make_tfrecords_from_json.py
""
"Given a coco-camera-traps .json file, creates tfrecords"
""
Thin wrapper for create_tfrecords_from_json.
""
%% Constants and imports
%% Constants and imports (interactive)
%%
%% Main tfrecord generation function
"check whether the input file has already been converted to the tfrecords format,"
"if not, convert"
"Calculate number of shards to get the desired number of images per record,"
ensure it is evenly divisible by the number of threads
%% Interactive driver
%%
%% Command-line driver
eMammal_make_splits.py
""
"Based on a tfrecords_format json file of the database, creates 3 splits according to"
the specified fractions based on location (images from the same location should be in
one split) or based on images.
""
"If a previous split is provided (append_to_previous_split is True), the entries in"
"each split will be preserved, and new entries will be appended, so that new models"
can warm start with a model trained on the original splits.
configurations and paths
approximate fraction for the new entries
read in the previous splits of image ID or location ID if available
"find new locations and assign them to a split, without reassigning any previous locations"
"find out which images are new, shuffle and split them"
do NOT sort the IDs to keep the shuffled order
export PYTHONPATH=$PYTHONPATH:tfmodels/research
add empty category
"add all images that don't have annotations, with cat empty"
print(detection_results.keys())
group the detections and gts by image id:
print(image_id)
print(len(scores))
print(len(labels))
recall_thresh = 0.9
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall])
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])"
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall])
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])"
exp_name = 'eccv_train'
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json'
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(len(detection_results['images']))
print(len(seqs))
"print(len(seqs[0]),len(seqs["
print(detection_results.keys())
group the detections by image id:
group the ground truth annotations by image id:
print(seq)
"for image_id, dets in per_image_detections.iteritems():"
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
print(num_gts)
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
if sum(detected_class_labels)>0:
print('detected animal box')
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])"
"print(scores, tp_fp_labels)"
num_total_gts += num_gts
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
print('valid box')
"print(best_im, best_score)"
exp_name = 'eccv_train'
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json'
export PYTHONPATH=$PYTHONPATH:tfmodels/research
need to loop over confidence values
"for each value, check if any detections on the image are > conf"
"If so, that image gets class ""animal"""
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0"
"for image_id, dets in per_image_detections.iteritems():"
"calc prec, rec for this confidence thresh"
group the ground truth annotations by image id:
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(detection_results.keys())
group the detections and gts by image id:
print(image_id)
print(len(scores))
print(len(labels))
recall_thresh = 0.9
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall])
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])"
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall])
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])"
print(sorted_ap)
plt.bar(sorted_ap)
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(detection_results.keys())
group the detections by image id:
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
print(gts)
print(num_gts)
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[:2])"
"print(scores, tp_fp_labels)"
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
export PYTHONPATH=$PYTHONPATH:tfmodels/research
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
print('detected animal box')
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])"
"print(scores, tp_fp_labels)"
num_total_gts += num_gts
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
print('valid box')
best_im = np.argmax(max_im_scores)
"print(best_im, best_score)"
"temp_labels = np.zeros(len(im_detection_labels),  dtype=np.int32)"
"temp_scores = np.zeros(len(im_detection_scores), dtype=np.float32)"
"for j in range(min(im_num_gts, len(im_detection_labels))):"
temp_labels[j] = True #TODO: this currently only works for oneclass?
temp_scores[j] = best_score
im_detection_labels = temp_labels
im_detection_scores = temp_scores
num_total_gts+=im_num_gts
"print(len(detection_scores), len(detection_scores[0]), len(detection_scores[1]))"
exp_name = 'small_balanced_cct'
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(seq)
"for image_id, dets in per_image_detections.iteritems():"
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
seq_num_gts.append(num_gts)
print(num_gts)
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
num_total_gts += 1
print('valid box')
"print(best_im, best_score)"
print('no valid box')
if sum(seq_num_gts)>0:
export PYTHONPATH=$PYTHONPATH:tfmodels/research
need to loop over confidence values
"for each value, check if any detections on the image are > conf"
"If so, that image gets class ""animal"""
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0"
"calc prec, rec for this confidence thresh"
export PYTHONPATH=$PYTHONPATH:tfmodels/research
group the detections by image id:
group the ground truth annotations by image id:
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
print('detected animal box')
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])"
"print(scores, tp_fp_labels)"
num_total_gts += num_gts
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
print('valid box')
"print(best_im, best_score)"
""
evaluate_detections.py
""
Adapted from analyze_detection.py which is now archived.
""
%% Imports and constants
%% Functions
"labels input to compute_object_detection_metrics() needs to start at 0, not 1"
num_detections = len(dets['boxes'])
to prevent 'Invalid dimensions for box data.' error
this box will not match any detections
compute one-class precision/recall/average precision (if every box is just of an object class)
%% Command-line driver
""
detection_eval_utils.py
""
Utility functions used in evaluate_detections.py
""
group the ground truth annotations by image id
#####
""
detect_and_predict_image.py
""
"Functions to load a TensorFlow detection and a classification model, run inference,"
"render bounding boxes on images, and write out the resulting"
images (with bounding boxes and classes).
""
"See the ""test driver"" cell for example invocation."
""
""
#####
"%% Constants, imports, environment"
Minimum detection confidence for showing a bounding box on the output image
Stick this into filenames before the extension for the rendered result
Number of top-scoring classes to show at each bounding box
%% Core detection functions
Load images if they're not already numpy arrays
iImage = 0; image = images[iImage]
"Load the image as an nparray of size h,w,nChannels"
"There was a time when I was loading with PIL and switched to mpimg,"
"but I can't remember why, and converting to RGB is a very good reason"
"to load with PIL, since mpimg doesn't give any indication of color"
"order, which basically breaks all .png files."
""
"So if you find a bug related to using PIL, update this comment"
"to indicate what it was, but also disable .png support."
image = mpimg.imread(image)
This shouldn't be necessarily when loading with PIL and converting to RGB
Actual detection
...for each image
"Currently ""boxes"" is a list of length nImages, where each element is shaped as"
""
"1,nDetections,4"
""
"This implicitly banks on TF giving us back a fixed number of boxes, let's assert on this"
to make sure this doesn't silently break in the future.
iBox = 0; box = boxes[iBox]
"""scores"" is a length-nImages list of elements with size 1,nDetections"
"""classes"" is a length-nImages list of elements with size 1,nDetections"
""
"Still as floats, but really representing ints"
Squeeze out the empty axis
boxes is nImages x nDetections x 4
scores and classes are both nImages x nDetections
Get input and output tensors of classification model
"imsize = cur_image['width'], cur_image['height']"
Select detections with a confidence larger 0.5
Get these boxes and convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes"
"However, we need to make sure that it box coordinates are still within the image"
For convenience:
Create an array with contains the index of the corresponding crop_box for each selected box
i.e. [False False 0 False 1 2 3 False False]
For each box
If this box should be classified
Run inference
if box should not be classified
...for each box
species_scores should have shape len(images) x len(boxes) x num_species
...for each image
...with tf.Session
with classification_graph
species_scores should have shape len(images) x len(boxes) x num_species
%% Rendering functions
Display the image
plt.show()
"top, left, bottom, right"
""
"x,y origin is the upper-left"
Generate bounding box text
Choose color based on class
Location is the bottom-left of the rect
""
Origin is the upper-left
iRight = x + w
iTop = y + h
Add the patch to the Axes
Add class description
First determine best location by finding the corner that is closest to the image center
relative corner coordinates
relative coordinates of image center
Compute pair-wise squared distance and get the index of the one with minimal distance
Get the corresponding coordinates ...
... and alignment for the text box
Plot the text box with background
...for each box
This is magic goop that removes whitespace around image plots (sort of)
"plt.savefig(outputFileName, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)"
os.startfile(outputFileName)
...for each image
...def render_bounding_boxes
Load and run detector on target images
Read the name of all classes
remove empty lines
%% Interactive driver
%%
%%
%% File helper functions
%% Command-line driver
Hack to avoid running on already-detected images
""
"Script for evaluating a frozen graph given a COCO-style dataset, which was generated by"
the ../data_management/databases/classification/make_classification_dataset.py script.
This is useful for debugging as the accuracy reported by this script should match the
accuracy reported by the Tensorflow training.
""
Check that all files exists for easier debugging
Load frozen graph
Collect tensors for input and output
Read image
Run inference
Check if correct
""
Script for evaluating precision/recall on a two-class problem given
"a frozen graph and a COCO-style dataset, which was generated by"
the ../data_management/databases/classification/make_classification_dataset.py script.
We assume the positive class is at index 0 (with a zero-based indexing)
""
Check that all files exists for easier debugging
Load frozen graph
Collect tensors for input and output
Read image
"with open(image_path, 'rb') as fi:"
"image =  sess.run(tf.image.decode_jpeg(fi.read(), channels=3))"
image = image / 255.
Run inference
predicted_class = np.argmax(predictions)
Check if correct
if coco.imgToAnns[image_id][0]['category_id'] == predicted_class:
correct = correct + 1
"In matplotlib < 1.5, plt.fill_between does not have a 'step' argument"
""
Script for selecting testing images of a COCO-style dataset generated by the script
../data_management/databases/classification/make_classification_dataset.py in a consistent
manner and predicting the class for it.
""
Mandatory parameters
Optional parameters
Validate parameters
Derived parameters
"We assume that the dataset was generated with the make_classification_dataset.py script,"
hence the images should be located in the same folder as the json
Make seletion deterministic
Load frozen graph
Get dataset information
Get classes
...and the class list corresponding to the model outputs by assuming
that they are in order of their ids
Get images of each class
Shuffle the image list
Start the image sampling
"Set of avaiable class IDs, will be filled below"
If there are still images left for that class
"Get image for the sampled class, we already shuffled the class images before so"
we can simply pop()
Start prediction
Collect tensors for input and output
Read image
Run inference
Print output to log file
#####
""
api_apply_classifier_single_node.py
""
Takes the JSON file produced by the detection API and
classifies all boxes above a confidence threshold.
""
#####
"%% Constants, imports, environment"
Assumes that the root of the CameraTraps repo is on the PYTHONPATH
Minimum detection confidence for classifying an object
Number of top-scoring classes to show at each bounding box
Enlargment factor applied to boxes before passing them to the classifier
""
Provides more context and can lead to better results
List of detection categories for which we will run the classification
""
"Currently there are {""1"": ""animal"", ""2"": ""person"", ""4"": ""vehicle""}"
""
Should be a list of string-formatted ints.
Number of significant float digits in JSON output
%% Core detection functions
Read the name of all classes
remove empty lines
Create field with name *classification_categories*
Add classes using 0-based indexing
def add_classification_categories
Make sure we have the right json object
Get input and output tensors of classification model
For each image
Read image
"Scale pixel values to [0,1]"
For each box
Skip detections with low confidence
Skip if detection category is not in whitelist
Skip if already classified
"Get current box in relative coordinates and format [x_min, y_min, width_of_box, height_of_box]"
"Convert to [ymin, xmin, ymax, xmax] and store it as 1x4 numpy array so we can"
re-use the generic multi-box padding code
Convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes."
""
"However, we need to make sure that it box coordinates are still within the image."
Get the first (and only) row as our bbox to classify
Get the image data for that box
Run inference
Add an empty list to the json for our predictions
Add the *num_annotated_classes* top scoring classes
...for each box
...for each image
...with tf.Session
with classification_graph
def classify_boxes
Load classification model
Load detector json
Add classes to detector_json
"Run classifier on all images, changes will be writting directly to the json"
Write output json
def load_and_run_classifier
%% Command-line driver
""
Script for selecting testing images of a COCO-style dataset generated by the script
../data_management/databases/classification/make_classification_dataset.py in a consistent
manner and predicting the class for it.
""
Assumes the cameratraps repo root is on the path
Make seletion deterministic
Mandatory parameters
Optional parameters
Validate parameters
Tranfer parameters to post-processing format
Load frozen graph
Reading image list
Reading class list
Image sampling
Start prediction
Collect tensors for input and output
Array for collecting infos for rendering the html
Read image
"Scale pixel values to [0,1]"
Run inference
Add links to all available classes
""
predict_image.py
""
"Given a pointer to a frozen detection graph, runs inference on a single image,"
printing the top classes to the console
""
%% Imports
%% Command-line processing
Check that all files exist for easier debugging
%% Inference
Load frozen graph
Load class list
Remove empty lines
Collect tensors for input and output
Read image
Run inference
Print output
""
Mostly unmodified script for freezing a model
Added for convenience and for possible future optimizations
""
Copyright 2015 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
'input_checkpoint' may be a prefix if we're using Saver V2 format
Remove all the explicit device specifications for this node. This helps to
make the graph more portable.
List of all partition variables. Because the condition is heuristic
"based, the list could include false positives."
This tensor doesn't exist in the graph (for example it's
'global_step' or a similar housekeeping element) so skip it.
`var_list` is required to be a map of variable names to Variable
tensors. Partition variables are Identity tensors that cannot be
handled by Saver.
Models that have been frozen previously do not contain Variables.
optimized_output_graph_def = optimize_for_inference_lib.optimize_for_inference(
"output_graph_def,"
"input_node_names.replace("" "", """").split("",""),"
"output_node_names.replace("" "", """").split("",""),"
tf.float32.as_datatype_enum)
Write GraphDef to file if output path has been given.
""
"Creates a graph description, which is required to create a frozen graph."
Adapted from from ./tf-slim/export_inference_graph.py
Added preprocessing to the definition for easier handling
""
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
tf.app.flags.DEFINE_integer(
"'batch_size', None,"
"'Batch size for the exported model. Defaulted to ""None"" so batch size can '"
'be specified at model runtime.')
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
#####################
Optimization Flags #
#####################
######################
Learning Rate Flags #
######################
######################
Dataset Flags #
######################
####################
Fine-Tuning Flags #
####################
"Note: when num_clones is > 1, this will actually have each clone to go"
over each epoch FLAGS.num_epochs_per_decay times. This is different
behavior from sync replicas and is expected to produce different results.
Warn the user if a checkpoint exists in the train_dir. Then we'll be
ignoring the checkpoint anyway.
TODO(sguada) variables.filter_variables()
######################
Config model_deploy #
######################
Create global_step
#####################
Select the dataset #
#####################
#####################
Select the network #
#####################
####################################
Select the preprocessing function #
####################################
#############################################################
Create a dataset provider that loads data from the dataset #
#############################################################
###################
Define the model #
###################
############################
Specify the loss function #
############################
Gather initial summaries.
"Gather update_ops from the first clone. These contain, for example,"
the updates for the batch_norm variables created by network_fn.
Add summaries for end_points.
Add summaries for losses.
Add summaries for variables.
################################
Configure the moving averages #
################################
########################################
Configure the optimization procedure. #
########################################
"If sync_replicas is enabled, the averaging will be done in the chief"
queue runner.
Update ops executed locally by trainer.
Variables to train.
and returns a train_tensor and summary_op
Add total_loss to summary.
Create gradient updates.
Add the summaries from the first clone. These contain the summaries
created by model_fn and either optimize_clones() or _gather_clone_loss().
Merge all summaries together.
##########################
Kicks off the training. #
##########################
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
#####################
Select the dataset #
#####################
###################
Select the model #
###################
#############################################################
Create a dataset provider that loads data from the dataset #
#############################################################
####################################
Select the preprocessing function #
####################################
###################
Define the model #
###################
Define the metrics:
Print the summaries to screen.
TODO(sguada) use num_epochs=1
This ensures that we make a single pass over all of the data.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Transform the image to floats.
"Randomly crop a [height, width] section of the image."
Randomly flip the image horizontally.
"Because these operations are not commutative, consider randomizing"
the order their operation.
Subtract off the mean and divide by the variance of the pixels.
Transform the image to floats.
Resize and crop if needed.
Subtract off the mean and divide by the variance of the pixels.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Use tf.slice instead of crop_to_bounding box as it accepts tensors to
define the crop size.
Compute the rank assertions.
Create a random bounding box.
""
Use tf.random_uniform and not numpy.random.rand as doing the former would
"generate random numbers at graph eval time, unlike the latter which"
generates random numbers at graph definition time.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Pass the real x only to one of the func calls.
The random_* ops do not necessarily clamp.
"Each bounding box has shape [1, num_boxes, box coords] and"
"the coordinates are ordered [ymin, xmin, ymax, xmax]."
A large fraction of image datasets contain a human-annotated bounding
box delineating the region of the image containing the object of interest.
We choose to create a new bounding box for the object which is a randomly
distorted version of the human-annotated bounding box that obeys an
"allowed range of aspect ratios, sizes and overlap with the human-annotated"
"bounding box. If no box is supplied, then we assume the bounding box is"
the entire image.
Crop the image to the specified bounding box.
"Each bounding box has shape [1, num_boxes, box coords] and"
"the coordinates are ordered [ymin, xmin, ymax, xmax]."
Restore the shape since the dynamic slice based upon the bbox_size loses
the third dimension.
This resizing operation may distort the images because the aspect
ratio is not respected. We select a resize method in a round robin
fashion based on the thread number.
Note that ResizeMethod contains 4 enumerated resizing methods.
We select only 1 case for fast_mode bilinear.
Randomly flip the image horizontally.
Randomly distort the colors. There are 1 or 4 ways to do it.
Crop the central region of the image with an area containing 87.5% of
the original image.
Resize the image to the specified height and width.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(sguada) docstring paragraph by (a) motivating the need for the file and
(b) defining clones.
TODO(sguada) describe the high-level components of model deployment.
"E.g. ""each model deployment is composed of several parts: a DeploymentConfig,"
"which captures A, B and C, an input_fn which loads data.. etc"
Set up DeploymentConfig
Create the global step on the device storing the variables.
Define the inputs
Define the optimizer.
Define the model including the loss.
Run training.
Namedtuple used to represent a clone during deployment.
"Namedtuple used to represent a DeployedModel, returned by deploy()."
Default parameters for DeploymentConfig
Create clones.
The return value.
Individual components of the loss that will need summaries.
Compute and aggregate losses on the clone device.
Add the summaries out of the clone device block.
Only use regularization_losses for the first clone
Compute the total_loss summing all the clones_losses.
Sum the gradients across clones.
Gather initial summaries.
Create Clones.
"Gather update_ops from the first clone. These contain, for example,"
the updates for the batch_norm variables created by model_fn.
Place the global step on the device storing the variables.
Compute the gradients for the clones.
Add summaries to the gradients.
Create gradient updates.
Only use regularization_losses for the first clone
Add the summaries from the first clone. These contain the summaries
created by model_fn and either optimize_clones() or _gather_clone_loss().
Add total_loss to summary.
Merge all summaries together.
Note that each grad_and_vars looks like the following:
"((grad_var0_clone0, var0), ... (grad_varN_cloneN, varN))"
!/usr/bin/python
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Failed to find ""index"" occurrence of item."
pylint: disable=broad-except
pylint: enable=broad-except
Grab the 'index' annotation.
Some images contain bounding box annotations that
"extend outside of the supplied image. See, e.g."
n03127925/n03127925_147.xml
"Additionally, for some bounding boxes, the min > max"
or the box is entirely outside of the image.
Example: <...>/n06470073/n00141669_6790.xml
Determine if the annotation is from an ImageNet Challenge label.
Note: There is a slight bug in the bounding box annotation data.
Many of the dog labels have the human label 'Scottish_deerhound'
instead of the synset ID 'n02092002' in the bbox.label field. As a
"simple hack to overcome this issue, we only exclude bbox labels"
*which are synset ID's* that do not match original synset label for
the XML file.
Guard against improperly specified boxes.
Note bbox.filename occasionally contains '%s' in the name. This is
data set noise that is fixed by just using the basename of the XML file.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The URLs where the MNIST data can be downloaded.
The names of the classes.
"First, process the training data:"
"Next, process the testing data:"
"Finally, write the labels file:"
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The URL where the Flowers data can be downloaded.
The number of images in the validation set.
Seed for repeatability.
The number of shards per dataset split.
Initializes function that decodes RGB JPEG data.
Read the filename:
Divide into train and test:
"First, convert the training and validation sets."
"Finally, write the labels file:"
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(nsilberman): Add tfrecord file type once the script is updated.
"If set to false, will not try to set label_to_names in dataset"
by reading them from labels.txt or github.
n01440764
n01443537
n02119247    black fox
n02119359    silver fox
pylint: disable=g-line-too-long
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The URL where the CIFAR data can be downloaded.
The number of training files.
The height and width of each image.
The names of the classes.
"First, process the training data:"
"Next, process the testing data:"
"Finally, write the labels file:"
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The labels file contains a list of valid labels are held in this file.
Assumes that the file contains entries as such:
n01440764
n01443537
n01484850
where each line corresponds to a label expressed as a synset. We map
each synset contained in the file to an integer (based on the alphabetical
ordering). See below for details.
This file containing mapping from synset to human-readable label.
Assumes each line of the file looks like:
""
n02119247    black fox
n02119359    silver fox
"n02119477    red fox, Vulpes fulva"
""
where each line corresponds to a unique mapping. Note that each line is
formatted as <synset>\t<human readable label>.
This file is the output of process_bounding_box.py
Assumes each line of the file looks like:
""
"n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940"
""
where each line corresponds to one bounding box annotation associated
with an image. Each line can be parsed as:
""
"<JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>"
""
Note that there might exist mulitple bounding box annotations associated
with an image file.
pylint: disable=expression-not-assigned
pylint: enable=expression-not-assigned
Create a single Session to run all image coding calls.
Initializes function that converts PNG to JPEG data.
Initializes function that converts CMYK JPEG data to RGB JPEG data.
Initializes function that decodes RGB JPEG data.
File list from:
https://groups.google.com/forum/embed/?place=forum/torch7#!topic/torch7/fOSTXHIESSU
File list from:
https://github.com/cytsai/ilsvrc-cmyk-image-list
Read the image file.
Clean the dirty data.
1 image is a PNG.
22 JPEG images are in CMYK colorspace.
Decode the RGB JPEG.
Check that image converted to RGB
Each thread produces N shards where N = int(num_shards / num_threads).
"For instance, if num_shards = 128, and the num_threads = 2, then the first"
"thread would produce shards [0, 64)."
"Generate a sharded version of the file name, e.g. 'train-00002-of-00010'"
"Break all images into batches with a [ranges[i][0], ranges[i][1]]."
Launch a thread for each batch.
Create a mechanism for monitoring when all threads are finished.
Create a generic TensorFlow-based utility for converting all image codings.
Wait for all the threads to terminate.
Leave label index 0 empty as a background class.
Construct the list of JPEG files and labels.
Shuffle the ordering of all image files in order to guarantee
random ordering of the images with respect to label in the
saved TFRecord files. Make the randomization repeatable.
Build a map from synset to human-readable label.
Run it!
Allowing None in the signature so that dataset_factory can use the default.
!/usr/bin/python
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Read in the 50000 synsets associated with the validation data set.
Make all sub-directories in the validation data dir.
Move all of the image to the appropriate sub-directory.
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
"The dataset has classes with no images (empty and human), which have ID 0 and 1,"
so we need to specify 49 here despite having only 47 classes with images
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Final pooling and prediction
Global average pooling.
Pooling with a fixed kernel size.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The current_stride variable keeps track of the effective stride of the
activations. This allows us to invoke atrous convolution whenever applying
the next residual unit would result in the activations having stride larger
than the target output_stride.
The atrous convolution rate parameter.
Move stride from the block's last unit to the end of the block.
"If we have reached the target output_stride, then we need to employ"
atrous convolution with stride=1 and multiply the atrous rate by the
current unit's stride for use in subsequent layers.
Collect activations at the block's end before performing subsampling.
Subsampling of the block's output activations.
"The following implies padding='SAME' for pool1, which makes feature"
alignment easier for dense prediction tasks. This is also used in
https://github.com/facebook/fb.resnet.torch. However the accompanying
code of 'Deep Residual Learning for Image Recognition' uses
padding='VALID' for pool1. You can switch to that choice by setting
"slim.arg_scope([slim.max_pool2d], padding='VALID')."
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Collect outputs for conv2d, fully_connected and max_pool2d"
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"inputs has shape [batch, 224, 224, 3]"
"inputs has shape [batch, 513, 513, 3]"
We do not include batch normalization or activation functions in
conv1 because the first ResNet unit will perform these. Cf.
Appendix of [2].
This is needed because the pre-activation variant does not have batch
normalization or activation functions in the residual unit output. See
Appendix of [2].
Convert end_points_collection into a dictionary of end_points.
Global average pooling.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
=============================================================================
"These parameters come from the online port, which don't necessarily match"
those in the paper.
TODO(nsilberman): confirm these values with Philip.
##########
Encoder #
##########
"No normalizer for the first encoder layers as per 'Image-to-Image',"
Section 5.1.1
First layer doesn't use normalizer_fn
Last layer doesn't use activation_fn nor normalizer_fn
##########
Decoder #
##########
"Dropout is used at both train and test time as per 'Image-to-Image',"
Section 2.1 (last paragraph).
The Relu comes BEFORE the upsample op:
Explicitly set the normalizer_fn to None to override any default value
"that may come from an arg_scope, such as pix2pix_arg_scope."
No normalization on the input layer.
Stride 1 on the last layer.
"1-dim logits, stride 1, no activation, no normalization."
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
Alias
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
pylint: disable=unused-import
pylint: enable=unused-import
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
If we are fine tuning a checkpoint we need to start at a lower learning
rate since we are farther along on training.
We can start quantizing immediately if we are finetuning.
We need to wait for the model to train a bit before we quantize if we are
training from scratch.
Call rewriter to produce graph with fake quant ops and folded batch norms
"quant_delay delays start of quantization till quant_delay steps, allowing"
for better model accuracy.
Configure the learning rate using an exponential decay.
"When restoring from a floating point model, the min/max values for"
quantized weights and activations are not present.
We instruct slim to ignore variables that are missing during restoration
by setting ignore_missing_vars=True
"If we are restoring from a floating point model, we need to initialize"
the global step to zero for the exponential decay to result in
reasonable learning rates.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Orignaly, arg_scope = slim.arg_scope and layers = slim, now switch to more"
update-to-date tf.contrib.* API.
Decay for the moving averages.
epsilon to prevent 0s in variance.
Turns off fused batch norm.
collection containing the moving mean and moving variance.
"For inverted pyramid models, we start with gating switched off."
batch_size x 32 x 112 x 112 x 64
Separable conv is slow when used at first conv layer.
batch_size x 32 x 56 x 56 x 64
batch_size x 32 x 56 x 56 x 64
batch_size x 32 x 56 x 56 x 192
batch_size x 32 x 28 x 28 x 192
batch_size x 32 x 28 x 28 x 256
batch_size x 16 x 14 x 14 x 512
batch_size x 16 x 14 x 14 x 512
batch_size x 16 x 14 x 14 x 512
batch_size x 16 x 14 x 14 x 528
batch_size x 16 x 14 x 14 x 832
batch_size x 8 x 7 x 7 x 832
batch_size x 8 x 7 x 7 x 1024
Final pooling and prediction
Temporal average pooling.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
299 x 299 x 3
149 x 149 x 32
147 x 147 x 32
147 x 147 x 64
73 x 73 x 160
71 x 71 x 192
35 x 35 x 384
4 x Inception-A blocks
35 x 35 x 384
Reduction-A block
17 x 17 x 1024
7 x Inception-B blocks
17 x 17 x 1024
Reduction-B block
8 x 8 x 1536
3 x Inception-C blocks
Auxiliary Head logits
17 x 17 x 1024
Final pooling and prediction
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which"
can be set to False to disable pooling here (as in resnet_*()).
8 x 8 x 1536
1 x 1 x 1536
1536
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN
setups need the gradient of gradient FusedBatchNormGrad.
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN
setups need the gradient of gradient FusedBatchNormGrad.
First upscaling is different because it takes the input vector.
Last layer has different normalizer and activation.
Convert to proper channels.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Use clip_by_value to simulate bandpass activation.
Use clip_by_value to simulate bandpass activation.
Use clip_by_value to simulate bandpass activation.
149 x 149 x 32
147 x 147 x 32
147 x 147 x 64
73 x 73 x 64
73 x 73 x 80
71 x 71 x 192
35 x 35 x 192
35 x 35 x 320
TODO(alemi): Register intermediate endpoints
"17 x 17 x 1088 if output_stride == 8,"
33 x 33 x 1088 if output_stride == 16
TODO(alemi): register intermediate endpoints
TODO(gpapan): Properly support output_stride for the rest of the net.
8 x 8 x 2080
TODO(alemi): register intermediate endpoints
8 x 8 x 1536
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which"
can be set to False to disable pooling here (as in resnet_*()).
Set weight_decay for weights in conv2d and fully_connected layers.
Set activation_fn and parameters for batch_norm.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Decay for the moving averages.
epsilon to prevent 0s in variance.
collection containing update_ops.
use fused batch norm if possible.
Set weight_decay for weights in Conv and FC layers.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"inputs has shape [batch, 224, 224, 3]"
"inputs has shape [batch, 513, 513, 3]"
Use clip_by_value to simulate bandpass activation.
Convert end_points_collection into a dictionary of end_points.
Global average pooling.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"end_points will collect relevant activations for external use, for example"
summaries or losses.
299 x 299 x 3
149 x 149 x 32
147 x 147 x 32
147 x 147 x 64
73 x 73 x 64
73 x 73 x 80.
71 x 71 x 192.
35 x 35 x 192.
Inception blocks
mixed: 35 x 35 x 256.
mixed_1: 35 x 35 x 288.
mixed_2: 35 x 35 x 288.
mixed_3: 17 x 17 x 768.
mixed4: 17 x 17 x 768.
mixed_5: 17 x 17 x 768.
mixed_6: 17 x 17 x 768.
mixed_7: 17 x 17 x 768.
mixed_8: 8 x 8 x 1280.
mixed_9: 8 x 8 x 2048.
mixed_10: 8 x 8 x 2048.
Auxiliary Head logits
Shape of feature map before the final layer.
Final pooling and prediction
Global average pooling.
Pooling with a fixed kernel size.
1 x 1 x 2048
2048
1000
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Reflection pad by 1 in spatial dimensions (axes 1, 2 = h, w) to make a 3x3"
'valid' convolution produce an output with the same dimension as the
input.
This corrects 1 pixel offset for images with even width and height.
conv2d is left aligned and conv2d_transpose is right aligned for even
sized images (while doing 'SAME' padding).
Note: This doesn't reflect actual model in paper.
Neither dropout nor batch norm -> dont need is_training
##########
Encoder #
##########
7x7 input stage
##################
Residual Blocks #
##################
##########
Decoder #
##########
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"end_points will collect relevant activations for external use, for example"
summaries or losses.
Used to find thinned depths for each layer.
Note that sizes in the comments below assume an input spatial size of
"224x224, however, the inputs can be of any size greater 32x32."
224 x 224 x 3
depthwise_multiplier here is different from depth_multiplier.
depthwise_multiplier determines the output channels of the initial
"depthwise conv (see docs for tf.nn.separable_conv2d), while"
depth_multiplier controls the # channels of the subsequent 1x1
convolution. Must have
in_channels * depthwise_multipler <= out_channels
so that the separable convolution is not overparameterized.
Use a normal convolution instead of a separable convolution.
112 x 112 x 64
56 x 56 x 64
56 x 56 x 64
56 x 56 x 192
28 x 28 x 192
Inception module.
28 x 28 x 256
28 x 28 x 320
14 x 14 x 576
14 x 14 x 576
14 x 14 x 576
14 x 14 x 576
14 x 14 x 576
7 x 7 x 1024
7 x 7 x 1024
Final pooling and prediction
Global average pooling.
Pooling with a fixed kernel size.
1 x 1 x 1024
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
=============================================================================
Tensorflow mandates these.
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture
Conv defines 3x3 convolution layers
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.
stride is the stride of the convolution
depth is the number of channels or filters in a layer
MOBILENETV1_CONV_DEFS specifies the MobileNet body
Used to find thinned depths for each layer.
The current_stride variable keeps track of the output stride of the
"activations, i.e., the running product of convolution strides up to the"
current network layer. This allows us to invoke atrous convolution
whenever applying the next convolution would result in the activations
having output stride larger than the target output_stride.
The atrous convolution rate parameter.
"If we have reached the target output_stride, then we need to employ"
atrous convolution with stride=1 and multiply the atrous rate by the
current unit's stride for use in subsequent layers.
By passing filters=None
separable_conv2d produces only a depthwise convolution layer
Global average pooling.
Pooling with a fixed kernel size.
1 x 1 x 1024
Set weight_decay for weights in Conv and DepthSepConv layers.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Decay for the moving averages.
epsilon to prevent 0s in variance.
Turns off fused batch norm.
collection containing the moving mean and moving variance.
Final pooling and prediction
Temporal average pooling.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Orignaly, add_arg_scope = slim.add_arg_scope and layers = slim, now switch to"
more update-to-date tf.contrib.* API.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Make sure that round down does not go down by more than 10%.
"Note: want to round down, we adjust each split to match the total."
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts."
which provide numbered scopes.
b1 -> b2 * r -> b2
i -> (o * r) (bottleneck) -> o
"Note in contrast with expansion, we always have"
projection to produce the desired output size.
stride check enforces that we don't add residuals when spatial
dimensions are None
Depth matches
Don't do any splitting if we end up with less than 8 filters
on either side.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Make sure that round down does not go down by more than 10%.
Set conv defs defaults and overrides.
a) Set the tensorflow scope
b) set padding to default: note we might consider removing this
since it is also set by mobilenet_scope
c) set all defaults
d) set all extra overrides.
The current_stride variable keeps track of the output stride of the
"activations, i.e., the running product of convolution strides up to the"
current network layer. This allows us to invoke atrous convolution
whenever applying the next convolution would result in the activations
having output stride larger than the target output_stride.
The atrous convolution rate parameter.
Insert default parameters before the base scope which includes
any custom overrides set in mobilenet.
"If we have reached the target output_stride, then we need to employ"
atrous convolution with stride=1 and multiply the atrous rate by the
current unit's stride for use in subsequent layers.
Update params.
Only insert rate to params if rate > 1.
Set padding
Add all tensors that end with 'output' to
endpoints
1 x 1 x num_classes
Note: legacy scope name.
"Recover output shape, for unknown shape."
the network created will be trainble with dropout/batch norm
initialized appropriately.
Note: do not introduce parameters that would change the inference
"model here (for example whether to use bias), modify conv_def instead."
Set weight_decay for weights in Conv and FC layers.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
pyformat: disable
Architecture: https://arxiv.org/abs/1801.04381
Note: these parameters of batch norm affect the architecture
that's why they are here and not in training_scope.
pyformat: enable
NB: do not set depth_args unless they are provided to avoid overriding
whatever default depth_multiplier might have thanks to arg_scope.
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that
"'finegrain_classification_mode' is set to True, which means the embedding"
layer will not be shrinked when given a depth-multiplier < 1.0.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Notes for training NASNet Cifar Model
-------------------------------------
batch_size: 32
learning rate: 0.025
cosine (single period) learning rate decay
auxiliary head loss weighting: 0.4
clip global norm of all gradients by 5
600 epochs with a batch size of 32
This is used for the drop path probabilities since it needs to increase
the drop out probability over the course of training.
Notes for training large NASNet model on ImageNet
-------------------------------------
batch size (per replica): 16
learning rate: 0.015 * 100
learning rate decay factor: 0.97
num epochs per decay: 2.4
sync sgd with 100 replicas
auxiliary head loss weighting: 0.4
label smoothing: 0.1
clip global norm of all gradients by 10
Notes for training the mobile NASNet ImageNet model
-------------------------------------
batch size (per replica): 32
learning rate: 0.04 * 50
learning rate scaling factor: 0.97
num epochs per decay: 2.4
sync sgd with 50 replicas
auxiliary head weighting: 0.4
label smoothing: 0.1
clip global norm of all gradients by 10
Decay for the moving averages.
epsilon to prevent 0s in variance.
Decay for the moving averages.
epsilon to prevent 0s in variance.
Decay for the moving averages.
epsilon to prevent 0s in variance.
Shape of feature map before the final layer.
149 x 149 x 32
Run the reduction cells
Calculate the total number of cells in the network
Add 2 for the reduction cells
Calculate the total number of cells in the network
Add 2 for the reduction cells
"If ImageNet, then add an additional two for the stem cells"
Calculate the total number of cells in the network
Add 2 for the reduction cells
"If ImageNet, then add an additional two for the stem cells"
Find where to place the reduction cells or stride normal cells
Setup for building in the auxiliary head.
Run the cells
true_cell_num accounts for the stem cells
Final softmax layer
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Find where to place the reduction cells or stride normal cells
pylint: disable=protected-access
pylint: enable=protected-access
Setup for building in the auxiliary head.
Run the cells
true_cell_num accounts for the stem cells
pylint: disable=protected-access
pylint: enable=protected-access
Final softmax layer
pylint: disable=protected-access
pylint: enable=protected-access
Calculate the total number of cells in the network.
There is no distinction between reduction and normal cells in PNAS so the
total number of cells is equal to the number normal cells plus the number
of stem cells (two by default).
pylint: disable=protected-access
pylint: enable=protected-access
Calculate the total number of cells in the network.
There is no distinction between reduction and normal cells in PNAS so the
total number of cells is equal to the number normal cells plus the number
of stem cells (two by default).
Configuration for the PNASNet-5 model.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"The cap for tf.clip_by_value, it's hinted from the activation distribution"
"that the majority of activation values are in the range [-6, 6]."
Skip path 1
Skip path 2
"First pad with 0's on the right and bottom, then shift the filter to"
include those 0's that were added.
"If odd number of filters, add an additional one to the second path."
Concat and apply BN
Set the prev layer to the current layer if it is none
Check to be sure prev layer stuff is setup correctly
num_or_size_splits=1
Apply conv operations
Combine hidden states using 'add'.
Add hiddenstate to the list of hiddenstates we can choose from
Dont stride if this is not one of the original hiddenstates
"Check if a stride is needed, then use a strided 1x1 here"
Determine if a reduction should be applied to make the number of
filters match.
Return the concat of all the states
Scale keep prob by layer number
The added 2 is for the reduction cells
Decrease the keep probability over time
""
Script for generating a two-class dataset in COCO format for training an obscured image classifier
""
Requires Python >= 3.6 because of the glob ** expression
""
Collect images and labels
"Labels: clean = 0, obscured = 1"
""
top_folders_to_bottom.py
""
Given a base folder with files like:
""
A/1/2/a.jpg
B/3/4/b.jpg
""
"...moves the top-level folders to the bottom in a new output folder, i.e., creates:"
""
1/2/A/a.jpg
3/4/B/b.jpg
""
"In practice, this is used to make this:"
""
animal/camera01/image01.jpg
""
...look like:
""
camera01/animal/image01.jpg
""
%% Constants and imports
%% Support functions
%% Main functions
Find top-level folder
Find file/folder names
Move or copy
...def process_file()
Enumerate input folder
Convert absolute paths to relative paths
Standardize delimiters
Make sure each input file maps to a unique output file
relative_filename = relative_files[0]
Loop
...def top_folders_to_bottom()
%% Interactive driver
%%
%%
%% Command-line driver
"python top_folders_to_bottom.py ""g:\temp\separated_images"" ""g:\temp\separated_images_inverted"" --n_threads 100"
Convert to an options object
%% Constants and imports
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils)
Assumes the cameratraps repo root is on the path
%% Options
"Make sure there is no overlap between the two sets, because this will cause"
issues in the code
## Required inputs
## Options
Can be a folder or a SAS URL
# These apply only when we're doing ground-truth comparisons
Classes we'll treat as negative
""
"Include the token ""#NO_LABELS#"" to indicate that an image with no annotations"
should be considered empty.
Classes we'll treat as neither positive nor negative
"A list of output sets that we should count, but not render images for."
""
"Typically used to preview sets with lots of empties, where you don't want to"
"subset but also don't want to render 100,000 empty images."
""
"detections, non_detections"
"detections_animal, detections_person, detections_vehicle"
"By default, choose a confidence threshold based on the detector version"
Used for summary statistics only
"Number of images to sample, -1 for ""all images"""
"Random seed for sampling, or None"
Optionally separate detections into categories (animal/vehicle/human)
""
Currently only supported when ground truth is unavailable
Optionally replace one or more strings in filenames with other strings;
useful for taking a set of results generated for one folder structure
and applying them to a slightly different folder structure.
Allow bypassing API output loading when operating on previously-loaded
results
Should we also split out a separate report about the detections that were
just below our main confidence threshold?
""
Currently only supported when ground truth is unavailable
Control rendering parallelization
...PostProcessingOptions
#%% Helper classes and functions
Anything greater than this isn't clearly positive or negative
image has annotations suggesting both negative and positive
"image is not annotated or is annotated with 'unknown', 'unlabeled', ETC."
image has not yet been assigned a state
"In some analyses, we add an additional class that lets us look at"
detections just below our main confidence threshold
count the # of images with each type of DetectionStatus
Check whether this image has:
- unknown / unassigned-type labels
- negative-type labels
"- positive labels (i.e., labels that are neither unknown nor negative)"
"assert has_unknown_labels is False, '{} has unknown labels'.format(annotations)"
If there are no image annotations...
n_negative += 1
im['_detection_status'] = DetectionStatus.DS_NEGATIVE
"If the image has more than one type of labels, it's ambiguous"
"note: bools are automatically converted to 0/1, so we can sum"
"After the check above, we can be sure it's only one of positive,"
"negative, or unknown."
""
Important: do not merge the following 'unknown' branch with the first
"'unknown' branch above, where we tested 'if len(categories) == 0'"
""
If the image has only unknown labels
If the image has only negative labels
If the images has only positive labels
"Annotate the category, if it is unambiguous"
...for each image
...mark_detection_status()
"Leaving code in place for reading from blob storage, may support this"
in the future.
resize is to display them in this notebook or in the HTML more quickly
os.path.isfile() is slow when mounting remote directories; much faster
to just try/except on the image open.
return ''
Render images to a flat folder... we can use os.sep here because we've
already normalized paths
"Create class labels like ""gt_1"" or ""gt_27"""
"for i_box,box in enumerate(ground_truth_boxes):"
gt_classes.append('_' + str(box[-1]))
"errno.ENAMETOOLONG doesn't get thrown properly on Windows, so"
we awkwardly check against a hard-coded limit
Use slashes regardless of os
Optionally add links back to the original images
...render_bounding_boxes
Count items in each category
Optionally sort by filename before writing to html
Write the individual HTML files
...prepare_html_subpages()
Get unique categories above the threshold for this image
Render an image (with no ground truth information)
"This is a list of [class,confidence] pairs, sorted by confidence"
"If we either don't have a confidence threshold, or we've met our"
confidence threshold
...if this detection has classification info
...for each detection
...def render_image_no_gt()
This should already have been normalized to either '/' or '\'
...def render_image_with_gt()
%% Main function
#%% Expand some options for convenience
#%% Prepare output dir
#%% Load ground truth if available
Mark images in the ground truth as positive or negative
#%% Load detection (and possibly classification) results
"If the caller hasn't supplied results, load them"
Determine confidence thresholds if necessary
Remove failed rows
Convert keys and values to lowercase
"Add column 'pred_detection_label' to indicate predicted detection status,"
not separating out the classes
#%% Pull out descriptive metadata
This is rare; it only happens during debugging when the caller
is supplying already-loaded API results.
"#%% If we have ground truth, remove images we can't match to ground truth"
#%% Sample images for visualization
#%% Fork here depending on whether or not ground truth is available
"If we have ground truth, we'll compute precision/recall and sample tp/fp/tn/fn."
""
Otherwise we'll just visualize detections/non-detections.
#%% Detection evaluation: compute precision/recall
numpy array of detection probabilities
"numpy array of bools (0.0/1.0), and -1 as null value"
Don't include ambiguous/unknown ground truth in precision/recall analysis
"For completeness, include the result at a confidence threshold of 1.0"
Compute and print summary statistics
Thresholds go up throughout precisions/recalls/thresholds; find the last
value where recall is at or above target.  That's our precision @ target recall.
"np.where returns a tuple of arrays, but in this syntax where we're"
"comparing an array with a scalar, there will only be one element."
Convert back to a list
Flatten the confusion matrix
"#%% Collect classification results, if they exist"
Mapping of classnames to idx for the confusion matrix.
""
"The lambda is actually kind of a hack, because we use assume that"
the following code does not reassign classname_to_idx
Confusion matrix as defaultdict of defaultdict
""
"Rows / first index is ground truth, columns / second index is predicted category"
iDetection = 0; fn = detector_files[iDetection]; print(fn)
"If this image has classification predictions, and an unambiguous class"
"annotated, and is a positive image..."
"The unambiguous category, we make this a set for easier handling afterward"
"Compute the accuracy as intersection of union,"
i.e. (# of categories in both prediciton and GT)
divided by (# of categories in either prediction or GT
""
"In case of only one GT category, the result will be 1.0, if"
prediction is one category and this category matches GT
""
"It is 1.0/(# of predicted top-1 categories), if the GT is"
one of the predicted top-1 categories.
""
"It is 0.0, if none of the predicted categories is correct"
Distribute this accuracy across all predicted categories in the
confusion matrix
...for each file in the detection results
If we have classification results
Build confusion matrix as array from classifier_cm
Print some statistics
Prepare confusion matrix output
Get confusion matrix as string
Get fixed-size classname for each idx
Prepend class name on each line and add to the top
Print formatted confusion matrix
"Actually don't, this gets really messy in all but the widest consoles"
Plot confusion matrix
To manually add more space at bottom: plt.rcParams['figure.subplot.bottom'] = 0.1
""
"Add 0.5 to figsize for every class. For two classes, this will result in"
"fig = plt.figure(figsize=[4,4])"
...if we have classification results
#%% Render output
Write p/r table to .csv file in output directory
Write precision/recall plot to .png file in output directory
#%% Sampling
Sample true/false positives/negatives with correct/incorrect top-1
classification and render to html
Accumulate html image structs (in the format expected by write_html_image_lists)
"for each category, e.g. 'tp', 'fp', ..., 'class_bird', ..."
Add default entries by accessing them for the first time
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]"
"Each element will be a three-tuple with elements file,max_conf,detections"
"Assemble the information we need for rendering, so we can parallelize without"
dealing with Pandas
i_row = 0; row = images_to_visualize.iloc[0]
Filenames should already have been normalized to either '/' or '\'
Map all the rendering results in the list rendering_results into the
dictionary images_html
Prepare the individual html image files
Write index.html
Show links to each GT class
""
We could do this without classification results; currently we don't.
Add links to all available classes
Close body and html tags
...for each image
"#%% Otherwise, if we don't have ground truth..."
#%% Sample detections/non-detections
Accumulate html image structs (in the format expected by write_html_image_list)
for each category
Add default entries by accessing them for the first time
"Maps detection categories - e.g. ""human"" - to result set names, e.g."
"""detections_human"""
Add a set of results for each category and combination of categories
Create output directories
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]"
"list of 3-tuples with elements (file, max_conf, detections)"
"Assemble the information we need for rendering, so we can parallelize without"
dealing with Pandas
i_row = 0; row = images_to_visualize.iloc[0]
Filenames should already have been normalized to either '/' or '\'
"render_image_no_gt(file_info,detection_categories_to_results_name,"
"detection_categories,classification_categories)"
Map all the rendering results in the list rendering_results into the
dictionary images_html
Prepare the individual html image files
Write index.html
"We can't just sum these, because image_counts includes images in both their"
detection and classification classes
total_images = sum(image_counts.values())
Don't print classification classes here; we'll do that later with a slightly
different structure
Add links to all available classes
os.startfile(output_html_file)
...if we do/don't have ground truth
...process_batch_results
%% Interactive driver(s)
%%
options.unlabeled_classes = ['human']
os.start(ppresults.output_html_file)
%% Command-line driver
""
load_api_results.py
""
Loads the output of the batch processing API (json) into a pandas dataframe.
""
Also functions to group entries by seq_id.
""
Includes the deprecated functions that worked with the old CSV API output format.
""
%% Constants and imports
%% Functions for grouping by sequence_id
example
input 'file': 'SER/S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG'
output 'id': 'S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG'
%% Functions for loading the result as a Pandas DataFrame
Validate that this is really a detector output file
Fields in the API output json other than 'images'
Normalize paths to simplify comparisons later
"image['file'] = image['file'].replace('\\','/')"
Replace some path tokens to match local paths to original blob structure
"If this is a newer file that doesn't include maximum detection confidence values,"
"add them, because our unofficial internal dataframe format includes this."
Pack the json output into a Pandas DataFrame
Sanity-check that this is really a detector output file
Normalize paths to simplify comparisons later
De-serialize detections
Optionally replace some path tokens to match local paths to the original blob structure
string_to_replace = list(options.detector_output_filename_replacements.keys())[0]
"TODO: hit some silly issues with vectorized str() and escaped characters, vectorize"
this later.
""
"detection_results['image_path'].str.replace(string_to_replace,replacement_string)"
iRow = 0
%% Imports
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils)
%% Constants and support classes
We will confirm that this matches what we load from each file
Process-based parallelization isn't supported yet
%% Main function
"Warn the user if some ""detections"" might not get rendered"
#%% Validate inputs
#%% Load both result sets
assert results_a['detection_categories'] == default_detection_categories
assert results_b['detection_categories'] == default_detection_categories
#%% Make sure they represent the same set of images
#%% Find differences
"Each of these maps a filename to a two-element list (the image in set A, the image in set B)"
""
"Right now, we only handle a very simple notion of class transition, where the detection"
of maximum confidence changes class *and* both images have an above-threshold detection.
fn = filenames_a[0]
We shouldn't have gotten this far if error_on_non_matching_lists is set
det = im_a['detections'][0]
...for each filename
#%% Sample and plot differences
"Render two sets of results (i.e., a comparison) for a single"
image.
...def render_image_pair()
fn = image_filenames[0]
...def render_detection_comparisons()
"For each category, generate comparison images and the"
comparison HTML page.
""
category = 'common_detections'
Choose detection pairs we're going to render for this category
...for each category
#%% Write the top-level HTML file content
...def compare_batch_results()
%% Interactive driver
%% KRU
%% Command-line driver
# TODO
""
"Merge high-confidence detections from one results file into another file,"
when the target file does not detect anything on an image.
""
Does not currently attempt to merge every detection based on whether individual
detections are missing; only merges detections into images that would otherwise
be considered blank.
""
"If you want to literally merge two .json files, see combine_api_outputs.py."
""
%% Constants and imports
%% Structs
Don't bother merging into target images where the max detection is already
higher than this threshold
"If you want to merge only certain categories, specify one"
(but not both) of these.
%% Main function
im = output_data['images'][0]
"Determine whether we should be processing all categories, or just a subset"
of categories.
i_source_file = 0; source_file = source_files[i_source_file]
source_im = source_data['images'][0]
detection_category = list(detection_categories)[0]
"This is already a detection, no need to proceed looking for detections to"
transfer
Boxes are x/y/w/h
source_sizes = [det['bbox'][2]*det['bbox'][3] for det in source_detections_this_category_raw]
Only look at boxes below the size threshold
...for each detection category
"print('Adding {} detections to image {}'.format(len(detections_to_transfer),image_filename))"
Update the max_detection_conf field (if present)
...for each image
...for each source file
%% Test driver
%%
%% Command-line driver (TODO)
""
separate_detections_into_folders.py
""
## Overview
""
"Given a .json file with batch processing results, separate the files in that"
"set of results into folders that contain animals/people/vehicles/nothing,"
according to per-class thresholds.
""
"Image files are copied, not moved."
""
""
## Output structure
""
Preserves relative paths within each of those folders; cannot be used with .json
files that have absolute paths in them.
""
"For example, if your .json file has these images:"
""
a/b/c/1.jpg
a/b/d/2.jpg
a/b/e/3.jpg
a/b/f/4.jpg
a/x/y/5.jpg
""
And let's say:
""
"* The results say that the first three images are empty/person/vehicle, respectively"
"* The fourth image is above threshold for ""animal"" and ""person"""
* The fifth image contains an animal
""
* You specify an output base folder of c:\out
""
You will get the following files:
""
c:\out\empty\a\b\c\1.jpg
c:\out\people\a\b\d\2.jpg
c:\out\vehicles\a\b\e\3.jpg
c:\out\animal_person\a\b\f\4.jpg
c:\out\animals\a\x\y\5.jpg
""
## Rendering bounding boxes
""
"By default, images are just copied to the target output folder.  If you specify --render_boxes,"
bounding boxes will be rendered on the output images.  Because this is no longer strictly
"a copy operation, this may result in the loss of metadata.  More accurately, this *may*"
result in the loss of some EXIF metadata; this *will* result in the loss of IPTC/XMP metadata.
""
Rendering boxes also makes this script a lot slower.
""
## Classification-based separation
""
"If you have a results file with classification data, you can also specify classes to put"
"in their own folders, within the ""animals"" folder, like this:"
""
"--classification_thresholds ""deer=0.75,cow=0.75"""
""
"So, e.g., you might get:"
""
c:\out\animals\deer\a\x\y\5.jpg
""
"In this scenario, the folders within ""animals"" will be:"
""
"deer, cow, multiple, unclassified"
""
"""multiple"" in this case only means ""deer and cow""; if an image is classified as containing a"
"bird and a bear, that would end up in ""unclassified"", since the folder separation is based only"
on the categories you provide at the command line.
""
"No classification-based separation is done within the animal_person, animal_vehicle, or"
animal_person_vehicle folders.
""
%% Constants and imports
Occasionally we have near-zero confidence detections associated with COCO classes that
didn't quite get squeezed out of the model in training.  As long as they're near zero
"confidence, we just ignore them."
%% Options class
"Dictionary mapping categories (plus combinations of categories, and 'empty') to output folders"
Populated only when using classification results
"Originally specified as a string, converted to a dict mapping name:threshold"
...__init__()
...class SeparateDetectionsIntoFoldersOptions
%% Support functions
Find the maximum confidence for each category
""
det = detections[0]
"For zero-confidence detections, we occasionally have leftover goop"
from COCO classes
assert det['conf'] < invalid_category_epsilon
...for each detection on this image
Count the number of thresholds exceeded
...for each category
If this is above multiple thresholds
"TODO: handle species-based separation in, e.g., the animal_person case"
"Are we making species classification folders, and is this an animal?"
Do we need to put this into a specific species folder?
Find the animal-class detections that are above threshold
Count the number of classification categories that are above threshold for at
least one detection
d = valid_animal_detections[0]
classification = d['classifications'][0]
"Do we have a threshold for this category, and if so, is"
this classification above threshold?
...for each classification
...for each detection
...if we have to deal with classification subfolders
...if we have 0/1/more categories above threshold
...if this is/isn't a failure case
Skip this image if it's empty and we're not processing empty images
"At this point, this image is getting copied; we may or may not also need to"
draw bounding boxes.
Do a simple copy operation if we don't need to render any boxes
Open the source image
"Render bounding boxes for each category separately, beacuse"
we allow different thresholds for each category.
"When we're not using classification folders, remove classification"
information to maintain standard detection colors.
...for each category
Read EXIF metadata
"Write output with EXIF metadata if available, and quality='keep' if this is a JPEG"
"image.  Unfortunately, neither parameter likes ""None"", so we get a slightly"
icky cascade of if's here.
Also see:
""
https://discuss.dizzycoding.com/determining-jpg-quality-in-python-pil/
""
...for more ways to preserve jpeg quality if quality='keep' doesn't do the trick.
...if we don't/do need to render boxes
...def process_detections()
%% Main function
Create output folder if necessary
Load detection results
Map class names to output folders
Create all combinations of categories
category_name = category_names[0]
Do we have a custom threshold for this category?
Create folder mappings for each category
Create the actual folders
"Handle species classification thresholds, if specified"
"E.g. deer=0.75,cow=0.75"
token = tokens[0]
...for each token
...if classification thresholds are still in string format
Validate the classes in the threshold list
...if we need to deal with classification categories
i_image = 14; im = images[i_image]; im
...for each image
...def separate_detections_into_folders
%% Interactive driver
%%
%%
%%
%% Testing various command-line invocations
"With boxes, no classification"
"No boxes, no classification (default)"
"With boxes, with classification"
"No boxes, with classification"
%% Command-line driver
"python api\batch_processing\postprocessing\separate_detections_into_folders.py ""d:\temp\rspb_mini.json"" ""d:\temp\demo_images\rspb_2018_2019_mini"" ""d:\temp\separation_test"" --nthreads 2"
Convert to an options object
"print('{} {}'.format(v,name))"
List of category numbers to use in separation; uses all categories if None
"Can be ""size"", ""width"", or ""height"""
For each image...
""
im = images[0]
d = im['detections'][0]
Are there really any detections here?
Is this a category we're supposed to process?
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing#detector-outputs
...for each detection
...for each image
...def categorize_detections_by_size()
""
add_max_conf.py
""
"The MD output format included a ""max_detection_conf"" field with each image"
up to and including version 1.2; it was removed as of version 1.3 (it's
redundant with the individual detection confidence values).
""
"Just in case someone took a dependency on that field, this script allows you"
to add it back to an existing .json file.
""
%% Imports and constants
%% Main function
%% Driver
#######
""
remove_repeat_detections.py
""
"Used after running find_repeat_detections, then manually filtering the results,"
to create a final filtered output file.
""
"If you want to use this script, we recommend that you read the user's guide:"
""
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms
""
#######
%% Constants and imports
%% Main function
%% Interactive driver
%%
%% Command-line driver
#######
""
repeat_detections_core.py
""
Core utilities shared by find_repeat_detections and remove_repeat_detections.
""
#######
%% Imports and environment
"from ai4eutils; this is assumed to be on the path, as per repo convention"
"""PIL cannot read EXIF metainfo for the images"""
"""Metadata Warning, tag 256 had too many entries: 42, expected 1"""
%% Constants
%% Classes
Relevant for rendering the folder of images for filtering
""
"imageBase can also be a SAS URL, in which case some error-checking is"
disabled.
Don't consider detections with confidence lower than this as suspicious
Don't consider detections with confidence higher than this as suspicious
What's the IOU threshold for considering two boxes the same?
How many occurrences of a single location (as defined by the IOU threshold)
are required before we declare it suspicious?
"Ignore ""suspicious"" detections larger than some size; these are often animals"
taking up the whole image.  This is expressed as a fraction of the image size.
"Ignore ""suspicious"" detections smaller than some size"
Ignore folders with more than this many images in them
A list of classes we don't want to treat as suspicious. Each element is an int.
Load detections from a filter file rather than finding them from the detector output
".json file containing detections, should be called detectionIndex.json in the filtering_* folder"
produced in the first pass
(optional) List of filenames remaining after deletion of identified
repeated detections that are actually animals.  This should be a flat
"text file, one relative filename per line.  See enumerate_images()."
Turn on/off optional outputs
Determines whether bounding-box rendering errors (typically network errors) should
be treated as failures
Box rendering options
State variables
"Replace filename tokens after reading, useful when the directory structure"
has changed relative to the structure the detector saw
How many folders up from the leaf nodes should we be going to aggregate images?
An optional function that takes a string (an image file name) and returns
"a string (the corresponding  folder ID), typically used when multiple folders"
actually correspond to the same camera in a manufacturer-specific way (e.g.
a/b/c/RECONYX100 and a/b/c/RECONYX101 may really be the same camera).
Include/exclude specific folders... only one of these may be
"specified; ""including"" folders includes *only* those folders."
"Optionally show *other* detections (i.e., detections other than the"
one the user is evaluating) in a light gray
"If bRenderOtherDetections is True, what color should we use to render the"
(hopefully pretty subtle) non-target detections?
""
"In theory I'd like these ""other detection"" rectangles to be partially"
"transparent, but this is not straightforward, and the alpha is ignored"
"here.  But maybe if I leave it here and wish hard enough, someday it"
will work.
""
otherDetectionsColors = ['dimgray']
Sort detections within a directory so nearby detections are adjacent
"in the list, for faster review."
""
"Can be None, 'xsort', or 'clustersort'"
""
* None sorts detections chronologically by first occurrence
* 'xsort' sorts detections from left to right
* 'clustersort' clusters detections and sorts by cluster
Only relevant if smartSort == 'clustersort'
"The data table (Pandas DataFrame), as loaded from the input json file via"
load_api_results()
"The other fields in the input json file, loaded via load_api_results()"
The data table after modification
dict mapping folder names to whole rows from the data table
dict mapping filenames to rows in the master table
"An array of length nDirs, where each element is a list of DetectionLocation"
objects for that directory that have been flagged as suspicious
"This is a bit of a hack right now, but for future-proofing, I don't want to call this"
"to retrieve anything other than the highest-confidence detection, and I'm assuming this"
"is already sorted, so assert() that."
It's not clear whether it's better to use instances[0].bbox or self.bbox
"here... they should be very similar, unless iouThreshold is very low."
self.bbox is a better representation of the overal DetectionLocation.
%% Helper functions
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t"
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t"
%% Sort a list of candidate detections to make them visually easier to review
Just sort by the X location of each box
"Prepare a list of points to represent each box,"
that's what we'll use for clustering
Upper-left
"points.append([det.bbox[0],det.bbox[1]])"
Center
"Labels *could* be any unique labels according to the docs, but in practice"
they are unique integers from 0:nClusters
Make sure the labels are unique incrementing integers
Store the label assigned to each cluster
"Now sort the clusters by their x coordinate, and re-assign labels"
so the labels are sortable
"Compute the centroid for debugging, but we're only going to use the x"
"coordinate.  This is the centroid of points used to represent detections,"
which may be box centers or box corners.
old_cluster_label_to_new_cluster_label[old_cluster_label] =\
new_cluster_labels[old_cluster_label]
%% Look for matches (one directory)
List of DetectionLocations
candidateDetections = []
Create a tree to store candidate detections
For each image in this directory
""
iDirectoryRow = 0; row = rows.iloc[iDirectoryRow]
""
"iDirectoryRow is a pandas index, so it may not start from zero;"
"for debugging, we maintain i_iteration as a loop index."
print('Searching row {} of {} (index {}) in dir {}'.\
"format(i_iteration,len(rows),iDirectoryRow,dirName))"
Don't bother checking images with no detections above threshold
"Array of dicts, where each element is"
{
"'category': '1',  # str value, category ID"
"'conf': 0.926,  # confidence of this detections"
""
"# (x_min, y_min) is upper-left, all in relative coordinates"
"'bbox': [x_min, y_min, width_of_box, height_of_box]"
""
}
For each detection in this image
"This is no longer strictly true; I sometimes run RDE in stages, so"
some probabilities have already been made negative
""
assert confidence >= 0.0 and confidence <= 1.0
Optionally exclude some classes from consideration as suspicious
Is this detection too big to be suspicious?
print('Illegal zero-size bounding box on image {}'.format(filename))
These are relative coordinates
print('Ignoring very small detection with area {}'.format(area))
print('Ignoring very large detection with area {}'.format(area))
This will return candidates of all classes
For each detection in our candidate list
Don't match across categories
Is this a match?
"If so, add this example to the list for this detection"
We *don't* break here; we allow this instance to possibly
match multiple candidates.  There isn't an obvious right or
wrong here.
...for each detection on our candidate list
"If we found no matches, add this to the candidate list"
candidateDetections.append(candidate)
pyqtree
...for each detection
...for each row
Get all candidate detections
print('Found {} candidate detections for folder {}'.format(
"len(candidateDetections),dirName))"
"For debugging only, it's convenient to have these sorted"
as if they had never gone into a tree structure.  Typically
this is in practce a sort by filename.
...def find_matches_in_directory(dirName)
"%% Update the detection table based on suspicious results, write .csv output"
"An array of length nDirs, where each element is a list of DetectionLocation"
objects for that directory that have been flagged as suspicious
For each directory
For each suspicious detection group in this directory
For each instance of this suspicious detection
This should match the bbox for the detection event
The bbox for this instance should be almost the same as the bbox
"for this detection group, where ""almost"" is defined by the IOU"
threshold.
if iou < options.iouThreshold:
"print('IOU warning: {},{}'.format(iou,options.iouThreshold))"
Make sure the bounding box matches
"Make the probability negative, if it hasn't been switched by"
another bounding box
...for each instance
...for each detection
...for each directory
Update maximum probabilities
For each row...
No longer strictly true; sometimes I run RDE on RDE output
assert maxPOriginal >= 0
We should only be making detections *less* likely in this process
"If there was a meaningful change, count it"
"Negative probabilities should be the only reason maxP changed, so"
we should have found at least one negative value if we reached
this point.
...if there was a meaningful change to the max probability for this row
...for each row
If we're also writing output...
"...def update_detection_table(RepeatDetectionResults,options)"
%% Main function
#%% Input handling
Validate some options
Load the filtering file
Load the same options we used when finding repeat detections
...except for things that explicitly tell this function not to
find repeat detections.
...if we're loading from an existing filtering file
Check early to avoid problems with the output folder
"Load file to a pandas dataframe.  Also populates 'max_detection_conf', even if it's"
not present in the .json file.
detectionResults[detectionResults['failure'].notna()]
"Before doing any real work, make sure we can *probably* access images"
"This is just a cursory check on the first image, but it heads off most"
"problems related to incorrect mount points, etc.  Better to do this before"
spending 20 minutes finding repeat detections.
#%% Separate files into directories
This will be a map from a directory name to smaller data frames
This is a mapping back into the rows of the original table
iRow = 0; row = detectionResults.iloc[0]
Create a new DataFrame with just this row
rowsByDirectory[dirName] = pd.DataFrame(row)
...for each unique detection
Convert lists of rows to proper DataFrames
#% Look for matches (or load them from file)
"Are we actually looking for matches, or just loading from a file?"
length-nDirs list of lists of DetectionLocation objects
We're actually looking for matches...
"We get slightly nicer progress bar behavior using threads, by passing a pbar"
object and letting it get updated.  We can't serialize this object across
processes.
#%% Find suspicious locations based on match results
For each directory
""
iDir = 51
A list of DetectionLocation objects
A list of DetectionLocation objects
occurrenceList is a list of file/detection pairs
Sort the above-threshold detections for easier review
...for each directory
If we're just loading detections from a file...
"We're skipping detection-finding, but to see which images are actually legit false"
"positives, we may be looking for physical files or loading from a text file."
For each directory
iDir = 0; detections = suspiciousDetections[0]
""
"suspiciousDetections is an array of DetectionLocation objects,"
one per directory.
For each detection that was present before filtering
iDetection = 0; detection = detections[iDetection]
Are we checking the directory to see whether detections were actually false
"positives, or reading from a list?"
Is the image still there?
"If not, remove this from the list of suspicious detections"
...for each detection
...for each directory
...if we are/aren't finding detections (vs. loading from file)
Create filtering directory
iDir = 0; suspiciousDetectionsThisDir = suspiciousDetections[iDir]
suspiciousDetectionsThisDir is a list of DetectionLocation objects
iDetection = 0; detection = suspiciousDetectionsThisDir[0]
Sort instances in descending order by confidence
Choose the highest-confidence index
Should we render (typically in a very light color) detections
*other* than the one we're highlighting here?
Render other detections first (typically in a thin+light box)
Now render the example detection (on top of at least one
of the other detections)
This converts the *first* instance to an API standard detection;
"because we just sorted this list in descending order by confidence,"
this is the highest-confidence detection.
...if we are/aren't rendering other bounding boxes
...for each detection in this folder
...for each folder
Write out the detection index
...if we're writing filtering info
...find_repeat_detections()
#######
""
find_repeat_detections.py
""
"If you want to use this script, we recommend that you read the user's guide:"
""
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms
""
"Really, don't try to run this script without reading the user's guide, you'll think"
it's more magical than it is.
""
"This script looks through a sequence of detections in the API output json file, and finds"
"candidates that might be ""repeated false positives"", i.e. that random branch that the"
detector thinks is an animal/person/vehicle.
""
"Typically after running this script, you would do a manual step to remove"
"true positives, then run remove_repeat_detections to produce a final output file."
""
There's no way that statement was self-explanatory; see the user's guide.
""
#######
%% Constants and imports
%% Interactive driver
%%
%% Command-line driver
Convert to an options object
""
If a request has been sent to AML for batch scoring but the monitoring thread of the API was
"interrupted (uncaught exception or having to re-start the API container), we could manually"
"aggregate results from each shard using this script, assuming all jobs submitted to AML have finished."
""
Need to have set environment variables STORAGE_ACCOUNT_NAME and STORAGE_ACCOUNT_KEY to those of the
"storage account backing the API. Also need to adjust the INTERNAL_CONTAINER, AML_CONTAINER and"
AML_CONFIG fields in api_core/orchestrator_api/api_config.py to match the instance of the API that this
request was submitted to.
""
May need to change the import statement in api_core/orchestrator_api/orchestrator.py
"""from sas_blob_utils import SasBlob"" to"
"""from .sas_blob_utils import SasBlob"" to not confuse with the module in AI4Eutils;"
"and change ""import api_config"" to"
"""from api.batch_processing.api_core.orchestrator_api import api_config"""
Execute this script from the root of the repository. You may need to add the repository to PYTHONPATH.
"list_jobs_submitted cannot be serialized (""can't pickle _thread.RLock objects ""), but"
do not need it for aggregating results
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%% helper classes and functions
TODO log exception when we have more telemetry
TODO check that the expiry date of input_container_sas is at least a month
into the future
"if no permission specified explicitly but has an access policy, assumes okay"
TODO - check based on access policy as well
return current UTC time as a string in the ISO 8601 format (so we can query by
timestamp in the Cosmos DB job status table.
example: '2021-02-08T20:02:05.699689Z'
"image_paths will have length at least 1, otherwise would have ended before this step"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
a job moves from created to running/problem after the Batch Job has been submitted
"job_id should be unique across all instances, and is also the partition key"
TODO do not read the entry first to get the call_params when the Cosmos SDK add a
patching functionality:
https://feedback.azure.com/forums/263030-azure-cosmos-db/suggestions/6693091-be-able-to-do-partial-updates-on-document
need to retain other fields in 'status' to be able to restart monitoring thread
retain existing fields; update as needed
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
sentinel should change if new configurations are available
configs have not changed
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Gunicorn logger handler will get attached if needed in server.py
set for all tasks in the job
cannot execute the scoring script that is in the mounted directory; has to be copied to cwd
not luck giving the commandline arguments via formatted string - set as env vars instead
form shards of images and assign each shard to a Task
for persisting stdout and stderr
persist stdout and stderr (will be removed when node removed)
paths are relative to the Task working directory
can also just upload on failure
first try submitting Tasks
retry submitting Tasks
Change the Job's on_all_tasks_complete option to 'terminateJob' so the Job's status changes automatically
after all submitted tasks are done
This is so that we do not take up the quota for active Jobs in the Batch account.
return type: TaskAddCollectionResult
actually we should probably only re-submit if it's a server_error
docs: # https://docs.microsoft.com/en-us/rest/api/batchservice/odata-filters-in-batch#list-tasks
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%% Flask app
reference: https://trstringer.com/logging-flask-gunicorn-the-manageable-way/
%% Helper classes
%% Flask endpoints
required params
can be an URL to a file not hosted in an Azure blob storage container
"if use_url, then images_requested_json_sas is required"
optional params
check model_version is among the available model versions
check request_name has only allowed characters
optional params for telemetry collection - logged to status table for now as part of call_params
All API instances / node pools share a quota on total number of active Jobs;
we cannot accept new Job submissions if we are at the quota
required fields
request_status is either completed or failed
the create_batch_job thread will stop when it wakes up the next time
"Fix for Zooniverse - deleting any ""-"" characters in the job_id"
"If the status is running, it could be a Job submitted before the last restart of this"
"API instance. If that is the case, we should start to monitor its progress again."
WARNING model_version could be wrong (a newer version number gets written to the output file) around
"the time that  the model is updated, if this request was submitted before the model update"
and the API restart; this should be quite rare
conform to previous schemes
%% undocumented endpoints
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Gunicorn logger handler will get attached if needed in server.py
request_name and request_submission_timestamp are for appending to
output file names
image_paths can be a list of strings (Azure blob names or public URLs)
"or a list of length-2 lists where each is a [image_id, metadata] pair"
Case 1: listing all images in the container
- not possible to have attached metadata if listing images in a blob
list all images to process
+ 1 so if the number of images listed > MAX_NUMBER_IMAGES_ACCEPTED_PER_JOB
we will know and not proceed
Case 2: user supplied a list of images to process; can include metadata
filter down to those conforming to the provided prefix and accepted suffixes (image file types)
prefix is case-sensitive; suffix is not
"Although urlparse(p).path preserves the extension on local paths, it will not work for"
"blob file names that contains ""#"", which will be treated as indication of a query."
"If the URL is generated via Azure Blob Storage, the ""#"" char will be properly encoded"
apply the first_n and sample_n filters
OK if first_n > total number of images
sample by shuffling image paths and take the first sample_n images
"upload the image list to the container, which is also mounted on all nodes"
all sharding and scoring use the uploaded list
now request_status moves from created to running
an extra field to allow the monitoring thread to restart after an API restart: total number of tasks
also record the number of images to process for reporting
start the monitor thread with the same name
"both succeeded and failed tasks are marked ""completed"" on Batch"
"preserving format from before, but SAS URL to 'failed_images' and 'images' are no longer provided"
"failures should be contained in the output entries, indicated by an 'error' field"
"when people download this, the timestamp will have : replaced by _"
check if the result blob has already been written (could be another instance of the API / worker thread)
"and if so, skip aggregating and uploading the results, and just generate the SAS URL, which"
could be needed still if the previous request_status was `problem`.
upload the output JSON to the Job folder
%% Helper functions *copied* from ct_utils.py and visualization/visualization_utils.py
"Determine the factor, which shifts the decimal point of x"
just behind the last significant digit
"Shift decimal point by multiplicatipon with factor, flooring, and"
division by factor
PIL.Image.convert() returns a converted copy of this image
alter orientation as needed according to EXIF tag 0x112 (274) for Orientation
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28
https://www.media.mit.edu/pia/Research/deepview/exif.html
"%% TFDetector class, an unmodified *copy* of the class in detection/tf_detector.py,"
so we do not have to import the packages required by run_detector.py
Number of decimal places to round to for confidence and bbox coordinates
"MegaDetector was trained with batch size of 1, and the resizing function is a part"
of the inference graph
An enumeration of failure reasons
"change from [y1, x1, y2, x2] to [x1, y1, width, height]"
convert numpy floats to Python floats
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size
"np_images = [np.asarray(image, np.uint8) for image in images]"
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)"
performs inference
our batch size is 1; need to loop the batch dim if supporting batch size > 1
%% Scoring script
determine if there is metadata attached to each image_id
information to determine input and output locations
other parameters for the task
test that we can write to output path; also in case there is no image to process
list images to process
"items in this list can be strings or [image_id, metadata]"
model path
"Path to .pb TensorFlow detector model file, relative to the"
models/megadetector_copies folder in mounted container
score the images
""
manage_video_batch.py
""
Notebook-esque script to manage the process of running a local batch of videos
through MD.  Defers most of the heavy lifting to manage_local_batch.py .
""
%% Imports and constants
%% Split videos into frames
"%% List frame files, break into folders"
Find unique (relative) folders
fn = frame_files[0]
%% List videos
%% Check for videos that are missing entirely
list(folder_to_frame_files.keys())[0]
video_filenames[0]
fn = video_filenames[0]
%% Check for videos with very few frames
%% Print the list of videos that are problematic
%% Process images like we would for any other camera trap job
"...typically using manage_local_batch.py, but do this however you like, as long"
as you get a results file at the end.
""
"If you do RDE, remember to use the second folder from the bottom, rather than the"
bottom-most folder.
%% Convert frame results to video results
%% Confirm that the videos in the .json file are what we expect them to be
%% Scrap
%% Test a possibly-broken video
%% List videos in a folder
%% Imports
%% Constants
%% Classes
class variables
"instance variables, in order of when they are typically set"
Leaving this commented out to remind us that we don't want this check here; let
the API fail on these images.  It's a huge hassle to remove non-image
files.
""
for path_or_url in images_list:
if not is_image_file_or_url(path_or_url):
raise ValueError('{} is not an image'.format(path_or_url))
Commented out as a reminder: don't check task status (which is a rest API call)
in __repr__; require the caller to explicitly request status
"status=getattr(self, 'status', None))"
estimate # of failed images from failed shards
Download all three JSON urls to memory
Remove files that were submitted but don't appear to be images
assert all(is_image_file_or_url(s) for s in submitted_images)
Diff submitted and processed images
Confirm that the procesed images are a subset of the submitted images
https://www.geeksforgeeks.org/break-list-chunks-size-n-python/
%% Interactive driver
%%
%%
%%
%% Imports and constants
%% Constants I set per script
## Required
base_output_folder_name = os.path.expanduser('~/postprocessing/' + organization_name_short)
Shared Access Signature (SAS) tokens for the Azure Blob Storage container.
Leading question mark is optional.
""
The read-only token is used for accessing images; the write-enabled token is
used for writing file lists.
## Typically left as default
"Pre-pended to all folder names/prefixes, if they're defined below"
"This is how we break the container up into multiple taskgroups, e.g., for"
separate surveys. The typical case is to do the whole container as a single
taskgroup.
"If your ""folders"" are really logical folders corresponding to multiple folders,"
map them here
"A list of .json files to load images from, instead of enumerating.  Formatted as a"
"dictionary, like folder_prefixes."
This is only necessary if you will be performing postprocessing steps that
"don't yet support SAS URLs, specifically the ""subsetting"" step, or in some"
cases the splitting of files into multiple output directories for
empty/animal/vehicle/people.
""
"For those applications, you will need to mount the container to a local drive."
For this case I recommend using rclone whether you are on Windows or Linux;
rclone is much easier than blobfuse for transient mounting.
""
"But most of the time, you can ignore this."
"Supported model_versions: '4', '3', '4_prelim'"
""
Also available at the /supported_model_versions and /default_model_version
endpoints
""
"Unless you have any specific reason to set this to a non-default value, leave"
"it at the default, which as of 2020.04.28 is MegaDetector 4.1"
""
"additional_task_args = {""model_version"":""4_prelim""}"
""
"file_lists_by_folder will contain a list of local JSON file names,"
each JSON file contains a list of blob names corresponding to an API taskgroup
"%% Derived variables, path setup"
local folders
Turn warnings into errors if more than this many images are missing
%% Support functions
"scheme, netloc, path, query, fragment"
%% Read images from lists or enumerate blobs to files
folder_name = folder_names[0]
"Load file lists for this ""folder"""
""
file_list = input_file_lists[folder][0]
Write to file
A flat list of blob paths for each folder
folder_name = folder_names[0]
"If we don't/do have multiple prefixes to enumerate for this ""folder"""
"If this is intended to be a folder, it needs to end in '/', otherwise"
files that start with the same string will match too
...for each prefix
Write to file
...for each folder
%% Some just-to-be-safe double-checking around enumeration
Make sure each folder has at least one image matched; the opposite is usually a sign of a copy/paste issue
...for each image
...for each prefix
...for each folder
Make sure each image comes from one of our folders; the opposite is usually a sign of a bug up above
...for each prefix
...for each folder
...for each image
%% Divide images into chunks for each folder
The JSON file at folder_chunks[i][j] corresponds to task j of taskgroup i
list_file = file_lists_by_folder[0]
"%% Create taskgroups and tasks, and upload image lists to blob storage"
periods not allowed in task names
%% Generate API calls for each task
clipboard.copy(request_strings[0])
clipboard.copy('\n\n'.join(request_strings))
%% Run the tasks (don't run this cell unless you are absolutely sure!)
I really want to make sure I'm sure...
%% Estimate total time
Around 0.8s/image on 16 GPUs
%% Manually create task groups if we ran the tasks manually
%%
"%% Write task information out to disk, in case we need to resume"
%% Status check
print(task.id)
%% Resume jobs if this notebook closes
%% For multiple tasks (use this only when we're merging with another job)
%% For just the one task
%% Load into separate taskgroups
p = task_cache_paths[0]
%% Typically merge everything into one taskgroup
"%% Look for failed shards or missing images, start new tasks if necessary"
List of lists of paths
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup];
"Make a copy, because we append to taskgroup"
i_task = 0; task = tasks[i_task]
Each taskgroup corresponds to one of our folders
Check that we have (almost) all the images
Now look for failed images
Write it out as a flat list as well (without explanation of failures)
...for each task
...for each task group
%% Pull results
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup]; task = taskgroup[0]
Each taskgroup corresponds to one of our folders
...for each task
...for each task group
%% Combine results from task groups into final output files
Check that we have (almost) all the images
The only reason we should ever have a repeated request is the case where an
"image was missing and we reprocessed it, or where it failed and later succeeded"
"There may be non-image files in the request list, ignore those"
Something has gone bonkers if there are images in the results that
aren't in the request
...for each folder
%% Post-processing (no ground truth)
i_folder = 0; folder_name_raw = folder_names[i_folder]
"Omit some pages from the output, useful when animals are rare"
%% Manual processing follows
""
"Everything after this should be considered mostly manual, and no longer includes"
looping over folders.
""
%% RDE (sample directory collapsing)
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images"
"%% Repeat detection elimination, phase 1"
"Deliberately leaving these imports here, rather than at the top, because this cell is not"
typically executed
To invoke custom collapsing of folders for a particular manufacturer's naming scheme
options.customDirNameFunction = remove_overflow_folders
Exclude people and vehicles from RDE
"options.excludeClasses = [2,3]"
%% Manual RDE step
# DELETE THE VALID DETECTIONS ##
%% Re-filtering
%% Post-processing (post-RDE)
i_folder = 0; folder_name_raw = folder_names[i_folder]
"Omit some pages from the output, useful when animals are rare"
"print('No RDE file available for {}, skipping'.format(folder_name))"
continue
%% Create a new category for large boxes
%% Subsetting
i_folder = 0; folder_name = folders[i_folder]
%% String replacement
%% Folder splitting
i_folder = 0; folder_name_raw = folder_names[i_folder]
api_output_file = folder_name_to_combined_output_file[folder_name]
%% Imports and constants
from ai4eutils
To specify a non-default confidence threshold for including detections in the .json file
Turn warnings into errors if more than this many images are missing
Only relevant when we're using a single GPU
"Specify a target image size when running MD... strongly recommended to leave this at ""None"""
Only relevant when running on CPU
OS-specific script line continuation character
OS-specific script comment character
"Prefer threads on Windows, processes on Linux"
"This is for things like image rendering, not for MegaDetector"
Should we use YOLOv5's val.py instead of run_detector_batch.py?
Directory in which to run val.py.  Only relevant if use_yolo_inference_scripts is True.
Should we remove intermediate files used for running YOLOv5's val.py?
""
Only relevant if use_yolo_inference_scripts is True.
Should we apply YOLOv5's augmentation?  Only allowed when use_yolo_inference_scripts
is True.
%% Constants I set per script
Optional descriptor
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v5.0.0/md_v5b.0.0.pt')
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v4.1.0/md_v4.1.0.pb')
"Number of jobs to split data into, typically equal to the number of available GPUs"
Only used to print out a time estimate
"%% Derived variables, constant validation, path setup"
%% Enumerate files
%% Load files from prior enumeration
%% Divide images into chunks
%% Estimate total time
%% Write file lists
%% Generate commands
"A list of the scripts tied to each GPU, as absolute paths.  We'll write this out at"
the end so each GPU's list of commands can be run at once.  Generally only used when
"running lots of small batches via YOLOv5's val.py, which doesn't support checkpointing."
i_task = 0; task = task_info[i_task]
Generate the script to run MD
Check whether this output file exists
Generate the script to resume from the checkpoint (only supported with MD inference code)
...for each task
Write out a script for each GPU that runs all of the commands associated with
that GPU.  Typically only used when running lots of little scripts in lieu
of checkpointing.
...for each GPU
%% Run the tasks
%%% Run the tasks (commented out)
i_task = 0; task = task_info[i_task]
"This will write absolute paths to the file, we'll fix this later"
...for each chunk
...if False
"%% Load results, look for failed or missing images in each task"
i_task = 0; task = task_info[i_task]
im = task_results['images'][0]
...for each task
%% Merge results files and make images relative
im = combined_results['images'][0]
%% Post-processing (pre-RDE)
options.sample_seed = 0
"Omit some pages from the output, useful when animals are rare"
%% RDE (sample directory collapsing)
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images"
%%
%%
%%
relativePath = image_filenames[0]
"%% Repeat detection elimination, phase 1"
"Deliberately leaving these imports here, rather than at the top, because this"
cell is not typically executed
options.minSuspiciousDetectionSize = 0.05
This will cause a very light gray box to get drawn around all the detections
we're *not* considering as suspicious.
options.lineThickness = 5
options.boxExpansion = 8
To invoke custom collapsing of folders for a particular manufacturer's naming scheme
options.customDirNameFunction = remove_overflow_folders
Exclude people and vehicles from RDE
"options.excludeClasses = [2,3]"
options.maxImagesPerFolder = 50000
options.includeFolders = ['a/b/c']
options.excludeFolder = ['a/b/c']
"Can be None, 'xsort', or 'clustersort'"
import clipboard; clipboard.copy(os.path.dirname(suspiciousDetectionResults.filterFile))
path_utils.open_file(os.path.dirname(suspiciousDetectionResults.filterFile))
%% Manual RDE step
# DELETE THE VALID DETECTIONS ##
%% Re-filtering
%% Post-processing (post-RDE)
options.sample_seed = 0
"Omit some pages from the output, useful when animals are rare"
"%% Run MegaClassifier (actually, write out a script that runs MegaClassifier)"
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
#%% Set up environment
commands.append('cd CameraTraps/classification\n')
commands.append('conda activate cameratraps-classifier\n')
#%% Crop images
fn = input_files[0]
#%% Run classifier
fn = input_files[0]
#%% Remap classifier outputs
fn = input_files[0]
#%% Merge classification and detection outputs
fn = input_files[0]
#%% Write  out classification script
"%% Run a non-MegaClassifier classifier (i.e., a classifier with no output mapping)"
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
#%% Set up environment
#%% Crop images
fn = input_files[0]
#%% Run classifier
fn = input_files[0]
#%% Merge classification and detection outputs
fn = input_files[0]
#%% Write everything out
%% Run the classifier(s) via the .sh script(s) or batch file(s) we just wrote
...
%% Within-image classification smoothing
""
Only count detections with a classification confidence threshold above
"*classification_confidence_threshold*, which in practice means we're only"
looking at one category per detection.
""
If an image has at least *min_detections_above_threshold* such detections
"in the most common category, and no more than *max_detections_secondary_class*"
"in the second-most-common category, flip all detections to the most common"
category.
""
"Optionally treat some classes as particularly unreliable, typically used to overwrite an"
"""other"" class."
""
This cell also removes everything but the non-dominant classification for each detection.
""
How many detections do we need above the classification threshold to determine a dominant category
for an image?
"Even if we have a dominant class, if a non-dominant class has at least this many classifications"
"in an image, leave them alone."
"If the dominant class has at least this many classifications, overwrite ""other"" classifications"
What confidence threshold should we use for assessing the dominant category in an image?
Which classifications should we even bother over-writing?
Detection confidence threshold for things we count when determining a dominant class
Which detections should we even bother over-writing?
"Before we do anything else, get rid of everything but the top classification"
for each detection.
...for each detection in this image
...for each image
im = d['images'][0]
...for each classification
...if there are classifications for this detection
...for each detection
"Handle a quirky special case: if the most common category is ""other"" and"
"it's ""tied"" with the second-most-common category, swap them"
secondary_count = category_to_count[keys[1]]
The 'secondary count' is the most common non-other class
If we have at least *min_detections_to_overwrite_other* in a category that isn't
"""other"", change all ""other"" classifications to that category"
...for each classification
...if there are classifications for this detection
...for each detection
"...if we should overwrite all ""other"" classifications"
"At this point, we know we have a dominant category; change all other above-threshold"
"classifications to that category.  That category may have been ""other"", in which"
case we may have already made the relevant changes.
det = detections[0]
...for each classification
...if there are classifications for this detection
...for each detection
...for each image
...for each file we want to smooth
"%% Post-processing (post-classification, post-within-image-smoothing)"
classification_detection_file = classification_detection_files[1]
%% Read EXIF data from all images
%% Prepare COCO-camera-traps-compatible image objects for EXIF results
import dateutil
"This is a standard format for EXIF datetime, and dateutil.parser"
doesn't handle it correctly.
return dateutil.parser.parse(s)
exif_result = exif_results[0]
Currently we assume that each leaf-node folder is a location
"We collected this image this century, but not today, make sure the parsed datetime"
jives with that.
""
The latter check is to make sure we don't repeat a particular pathological approach
"to datetime parsing, where dateutil parses time correctly, but swaps in the current"
date when it's not sure where the date is.
...for each exif image result
%% Assemble into sequences
Make a list of images appearing at each location
im = image_info[0]
%% Load classification results
Map each filename to classification results for that file
%% Smooth classification results over sequences (prep)
These are the only classes to which we're going to switch other classifications
Only switch classifications to the dominant class if we see the dominant class at least
this many times
"If we see more than this many of a class that are above threshold, don't switch those"
classifications to the dominant class.
"If the ratio between a dominant class and a secondary class count is greater than this,"
"regardless of the secondary class count, switch those classificaitons (i.e., ignore"
max_secondary_class_classifications_above_threshold_for_class_smoothing).
""
"This may be different for different dominant classes, e.g. if we see lots of cows, they really"
"tend to be cows.  Less so for canids, so we set a higher ""override ratio"" for canids."
"If there are at least this many classifications for the dominant class in a sequence,"
"regardless of what that class is, convert all 'other' classifications (regardless of"
confidence) to that class.
"If there are at least this many classifications for the dominant class in a sequence,"
"regardless of what that class is, classify all previously-unclassified detections"
as that class.
Only count classifications above this confidence level when determining the dominant
"class, and when deciding whether to switch other classifications."
Confidence values to use when we change a detection's classification (the
original confidence value is irrelevant at that point)
%% Smooth classification results over sequences (supporting functions)
im = images_this_sequence[0]
det = results_this_image['detections'][0]
Only process animal detections
Only process detections with classification information
"We only care about top-1 classifications, remove everything else"
Make sure the list of classifications is already sorted by confidence
...and just keep the first one
"Confidence values should be sorted within a detection; verify this, and ignore"
...for each detection in this image
...for each image in this sequence
...top_classifications_for_sequence()
Count above-threshold classifications in this sequence
Sort the dictionary in descending order by count
"Handle a quirky special case: if the most common category is ""other"" and"
"it's ""tied"" with the second-most-common category, swap them."
...def count_above_threshold_classifications()
%% Smooth classifications at the sequence level (main loop)
Break if this token is contained in a filename (set to None for normal operation)
i_sequence = 0; seq_id = all_sequences[i_sequence]
Count top-1 classifications in this sequence (regardless of confidence)
Handy debugging code for looking at the numbers for a particular sequence
Count above-threshold classifications for each category
"If our dominant category ID isn't something we want to smooth to, don't mess around with this sequence"
"# Smooth ""other"" classifications ##"
"By not re-computing ""max_count"" here, we are making a decision that the count used"
"to decide whether a class should overwrite another class does not include any ""other"""
classifications we changed to be the dominant class.  If we wanted to include those...
""
sorted_category_to_count = count_above_threshold_classifications(classifications_this_sequence)
max_count = get_first_value_from_sorted_dictionary(sorted_category_to_count)
assert dominant_category_id == get_first_key_from_sorted_dictionary(sorted_category_to_count)
# Smooth non-dominant classes ##
Don't flip classes to the dominant class if they have a large number of classifications
"Don't smooth over this class if there are a bunch of them, and the ratio"
if primary to secondary class count isn't too large
Default ratio
Does this dominant class have a custom ratio?
# Smooth unclassified detections ##
...for each sequence
%% Write smoothed classification results
"%% Post-processing (post-classification, post-within-image-and-within-sequence-smoothing)"
%% Zip .json files
%% 99.9% of jobs end here
Everything after this is run ad hoc and/or requires some manual editing.
%% Compare results files for different model versions (or before/after RDE)
Choose all pairwise combinations of the files in [filenames]
%% Merge in high-confidence detections from another results file
%% Create a new category for large boxes
"This is a size threshold, not a confidence threshold"
size_options.categories_to_separate = [3]
%% Preview large boxes
%% .json splitting
options.query = None
options.replacement = None
Reminder: 'n_from_bottom' with a parameter of zero is the same as 'bottom'
%% Custom splitting/subsetting
i_folder = 0; folder_name = folders[i_folder]
"This doesn't do anything in this case, since we're not splitting folders"
options.make_folder_relative = True
%% String replacement
%% Splitting images into folders
%% Generate commands for a subset of tasks
i_task = 8
...for each task
%% End notebook: turn this script into a notebook (how meta!)
Exclude everything before the first cell
Remove the first [first_non_empty_lines] from the list
Add the last cell
""
xmp_integration.py
""
"Tools for loading MegaDetector batch API results into XMP metadata, specifically"
for consumption in digiKam:
""
https://cran.r-project.org/web/packages/camtrapR/vignettes/camtrapr2.html
""
%% Imports and constants
%% Class definitions
Folder where images are stored
.json file containing MegaDetector output
"String to remove from all path names, typically representing a"
prefix that was added during MegaDetector processing
Optionally *rename* (not copy) all images that have no detections
above [rename_conf] for the categories in rename_cats from x.jpg to
x.check.jpg
"Comma-deleted list of category names (or ""all"") to apply the rename_conf"
behavior to.
"Minimum detection threshold (applies to all classes, defaults to None,"
i.e. 0.0
%% Functions
Relative image path
Absolute image path
List of categories to write to XMP metadata
Categories with above-threshold detections present for
this image
Maximum confidence for each category
Have we already added this to the list of categories to
write out to this image?
If we're supposed to compare to a threshold...
Else we treat *any* detection as valid...
Keep track of the highest-confidence detection for this class
If we're doing the rename/.check behavior...
Legacy code to rename files where XMP writing failed
%% Interactive/test driver
%%
%% Command-line driver
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Cosmos DB `batch-api-jobs` table for job status
"aggregate the number of images, country and organization names info from each job"
submitted during yesterday (UTC time)
create the card
""
api_frontend.py
""
"Defines the Flask app, which takes requests (one or more images) from"
"remote callers and pushes the images onto the shared Redis queue, to be processed"
by the main service in api_backend.py .
""
%% Imports
%% Initialization
%% Support functions
Make a dict that the request_processing_function can return to the endpoint
function to notify it of an error
Verify that the content uploaded is not too big
""
request.content_length is the length of the total payload
Verify that the number of images is acceptable
...def check_posted_data(request)
%% Main loop
Check whether the request_processing_function had an error
Write images to temporary files
""
TODO: read from memory rather than using intermediate files
"Submit the image(s) for processing by api_backend.py, who is waiting on this queue"
TODO: convert to a blocking read and eliminate the sleep() statement in this loop
"image = Image.open(os.path.join(temp_direc, image_name))"
...if we do/don't have a request available on the queue
...while(True)
...def detect_sync()
%% Command-line driver
use --non-docker if you are testing without Docker
""
python api_frontend.py --non-docker
# Camera trap real-time API configuration
"Full path to the temporary folder for image storage, only meaningful"
within the Docker container
Upper limit on total content length (all images and parameters)
Minimum confidence threshold for detections
Minimum confidence threshold for showing a bounding box on the output image
Use this when testing without Docker
""
api_backend.py
""
"Defines the model execution service, which pulls requests (one or more images)"
"from the shared Redis queue, and runs them through the TF model."
""
%% Imports
%% Initialization
%% Main loop
TODO: convert to a blocking read and eliminate the sleep() statement in this loop
Filter the detections by the confidence threshold
""
"Each result is [ymin, xmin, ymax, xmax, confidence, category]"
""
"Coordinates are relative, with the origin in the upper-left"
...if serialized_entry
...while(True)
...def detect_process()
%% Command-line driver
use --non-docker if you are testing without Docker
""
python api_frontend.py --non-docker
run detections on a test image to load the model
%%
Importing libraries
%%
%%
%%
app = typer.Typer()
%%
@app.command()
GPU configuration: set up GPUs based on availability and user specification
Environment variable setup for numpy multi-threading
Load and set configurations from the YAML file
Set a global seed for reproducibility
"If the annotation directory does not have a data split, split the data first"
Replace annotation dir from config with the directory containing the split files
Split the data according to the split type
Get the path to the annotation files
Split training data
Split validation and test data
Dataset and algorithm loading based on the configuration
Logger setup based on the specified logger type
Callbacks for model checkpointing and learning rate monitoring
Trainer configuration in PyTorch Lightning
"Training, validation, or evaluation execution based on the mode"
%%
%%
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
PyTorch imports
"Importing the model, dataset, transformations and utility functions from PytorchWildlife"
Importing the utility function for saving cropped images
Setting the device to use for computations ('cuda' indicates GPU)
Initializing the MegaDetectorV5 model for image detection
Creating a dataset of images with the specified transform
Creating a DataLoader for batching and parallel processing of the images
Performing batch detection on the images
Saving the detected objects as cropped images
%%
Read the original CSV file
Prepare a list to store new records for the new CSV
Process the data if the name of the file is in the dataframe
Save the crop into a new csv
Add record to the new CSV data
Create a DataFrame from the new records
Define the path for the new CSV file
Save the new DataFrame to CSV
# DATA SPLITTING
Load the data from the csv file
Separate the features and the targets
First split to separate out the test set
Adjust val_size to account for the initial split
Second split to separate out the validation set
"Combine features, labels, and classification back into dataframes"
Create the output directory in case that it does not exist
Save the splits to new CSV files
Return the dataframes
Load the data from the csv file
Calculate train size based on val and test size
Get unique locations
Split locations into train and temp (temporary holding for val and test)
Adjust the proportions for val and test based on the remaining locations
"Allocate images to train, validation, and test sets based on their location"
Save the datasets to CSV files
Return the split datasets
Load the data from the csv file
Convert 'Photo_Time' from string to datetime
Calculate train size based on val and test size
Sort by 'Photo_Time' to ensure chronological order
Group photos into sequences based on a 30-second interval
Assign unique sequence IDs to each group
Get unique sequence IDs
Split sequence IDs into train and temp (temporary holding for val and test)
Adjust the proportions for val and test based on the remaining sequences
"Allocate images to train, validation, and test sets based on their sequence ID"
Save the datasets to CSV files
Return the split datasets
Exportable class names for external use
Applying the ResNet layers and operations
Initialize the network with the specified settings
Selecting the appropriate ResNet architecture and pre-trained weights
self.pretrained_weights = ResNet18_Weights.IMAGENET1K_V1
self.pretrained_weights = ResNet50_Weights.IMAGENET1K_V1
Constructing the feature extractor and classifier
Criterion for binary classification
Load pre-trained weights and adjust for the current model
init_weights = self.pretrained_weights.get_state_dict(progress=True)
Load the weights into the feature extractor
Identify missing and unused keys in the loaded weights
Import necessary libraries
Exportable class names for external use
Define normalization mean and standard deviation for image preprocessing
Define data transformations for training and validation datasets
Load data for prediction
Load data for training/validation
"Load datasets for different modes (training, validation, testing, prediction)"
Calculate class counts and label mappings
Define parameters for the optimizer
Optimizer parameters for feature extraction
Optimizer parameters for the classifier
Setup optimizer and optimizer scheduler
Forward pass
Calculate loss
Forward pass
Forward pass
Concatenate outputs from all test steps
Calculate the metrics and save the output
Forward pass
Concatenate outputs from all predict steps
Compute the confusion matrix from true labels and predictions
Calculate class-wise accuracy (accuracy for each class)
Calculate micro accuracy (overall accuracy)
Calculate macro accuracy (mean of class-wise accuracies)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Importing necessary basic libraries and modules
%%
PyTorch imports for tensor operations
%%
"Importing the models, transformations, and utility functions from PytorchWildlife"
%%
Setting the device to use for computations ('cuda' indicates GPU)
%%
Initializing the model for image detection
%%
Initializing the model for image classification
%%
Defining transformations for detection and classification
%%
Initializing a box annotator for visualizing detections
Processing the video and saving the result with annotated detections and classifications
%%
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Importing basic libraries
%%
"Importing the models, dataset, transformations, and utility functions from PytorchWildlife"
%%
Setting the device to use for computations ('cuda' indicates GPU)
Initializing a supervision box annotator for visualizing detections
Create a temp folder
Initializing the detection and classification models
Defining transformations for detection and classification
%% Defining functions for different detection scenarios
Only run classifier when detection class is animal
Clean the temp folder if it contains files
Check the contents of the extracted folder
%% Building Gradio UI
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Importing necessary basic libraries and modules
%%
PyTorch imports
%%
"Importing the model, dataset, transformations and utility functions from PytorchWildlife"
%%
Setting the device to use for computations ('cuda' indicates GPU)
%%
Initializing the MegaDetectorV5 model for image detection
%% Single image detection
Specifying the path to the target image TODO: Allow argparsing
Opening and converting the image to RGB format
Initializing the Yolo-specific transform for the image
Performing the detection on the single image
Saving the detection results
%% Batch detection
Specifying the folder path containing multiple images for batch detection
Creating a dataset of images with the specified transform
Creating a DataLoader for batching and parallel processing of the images
Performing batch detection on the images
%% Output to annotated images
Saving the batch detection results as annotated images
%% Output to cropped images
Saving the detected objects as cropped images
%% Output to JSON results
Saving the detection results in JSON format
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Making the DetectionImageFolder class available for import from this module
Listing and sorting all image files in the specified directory
Get image filename and path
Load and convert image to RGB
Apply transformation if specified
Only run recognition on animal detections
Get image path and corresponding bbox xyxy for cropping
Load and crop image with supervision
Apply transformation if specified
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
from yolov5.utils.augmentations import letterbox
Making the provided classes available for import from this module
Convert PIL Image to Torch Tensor
Original shape
New shape
Scale ratio (new / old) and compute padding
Resize image
Pad image
Convert the image to a PyTorch tensor and normalize it
Resize and pad the image using a customized letterbox function.
Normalization constants
Define the sequence of transformations
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
!!! Output paths need to be optimized !!!
!!! Output paths need to be optimized !!!
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Importing basic libraries
Placeholder class-level attributes to be defined in derived classes
"If there are size differences in the input images, use a for loop instead of matrix processing for scaling"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Image size for the Opossum classifier
Class names for prediction
"If pretrained, use the provided URL to fetch the weights"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Making the PlainResNetInference class available for import from this module
Following the ResNet structure to extract features
Initialize the network and weights
... [Missing weight URL definition for ResNet18]
... [Missing weight URL definition for ResNet50]
Print missing and unused keys for debugging purposes
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Image size for the Opossum classifier
Class names for prediction
"If pretrained, use the provided URL to fetch the weights"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Image size for the Opossum classifier
Class names for prediction
"If pretrained, use the provided URL to fetch the weights"
Configuration file for the Sphinx documentation builder.
""
"For the full list of built-in configuration values, see the documentation:"
https://www.sphinx-doc.org/en/master/usage/configuration.html
-- Project information -----------------------------------------------------
https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information
-- General configuration ---------------------------------------------------
https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration
-- Options for HTML output -------------------------------------------------
https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output
-- Options for todo extension ----------------------------------------------
https://www.sphinx-doc.org/en/master/usage/extensions/todo.html#configuration
"Determine the factor, which shifts the decimal point of x"
just behind the last significant digit
"Shift decimal point by multiplicatipon with factor, flooring, and"
division by factor
_ = pretty_print_object(obj)
Sloppy that I'm making a module-wide change here...
Determine the coordinates of the intersection rectangle
The intersection of two axis-aligned bounding boxes is always an
axis-aligned bounding box
Compute the area of both AABBs
Compute the intersection over union by taking the intersection
area and dividing it by the sum of prediction + ground-truth
areas - the intersection area.
%% Functions for running commands as subprocesses
%%
%% Test driver for execute_and_print
%% Parallel test driver for execute_command_and_print
Should we use threads (vs. processes) for parallelization?
"Only relevant if n_workers == 1, i.e. if we're not parallelizing"
%% Constants and imports
%% Classes
Collect all names
Make names unique and sort
cast location to string as the entries in locations are strings
Convert classnames to lowercase to simplify comparisons later
Normalize paths to simplify comparisons later
"Make custom replacements in filenames, typically used to"
accommodate changes in root paths after DB construction
## Build useful mappings to facilitate working with the DB
Category ID <--> name
Image filename --> ID
Image ID --> image object
Image ID --> annotations
"Each image can potentially multiple annotations, hence using lists"
...__init__
...class IndexedJsonDb
%% Functions
Find all unique locations
i_location = 0; location = locations[i_location]
Sorting datetimes fails when there are None's in the list.  So instead of sorting datetimes
"directly, sort tuples with a boolean for none-ness, then the datetime itself."
""
https://stackoverflow.com/questions/18411560/sort-list-while-pushing-none-values-to-the-end
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_location[1]
"Start a new sequence if necessary, including the case where this datetime is invalid"
"If this was an invalid datetime, this will record the previous datetime"
"as None, which will force the next image to start a new sequence."
...for each image in this location
Fill in seq_num_frames
...for each location
...create_sequences()
""
cct_to_md.py
""
"""Converts"" a COCO Camera Traps file to a MD results file.  Currently ignores"
"non-bounding-box annotations, and gives all annotations a confidence of 1.0."
""
The only reason to do this is if you are going to add information to an existing
"CCT-formatted dataset, and want to do that in Timelapse."
""
"Currently assumes that width and height are present in the input data, does not"
read them from images.
""
%% Constants and imports
%% Functions
# Validate input
# Read input
# Prepare metadata
ann = d['annotations'][0]
# Process images
im = d['images'][0]
"MegaDetector: [x,y,width,height] (normalized, origin upper-left)"
"CCT: [x,y,width,height] (absolute, origin upper-left)"
...if there's a bounding box
...for each annotation
This field is no longer included in MD output files by default
im_out['max_detection_conf'] = max_detection_conf
...for each image
# Write output
...cct_to_md()
%% Command-line driver
TODO
%% Interactive driver
%%
%%
""
cct_json_to_filename_json.py
""
"Given a .json file in COCO Camera Traps format, outputs a .json-formatted list of"
relative file names present in the CCT file.
""
%% Constants and environment
%% Main function
"json.dump(s,open(outputFilename,'w'))"
%% Command-line driver
%% Interactive driver
%%
""
cct_to_csv.py
""
"""Converts"" a COCO Camera Traps .json file to .csv, in quotes because"
"all kinds of assumptions are made here, and if you have a particular .csv"
"format in mind, YMMV.  Most notably, does not include any bounding box information"
or any non-standard fields that may be present in the .json file.  Does not
propagate information about sequence-level vs. image-level annotations.
""
"Does not assume access to the images, therefore does not open .jpg files to find"
"datetime information if it's not in the metadata, just writes datetime as 'unknown'."
""
%% Imports
%% Main function
#%% Read input
#%% Build internal mappings
annotation = annotations[0]
#%% Write output file
im = images[0]
Write out one line per class:
...for each class name
...for each image
...with open(output_file)
...def cct_to_csv
%% Interactive driver
%%
%% Command-line driver
""
remove_exif.py
""
"Removes all EXIF/IPTC/XMP metadata from a folder of images, without making"
"backup copies, using pyexiv2."
""
%% Imports and constants
%% List files
%% Remove EXIF data (support)
PYEXIV2 IS NOT THREAD SAFE; DO NOT CALL THIS IN PARALLEL FROM A SINGLE PROCESS
data = img.read_exif(); print(data)
%% Debug
%%
%%
%% Remove EXIF data (execution)
fn = image_files[0]
"joblib.Parallel defaults to a process-based backend, but let's be sure"
"results = Parallel(n_jobs=n_exif_threads,verbose=2,prefer='processes')(delayed(remove_exif)(fn) for fn in image_files[0:10])"
""
yolo_to_coco.py
""
Converts a YOLO-formatted dataset to a COCO-formatted dataset.
""
"Currently supports only a single folder (i.e., no recursion).  Treats images without"
corresponding .txt files as empty.
""
%% Imports and constants
from ai4eutils
%% Support functions
Validate input
Class names
Blank lines should only appear at the end
Enumerate images
fn = image_files[0]
Create the image object for this image
Is there an annotation file for this image?
"This is an image with no annotations, currently don't do anything special"
here
s = lines[0]
"COCO: [x_min, y_min, width, height] in absolute coordinates"
"YOLO: [class, x_center, y_center, width, height] in normalized coordinates"
...for each annotation
...if this image has annotations
...for each image
...def yolo_to_coco()
%% Interactive driver
%% Convert YOLO folders to COCO
%% Check DB integrity
%% Preview some images
%% Command-line driver
TODO
""
read_exif.py
""
"Given a folder of images, read relevant metadata (EXIF/IPTC/XMP) fields from all images,"
and write them to  a .json or .csv file.
""
This module can use either PIL (which can only reliably read EXIF data) or exiftool (which
can read everything).  The latter approach expects that exiftool is available on the system
path.  No attempt is made to be consistent in format across the two approaches.
""
%% Imports and constants
From ai4eutils
%% Options
Number of concurrent workers
Should we use threads (vs. processes) for parallelization?
""
Not relevant if n_workers is 1.
Should we use exiftool or pil?
%% Functions
exif_tags = img.info['exif'] if ('exif' in img.info) else None
print('Warning: unrecognized EXIF tag: {}'.format(k))
"-G means ""Print group name for each tag"", e.g. print:"
""
[File]          Bits Per Sample                 : 8
""
...instead of:
""
Bits Per Sample                 : 8
A list of three-element lists (type/tag/value)
line_raw = exif_lines[0]
A typical line:
""
[ExifTool]      ExifTool Version Number         : 12.13
"Split on the first occurrence of "":"""
...for each output line
...which processing library are we using?
...read_exif_tags_for_image()
...populate_exif_data()
Enumerate *relative* paths
Find all EXIF tags that exist in any image
...for each tag in this image
...for each image
Write header
...for each key that *might* be present in this image
...for each image
...with open()
...if we're writing to .json/.csv
https://stackoverflow.com/questions/11210104/check-if-a-program-exists-from-a-python-script
%% Interactive driver
%%
output_file = os.path.expanduser('~/data/test-exif.csv')
options.processing_library = 'pil'
"file_path = os.path.join(input_folder,'KRU_S1_11_R1_IMAG0148.JPG')"
%%
%% Command-line driver
""
"Given a json-formatted list of image filenames, retrieve the width and height of every image."
""
%% Constants and imports
%% Processing functions
Is this image on disk?
"all_results = list(tqdm(pool.imap(process_image, filenames), total=len(filenames)))"
%% Interactive driver
%%
List images in a test folder
%%
"process_list_file(image_list_file,image_size_file,image_prefix=base_dir)"
%% Command-line driver
"python sanity_check_json_db.py ""e:\wildlife_data\wellington_data\wellington_camera_traps.json"" --baseDir ""e:\wildlife_data\wellington_data\images"" --bFindUnusedImages --bCheckImageSizes"
"python sanity_check_json_db.py ""D:/wildlife_data/mcgill_test/mcgill_test.json"" --baseDir ""D:/wildlife_data/mcgill_test"" --bFindUnusedImages --bCheckImageSizes"
"Here the '-u' prevents buffering, which makes tee happier"
""
python -u sanity_check_json_db.py '/datadrive1/nacti_metadata.json' --baseDir '/datadrive1/nactiUnzip/' --bFindUnusedImages --bCheckImageSizes | tee ~/nactiTest.out
""
Partially-formed stub to get from MegaDetector output files to COCO Camera Traps data.
""
"Was actually written to convert *many* MD .json files to a single CCT file, hence"
the loop over .json files.
""
"THIS CODE HAS NEVER BEEN RUN, it was added as a demonstration of how to do this.  YMMV."
""
"You may find a more polished, command-line-ready version of this code at:"
""
https://github.com/StewartWILDlab/mdtools
""
%% Constants and imports
"Images sizes are required to convert between absolute and relative coordinates,"
so we need to read the images.
Only required if you want to write a database preview
%% Create CCT dictionaries
image_ids_to_images = {}
Force the empty category to be ID 0
Load .json annotations for this data set
i_entry = 0; entry = data['images'][i_entry]
""
"PERF: Not exactly trivially parallelizable, but about 100% of the"
time here is spent reading image sizes (which we need to do to get from
"absolute to relative coordinates), so worth parallelizing."
Generate a unique ID from the path
detection = detections[0]
Have we seen this category before?
Create an annotation
"MegaDetector: [x,y,width,height] (normalized, origin upper-left)"
"CCT: [x,y,width,height] (absolute, origin upper-left)"
...for each detection
...for each image
Remove non-reviewed images and associated annotations
%% Create info struct
%% Write .json output
%% Clean start
## Everything after this should work from a clean start ###
%% Validate output
%% Preview animal labels
%% Preview empty labels
"viz_options.classes_to_exclude = ['empty','human']"
""
generate_crops_from_cct.py
""
"Given a .json file in COCO Camera Traps format, create a cropped image for"
each bounding box.
""
%% Imports and constants
%% Functions
# Read and validate input
# Find annotations for each image
"This actually maps image IDs to annotations, but only to annotations"
containing boxes
# Generate crops
TODO: parallelize this loop
im = d['images'][0]
Load the image
Generate crops
i_ann = 0; ann = annotations_this_image[i_ann]
"x/y/w/h, origin at the upper-left"
...for each box
...for each image
...generate_crops_from_cct()
%% Interactive driver
%%
%%
%%
%% Command-line driver
TODO
%% Scrap
%%
""
coco_to_yolo.py
""
Converts a COCO-formatted dataset to a YOLO-formatted dataset.
""
"If the input and output folders are the same, writes .txt files to the input folder,"
and neither moves nor modifies images.
""
"Currently ignores segmentation masks, and errors if an annotation has a"
segmentation polygon but no bbox
""
Has only been tested on a handful of COCO Camera Traps data sets; if you
"use it for more general COCO conversion, YMMV."
""
%% Imports and constants
%% Support functions
Validate input
Read input data
Parse annotations
i_ann = 0; ann = data['annotations'][0]
Make sure no annotations have *only* segmentation data
Re-map class IDs to make sure they run from 0...n-classes-1
""
"TODO: this allows unused categories in the output data set, which I *think* is OK,"
but I'm only 81% sure.
Process images (everything but I/O)
"List of dictionaries with keys 'source_image','dest_image','bboxes','dest_txt'"
i_image = 0; im = data['images'][i_image]
"assert os.path.isfile(source_image), 'Could not find image {}'.format(source_image)"
If this annotation has no bounding boxes...
"This is not entirely clear from the COCO spec, but it seems to be consensus"
"that if you want to specify an image with no objects, you don't include any"
annotations for that image.
We allow empty bbox lists in COCO camera traps; this is typically a negative
"example in a dataset that has bounding boxes, and 0 is typically the empty"
category.
...if this is an empty annotation
"COCO: [x_min, y_min, width, height] in absolute coordinates"
"YOLO: [class, x_center, y_center, width, height] in normalized coordinates"
Convert from COCO coordinates to YOLO coordinates
...for each annotation
...if this image has annotations
...for each image
Write output
Category IDs should range from 0..N-1
TODO: parallelize this loop
""
output_info = images_to_copy[0]
Only write an annotation file if there are bounding boxes.  Images with
"no .txt files are treated as hard negatives, at least by YOLOv5:"
""
https://github.com/ultralytics/yolov5/issues/3218
""
"I think this is also true for images with empty annotation files, but"
"I'm using the convention suggested on that issue, i.e. hard negatives"
are expressed as images without .txt files.
bbox = bboxes[0]
...for each image
...def coco_to_yolo()
%% Interactive driver
%% CCT data
"If preview_export is True, I'm exporting to preview these with BoundingBoxEditor:"
""
https://github.com/mfl28/BoundingBoxEditor
""
"This export will be compatible, other than the fact that you need to move"
"""object.data"" into the ""labels"" folder."
""
"Otherwise I'm exporting for training, in the YOLOv5 flat format."
%% Command-line driver
TODO
""
cct_to_wi.py
""
Converts COCO Camera Traps .json files to the Wildlife Insights
batch upload format
""
Also see:
""
https://github.com/ConservationInternational/Wildlife-Insights----Data-Migration
""
https://data.naturalsciences.org/wildlife-insights/taxonomy/search
""
%% Imports
%% Paths
A COCO camera traps file with information about this dataset
A .json dictionary mapping common names in this dataset to dictionaries with the
"WI taxonomy fields: common_name, wi_taxon_id, class, orer, family, genus, species"
%% Constants
%% Project information
%% Read templates
%% Compare dictionary to template lists
Write the header
Write values
%% Project file
%% Camera file
%% Deployment file
%% Images file
Read .json file with image information
Read taxonomy dictionary
Populate output information
df = pd.DataFrame(columns = images_fields)
annotation = annotations[0]
im = input_data['images'][0]
"We don't have counts, but we can differentiate between zero and 1"
This is the label mapping used for our incoming iMerit annotations
"Only used to parse the incoming annotations. In our database, the string name is used to avoid confusion"
MegaDetector outputs
""
add_bounding_boxes_to_megadb.py
""
"Given COCO-formatted JSONs containing manually labeled bounding box annotations, add them to"
"MegaDB sequence entries, which can then be ingested into MegaDB."
the category map that comes in the COCO JSONs for iMerit batch 12 - to check that each
JSON
"dataset name : (seq_id, frame_num) : [bbox, bbox]"
where bbox is a dict with str 'category' and list 'bbox'
iterate over image_id_to_image rather than image_id_to_annotations so we include
the confirmed empty images
The file_name field in the incoming json looks like
alka_squirrels.seq2020_05_07_25C.frame119221.jpg
there seems to be a bug in the annotations where sometimes there's a
non-empty label along with a label of category_id 5
ignore the empty label (they seem to be actually non-empty)
""
add_bounding_boxes_to_json.py
""
This script takes a image database in the COCO Camera Traps format and merges in a set of bounding
box annotations in the format that iMerit uses (a .json where actually only each row is a valid json).
""
"If you need to update an existing bbox database, please get all the original annotation files and"
re-generate from scratch
""
%% Imports
%% Configurations and paths
images database
output bboxes database
annotation files (pseudo json) obtained from our annotation vendor that contain annotations for this dataset
None or a string or tuple of strings that is the prefix to all file_name of interest / in this dataset in the annotation files
functions for mapping the image_id in the annotation files (pseudo jsons) to the image_id used in the image DB
our img_id doesn't contain frame info
"batch3 - ""file_name"":""ASG0000019_0_S1_B06_R1_PICT0007.JPG"""
"batch5 and 7 - ""file_name"":""datasetsnapshotserengeti.seqASG000002m-frame0.imgS1_B06_R1_PICT0056.JPG"""
sometimes - 'datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG'
"id in DB (old_token): 'S6/J01/J01_R1/S6_J01_R1_IMAG0001', 'S1/B05/B05_R1/S1_B05_R1_PICT0036'"
specify which one to use for your dataset here
%% Load the image database and fill in DB info for the output bbox database
load the images database
%% Find the height and width of images from the annotation files
""
...if they are not available in the images DB
each row in this pseudo-json is a COCO formatted entry for an image sequence
%% Other functions required by specific datasets
the IDFG image database does not include images from unlabeled folders that were annotated with bounding boxes
%% Create the bbox database from all annotation files pertaining to this dataset
"for the incoming annotations, look up by category name (common) and convert to the numerical id used in our databases"
"for each annotation pseudo-json, check that the image it refers to exists in the original database"
each row in this pseudo-json is a COCO formatted entry for an image sequence
check that entry is for this dataset
category map for this entry in the annotation file - usually the same across all entries but just in case
rspb_add_image_entry(img_id)
use the image length and width in the image DB
"[top left x, top left y, width, height] in relative coordinates"
"add all images that have been sent to annotation, some of which may be empty of bounding boxes"
rspb_add_image_entry(db_image_id)
%% Common queries
This query is used when preparing tfrecords for object detector training.
We do not want to get the whole seq obj where at least one image has bbox because
some images in that sequence will not be bbox labeled so will be confusing.
Include images with bbox length 0 - these are confirmed empty by bbox annotators.
"If frame_num is not available, it will not be a field in the result iterable."
"Note that the seq_id is the Cosmos DB assigned ID for that sequence, not the"
"seq_id field, which may contain ""/"" characters."
"For public datasets to be converted to the CCT format, we get the whole seq object because"
sequence level attributes need to be included too. megadb/converters/megadb_to_cct.py handles
the case of bbox-only JSONs with the flag exclude_species_class
Getting all sequences in a dataset - for updating or deleting entries which need the id field
%% Parameters
Use None if querying across all partitions
"The `sequences` table has the `dataset` as the partition key, so if only querying"
"entries from one dataset, set the dataset name here."
"e.g. {'name': '@top_n', 'value': 100} - see query_and_upsert_examples/query_for_data.ipynb"
Use False if do not want all results stored in a single JSON.
%% Script
execute the query
loop through and save the results
MODIFY HERE depending on the query
wiitigers Unicode issue - no good mapping from DB file names to file names in blob URL
build filename
if need to re-download a dataset's images in case of corruption
entries_to_download = {
"filename: entry for filename, entry in entries_to_download.items()"
if entry['dataset'] == DATASET
}
input validation
"existing files, with paths relative to <store_dir>"
parse JSON or TXT file
"create a new storage container client for this dataset,"
and cache it
schema already checks that the min possible value of frame_num is 0
"if there are more than one image item, each needs a frame_num"
checks across all sequence items
per sequence item checks
load the schema
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory
load the schema
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory
checks across all datasets items
check for expiry date of the SAS keys
update the sequences
"`id` is from the database, as well as all attributes starting with _"
"if valuable sequence information is available, add them to the image"
required fields for an image object
add seq-level class labels for this image
add other sequence-level properties to each image too
add other image-level properties
... for im in seq['images']
... for seq in mega_db
consolidate categories
some property names have changed in the new schema
a dummy sequence ID will be generated if the image entry does not have a seq_id field
seq_id only needs to be unique within this dataset; MegaDB does not rely on it as the _id field
"""annotations"" fields are opened and have its sub-field surfaced one level up"
set the `dataset` property on each sequence to the provided dataset_name
check that the location field is the same for all images in a sequence
check which fields in a CCT image entry are sequence-level
image-level properties that really should be sequence-level
need to add (misidentified) seq properties not present for each image in a sequence to img_level_properties
"(some properties act like flags - all have the same value, but not present on each img)"
add the sequence-level properties to the sequence objects
not every sequence have to have all the seq_level_properties
get the value of this sequence-level property from the first image entry
check which fields are really dataset-level and should be included in the dataset table instead.
delete sequence-level properties that should be dataset-level
make all `class` fields lower-case; cast `seq_id` to type string in case they're integers
turn all float NaN values into None so it gets converted to null when serialized
this was an issue in the Snapshot Safari datasets
%% validation
"at first a dict of image_id: image_obj with annotations embedded, then it becomes"
an array of image objects
%% integrate the image DB
takes in image entries and species and other annotations in the image DB
convert the species category to explicit string name
there may be other fields in the annotation object
these fields should already be gotten from the image object
%% integrate the bbox DB
add any images that are not in the image DB
also add any fields in the image object that are not present already
add bbox to the annotations field
for any newly added images
"'bbox_abs': bbox_anno['bbox'],"
not keeping height and width
""
jb_csv_to_json.py
""
Convert a particular .csv file to CCT format.  Images were not available at
"the time I wrote this script, so this is much shorter than other scripts"
in this folder.
""
%% Constants and environment
%% Read source data
%% Confirm filename uniqueness (this data set has one label per image)
"Build up a map from filenames to a list of rows, checking image existence as we go"
%% Create CCT dictionaries
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
imageName = imageFilenames[0]
"As per above, this is convenient and appears to be true; asserting to be safe"
"Filenames look like ""290716114012001a1116.jpg"""
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
%% Create info struct
%% Write output
%% Sanity-check
%% Imports and constants
%% Load data
"with open(image_json,'r') as f:"
data = json.load(f)
%% Sanity-check data
%% Label previews
%% Collect images to annotate
%% Sort by sequence and frame
%% Copy to a folder by GUID
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension
im = images_to_annotate[0]
...for each image
%% Write out the annotation list
%% Constants and imports
%% Enumerate files
edited_image_folder = edited_image_folders[0]
fn = edited_image_files[0]
%% Read metadata and capture location information
i_row = 0; row = df.iloc[i_row]
Sometimes '2017' was just '17' in the date column
%% Read the .json files and build output dictionaries
json_fn = json_files[0]
if 'partial' in json_fn:
continue
line = lines[0]
if image_fn == 'SD1_238_6_26_17_16_76.73.jpg':
asdfad
SD29_079_5_14_2018_17_52.85.jpg
Re-write two-digit years as four-digit years
Sometimes the year was written with two digits instead of 4
assert len(tokens[4]) == 4 and tokens[4].startswith('20')
Have we seen this location already?
"Not a typo, it's actually ""formateddata"""
An image shouldn't be annotated as both empty and non-empty
An image shouldn't be annotated as both empty and non-empty
box = formatteddata[0]
"open_file(os.path.join(base_folder,jpeg_image_folder,image_fn))"
...for each box
...if there are boxes on this image
...for each line
...with open()
...for each json file
%% Prepare the output .json
%% Check DB integrity
%% Print unique locations
SD12_202_6_23_2017_1_31.85.jpg
%% Preview some images
%% Statistics
""
"Import a Snapshot Safari project (one project, one season)"
""
Before running this script:
""
"* Mount the blob container where the images live, or copy the"
images to local storage
""
What this script does:
""
* Creates a .json file
* Creates zip archives of the season without humans.
* Copies animals and humans to separate folders
""
After running this script:
""
* Create or update LILA page
* Push zipfile and unzipped images to LILA
* Push unzipped humans to wildlifeblobssc
* Delete images from UMN uplaod storage
""
"Snapshot Serengeti is handled specially, because we're dealing with bounding"
boxes too.  See snapshot_serengeti_lila.py.
""
%% Imports
From ai4eutils
From CameraTraps
%% Constants
project_name = 'XXX'; season_name = 'S1'; project_friendly_name = 'Snapshot Unknown'
project_name = 'SER'; season_name = 'S1-11'; project_friendly_name = 'Snapshot Serengeti'
project_name = 'KRU'; season_name = 'S1'; project_friendly_name = 'Snapshot Kruger'
project_name = 'CDB'; season_name = 'S1'; project_friendly_name = 'Snapshot Camdeboo'
project_name = 'MTZ'; season_name = 'S1'; project_friendly_name = 'Snapshot Mountain Zebra'
project_name = 'ENO'; season_name = 'S1'; project_friendly_name = 'Snapshot Enonkishu'
project_name = 'KAR'; season_name = 'S1'; project_friendly_name = 'Snapshot Karoo'
project_name = 'KGA'; season_name = 'S1'; project_friendly_name = 'Snapshot Kgalagadi'
%% Folder/file creation
E.g. KRU_S1
E.g. Z:\KRU
E.g. Z:\KRU\KRU_S1
Contains annotations for each capture event (sequence)
Maps image IDs to filenames; each line looks like:
""
"KRU_S1#1#1#2,3,KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0004.JPG"
"Total number of each answer to each question, e.g. total number of times each species was identified"
""
Not used here
Create output folders
Images will be placed in a season-specific folder inside this (the source data includes
this in path names)
%% Load metadata files
%% Convert to dictionaries (prep)
%% Convert to dictionaries (loops)
"TODO: iterrows() is a terrible way to do this, but this is one of those days"
"where I want to get this done, not get better at Python."
irow = 0; row = image_table.iloc[0]
"Loaded as an int64, converting to int here"
...for each row in the image table
Make sure image IDs are what we think they are
...for each row in the annotation table
%% Take a look at categories (just sanity-checking)
print('\nCategories by species:')
pp.pprint(categories_by_species)
%% Fill in some image fields we didn't have when we created the image table
"width, height, corrupt, seq_num_frames, location, datetime"
Every annotation in this list should have the same sequence ID
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;"
there's no reason to do this redundantly for every image
Every image in this sequence should point back to the same equence
Every annotation in this list should have the same location
Every annotation in this list should have the same datetime
Is this image on disk?
iImage = 0; im = images[0]
...for each image
"images_processed = pool.map(process_image, images)"
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))"
%% Count missing/corrupted images
%% Print distribution of sequence lengths (sanity-check)
%% Replicate annotations across images
iAnn = 0; ann = annotations[iAnn]
%% See what files are on disk but not annotated
%% Sanity-check image and annotation uniqueness
%% Minor updates to fields
%% Write .json file
%% Create a list of human files
ann = annotations[0]
%% Create public archive and public/private folders
im = images[0]
E.g. KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0001.JPG
Copy to private output folder
Add to zipfile
Possibly start a new archive
Copy to public output folder
...for each image
%% Sanity-check .json file
"This will produce some validation errors, because this zipfile doesn't include humans"
%% Zip up .json and .csv files
%% When I skip to this part (using a pre-rendered .json file)
%%
%%
ann = annotations[0]
%% Summary prep for LILA
"%% Generate preview, sanity-check labels"
"viz_options.classes_to_include = ['jackalblackbacked','bustardkori']"
%% Scrap
%% Find annotations for a particular image
%% Write a list of missing images
""
save_the_elephants_survey_A.py
""
Convert the .csv file provided for the Save the Elephants Survey A data set to a
COCO-camera-traps .json file
""
%% Constants and environment
Handle all unstructured fields in the source data as extra fields in the annotations
"photo_type really should be an image property, but there are a few conflicts"
that forced me to handle it as an annotation proprerty
%% Read source data
"%% Map filenames to rows, verify image existence"
Maps relative paths to row indices in input_metadata
"Build up a map from filenames to a list of rows, checking image existence as we go"
Ignore directories
%% Make sure the multiple-annotation cases make sense
%%
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
Force the empty category to be ID 0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image]
Example filename:
""
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG'
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG'
i_row = row_indices[0]
There are a small handful of datetime mismatches across annotations
for the same image
assert im['datetime'] == timestamp
Special cases based on the 'photo type' field
Various spellings of 'community'
Have we seen this category before?
Create an annotation
fieldname = list(mapped_fields.keys())[0]
...for each row
...for each image
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
%% Scrap
%% Find unique photo types
##
""
timelapse_csv_set_to_json.py
""
Given a directory full of reasonably-consistent Timelapse-exported
".csvs, assemble a CCT .json."
""
"Assumes that you have a list of all files in the directory tree, including"
image and .csv files.
""
##
%% Constants and imports
Text file with relative paths to all files (images and .csv files)
"%% Read file list, make a list of all image files and all .csv files"
"%% Verify column consistency, create a giant array with all rows from all .csv files"
i_csv = 0; csv_filename = csv_files[0]
%% Prepare some data structures we'll need for mapping image rows in .csv files to actual image files
Enumerate all folders containing image files
"In this data set, a site folder looks like:"
""
Processed Images\\site_name
%% Map .csv files to candidate camera folders
fn = valid_csv_files[0]
"Some site folders appear as ""XXNNNN"", some appear as ""XXNNNN_complete"""
...for each .csv file
%% Map camera folders to candidate image folders
%% Main loop over labels (prep)
Force the empty category to be ID 0
Images that are marked empty and also have a species label
%% Main loop over labels (loop)
i_row = 0; row = input_metadata.iloc[i_row]
"for i_row,row in input_metadata.iterrows():"
"Usually this is just a single folder name, sometimes it's a full path,"
which we don't want
Check whether this file exists on disk
Retrieve image width and height
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each row in the big table of concatenated .csv files
%% Fix cases where an image was annotated as 'unlabeled' and as something else
This annotation is 'unlabeled'
Was there another category associated with this image?
%% Check for un-annnotated images
Enumerate all images
list(relative_path_to_image.keys())[0]
%% Create info struct
%% Write output
%% Sanity-check the database's integrity
%% Render a bunch of images to make sure the labels got carried along correctly
""
auckland_doc_to_json.py
""
Convert Auckland DOC data set to COCO camera traps format.  This was
for a training data set where class names were encoded in path names.
""
%% Constants and imports
Filenames will be stored in the output .json relative to this base dir
%% Enumerate files
%% Assemble dictionaries
Force the empty category to be ID 0
fn = image_files[0]; print(fn)
Typically y:\Maukahuka_Auckland_Island\1_Training\Winter_Trial_2019\cat\cat\eat\20190903_IDdY_34_E3_tmp_201908240051.JPG
"This data set has two top-level folders, ""1_Training"" (which has class names encoded"
"in paths) and ""2_Testing"" (which has no class information)."
...for each image
%% Write output .json
%% Write train/test .jsons
%% Validate .json files
%% Preview labels
checkpoint
""
awc_to_json.py
""
Convert a particular .csv file to CCT format.
""
%% Constants and environment
%% Read source data
%% Main loop over labels
Force the empty category to be ID 0
iRow = 0; row = input_metadata.iloc[iRow]
"ImageID,FileName,FilePath,SpeciesID,CommonName"
"This makes an assumption of one annotation per image, which happens to be"
true in this data set.
Retrieve image width and height
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
%% Check for images that aren't included in the metadata file
Enumerate all images
list(relativePathToImage.keys())[0]
%% Create info struct
%% Write output
%% Sanity-check the database's integrity
%% Render a bunch of images to make sure the labels got carried along correctly
""
idaho-camera-traps.py
""
Prepare the Idaho Camera Traps dataset for release on LILA.
""
%% Imports and constants
Multi-threading for .csv file comparison and image existence validation
"We are going to map the original filenames/locations to obfuscated strings, but once"
"we've done that, we will re-use the mappings every time we run this script."
This is the file to which mappings get saved
The maximum time (in seconds) between images within which two images are considered the
same sequence.
"This is a two-column file, where each line is [string in the original metadata],[category name we want to map it to]"
"The output file, using the original strings"
"The output file, using obfuscated strings for everything but filenamed"
"The output file, using obfuscated strings and obfuscated filenames"
"One time only, I ran MegaDetector on the whole dataset..."
...then set aside any images that *may* have contained humans that had not already been
annotated as such.  Those went in this folder...
...and the ones that *actually* had humans (identified via manual review) got
copied to this folder...
"...which was enumerated to this text file, which is a manually-curated list of"
images that were flagged as human.
Unopinionated .json conversion of the .csv metadata
%% List files (images + .csv)
Ignore .csv files in folders with multiple .csv files
...which would require some extra work to decipher.
fn = csv_files[0]
%% Parse each .csv file into sequences (function)
csv_file = csv_files[-1]
os.startfile(csv_file_absolute)
survey = csv_file.split('\\')[0]
Sample paths from which we need to derive locations:
""
St.Joe_elk\AM99\Trip 1\100RECNX\TimelapseData.csv
Beaverhead_elk\AM34\Trip 1\100RECNX\TimelapseData.csv
""
ClearCreek_mustelids\Winter2015-16\FS-001-P\FS-001-P.csv
ClearCreek_mustelids\Summer2015\FS-001\FS-001.csv
ClearCreek_mustelids\Summer2016\IDFG-016\IDFG-016.csv
""
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017b
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017a
Load .csv file
Validate the opstate column
# Create datetimes
print('Creating datetimes')
i_row = 0; row = df.iloc[i_row]
Make sure data are sorted chronologically
""
"In odd circumstances, they are not... so sort them first, but warn"
Debugging when I was trying to see what was up with the unsorted dates
# Parse into sequences
print('Creating sequences')
i_row = 0; row = df.iloc[i_row]
Start a new sequence if:
""
* This image has no timestamp
* This image has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each row
# Parse labels for each sequence
sequence_id = location_sequences[0]
Row indices in a sequence should be adjacent
sequence_df = df[df['seq_id']==sequence_id]
# Determine what's present
Be conservative; assume humans are present in all maintenance images
The presence columns are *almost* always identical for all images in a sequence
assert single_presence_value
print('Warning: presence value for {} is inconsistent for {}'.format(
"presence_column,sequence_id))"
...for each presence column
Tally up the standard (survey) species
"If no presence columns are marked, all counts should be zero"
count_column = count_columns[0]
Occasionally a count gets entered (correctly) without the presence column being marked
"assert len(values) == 1 and values[0] == 0, 'Non-zero counts with no presence"
columns marked for sequence {}'.format(sequence_id)
"Handle this by virtually checking the ""right"" box"
Make sure we found a match
Handle 'other' tags
column_name = otherpresent_columns[0]
print('Found non-survey counted species column: {}'.format(column_name))
...for each non-empty presence column
"Very rarely, the ""otherpresent"" column is checked, but no more detail is available"
...handling non-survey species
Build the sequence data
i_row = 0; row = sequence_df.iloc[i_row]
Only one folder used a single .csv file for two subfolders
...for each sequence
...def csv_to_sequences()
%% Parse each .csv file into sequences (loop)
%%
%%
i_file = -1; csv_file = csv_files[i_file]
%% Save sequence data
%% Load sequence data
%%
%% Validate file mapping (based on the existing enumeration)
sequences = sequences_by_file[0]
sequence = sequences[0]
"Actually, one folder has relative paths"
assert '\\' not in image_file_relative and '/' not in image_file_relative
os.startfile(csv_folder)
assert os.path.isfile(image_file_absolute)
found_file = os.path.isfile(image_file_absolute)
...for each image
...for each sequence
...for each .csv file
%% Load manual category mappings
The second column is blank when the first column already represents the category name
%% Convert to CCT .json (original strings)
Force the empty category to be ID 0
For each .csv file...
""
sequences = sequences_by_file[0]
For each sequence...
""
sequence = sequences[0]
Find categories for this image
"When 'unknown' is used in combination with another label, use that"
"label; the ""unknown"" here doesn't mean ""another unknown species"", it means"
there is some other unknown property about the main species.
category_name_string = species_present[0]
"This piece of text had a lot of complicated syntax in it, and it would have"
been too complicated to handle in a general way
print('Ignoring category {}'.format(category_name_string))
Don't process redundant labels
category_name = category_names[0]
If we've seen this category before...
If this is a new category...
print('Adding new category for {}'.format(category_name))
...for each category (inner)
...for each category (outer)
...if we do/don't have species in this sequence
"We should have at least one category assigned (which may be ""empty"" or ""unknown"")"
assert len(sequence_category_ids) > 0
Was any image in this sequence manually flagged as human?
print('Flagging sequence {} as human based on manual review'.format(sequence_id))
For each image in this sequence...
""
i_image = 0; im = images[i_image]
Create annotations for this image
...for each image in this sequence
...for each sequence
...for each .csv file
Verify that all images have annotations
ann = ict_data['annotations'][0]
For debugging only
%% Create output (original strings)
%% Validate .json file
%% Preview labels
%% Look for humans that were found by MegaDetector that haven't already been identified as human
This whole step only needed to get run once
%%
Load MD results
Get a list of filenames that MD tagged as human
im = md_results['images'][0]
...for each detection
...for each image
Map images to annotations in ICT
ann = ict_data['annotations'][0]
For every image
im = ict_data['images'][0]
Does this image already have a human annotation?
...for each annotation
...for each image
%% Copy images for review to a new folder
fn = missing_human_images[0]
%% Manual step...
Copy any images from that list that have humans in them to...
%% Create a list of the images we just manually flagged
fn = human_tagged_filenames[0]
E.g. '0000_Beaverhead_elk~AM174~Trip 1~100RECNX~IMG_1397.JPG'
"%% Translate location, image, sequence IDs"
Load mappings if available
Generate mappings
If we've seen this location before...
Otherwise assign a string-formatted int as the ID
If we've seen this sequence before...
Otherwise assign a string-formatted int as the ID
Assign an image ID
...for each image
Assign annotation mappings
Save mappings
"Back this file up, lest we should accidentally re-run this script"
with force_generate_mappings = True and overwrite the mappings we used.
...if we are/aren't re-generating mappings
%% Apply mappings
"%% Write new dictionaries (modified strings, original files)"
"%% Validate .json file (modified strings, original files)"
%% Preview labels (original files)
"viz_options.classes_to_exclude = ['empty','deer','elk']"
viz_options.classes_to_include = ['bobcat']
%% Copy images to final output folder (prep)
ann = d['annotations'][0]
Is this a public or private image?
Generate absolute path
Copy to output
Update the filename reference
...def process_image(im)
%% Copy images to final output folder (execution)
For each image
im = images[0]
Write output .json
%% Make sure the right number of images got there
%% Validate .json file (final filenames)
%% Preview labels (final filenames)
"viz_options.classes_to_exclude = ['empty','deer','elk']"
viz_options.classes_to_include = ['horse']
viz_options.classes_to_include = [viz_options.multiple_categories_tag]
"viz_options.classes_to_include = ['human','vehicle','domestic dog']"
%% Create zipfiles
%% List public files
%% Find the size of each file
fn = all_public_output_files[0]
%% Split into chunks of approximately-equal size
...for each file
%% Create a zipfile for each chunk
...for each filename
with ZipFile()
...def create_zipfile()
i_file_list = 0; file_list = file_lists[i_file_list]
"....if __name__ == ""__main__"""
""
wellington_to_json.py
""
Convert the .csv file provided for the Wellington data set to a
COCO-camera-traps .json file
""
%% Constants and environment
%% Read source data
"Filenames were provided as *.jpg, but images were *.JPG, converting here"
"%% Map filenames to rows, verify image existence"
"Takes ~30 seconds, since it's checking the existence of ~270k images"
"Build up a map from filenames to a list of rows, checking image existence as we go"
"I didn't expect this to be true a priori, but it appears to be true, and"
it saves us the trouble of checking consistency across multiple occurrences
of an image.
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
imageName = imageFilenames[0]
"As per above, this is convenient and appears to be true; asserting to be safe"
"Filenames look like ""290716114012001a1116.jpg"""
This gets imported as an int64
"These appear as ""image1"", ""image2"", etc."
"In the form ""001a"""
Can be in the form '111' or 's46'
"In the form ""7/29/2016 11:40"""
Check image height and width
"Use 'empty', to be consistent with other data on lila"
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
""
mcgill_to_json.py
""
Convert the .csv file provided for the McGill test data set to a
COCO-camera-traps .json file
""
%% Constants and environment
%% Read source data
"%% Map filenames to rows, verify image existence"
Create an additional column for concatenated filenames
Maps relative filenames to rows
"Build up a map from filenames to a list of rows, checking image existence as we go"
row = input_metadata.iloc[0]
"I didn't expect this to be true a priori, but it appears to be true, and"
it saves us the trouble of checking consistency across multiple occurrences
of an image.
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
row = input_metadata.iloc[0]
"Filenames look like ""290716114012001a1116.jpg"""
"In the form ""001a"""
Can be in the form '111' or 's46'
"In the form ""7/29/2016 11:40"""
Check image height and width
NaN is the only thing we should see that's not a string
NaN is the only thing we should see that's not a string
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
""
bellevue_to_json.py
""
"""Bellevue Camera Traps"" is the rather unremarkable camera trap data set"
used by one of the repo's maintainers for testing.  It's organized as:
""
approximate_date/[loose_camera_specifier/]/species
""
E.g.:
""
"""2018.03.30\coyote\DSCF0091.JPG"""
"""2018.07.18\oldcam\empty\DSCF0001.JPG"""
""
%% Constants and imports
from the ai4eutils repo
Filenames will be stored in the output .json relative to this base dir
%% Exif functions
"%% Enumerate files, create image/annotation/category info"
Force the empty category to be ID 0
Keep track of unique camera folders
Each element will be a dictionary with fields:
""
"relative_path, width, height, datetime"
fname = image_files[0]
Corrupt or not an image
Store file info
E.g. 2018.03.30/coyote/DSCF0091.JPG
...for each image file
%% Synthesize sequence information
Sort images by time within each folder
camera_path = camera_folders[0]
previous_datetime = sorted_images_this_camera[0]['datetime']
im = sorted_images_this_camera[1]
Start a new sequence if necessary
...for each image in this camera
...for each camera
Fill in seq_num_frames
%% A little cleanup
%% Write output .json
%% Sanity-check data
%% Label previews
""
snapshot_safar_importer_reprise.py
""
This is a 2023 update to snapshot_safari_importer.py.  We do a bunch of things now that
we didn't do the last time we imported Snapshot data (like updating the big taxonomy)
"file, and we skip a bunch of things now that we used to do (like generating massive"
"zipfiles).  So, new year, new importer."
""
%% Constants and imports
%% List files
"Do a one-time enumeration of the entire drive; this will take a long time,"
but will save a lot of hassle later.
%% Create derived lists
Takes about 60 seconds
CSV files are one of:
""
"_report_lila.csv (this is the one we want to use, with the species/count/etc. for each sequence)"
_report_lila_image_inventory.csv (maps captures to images)
_report_lila_overview.csv (distrubution of species)
%% List project folders
Project folders look like one of these:
""
APN
Snapshot Cameo/DEB
%% Map report and inventory files to codes
fn = csv_files[0]
%% Make sure that every report has a corresponding inventory file
%% Count species based on overview and report files
%% Print counts
%% Make sure that capture IDs in the reports/inventory files match
...and that all the images in the inventory tables are actually present on disk.
assert image_path_relative in all_files_relative_set
Make sure this isn't just a case issue
...for each report on this project
...for each project
"%% For all the files we have on disk, see which are and aren't in the inventory files"
"There aren't any capital-P .PNG files, but if I don't include that"
"in this list, I'll look at this in a year and wonder whether I forgot"
to include it.
fn = all_files_relative[0]
print('Skipping project {}'.format(project_code))
""
plot_wni_giraffes.py
""
Plot keypoints on a random sample of images from the wni-giraffes data set.
""
%% Constants and imports
%% Load and select data
%% Support functions
https://stackoverflow.com/questions/32504246/draw-ellipse-in-python-pil-with-line-thickness
Use a single channel image (mode='L') as mask.
The size of the mask can be increased relative to the imput image
to get smoother looking results.
draw outer shape in white (color) and inner shape in black (transparent)
downsample the mask using PIL.Image.LANCZOS
(a high-quality downsampling filter).
paste outline color to input image through the mask
%% Plot some images
ann = annotations_to_plot[0]
i_tool = 0; tool_name = short_tool_names[i_tool]
Don't plot tools that don't have a consensus annotation
...for each tool
...for each annotation
""
idfg_iwildcam_lila_prep.py
""
Adding class labels (from the private test .csv) to the iWildCam 2019 IDFG
"test set, in preparation for release on LILA."
""
This version works with the public iWildCam release images.
""
"%% ############ Take one, from iWildCam .json files ############"
%% Imports and constants
%% Read input files
Remove the header line
%% Parse annotations
Lines look like:
""
"b005e5b2-2c0b-11e9-bcad-06f1011196c4,1,Private"
%% Minor cleanup re: images
%% Create annotations
%% Prepare info
%% Minor adjustments to categories
Remove unused categories
Name adjustments
%% Create output
%% Write output
%% Validate .json file
%% Preview labels
viz_options.classes_to_exclude = ['test']
"%% ############ Take two, from pre-iWildCam .json files created from IDFG .csv files ############"
%% Imports and constants
%% One-time line break addition
%% Read input files
%% Prepare info
%% Minor adjustments to categories
%% Minor adjustments to annotations
%% Create output
%% Write output
%% Validate .json file
%% Preview labels
viz_options.classes_to_exclude = ['test']
""
missouri_to_json.py
""
Create .json files from the original source files for the Missouri Camera Traps
data set.  Metadata was provided here in two formats:
""
"1) In one subset of the data, folder names indicated species names.  In Set 1,"
there are no empty sequences.  Set 1 has a metadata file to indicate image-level
bounding boxes.
""
2) A subset of the data (overlapping with (1)) was annotated with bounding
"boxes, specified in a whitespace-delimited text file.  In set 2, there are"
"some sequences omitted from the metadata file, which implied emptiness."
""
"In the end, set 2 labels were not reliable enough to publish, so LILA includes only set 1."
""
%% Constants and imports
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Filenames will be stored in the output .json relative to this base dir
This will be a list of filenames that need re-annotation due to redundant boxes
"%% Enumerate files, read image sizes (both sets)"
"Takes a few minutes, since we're reading image sizes."
Each element will be a list of relative path/full path/width/height
"Only process leaf nodes corresponding to sequences, which look like:"
""
E:\wildlife_data\missouri_camera_traps\Set1\1.02-Agouti\SEQ75583
E:\wildlife_data\missouri_camera_traps\Set2\p1d101
""
assert len(files) <= 2
Read the image
Not an image...
Store file info
"...if we didn't hit the max file limit, keep going"
...for each file
%% Add sequence lengths (both sets)
%% Load the set 1 metadata file
"List of lists, length varies according to number of bounding boxes"
""
Preserves original ordering
iLine = 0; line = metadataSet1Lines[0]
"Lines should be filename, number of bounding boxes, boxes (four values per box)"
Make sure we have image info for this image
%% Print missing files from Set 1 metadata
Manual changes I made to the metadata file:
""
'IMG' --> 'IMG_'
Red_Brocket_Deer --> Red_Deer
European-Hare --> European_Hare
Wood-Mouse --> Wood_Mouse
Coiban-Agouti --> Coiban_Agouti
%% Load the set 2 metadata file
"This metadata file contains most (but not all) images, and a class label (person/animal/empty)"
"for each, plus bounding boxes."
"List of lists, length varies according to number of bounding boxes"
""
Preserves original ordering
"Create class IDs for each *sequence*, which we'll use to attach classes to"
images for which we don't have metadata
""
This only contains mappings for sequences that appear in the metadata.
iLine = 0; line = metadataSet2Lines[0]
"Lines should be filename, number of bounding boxes, labeled boxes (five values per box)"
""
Empty images look like filename\t0\t0
E.g. 'Set2\\p1d101\\p1d101s100i10.JPG'
Make sure we don't have mixed classes within an image
"Figure out what class this *sequence* is, so we know how to handle unlabeled"
images from this sequence
Can't un-do a mixed sequence
Previously-empty sequences get the image class label
"If the sequence has a non-empty class, possibly change it"
Make sure we have image info for this image
...for each line in the set 2 metadata file
%% What Set 2 images do I not have metadata for?
These are *mostly* empty images
iImage = 0; imageID = set2ImageIDs[iImage]
%% Create categories and annotations for set 1
"Though we have no empty sequences, we do have empty images in this set"
For each image
""
iImage = 0; imageID = set1ImageIDs[iImage]
E.g. Set1\\1.80-Coiban_Agouti\\SEQ83155\\SEQ83155_IMG_0010.JPG
Find the species name
This image may still be empty...
category['count'] = category['count'] + 1
"If we have bounding boxes, create image-level annotations"
"filename, number of bounding boxes, boxes (four values per box)"
"Make sure the relative filename matches, allowing for the fact that"
some of the filenames in the metadata aren't quite right
This image is non-empty
"Some redundant bounding boxes crept in, don't add them twice"
Check this bbox against previous bboxes
""
Inefficient?  Yes.  In an important way?  No.
"Bounding box values are in absolute coordinates, with the origin"
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1]."
""
"Convert to floats and to x/y/w/h, as per CCT standard"
...for each box
if we do/don't have boxes for this image
Else create a sequence-level annotation
...for each image
%% Write out the list of images with redundant boxes
%% Create categories and annotations for set 2
For each image
""
iImage = 0; imageID = set2ImageIDs[iImage]
E.g. 'Set2\\p1d100\\p1d100s10i1.JPG'
"Find the sequence ID, sanity check filename against what we stored"
"If we have bounding boxes or an explicit empty label, create image-level annotations"
"filename, number of bounding boxes, labeled boxes (five values per box)"
"Make sure the relative filename matches, allowing for the fact that"
some of the filenames in the metadata aren't quite right
"Bounding box values are in absolute coordinates, with the origin"
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1]."
""
Convert to floats and to x/y/w/h
...for each box
...if we do/don't have boxes for this image
Else create a sequence-level annotation
...for each image
"%% The 'count' field isn't really meaningful, delete it"
"It's really the count of image-level annotations, not total images assigned to a class"
%% Write output .json files
%% Sanity-check final set 1 .json file
"python sanity_check_json_db.py --bCheckImageSizes --baseDir ""E:\wildlife_data\missouri_camera_traps"" ""E:\wildlife_data\missouri_camera_traps\missouri_camera_traps_set1.json"""
%% Generate previews
""
filenames_to_json.py
""
Take a directory of images in which species labels are encoded by folder
"names, and produces a COCO-style .json file"
""
%% Constants and imports
from the ai4eutils repo
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Filenames will be stored in the output .json relative to this base dir
"rawClassListFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_classes.csv')"
"classMappingsFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_class_mapping.csv')"
"%% Enumerate files, read image sizes"
Each element will be a list of relative path/full path/width/height
Read the image
Corrupt or not an image
Store file info
Write to output file
...for each image file
...csv file output
...if the file list is/isn't available
%% Enumerate classes
Maps classes to counts
We like 'empty' to be class 0
%% Assemble dictionaries
...for each category
Each element is a list of relative path/full path/width/height/className
...for each image
%% External class mapping
%% Write raw class table
cat = categories[0]
%% Read the mapped class table
"id, source, count, target"
"%% Make classMappings contain *all* classes, not just remapped classes"
cat = categories[0]
%% Create new class list
"Start at 1, explicitly assign 0 to ""empty"""
One-off issue with character encoding
%% Re-map annotations
ann = annotations[0]
%% Write output .json
%% Utilities
%%
Find images with a particular tag
%% Randomly sample annotations
""
umn_to_json.py
""
Prepare images and metadata for the Orinoquía Camera Traps dataset.
""
%% Imports and constants
"For two deployments, we're only processing imagse in the ""detections"" subfolder"
%% Enumerate deployment folders
%% Load ground truth
i_row = 0; row = ground_truth_df.iloc[i_row]
%% Create relative paths for ground truth data
"Some deployment folders have no subfolders, e.g. this is a valid file name:"
""
M00/01010132.JPG
""
"But some deployment folders have subfolders, e.g. this is also a valid file name:"
""
N17/100EK113/07160020.JPG
""
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly"
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths."
deployment_name = list(deployment_folders)[0]
Enumerate all files in this folder
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))"
filename = files[100]
...for each file in this deployment
...for each deployment
%% Add relative paths to our ground truth table
i_row = 0; row = ground_truth_df.iloc[i_row]
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's"
"just the deployment ID and the image name, separated by a slash."
Find the relative path for this image
Make sure this image file exists
...for each row in the ground truth table
%% Take everything out of Pandas
%% Convert string timestamps to Python datetimes
im = ground_truth_dicts[0]
"Filenames look like, e.g., N36/100EK113/06040726.JPG"
%% Synthesize sequence information
Sort images by time within each location
i_location=0; location = locations[i_location]
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_camera[1]
"Timestamp for this image, may be None"
Start a new sequence if:
""
* This image has no timestamp
* This iamge has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each image in this location
...for each location
%% Create category dict and category IDs
im = images[0]
%% Imports and constants (.json generation)
%% Count frames in each sequence
%% Double check images with multiple annotations
im = images[0]
%% Assemble dictionaries
Force the empty category to be ID 0
input_im = images[0]
...for each image
%% Write output .json
%% Validate .json file
%% Map relative paths to annotation categories
ann = data['annotations'][0]
%% Copy images to output
EXCLUDE HUMAN AND MISSING
im = data['images'][0]
im = images[0]
%% Preview labels
viz_options.classes_to_exclude = ['test']
""
nacti_fieldname_adjustments.py
""
"NACTI metadata was posted with ""filename"" in images instead of ""file_name"", and"
"used string (rather than int) category IDs (in categories, but not in annotations)."
""
This script fixes those issues and rev's the version number.
""
%% Constants and environment
%% Read .json file
"%% Rev version number, update field names and types"
%% Write json file
""
cct_field_adjustments.py
""
CCT metadata was posted with int locations instead of strings.
""
This script fixes those issues and rev's the version number.
""
%% Constants and environment
%% Read .json file
"%% Rev version number, update field names and types"
%% Write json file
%% Check output data file
""
carrizo_shrubfree_2018.py
""
Convert the .csv file provided for the Carrizo Mojave data set to a
COCO-camera-traps .json file
""
%% Constants and environment
%% Read source data
Original .csv file had superfluous spaces in column names
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
58 missing files (of 17652)
%% Check for images that aren't included in the metadata file
3012 of 20606 files are not in metadata
%% Create CCT dictionaries
Map categories to integer IDs
""
The category '0' is reserved for 'empty'
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
Each filename should just match one row
Don't include images that don't exist on disk
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
""
snapshot_serengeti_lila.py
""
Create zipfiles of Snapshot Serengeti S1-S11.
""
"Create a metadata file for S1-S10, plus separate metadata files"
"for S1-S11.  At the time this code was written, S11 was under embargo."
""
Create zip archives of each season without humans.
""
Create a human zip archive.
""
%% Constants and imports
import sys; sys.path.append(r'c:\git\ai4eutils')
import sys; sys.path.append(r'c:\git\cameratraps')
assert(os.path.isdir(metadata_base))
"There are two redundant categories, and we re-map ""blank"" to ""empty"" as per CCT convention"
"%% Load metadata files, concatenate into a single table"
iSeason = 1
%% Convert to dictionaries (prep)
%% Convert to dictionaries (loops)
"TODO: iterrows() is a terrible way to do this, but this is one of those days"
"where I want to get this done, not get better at Python."
irow = 0; row = image_table.iloc[0]
"Loaded as an int64, converting to int here"
...for each row in the image table
Make sure image IDs are what we think they are
...for each row in the annotation table
%% Load previously-saved dictionaries when re-starting mid-script
%%
%% Take a look at categories (just sanity-checking)
%%
%% Fill in some image fields we didn't have when we created the image table
"width, height, corrupt, seq_num_frames, location, datetime"
Every annotation in this list should have the same sequence ID
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;"
there's no reason to do this redundantly for every image
Every image in this sequence should point back to the same equence
Every annotation in this list should have the same location
Every annotation in this list should have the same datetime
Is this image on disk?
iImage = 0; im = images[0]
...for each image
"images_processed = pool.map(process_image, images)"
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))"
%%
%% Count missing/corrupted images
%% Print distribution of sequence lengths (sanity-check)
%% Replicate annotations across images
iAnn = 0; ann = annotations[iAnn]
%% See what files are on disk but not annotated (~15 mins)
247370 files not in the database (of 7425810)
%% Load old image database
%% Look for old images not in the new DB and vice-versa
"At the time this was written, ""old"" was S1-S6"
old_im = cct_old['images'][0]
new_im = images[0]
4 old images not in new db
12 new images not in old db
%% Save our work
%% Load our work
%%
%% Examine size mismatches
i_mismatch = -1; old_im = size_mismatches[i_mismatch]
%% Sanity-check image and annotation uniqueness
"%% Split data by seasons, create master list for public seasons"
ann = annotations[0]
%% Minor updates to fields
"%% Write master .json out for S1-10, write individual season .jsons (including S11)"
"Loop over all seasons, plus one iteration for the ""all public data"" iteration, and"
"one for the ""all data"" iteration"
%% Find categories that only exist in S11
List of categories in each season
Category 55 (fire) only in S11
Category 56 (hyenabrown) only in S11
Category 57 (wilddog) only in S11
Category 58 (kudu) only in S11
Category 59 (pangolin) only in S11
Category 60 (lioncub) only in S11
%% Prepare season-specific .csv files
iSeason = 1
%% Create a list of human files
ann = annotations[0]
%% Save our work
%% Load our work
%%
"%% Create archives (human, per-season) (prep)"
im = images[0]
im = images[0]
Don't include humans
Only include files from this season
Possibly start a new archive
...for each image
i_season = 0
"for i_season in range(0,nSeasons):"
create_season_archive(i_season)
%% Create archives (loop)
pool = ThreadPool(nSeasons+1)
"n_images = pool.map(create_archive, range(-1,nSeasons))"
"seasons_to_zip = range(-1,nSeasons)"
...for each season
%% Sanity-check .json files
%logstart -o r'E:\snapshot_temp\python.txt'
%% Zip up .json and .csv files
pool = ThreadPool(len(files_to_zip))
"pool.map(zip_single_file, files_to_zip)"
%% Super-sanity-check that S11 info isn't leaking
im = data_public['images'][0]
ann = data_public['annotations'][0]
iRow = 0; row = annotation_df.iloc[iRow]
iRow = 0; row = image_df.iloc[iRow]
%% Create bounding box archive
i_image = 0; im = data['images'][0]
i_box = 0; boxann = bbox_data['annotations'][0]
%% Sanity-check a few files to make sure bounding boxes are still sensible
import sys; sys.path.append(r'C:\git\CameraTraps')
%% Check categories
%% Summary prep for LILA
""
wi_to_json
""
Prepares CCT-formatted metadata based on a Wildlife Insights data export.
""
"Mostly assumes you have the images also, for validation/QA."
""
%% Imports and constants
"Because WI filenames are GUIDs, it's not practical to page through sequences in an"
image viewer.  So we're going to (optionally) create a copy of the data set where
images are ordered.
%% Load ground truth
%% Take everything out of Pandas
%% Synthesize common names when they're not available
"Blank rows should always have ""Blank"" as the common name"
"print('Warning: missing common name for row {} ({})'.format(i_row,row['filename']))"
%% Convert string timestamps to Python datetimes
im = ground_truth_dicts[0]
"The field called ""location"" in the WI .csv file is a URL, we want to reclaim"
"the ""location"" keyword for CCT output"
"Filenames look like, e.g., N36/100EK113/06040726.JPG"
%% Synthesize sequence information
Sort images by time within each location
i_location=0; location = locations[i_location]
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_camera[1]
"Timestamp for this image, may be None"
Start a new sequence if:
""
* This image has no timestamp
* This iamge has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each image in this location
...for each location
%% Create category dict and category IDs
im = images[0]
%% Count frames in each sequence
%% Build relative paths
im = images[0]
Sample URL:
""
gs://project-asfasdfd/deployment/21444549/asdfasdfd-616a-4d10-a921-45ac456c568a.jpg'
%% Double check images with multiple annotations
im = images[0]
%% Assemble dictionaries
Force the empty category to be ID 0
input_im = images[0]
"print('Warning: image ID {} ({}) has multiple annotations'.format(im['id'],im['id'].replace('_','/')))"
...for each image
%% Write output .json
%% Validate .json file
%% Preview labels
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))"
%%
%% Create ordered dataset
"Because WI filenames are GUIDs, it's not practical to page through sequences in an"
image viewer.  So we're going to create a copy of the data set where images are
ordered.
im = images_out[0]; im
%% Create ordered .json
%% Copy files to their new locations
im = ordered_images[0]
im = data_ordered['images'][0]
%% Preview labels in the ordered dataset
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))"
%% Open an ordered filename from the unordered filename
""
rspb_to_json.py
""
Convert the .csv file provided for the RSPB data set to a
COCO-camera-traps .json file
""
%% Constants and environment
[location] is an obfuscation
%% Create info struct
%% Read source data
metadataTable.columns.values
""
"array(['Project', 'inDir', 'FileName', 'Station', 'Camera',"
"'StationCameraFileName', 'DateTimeOriginal', 'DateReadable',"
"'outDir', 'filename_new', 'fileExistsAlready', 'CopyStatus',"
"'Species'], dtype=object)"
We'll populate these later
keys should be lowercase
"%% Enumerate images, confirm filename uniqueness"
"%% Update metadata filenames to include site and camera folders, check existence"
""
Takes ~1min
iRow = 0; row = metadataTable.iloc[iRow]
There's a bug in the metadata; the 'camera' column isn't correct.
camera = row['Camera']
"These appear as, e.g., '3.22e12'"
camera = str(int(float(camera)))
Let's pull this out of the file name instead
""
Filenames look like one of the following:
""
A1__03224850850507__2015-11-28__10-45-04(1).JPG
Bayama2PH__C05__NA(NA).JPG
assert(os.path.isfile(fullPath))
metadataTable.iloc[iRow] = row
Re-assemble into an updated table
%% Check for images that aren't included in the metadata file
Enumerate all images
Write to a text file
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
iRow = 0; row = metadataTable.iloc[iRow]
A1__03224850850507__2015-11-28__10-45-04(1).JPG
'A1\\03224850850507\\A1__03224850850507__2015-11-28__10-45-04(1).JPG'
Not currently populated
"Often -1, sometimes a semi-meaningful int"
A1
03224850850507
"In variable form, but sometimes '28/11/2015 10:45'"
Check image height and width
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Write output
%% Check database integrity
%% Preview a few images to make sure labels were passed along sensibly
%% One-time processing step: copy images to a flat directory for annotation
%%
""
pc_to_json.py
""
Convert a particular collection of .csv files to CCT format.
""
%% Constants and environment
%% Read and concatenate source data
List files
"List of dataframes, one per .csv file; we'll concatenate later"
i_file = 87; fn = input_files[i_file]
Concatenate into a giant data frame
%% List files
%% Main loop over labels (prep)
Force the empty category to be ID 0
%% Main loop over labels (loop)
iRow = 0; row = input_metadata.iloc[iRow]
"ImageID,FileName,FilePath,SpeciesID,CommonName"
assert os.path.isfile(full_path)
Retrieve image width and height
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
%% See what's up with missing files
s = list(image_relative_paths)[0]
s = missing_files[0]
%% Check for images that aren't included in the metadata file
%% Sample the database
%%
Collect the images we want
%% Create info struct
%% Write output
%% Sanity-check the database's integrity
%% Render a bunch of images to make sure the labels got carried along correctly
options.classes_to_exclude = ['unlabeled']
%% Write out a list of files to annotate
""
save_the_elephants_survey_B.py
""
Convert the .csv file provided for the Save the Elephants Survey B data set to a
COCO-camera-traps .json file
""
%% Constants and environment
input_base = r'/mnt/blobfuse/wildlifeblobssc/ste_2019_08_drop'
output_base = r'/home/gramener/survey_b'
Handle all unstructured fields in the source data as extra fields in the annotations
"photo_type really should be an image property, but there are a few conflicts"
that forced me to handle it as an annotation proprerty
%% Read source data
"%% Map filenames to rows, verify image existence"
"%% Map filenames to rows, verify image existence"
Maps relative paths to row indices in input_metadata
"Build up a map from filenames to a list of rows, checking image existence as we go"
Ignore directories
%% Make sure the multiple-annotation cases make sense
%%
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
Force the empty category to be ID 0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image]
Example filename:
""
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG'
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG'
i_row = row_indices[0]
timestamp = row['Date']
There are a small handful of datetime mismatches across annotations
for the same image
assert im['datetime'] == timestamp
Special cases based on the 'photo type' field
Various spellings of 'community'
Have we seen this category before?
Create an annotation
fieldname = list(mapped_fields.keys())[0]
...for each row
...for each image
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
%% Scrap
%% Find unique photo types
""
ubc_to_json.py
""
Convert the .csv file provided for the UBC data set to a
COCO-camera-traps .json file
""
"Images were provided in eight folders, each of which contained a .csv"
file with annotations.  Those annotations came in two slightly different
"formats, the two formats corresponding to folders starting with ""SC_"" and"
otherwise.
""
%% Constants and environment
Map Excel column names - which vary a little across spreadsheets - to a common set of names
%% Enumerate images
Load from file if we've already enumerated
%% Create CCT dictionaries
Force the empty category to be ID 0
To simplify debugging of the loop below
#%% Create CCT dictionaries (loop)
#%%
Read source data for this folder
Rename columns
Folder name is the first two characters of the filename
""
Create relative path names from the filename itself
Folder name is the camera name
""
Create relative path names from camera name and filename
Which of our images are in the spreadsheet?
i_row = 0; fn = input_metadata['image_relative_path'][i_row]
#%% Check for images that aren't included in the metadata file
Find all the images in this folder
Which of these aren't in the spreadsheet?
#%% Create entries in CCT dictionaries
Only process images we have on disk
"This is redundant, but doing this for clarity, at basically no performance"
cost since we need to *read* the images below to check validity.
i_row = row_indices[0]
"These generally represent zero-byte images in this data set, don't try"
to find the very small handful that might be other kinds of failures we
might want to keep around.
print('Error opening image {}'.format(image_relative_path))
If we've seen this category before...
...make sure it used the same latin --> common mapping
""
"If the previous instance had no mapping, use the new one."
assert common_name == category['common_name']
Create an annotation
...for each annotation we found for this image
...for each image
...for each dataset
Print all of our species mappings
"%% Copy images for which we actually have annotations to a new folder, lowercase everything"
im = images[0]
%% Create info struct
"%% Convert image IDs to lowercase in annotations, tag as sequence level"
"While there isn't any sequence information, the nature of false positives"
"here leads me to believe the images were labeled at the sequence level, so"
we should trust labels more when positives are verified.  Overall false
positive rate looks to be between 1% and 5%.
%% Write output
%% Validate output
%% Preview labels
""
helena_to_cct.py
""
Convert the Helena Detections data set to a COCO-camera-traps .json file
""
%% Constants and environment
This is one time process
%% Create Filenames and timestamps mapping CSV
import pdb;pdb.set_trace()
%% To create CCT JSON for RSPB dataset
%% Read source data
Original Excel file had timestamp in different columns
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
%% Skipping this check because one image has multiple species
assert len(duplicate_rows) == 0
%% Check for images that aren't included in the metadata file
%% Create CCT dictionaries
Map categories to integer IDs
""
The category '0' is reserved for 'empty'
For each image
Don't include images that don't exist on disk
Some filenames will match to multiple rows
assert(len(rows) == 1)
iRow = rows[0]
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
ann['datetime'] = row['datetime']
ann['site'] = row['site']
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
%% Imports and constants
from github.com/microsoft/ai4eutils
from github.com/ecologize/CameraTraps
A list of files in the lilablobssc container for this data set
The raw detection files provided by NOAA
A version of the above with filename columns added
%% Read input .csv
%% Read list of files
%% Convert paths to full paths
i_row = 0; row = df.iloc[i_row]
assert ir_image_path in all_files
...for each row
%% Write results
"%% Load output file, just to be sure"
%% Render annotations on an image
i_image = 2004
%% Download the image
%% Find all the rows (detections) associated with this image
"as l,r,t,b"
%% Render the detections on the image(s)
In pixel coordinates
In pixel coordinates
%% Save images
%% Clean up
""
carrizo_trail_cam_2017.py
""
"Convert the .csv files provided for the ""Trail Cam Carrizo"" 2017 data set to"
a COCO-camera-traps .json file.
""
%% Constants and environment
%% Read source data
Removing the empty records
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
908 missing files (of 60562)
%% Check for images that aren't included in the metadata file
105329 of 164983 files are not in metadata
%% Create CCT dictionaries
Map categories to integer IDs
""
The category '0' is reserved for 'empty'
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
Each filename should just match one row
Don't include images that don't exist on disk
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
""
channel_islands_to_cct.py
""
Convert the Channel Islands data set to a COCO-camera-traps .json file
""
"Uses the command-line tool ExifTool (exiftool.org) to pull EXIF tags from images,"
"because every Python package we tried failed to pull the ""Maker Notes"" field properly."
""
"%% Imports, constants, paths"
# Imports ##
# Constants ##
# Paths ##
Confirm that exiftool is available
"assert which(exiftool_command_name) is not None, 'Could not locate the ExifTool executable'"
%% Load information from every .json file
"Ignore the sample file... actually, first make sure there is a sample file"
...and now ignore that sample file.
json_file = json_files[0]
ann = annotations[0]
...for each annotation in this file
...for each .json file
"%% Confirm URL uniqueness, handle redundant tags"
Have we already added this image?
"One .json file was basically duplicated, but as:"
""
Ellie_2016-2017 SC12.json
Ellie_2016-2017-SC12.json
"If the new image has no output, just leave the old one there"
"If the old image has no output, and the new one has output, default to the one with output"
Don't worry about the cases where someone tagged 'fox' and someone tagged 'fox_partial'
...for each image we've already added
...if this URL is/isn't in the list of URLs we've already processed
...for each image
%% Save progress
%%
%%
%% Download files (functions)
https://www.quickprogrammingtips.com/python/how-to-download-multiple-files-concurrently-in-python.html
"This is returned with a leading slash, remove it"
%% Download files (execution)
%% Read required fields from EXIF data (functions)
"-G means ""Print group name for each tag"", e.g. print:"
""
[File]          Bits Per Sample                 : 8
""
...instead of:
""
Bits Per Sample                 : 8
"If we don't get any EXIF information, this probably isn't an image"
line_raw = exif_lines[0]
"Split on the first occurrence of "":"""
Typically:
""
"'[MakerNotes]    Sequence                        ', '1 of 3']"
Not a typo; we are using serial number as a location
"If there are multiple timestamps, make sure they're *almost* the same"
"If there are multiple timestamps, make sure they're *almost* the same"
...for each line in the exiftool output
"This isn't directly related to the lack of maker notes, but it happens that files that are missing"
maker notes also happen to be missing EXIF date information
...process_exif()
"This is returned with a leading slash, remove it"
Ignore non-image files
%% Read EXIF data (execution)
ann = images[0]
%% Save progress
Use default=str to handle datetime objects
%%
%%
"Not deserializing datetimes yet, will do this if I actually need to run this"
%% Check for EXIF read errors
%% Remove junk
Ignore non-image files
%% Fill in some None values
"...so we can sort by datetime later, and let None's be sorted arbitrarily"
%% Find unique locations
%% Synthesize sequence information
Sort images by time within each location
i_location=0; location = locations[i_location]
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_camera[1]
"Timestamp for this image, may be None"
Start a new sequence if:
""
* This image has no timestamp
* This image has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each image in this location
...for each location
%% Count frames in each sequence
images_this_sequence = [im for im in images if im['seq_id'] == seq_id]
"%% Create output filenames for each image, store original filenames"
i_location = 0; location = locations[i_location]
i_image = 0; im = sorted_images_this_location[i_image]
%% Save progress
Use default=str to handle datetime objects
%%
%%
%% Copy images to their output files (functions)
%% Copy images to output files (execution)
%% Rename the main image list for consistency with other scripts
%% Create CCT dictionaries
Make sure this is really a box
Transform to CCT format
Force the empty category to be ID 0
i_image = 0; input_im = all_image_info[0]
"This issue only impacted one image that wasn't a real image, it was just a screenshot"
"showing ""no images available for this camera"""
Convert datetime if necessary
Process temperature if available
Read width and height if necessary
I don't know what this field is; confirming that it's always None
Process object and bbox
os.startfile(output_image_full_path)
"Zero is hard-coded as the empty category, but check to be safe"
"I can't figure out the 'index' field, but I'm not losing sleep about it"
assert input_annotation['index'] == 1+i_ann
"Some annotators (but not all) included ""_partial"" when animals were partially obscured"
"Annotators *mostly* used 'none', but sometimes 'empty'.  'empty' is CCT-correct."
If we've seen this category before...
If this is a new category...
...if this is an empty/non-empty annotation
Create an annotation
...for each annotation on this image
...for each image
%% Change *two* annotations on images that I discovered contains a human after running MDv4
%% Move human images
ann = annotations[0]
%% Count images by location
%% Write output
%% Validate output
%% Preview labels
viz_options.classes_to_exclude = [0]
""
ena24_to_json_2017.py
""
Convert the ENA24 data set to a COCO-camera-traps .json file
""
%% Constants and environment
Temporary folders for human and non-human images
Clean existing output folders/zipfiles
%% Support functions
%% Read source data
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
Check image height and width
"Each row is category, [box coordinates]"
"If there's just one row, loadtxt reads it as a 1d array; make it a 2d array"
with one row
Each row is a bounding box
Have we seen this category before?
Create an annotation
...for each bounding box
"This was here for debugging; nearly every instance is Human+Horse, Human+Vehicle,"
"or Human+Dog, but there is one Rabbit+Opossium, and a few Deer+Chicken!"
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Create ZIP files for human and non human
%% Validate output
%% Preview labels
%% Imports and constants
configurations and paths
%% Helper functions
"dest_path = copy(source_path, dest_folder)"
num_workers = multiprocessing.cpu_count()
pool = ThreadPool(num_workers)
"results = pool.starmap(_copy_unzip, zip(sources, itertools.repeat(dest_folder)))"
""
print('Waiting for processes to finish...')
pool.close()
pool.join()
sequential
%% Command-line driver
if the blob container is already mounted on the VM
or you can download them using the storage Python SDK
store storage account key in environment variable AZ_STORAGE_KEY
""
eMammal_helpers.py
""
Support functions for processing eMammal metadata
""
%% Constants and imports
%% Support functions
"pad to a total of 3 digits if < 1000, or 4 digits otherwise"
img_frame is a string from the xml tree
"length 4 frame order is returned as is, others are left padded to be 3 digit long"
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels
pad to a total of 4 digits
img_frame is a string from the xml tree
"length 4 frame order is returned as is, others are left padded to be 3 digit long"
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels
others column
summer day hours: 6am - 7pm
others day hours: 7am - 6pm
""
make_eMammal_json.py
""
"Produces the COCO-formatted json database for an eMammal dataset, i.e. a"
"collection of folders, each of which contains a deployment_manifest.xml file."
""
"In this process, each image needs to be loaded to size it."
""
"To add bounding box annotations to the resulting database, use"
add_annotations_to_eMammal_json.py.
""
%% Constants and imports
"Either add the eMammal directory to your path, or run from there"
os.chdir(r'd:\git\CameraTraps\database_tools\eMammal')
import warnings
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
warnings.filterwarnings('ignore')
Should we run the image size retrieval in parallel?
%% Support functions
%% Main loop (metadata processing; image sizes are retrieved later)
deployment = folders[0]
sequence = image_sequences[0]
get species info for this sequence
add each image's info to database
img = images[0]
"some manifests don't have the ImageOrder info, but the info is in the file name"
full_img_id has no frame info
""
frame number only used in requests to iMerit for ordering
...for each image
...for each sequence
...for each deployment
%% Get image sizes
"'tasks' is currently a list of 2-tuples, with each entry as [image dictionary,path]."
""
"Go through that and copy just the image dictionaries to 'db_images', adding size"
information to each entry.  Takes a couple hours.
opening each image seems too fast for this multi-threaded version to be faster than sequential code.
%% Assemble top-level dictionaries
%% Write out .json
""
make_full_SS_json.py
""
Create a COCO-camera-traps .json file for Snapshot Serengeti data from
the original .csv files provided on Dryad.
""
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no"
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated
Snapshot Safari preparation process.
""
%% Imports and constants
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays"
Count the number of images with multiple species
"%% Read image .csv file, format into a dictionary mapping images to capture events"
%% Create CCT-style .json
still need image width and height
...for each image
%% Write output files
""
make_per_season_SS_json.py
""
Create a COCO-camera-traps .json file for each Snapshot Serengeti season from
the original .csv files provided on Dryad.
""
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no"
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated
Snapshot Safari preparation process.
""
%% Imports and constants
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays"
"%% Read image .csv file, format into a dictionary mapping images to capture events"
%% Create CCT-style .json
still need image width and height
...for each image ID
%% Write output files
...for each season
""
generate_lila_per_image_labels.py
""
"Generate a .csv file with one row per annotation, containing full URLs to every"
"camera trap image on LILA, with taxonomically expanded labels."
""
"Typically there will be one row per image, though images with multiple annotations"
will have multiple rows.
""
"Some images may not physically exist, particularly images that are labeled as ""human""."
This script does not validate image URLs.
""
Does not include bounding box annotations.
""
%% Constants and imports
"We'll write images, metadata downloads, and temporary files here"
"Some datasets don't have ""sequence_level_annotation"" fields populated, but we know their"
annotation level
%% Download and parse the metadata file
To select an individual data set for debugging
%% Download and extract metadata for the datasets we're interested in
%% Load taxonomy data
"%% Build a dictionary that maps each [dataset,query] pair to the full taxonomic label set"
i_row = 0; row = taxonomy_df.iloc[i_row]
%% Process annotations for each dataset
ds_name = list(metadata_table.keys())[0]
"Go through annotations, marking each image with the categories that are present"
""
ann = annotations[0]
im = images[10]
This field name was only used for Caltech Camera Traps
raise ValueError('Suspicious date parsing result')
Special case we don't want to print a warning about
"Location, sequence, and image IDs are only guaranteed to be unique within"
"a dataset, so for the output .csv file, include both"
category_name = list(categories_this_image)[0]
Only print a warning the first time we see an unmapped label
...for each category that was applied at least once to this image
...for each image in this dataset
print('Warning: no date information available for this dataset')
print('Warning: no location information available for this dataset')
...for each dataset
...with open()
%% Read the .csv back
%% Do some post-hoc integrity checking
"-1 isn't *really* valid, but we use it sometimes for sequences of unknown length"
i_row = 0; row = df.iloc[i_row]
%% Preview constants
%% Choose images to download
ds_name = list(metadata_table.keys())[2]
Find all rows for this dataset
...for each dataset
%% Download images
i_image = 0; image = images_to_download[i_image]
%% Write preview HTML
im = images_to_download[0]
""
Common constants and functions related to LILA data management/retrieval.
""
%% Imports and constants
LILA camera trap master metadata file
"wildlife_insights_taxonomy_url = 'https://api.wildlifeinsights.org/api/v1/taxonomy?fields=class,order,family,genus,species,authority,taxonomyType,uniqueIdentifier,commonNameEnglish&page[size]={}'.format(wildlife_insights_page_size)"
from ai4eutils
%% Common functions
"We haven't implemented paging, make sure that's not an issue"
d['data'] is a list of items that look like:
Put the master metadata file in the same folder where we're putting images
Read lines from the master metadata file
Parse those lines into a table
Each line in this file is name/sas_url/json_url/[bbox_json_url]
Create a separate entry for bounding boxes if they exist
Unzip if necessary
""
get_lila_category_list.py
""
Generates a .json-formatted dictionary mapping each LILA dataset to all categories
"that exist for that dataset, with counts for the number of occurrences of each category"
"(the number of *annotations* for each category, not the number of *images*)."
""
"Also loads the taxonomy mapping file, to include scientific names for each category."
""
get_lila_category_counts counts the number of *images* for each category in each dataset.
""
%% Constants and imports
array to fill for output
"We'll write images, metadata downloads, and temporary files here"
Created by get_lila_category_list.py... contains counts for each category
%% Load category and taxonomy files
%% Map dataset names and category names to scientific names
i_row = 1; row = taxonomy_df.iloc[i_row]; row
%% Download and parse the metadata file
%% Download and extract metadata for the datasets we're interested in
%% Get category names and counts for each dataset
ds_name = 'NACTI'
Open the metadata file
Collect list of categories and mappings to category name
ann = annotations[0]
c = categories[0]
"Don't do taxonomy mapping for bbox data sets, which are sometimes just binary and are"
always redundant with the class-level data sets.
"As of right now, this is the only quirky case"
...for each dataset
%% Save dict
%% Print the results
ds_name = list(dataset_to_categories.keys())[0]
...for each dataset
""
download_lila_subset.py
""
"Example of how to download a list of files from LILA, e.g. all the files"
in a data set corresponding to a particular species.
""
%% Constants and imports
LILA camera trap master metadata file
"In this example, we're using the Missouri Camera Traps data set and the Caltech Camera Traps dataset"
All lower-case; we'll convert category names to lower-case when comparing
"We'll write images, metadata downloads, and temporary files here"
"We will demonstrate two approaches to downloading, one that loops over files"
"and downloads directly in Python, another that uses AzCopy."
""
AzCopy will generally be more performant and supports resuming if the
transfers are interrupted.  This script assumes that azcopy is on the system path.
Number of concurrent download threads (when not using AzCopy) (AzCopy does its
own magical parallelism)
%% Support functions
remove the leading '/'
%% Download and parse the metadata file
Put the master metadata file in the same folder where we're putting images
Read lines from the master metadata file
Parse those lines into a table
Each line in this file is name/base_url/json_url/[box_url]
%% Download and extract metadata for the datasets we're interested in
Unzip if necessary
...for each dataset of interest
%% List of files we're going to download (for all data sets)
"Flat list or URLS, for use with direct Python downloads"
For use with azcopy
This may or may not be a SAS URL
# Open the metadata file
# Build a list of image files (relative path names) that match the target species
Retrieve all the images that match that category
Retrieve image file names
Convert to URLs
...for each dataset
%% Download those image files
ds_name = 'Caltech Camera Traps'
ds_name = 'SWG Camera Traps'
We want to use the whole relative path for this script (relative to the base of the container)
"to build the output filename, to make sure that different data sets end up in different folders."
This may or may not be a SAS URL
For example:
""
caltech-unzipped/cct_images
swg-camera-traps
Check whether the URL includes a folder
E.g. caltech-unzipped
E.g. cct_images
E.g. swg-camera-traps
"Write out a list of files, and use the azcopy ""list-of-files"" option to download those files."
""
This azcopy feature is unofficially documented at:
""
https://github.com/Azure/azure-storage-azcopy/wiki/Listing-specific-files-to-transfer
""
import clipboard; clipboard.copy(cmd)
Loop over files
""
get_lila_category_counts.py
""
Count the number of images and bounding boxes with each label in one or more LILA datasets.
""
"This script doesn't write these counts out anywhere other than the console, it's just intended"
as a template for doing operations like this on LILA data.  get_lila_category_list.py writes
"information out to a .json file, but it counts *annotations*, not *images*, for each category."
""
%% Constants and imports
"If None, will use all datasets"
"We'll write images, metadata downloads, and temporary files here"
%% Download and parse the metadata file
%% Download and extract metadata for the datasets we're interested in
%% Count categories
ds_name = datasets_of_interest[0]
"Go through annotations, marking each image with the categories that are present"
""
ann = annotations[0]
Now go through images and count categories
im = images[0]
...for each dataset
%% Print the results
...for each dataset
""
add_url_to_database.py
""
"Adds a ""url"" field to a coco-camera-traps .json database, specifically to allow the db to"
be reviewed in the Visipedia annotation tool.
""
""
remove_corrupted_images_from_database.py
""
"Given a coco-camera-traps .json file, checks all images for TF-friendliness and generates"
a new .json file that only contains the non-corrupted images.
""
%% Imports and constants
import multiprocessing
"I leave this at an annoying low number, since by definition weird stuff will"
"be happening in the TF kernel, and it's useful to keep having content in the console."
%% Function definitions
"I sometimes pass in a list of images, sometimes a dict with a single"
element mapping a job ID to the list of images
"We're about to start a lot of TF sessions, and we don't want gobs"
of debugging information printing out for every session.
At some point we were creating a single session and looping over images
"within that session, but the only way I found to reliably not run out"
of GPU memory was to create a session per image and gc.collect() after
each session.
Map Image IDs to boolean (should I keep this image?)
"Convert to lists, append job numbers to the image lists"
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)"
Merge results
%% Interactive driver
%%
base_dir = r'D:\temp\snapshot_serengeti_tfrecord_generation'
Load annotations
Check for corruption
Write out only the uncorrupted data
%% Command-line driver
#######
""
integrity_check_json_db.py
""
"Does some integrity-checking and computes basic statistics on a db, specifically:"
""
* Verifies that required fields are present and have the right types
* Verifies that annotations refer to valid images
* Verifies that annotations refer to valid categories
"* Verifies that image, category, and annotation IDs are unique"
""
* Optionally checks file existence
""
* Finds un-annotated images
* Finds unused categories
""
* Prints a list of categories sorted by count
""
#######
%% Constants and environment
%% Functions
"If baseDir is non-empty, checks image existence"
This is used in a medium-hacky way to share modified options across threads
print('Image path {} does not exist'.format(filePath))
"#%% Read .json file if necessary, integrity-check fields"
info = data['info']
"#%% Build dictionaries, checking ID uniqueness and internal validity as we go"
Confirm that required fields are present
Confirm ID uniqueness
...for each category
image = images[0]
Confirm that required fields are present
Confirm ID uniqueness
We previously supported ints here; this should be strings now
"assert isinstance(image['location'], str) or isinstance(image['location'], int), \"
'Illegal image location type'
Are we checking for unused images?
Recursively enumerate images
print('Image {} is unused'.format(p))
Are we checking file existence and/or image size?
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)"
...for each image
Confirm that required fields are present
Confirm ID uniqueness
Confirm validity
...for each annotation
#%% Print statistics
Find un-annotated images and multi-annotation images
Find unused categories
Prints a list of categories sorted by count
""
https://stackoverflow.com/questions/72899/how-do-i-sort-a-list-of-dictionaries-by-a-value-of-the-dictionary
...def integrity_check_json_db()
%% Command-line driver
%% Interactive driver(s)
%%
Integrity-check .json files for LILA
options.iMaxNumImages = 10
""
combine_two_json_files.py
""
"Merges two coco-camera-traps .json files. In particular, categories are combined and re-numbered."
""
Combined Info
Combined Images
Combined Categories
## categories to merge
Combined Annotations
""
make_detection_db_for_viewing.py
""
"Given a .json file with ground truth bounding boxes, and a .p file containing detections for the same images,"
"creates a new .json file with separate classes for ground truth and detection, suitable for viewing in the Visipedia"
annotation tool.
""
%% Imports and constants
%% Main function
im_id_to_im = {im['id']:im for im in images}
make new categories to distinguish between ground truth and detections
"update all gt annotations to be class ""gt"""
collect all detections by image
keep any detection with score above det_thresh
"need to convert bbox from [x1,y1,x2,y2] to [x,y,w,h]"
"add ""info"" and ""licenses"" for annotation tools to function"
create new db
%% Command-line handling
""
analyze_json_database.py
""
Plots location/class/etc. distributions for classes in a coco-camera-traps .json file.
""
Currently includes some one-off code for specific species.
""
%% Constants and imports
%% Path configuration
%% Load source data
%% Build image/category dictionaries
%% Make plot of category distribution
%% make plots of location distribution
"plt.title('Number of images per location, by category')"
"plt.tight_layout(rect=[0,0,1,0.9])"
#make plot of images per season
%% Make plot of lions per location
%% Make plot of elephants per location
for loc in sorted_by_total[:25]:
"print('Location:' + loc[0] +', Lions: ' + str(len(loc_to_lion_ims[loc[0]])) + ', Elephants: ' + str(len(loc_to_elephant_ims[loc[0]])) + ', Total ims: ' + str(len(loc_to_ims[loc[0]])))"
""
subset_json_db.py
""
Select a subset of images (and associated annotations) from a .json file
in COCO Camera Traps format.
""
"To subset the .json files produced by our batch processing API, see"
subset_json_detector_output.py
""
Sample invocation:
""
"python subset_json_db.py ""E:\Statewide_wolf_container\idfg_20190409.json"" ""E:\Statewide_wolf_container\idfg_20190409_clearcreek.json"" ""clearcreek"" --ignore_case"
""
%% Constants and imports
%% Functions
Load the input file if necessary
Find images matching the query
Find annotations referring to those images
Write the output file if requested
%% Interactive driver
%%
%% Command-line driver
%% Constants and imports
%% Merge functions
i_input_dict = 0; input_dict = input_dicts[i_input_dict]
We will prepend an index to every ID to guarantee uniqueness
Map detection categories from the original data set into the merged data set
...for each category
Merge original image list into the merged data set
Create a unique ID
...for each image
Same for annotations
...for each annotation
"Merge info dicts, don't check completion time fields"
...for each dictionary
Convert merged image dictionaries to a sorted list
%% Driver
""
add_width_and_height_to_database.py
""
Grabs width and height from actual image files for a .json database that is missing w/h.
""
Originally used when we created a .json file for snapshot serengeti from .csv.
""
"This file converts the JSON output of the batch processing API to a pickle file, which can be used by the"
script ./make_classification_dataset.py
"Convert boxes from JSON   [x_min, y_min, width_of_box, height_of_box]"
"to PICKLE [ymin,  xmin,  ymax,         xmax]"
Write detections to file with pickle
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the"
script ./make_classification_dataset.py
Write detections to file with pickle
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the"
script ./make_classification_dataset.py
Assumes that the root of the CameraTrap repo is in the PYTHONPATH
Minimum threshold to put a detection into the output JSON file
Parameter check
Load detections from input
Load COCO style annotations
Build output JSON in format version 1.0
Adding the only known metadata info
"The pickle file does not contain category information, so we assume the default"
For each image with detections
for each detection
"Convert boxes from [ymin, xmin, ymax, xmax] format to"
"[x_min, y_min, width_of_box, height_of_box]"
Write output json
global flag for whether or not we encounter missing images
"- will only print ""missing image"" warning once"
TFRecords variables
1.3 for the cropping during test time and 1.3 for the context that the
CNN requires in the left-over image
Create output directories
Load COCO style annotations from the input dataset
"Get all categories, their names, and create updated ID for the json file"
Prepare the coco-style json files
Split the dataset by locations
Load detections
Write out COCO-style json files to the output directory
Write detections to file with pickle
## Preparations: get all the output tensors
For all images listed in the annotations file
Skip the image if it is annotated with more than one category
"Get ""old"" and ""new"" category ID and category name for this image."
Skip if in excluded categories.
get path to image
"If we already have detection results, we can use them"
Otherwise run detector and add detections to the collection
Only select detections with confidence larger than DETECTION_THRESHOLD
Skip image if no detection selected
whether it belongs to a training or testing location
Skip images that we do not have available right now
- this is useful for processing parts of large datasets
Load image
Run inference
"remove batch dimension, and convert from float32 to appropriate type"
convert normalized bbox coordinates to pixel coordinates
Pad the detected animal to a square box and additionally by
"PADDING_FACTOR. The result will be in crop_boxes. However, we need to make"
sure that its box coordinates are still within the image.
"for each bounding box, crop the image to the padded box and save it"
"Create the file path as it will appear in the annotation json,"
adding the box number if there are multiple boxes
"if the cropped file already exists, verify its size"
Add annotations to the appropriate json
js_keys = ['/'.join(im['file_name'].split('/')[1:])[:-4] for im in js['images']]
for tk in js_keys:
"assert np.isclose(1, np.sum(detections[tk]['detection_scores'] > 0.5))"
""
separate_detections_by_size
""
Not-super-well-maintained script to break a list of API output files up
based on bounding box size.
""
%% Imports and constants
Folder with one or more .json files in it that we want to split up
Enumerate .json files
Define size thresholds and confidence thresholds
"Not used directly in this script, but useful if we want to generate previews"
%% Split by size
For each size threshold...
For each file...
fn = input_files[0]
Just double-checking; we already filtered this out above
Don't reprocess .json files we generated with this script
Load the input file
For each image...
1.1 is the same as infinity here; no box can be bigger than a whole image
What's the smallest detection above threshold?
"[x_min, y_min, width_of_box, height_of_box]"
""
size = w * h
...for each detection
Which list do we put this image on?
...for each image in this file
Make sure the number of images adds up
Write out all files
...for each size threshold
...for each file
""
tile_images.py
""
Split a folder of images into tiles.  Preserves relative folder structure in a
"new output folder, with a/b/c/d.jpg becoming, e.g.:"
""
a/b/c/d_row_0_col_0.jpg
a/b/c/d_row_0_col_1.jpg
""
%% Imports and constants
from ai4eutils
%% Main function
TODO: parallelization
""
i_fn = 2; relative_fn = image_relative_paths[i_fn]
Can we skip this image because we've already generated all the tiles?
TODO: super-sloppy that I'm pasting this code from below
From:
""
https://github.com/whiplashoo/split-image/blob/main/src/split_image/split.py
i_col = 0; i_row = 1
left/top/right/bottom
...for each row
...for each column
...for each image
%% Interactive driver
%%
%%
""
rde_debug.py
""
Some useful cells for comparing the outputs of the repeat detection
"elimination process, specifically to make sure that after optimizations,"
results are the same up to ordering.
""
%% Compare two RDE files
i_dir = 0
break
"Regardless of ordering within a directory, we should have the same"
number of unique detections
Re-sort
Make sure that we have the same number of instances for each detection
Make sure the box values match
""
aggregate_video.py
""
Aggregate results and render output video for a video that's already been run through MD
""
%% Constants
%% Processing
im = d['images'][0]
...for each detection
This is no longer included in output files by default
# Split into frames
# Render output video
## Render detections to images
## Combine into a video
""
ocr_sandbox.py
""
sandbox for experimenting with using OCR to pull metadata from camera trap images
""
The general approach is:
""
"* Crop a fixed percentage from the top and bottom of an image, slightly larger"
than the largest examples we've seen of how much space is used for metadata.
""
"* Refine that crop by blurring a little, then looking for huge peaks in the"
"color histogram suggesting a solid background, then finding rows that are"
mostly that color.
""
"* Crop to the refined crop, then run pytesseract to extract text"
""
"* Use regular expressions to find time and date, in the future can add, e.g.,"
"temperature (which is often present *only* in the images, unlike time/date which"
are also usually in EXIF but often wrong or lost in processing)
""
"The metadata extraction (EXIF, IPTC) here is just sample code that seemed to"
belong in this file.
""
Contact: Dan Morris (cameratraps@lila.science)
""
%% Constants and imports
pip install pytesseract
""
"Also intall tesseract from: https://github.com/UB-Mannheim/tesseract/wiki, and add"
"the installation dir to your path (on Windows, typically C:\Program Files (x86)\Tesseract-OCR)"
pip install IPTCInfo3
from the ai4eutils repo: https://github.com/Microsoft/ai4eutils
""
"Only used for writing out a summary, not important for the core metadata extraction"
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Using a semi-arbitrary metric of how much it feels like we found the
"text-containing region, discard regions that appear to be extraction failures"
Pad each crop with a few pixels to make tesseract happy
Discard text from the top
"When we're looking for pixels that match the background color, allow some"
tolerance around the dominant color
We need to see a consistent color in at least this fraction of pixels in our rough
crop to believe that we actually found a candidate metadata region.
"What fraction of the [top,bottom] of the image should we use for our rough crop?"
A row is considered a probable metadata row if it contains at least this fraction
"of the background color.  This is used only to find the top and bottom of the crop area,"
"so it's not that *every* row needs to hit this criteria, only the rows that are generally"
above and below the text.
%% Support functions
"%% Load some images, pull EXIF and IPTC data for fun"
%% Rough crop
"This will be an nImages x 1 list of 2 x 1 lists (image top, image bottom)"
image = images[0]
"l,t,r,b"
""
"0,0 is upper-left"
"%% Close-crop around the text, return a revised image and success metric"
Did we find a sensible mode that looks like a background value?
"This looks very scientific, right?  Definitely a probability?"
"print('Failed min background fraction test: {} of {}'.format(pBackGroundValue,minBackgroundFraction))"
"Notes to self, things I tried that didn't really go anywhere..."
"analysisImage = cv2.blur(analysisImage, (3,3))"
"analysisImage = cv2.medianBlur(analysisImage,5)"
"analysisImage = cv2.Canny(analysisImage,100,100)"
imagePil = Image.fromarray(analysisImage); imagePil
Use row heuristics to refine the crop
""
This egregious block of code makes me miss my fluency in Matlab.
"print('Cropping to {},{},{},{}'.format(x,y,w,h))"
Crop the image
"For some reason, tesseract doesn't like characters really close to the edge"
imagePil = Image.fromarray(croppedImage); imagePil
%% Go to OCR-town
"An nImages x 2 list of strings, extracted from the top and bottom of each image"
An nImages x 2 list of cropped images
iImage = 0; iRegion = 1; regionSet = imageRegions[iImage]; region = regionSet[iRegion]
text = pytesseract.image_to_string(region)
pil --> cv2
"image = cv2.medianBlur(image, 3)"
"image = cv2.erode(image, None, iterations=2)"
"image = cv2.dilate(image, None, iterations=4)"
"image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]"
"image = cv2.blur(image, (3,3))"
"image = cv2.copyMakeBorder(image,10,10,10,10,cv2.BORDER_CONSTANT,value=[0,0,0])"
"text = pytesseract.image_to_string(imagePil, lang='eng')"
https://github.com/tesseract-ocr/tesseract/wiki/Command-Line-Usage
"psm 6: ""assume a single uniform block of text"""
""
...for each cropped region
...for each image
%% Extract dates and times
s = '1:22 pm'
s = '1:23:44 pm'
%% Write results to a handy html file
Add image name and resized image
Add results and individual region images
"textStyle = ""font-family:calibri,verdana,arial;font-weight:bold;font-size:150%;text-align:left;margin-left:50px;"""
%% Scrap
Alternative approaches to finding the text/background  region
Using findCountours()
imagePil = Image.fromarray(analysisImage); imagePil
"analysisImage = cv2.erode(analysisImage, None, iterations=3)"
"analysisImage = cv2.dilate(analysisImage, None, iterations=3)"
"analysisImage = cv2.threshold(analysisImage, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]"
Find object with the biggest bounding box
Using connectedComponents()
analysisImage = image
print('Found {} components'.format(nb_components))
We just want the *background* image
""
"Takes a .json file with MD results for an individual video, and converts to a .csv that includes"
frame times.  This is very bespoke to animal detection and does not include other classes.
""
%% Imports and constants
Only necessary if you want to extract the sample rate from the video
%% Extract the sample rate if necessary
%% Load results
%% Convert to .csv
i_image = 0; im = results['images'][i_image]
""
umn-pr-analysis.py
""
Precision/recall analysis for UMN data
""
%% Imports and constants
results_file = results_file_filtered
"For two deployments, we're only processing imagse in the ""detections"" subfolder"
String to remove from MegaDetector results
%% Enumerate deployment folders
%% Load MD results
im = md_results['images'][0]
%% Load ground truth
i_row = 0; row = ground_truth_df.iloc[i_row]
%% Create relative paths for ground truth data
"Some deployment folders have no subfolders, e.g. this is a valid file name:"
""
M00/01010132.JPG
""
"But some deployment folders have subfolders, e.g. this is also a valid file name:"
""
N17/100EK113/07160020.JPG
""
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly"
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths."
deployment_name = list(deployment_folders)[0]
Enumerate all files in this folder
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))"
filename = files[100]
...for each file in this deployment
...for each deployment
%% Map relative paths to MD results
%% Add relative paths to our ground truth table
i_row = 0; row = ground_truth_df.iloc[i_row]
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's"
"just the deployment ID and the image name, separated by a slash."
Find the relative path for this image
Make sure we have MegaDetector results for this file
Make sure this image file exists
...for each row in the ground truth table
%% Take everything out of Pandas
%% Some additional error-checking of the ground truth
An early version of the data required consistency between common_name and is_blank
%% Combine MD and ground truth results
d = ground_truth_dicts[0]
Find the maximum confidence for each category
...for each detection
...for each image
%% Precision/recall analysis
...for each image
Thresholds go up throughout precisions/recalls/thresholds; find the last
value where recall is at or above target.  That's our precision @ target recall.
Flatten the confusion matrix
Write precision/recall plot to .png file in output directory
pr_figure_relative_filename = 'prec_recall.png'
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)"
plt.show(block=False)
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')"
%% Find and manually review all images of humans
%%
"...if this image is annotated as ""human"""
...for each image
%% Find and manually review all MegaDetector animal misses
%%
im = merged_images[0]
GT says this is not an animal
GT says this is an animal
%% Convert .json to .csv
%%
""
kga-pr-analysis.py
""
Precision/recall analysis for KGA data
""
%% Imports and constants
%% Load and filter MD results
%% Load and filter ground truth
%% Map images to image-level results
%% Map sequence IDs to images and annotations to images
Verify consistency of annotation within a sequence
TODO
%% Find max confidence values for each category for each sequence
seq_id = list(sequence_id_to_images.keys())[1000]
im = images_this_sequence[0]
det = md_result['detections'][]
...for each detection
...for each image in this sequence
...for each sequence
%% Prepare for precision/recall analysis
seq_id = list(sequence_id_to_images.keys())[1000]
cat_id = list(category_ids_this_sequence)[0]
...for each category in this sequence
...for each sequence
%% Precision/recall analysis
"Confirm that thresholds are increasing, recall is decreasing"
This is not necessarily true
assert np.all(precisions[:-1] <= precisions[1:])
Thresholds go up throughout precisions/recalls/thresholds; find the max
value where recall is at or above target.  That's our precision @ target recall.
"This is very slightly optimistic in its handling of non-monotonic recall curves,"
but is an easy scheme to deal with.
Flatten the confusion matrix
Write precision/recall plot to .png file in output directory
pr_figure_relative_filename = 'prec_recall.png'
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)"
plt.show(block=False)
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')"
%% Scrap
%% Find and manually review all sequence-level MegaDetector animal misses
input_base = '/media/user/lila-01/lila/snapshot-safari/KGA/KGA_public'
i_sequence = 0; seq_id = pr_sequence_ids[i_sequence]
i_seq = 0; seq_id = false_negative_sequences[i_seq]
"sequence_folder = os.path.join(sequence_preview_dir,'seq_{}'.format(str(i_seq).zfill(3)))"
fn = image_files[0]
"print('Copying {} to {}'.format(input_path,output_path))"
...for each file in this sequence.
...for each sequence
%% Image-level postprocessing
parse arguments
check if a GPU is available
load a pretrained embedding model
setup experiment
load the embedding model
setup the target dataset
setup finetuning criterion
setup an active learning environment
create a classifier
the main active learning loop
Active Learning
finetune the embedding model and load new embedding values
gather labeled pool and train the classifier
save a snapshot
Load a checkpoint if necessary
setup the training dataset and the validation dataset
setup data loaders
check if a GPU is available
create a model
setup loss criterion
define optimizer
load a checkpoint if provided
setup a deep learning engine and start running
train the model
train for one epoch
evaluate on validation set
save a checkpoint
open the file
read it
do the substitution
matplotlib.use('Agg')
from UIComponents.DBObjects import *
Initialize Database
# database connection credentials
# try to connect as USER to database DB_NAME through peewee
Load the saved embedding model
dataset_query = Detection.select().limit(5)
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})"
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getalllabels()) , dataset.getallpaths(), {})"
Random examples to start
"random_ids = np.random.choice(dataset.current_set, 1000, replace=False).tolist()"
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)"
print(random_ids)
Move Records
"moveRecords(dataset, DetectionKind.ModelDetection.value, DetectionKind.UserDetection.value, random_ids)"
#print([len(x) for x in dataset.set_indices])
# Finetune the embedding model
#dataset.set_kind(DetectionKind.UserDetection.value)
#dataset.train()
"#train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)"
"#finetune_embedding(model, checkpoint['loss_type'], dataset, 32, 4, 100)"
#save_checkpoint({
"#        'arch': model.arch,"
"#        'state_dict': model.state_dict(),"
"#        'optimizer' : optimizer.state_dict(),"
"#        'loss_type' : loss_type,"
"#        }, False, ""%s%s_%s_%04d.tar""%('finetuned', loss_type, model.arch, len(dataset.set_indices[DetectionKind.UserDetection.value])))"
Get indices of samples to get user to label
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
"kwargs[""already_selected""].extend(indices)"
Train on samples that have been labeled so far
Test on the samples that have not been labeled
"'optimizer' : optimizer.state_dict(),"
num_classes= len(train_dataset.getClassesInfo()[0])
"print(""Num Classes= ""+str(num_classes))"
define loss function (criterion) and optimizer
"optimizer = torch.optim.SGD(params, momentum = 0.9, lr = args.lr, weight_decay = args.weight_decay)"
optimizer.load_state_dict(checkpoint['optimizer'])
train for one epoch
"adjust_lr(optimizer,epoch)"
if epoch % 1 == 0 and epoch > 0:
"a, b, c = e.predict(train_embd_loader, load_info = True, dim = args.feat_dim)"
"plot_embedding(reduce_dimensionality(a), b, c, {})"
evaluate on validation set
--------some stuff needed to get AJAX to work with bottle?--------#
-------------------------------------------------------------------------------- #
PREPARE TO QUEUE IMAGES FOR LABELING
-------------------------------------------------------------------------------- #
# Connect as USER to database DB_NAME through peewee and initialize database proxy
# Load embedding model
---------------------------------------------------------------------- #
CREATE QUEUE OF IMAGES TO LABEL
---------------------------------------------------------------------- #
Use classifier to generate predictions
# Update model predicted class in PostgreSQL database
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
-------------------------------------------------------------------------------- #
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI
-------------------------------------------------------------------------------- #
"# static routes (to serve CSS, etc.)"
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps"
"return bottle.static_file(filename, root='../../../../../../../../../../../.')"
# dynamic routes
if data['display_grayscale']:
indices_to_exclude.update(set(color_indices))
elif not data['display_grayscale']:
indices_to_exclude.update(set(grayscale_indices))
data['display_images'] = {}
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices]
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices]
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices]
data['display_images']['detection_categories'] = []
for i in indices:
if str(dataset.samples[i][1]) == 'None':
data['display_images']['detection_categories'].append('None')
else:
existing_category_entries = {cat.id: cat.name for cat in Category.select()}
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()"
data['display_images']['detection_categories'].append(cat_name)
Use image ids in images_to_label to get the corresponding dataset indices
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
print(set(dataset.set_indices[4]).update(set(indices_to_label)))
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Use image id images_to_label to get the corresponding dataset index
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Train on samples that have been labeled so far
dataset.set_kind(DetectionKind.UserDetection.value)
print(y_train)
Predict on the samples that have not been labeled
print(y_pred)
Update model predicted class in PostgreSQL database
timer = time.time()
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
print('Updating the database took %0.2f seconds'%(time.time() - timer))
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
"once the classifier has been trained the first time, switch to AL sampling"
# sourceMappingURL=bootstrap.bundle.min.js.map
"print(ap_distances.size(),an_distances.size())"
losses = -(((-ap_distances)/128)+1+1e-16).log() - (((-(128-an_distances))/128)+1+1e-16).log()
import pdb
pdb.set_trace()
losses = ap_distances - an_distances + self.margin
print(losses.size())
from UIComponents.DBObjects import *
TODO: should this also change self.kind?
get the embedding representations for all samples (i.e. set current_set to all indices)
"return PILImage.open(os.path.join(self.img_base,path)).convert('RGB')"
"print(self.labels_set, self.n_classes)"
from sklearn.manifold import TSNE
embedding= TSNE(n_components=2).fit_transform(X)
embedding= PCA(n_components=2).fit_transform(X)
return X
"print(dir(event), type(sc))"
"print(label,bgcolor)"
"patches.append(mpatches.Patch(color=indexcolors[i], label=label))"
"plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,"
"ncol=12, mode=""expand"", borderaxespad=0., handles=patches)"
plt.legend(handles=patches)
"plt.xlabel('Dim 1', fontsize=12)"
"plt.ylabel('Dim 2', fontsize=12)"
plt.grid(True)
"plt.xlabel('Dim 1', fontsize=12)"
"plt.ylabel('Dim 2', fontsize=12)"
plt.grid(True)
print(thumb)
"img.thumbnail((16, 12), PILImage.ANTIALIAS)"
plt.show()
Add all negatives for all positive pairs
print(triplets.shape[0])
compute output
measure accuracy
compute loss on this batch
"train on a batch, record loss, and measure accuracy (if calc_accuracy)"
compute output
measure accuracy and record loss
switch to evaluate mode
compute output
switch to evaluate mode
"self.fc13 = nn.Linear(128, 64)"
self.bn2 = nn.BatchNorm1d(64)
x = F.relu(self.fc12(x))
x = F.relu(self.bn1(self.fc13(x)))
x = F.relu(self.fc13(x))
"x = F.dropout(x, training=self.training)"
save features last FC layer
x = F.relu(x)
save features last FC layer
import matplotlib.pyplot as plt
plt.switch_backend('agg')
class EmbeddingNet(nn.Module):
"def __init__(self, architecture, feat_dim, use_pretrained=False):"
"super(EmbeddingNet, self).__init__()"
self.feat_dim= feat_dim
self.inner_model = models.__dict__[architecture](pretrained=use_pretrained)
if architecture.startswith('resnet'):
in_feats= self.inner_model.fc.in_features
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)"
elif architecture.startswith('inception'):
in_feats= self.inner_model.fc.in_features
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)"
if architecture.startswith('densenet'):
in_feats= self.inner_model.classifier.in_features
"self.inner_model.classifier = nn.Linear(in_feats, feat_dim)"
if architecture.startswith('vgg'):
in_feats= self.inner_model.classifier._modules['6'].in_features
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)"
if architecture.startswith('alexnet'):
in_feats= self.inner_model.classifier._modules['6'].in_features
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)"
"def forward(self, x):"
return self.inner_model.forward(x)
class NormalizedEmbeddingNet(EmbeddingNet):
"def __init__(self, architecture, feat_dim, use_pretrained=False):"
"EmbeddingNet.__init__(self, architecture, feat_dim, use_pretrained = use_pretrained)"
"def forward(self, x):"
embedding =  F.normalize(self.inner_model.forward(x))*10.0
"return embedding, embedding"
"def get_random_images(num, image_dir, test_transforms):"
"data = datasets.ImageFolder(image_dir, transform=test_transforms) # slight abuse; this expects subfolders corresponding to classes but we have no classes here"
indices = list(range(len(data)))
np.random.shuffle(indices)
idx = indices[:num]
from torch.utils.data.sampler import SubsetRandomSampler
sampler = SubsetRandomSampler(idx)
"loader = torch.utils.data.DataLoader(data,"
"sampler=sampler, batch_size=num)"
dataiter = iter(loader)
"images, labels = dataiter.next()"
"return images, labels"
"def predict_image(image, model, test_transforms):"
"device = torch.device(""cuda"" if torch.cuda.is_available()"
"else ""cpu"")"
image_tensor = test_transforms(image).float()
image_tensor = image_tensor.unsqueeze_(0)
input = Variable(image_tensor)
input = input.to(device)
output = model(input)[0]
return output.data.cpu().numpy()
Connect to database and initialize db_proxy
# database connection credentials
# load the dataset
Load the saved embedding model from the checkpoint
# update the dataset embedding
# Create a folder for saving embedding visualizations with this model checkpoint
model_emb_dirname = os.path.basename(args.base_model).split('.')[0]
"os.makedirs(model_emb_dirname, exist_ok=True)"
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')"
dataset.embedding_mode()
"assert 2==3, 'break'"
datasetindices = list(range(len(dataset)))
np.random.shuffle(datasetindices)
random_indices = datasetindices[:args.num]
print(random_indices)
"selected_sample_features = np.array([]).reshape(0, 256)"
selected_sample_labels = []
for idx in random_indices:
"selected_sample_features = np.vstack([selected_sample_features, X_train[idx]])"
selected_sample_labels.append(y_train[idx])
img_path = imagepaths[idx].split('.JPG')[0]
image = dataset.loader(img_path)
selected_sample_images.append(image)
# TRY NEAREST NEIGHBORS WALK THROUGH EMBEDDING
nbrs = NearestNeighbors(n_neighbors=args.num).fit(selected_sample_features)
"distances, indices = nbrs.kneighbors(selected_sample_features)"
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')"
"idx_w_closest_nbr = np.where(distances[:,1] == min(distances[:,1]))[0][0]"
order = [idx_w_closest_nbr]
for ii in range(len(distances)):
"distances[ii, 0] = np.inf"
while len(order)<args.num:
curr_idx = order[-1]
curr_neighbors = indices[curr_idx]
curr_dists = list(distances[curr_idx])
# print(min(curr_dists))
next_closest_pos = curr_dists.index(min(curr_dists))
next_closest = curr_neighbors[next_closest_pos]
order.append(next_closest)
# make sure you can't revisit past nodes
for vi in order:
vi_pos = list(indices[next_closest]).index(vi)
"distances[next_closest, vi_pos] = np.inf"
for ii in range(len(order)):
imgidx = order[ii]
image = selected_sample_images[imgidx]
"image.save(model_emb_dirname+""/img""+str(ii)+""_""+str(selected_sample_labels[imgidx])+"".png"")"
# Specify the transformations on the input images before inference
"# test_transforms = transforms.Compose([transforms.Resize([224, 224]), transforms.ToTensor()])"
"test_transforms = transforms.Compose([transforms.Resize([256, 256]), transforms.RandomCrop([224, 224]), transforms.RandomHorizontalFlip(), transforms.ColorJitter(), transforms.ToTensor(), transforms.Normalize([0.407328, 0.407328, 0.407328], [0.118641, 0.118641, 0.118641])])"
"images, labels = get_random_images(num, image_dir, test_transforms)"
"all_features = np.array([]).reshape(0, 256)"
for ii in range(len(images)):
image = to_pil(images[ii])
"features = predict_image(image, model, test_transforms)"
"all_features = np.vstack([all_features, features])"
# for ii in range(len(images)):
#     image = to_pil(images[ii])
"#     image.save(""img""+str(ii)+"".png"")"
# TRY CLUSTERING
kmeans1 = KMeans(n_clusters=5).fit(StandardScaler().fit_transform(all_features))
print(kmeans1.labels_)
for ii in range(len(images)):
image = to_pil(images[ii])
"filename = str(kmeans1.labels_[ii])+""/img""+str(ii)+"".png"""
if not os.path.exists(os.path.dirname(filename)):
os.makedirs(os.path.dirname(filename))
image.save(filename)
class SaveFeatures():
"def __init__(self, module):"
self.hook = module.register_forward_hook(self.hook_fn)
"def hook_fn(self, module, input, output):"
"self.features = torch.tensor(output, requires_grad=True).cuda()"
def close(self):
self.hook.remove()
Load the saved embedding model from the checkpoint
"Get a sample from the database, with eval transforms applied, etc."
Connect to database and sample a dataset
output = model.forward(sample_image.unsqueeze(0))
print(output)
with torch.no_grad():
sample_image_input = sample_image.cuda(non_blocking=True)
"_, output = model(sample_image_input) # compute output"
print(output)
sample_image = PILImage.open(sample_image_path).convert('RGB')
"sample_image = transforms.Compose([Resize([256, 256]), CenterCrop(([[224,224]])), ToTensor(), Normalize([0.369875, 0.388726, 0.347536], [0.136821, 0.143952, 0.145229])])(sample_image)"
print(list(model_inner_resnet.children()))
print(model_inner_resnet.fc)
print(model_inner_resnet.fc0)
# print(model_inner_resnet.layer4[0].conv2)
# print(type(model))
# print(len(list(model_inner_resnet.children())))
# print(list(model.children()))
# print(list(list(model.children())[0].children()))
"img = np.uint8(np.random.uniform(150, 180, (56, 56, 3)))/255"
"img_tensor = torch.unsqueeze(torch.from_numpy(img), 0)"
full_out = model_inner_resnet.forward(img_tensor)
print(full_out)
model(img_tensor)
activations = SaveFeatures(model_inner_resnet.layer4[0].conv2)
print(activations.features)
print(type(activations.features))
activations.close()
Connect to database and sample a dataset
Load the saved embedding model from the checkpoint
Update the dataset embedding
Get a random query image
# # # IMAGES IN THE SAME SEQUENCE # # # #
"assert 2==3, 'break'"
# # # CLOSEST IN (EMBEDDING) FEATURE SPACE # # # #
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Variables for the hierarchical cluster
Variables for the AL algorithm
Data variables
"connectivity = kneighbors_graph(self.transformed_X,max_features)"
Fit cluster and update cluster variables
The sklearn hierarchical clustering algo numbers leaves which correspond
to actual datapoints 0 to n_points - 1 and all internal nodes have
ids greater than n_points - 1 with the root having the highest node id
"If no labels have been observed, simply return uniform distribution"
"If no observations, return worst possible upper lower bounds"
Loop through generations from bottom to top
Update admissible labels for node
Calculate score
Determine if node should be split
Make sure label set for node so that we can flow to children
if necessary
Only split if all ancestors are admissible nodes
This is part  of definition of admissible pruning
Check that pruning covers all leave nodes
Fill in labels
Observe labels for previously recommended batches
TODO(lishal): implement multiple selection methods
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
A list of initialized samplers is allowed as an input because
"for AL_methods that search over different mixtures, may want mixtures to"
have shared AL_methods so that initialization is only performed once for
computation intensive methods like HierarchicalClusteringAL and
states are shared between mixtures.
"If initialized samplers are not provided, initialize them ourselves."
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copy these objects to make sure they are not modified while simulating
trajectories as they are used later by the main run_experiment script.
Assumes that model has already by fit using all labeled data so
the probabilities can be used immediately to hallucinate labels
All models need to have predict_proba method
Hallucinate labels for selected datapoints to be label
using class probabilities from model
"Not saving already_selected here, if saving then should sort"
only for the input to fit but preserve ordering of indices in
already_selected
Useful to know how accuracy compares for model trained on hallucinated
labels vs trained on true labels.  But can remove this train to speed
up simulations.  Won't speed up significantly since many more models
are being trained inside the loop above.
Save trajectory for reference
Delete created copies
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER
Should check model but kernel_svm does not have coef_ so need better
handling here
Make sure that model object fed in did not change during simulations
Return indices based on return type specified
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Set gamma for gaussian kernel to be equal to 1/n_features
kneighbors graph is constructed using k=10
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of"
"another point, make it vice versa"
Graph edges are weighted by applying gaussian kernel to manhattan dist.
"By default, gamma for rbf kernel is equal to 1/n_features but may"
get better results if gamma is tuned.
Define graph density for an observation to be sum of weights for all
edges to the node representing the datapoint.  Normalize sum weights
by total number of neighbors.
"If a neighbor has already been sampled, reduce the graph density"
for its direct neighbors to promote diversity.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update observed reward and arm probabilities
Sample an arm
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
y only used for determining how many clusters there should be
probably not practical to assume we know # of classes before hand
should also probably scale with dimensionality of data
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
TODO(lishal): have MarginSampler and this share margin function
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update min_distances for all examples given new cluster center.
Assumes that the transform function takes in original data and not
flattened data.
Initialize centers with a randomly selected datapoint
New examples should not be in already selected since those points
should have min_distance of zero to a cluster center.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
This is uniform given the remaining pool but biased wrt the entire pool.
sample = [i for i in range(self.X.shape[0]) if i not in already_selected]
return sample[0:N]
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Fields for hierarchical clustering AL
Setting parent and storing nodes in dict for fast access
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Connect to database DB_NAME as USER and initialize tables
Populate Info table
Populate Category table
"# For now, we have a predefined list of species we expect to see in the camera trap database (e.g. maybe from a quick look through the images)"
# TODO: allow user to update the class list through the labeling tool UI as they see different species
Populate Image and Detection tables
Image entry data
Detection entry data
Connect to database DB_NAME as USER and initialize tables
Populate Info table
Populate Category table
# Create category records from species present in the COCO camera trap classification dataset (COCO classes minus any excluded classes)
# TODO: allow user to update the class list through the labeling tool UI as they see different species
Populate Image and Detection tables
"with open(os.path.join(args.crop_dir,'crops.json'), 'r') as infile:"
crops_json = json.load(infile)
counter = 0
timer = time.time()
num_detections = len(crops_json)
for detectionid in crops_json:
counter += 1
detection_data = crops_json[detectionid]
# Image entry data
existing_image_entries = Image.select().where((Image.file_name == detection_data['file_name']))
try:
existing_image_entry = existing_image_entries.get()
except:
"image_entry = Image.create(id=detectionid, file_name=detection_data['file_name'], width=detection_data['width'], height=detection_data['height'], grayscale=detection_data['grayscale'],"
"source_file_name=detection_data['source_file_name'], relative_size=detection_data['relative_size'],"
"seq_id=detection_data['seq_id'], seq_num_frames=detection_data['seq_num_frames'], frame_num=detection_data['frame_num'])"
image_entry.save()
# Detection entry data
"detection_entry = Detection.create(id=detectionid, image=detectionid, bbox_confidence=detection_data['bbox_confidence'],"
"bbox_X1=detection_data['bbox_X1'], bbox_Y1=detection_data['bbox_Y1'], bbox_X2=detection_data['bbox_X2'], bbox_Y2=detection_data['bbox_Y2'],"
kind=DetectionKind.ModelDetection.value)
detection_entry.save()
if counter%100 == 0:
"print('Updated database with Image and Detection table entries for %d out of %d crops in %0.2f seconds'%(counter, num_detections, time.time() - timer))"
# data related to original image
Get class names from .txt list
Initialize Oracle table
Map filenames to classes (NOTE: we assume a single image does not contain more than one class)
"For each detection, use source image path to get class"
TODO update: Assumes that crops have already
been generated for the images using make_active_learning_classification_dataset.py. The created DB contains tables:
- info: information about the dataset
- image: images present in the dataset
- detections: crops of images with detections with confidence greater than a specified threshold
Initialize Database
# database connection credentials
HOST = 'localhost'
PORT = 5432
"# first, make sure the (user, password) has been created"
"# sudo -u postgres psql -c ""CREATE USER <db_user> WITH PASSWORD <db_password>;"""
"# sudo -u postgres psql -c ""CREATE DATABASE <db_name> WITH OWNER <db_user> CONNECTION LIMIT -1;"""
"# sudo -u postgres psql -c ""GRANT CONNECT ON DATABASE <db_name> TO <db_user>;"""
"# sudo -u postgres psql -d <db_name> -c ""CREATE EXTENSION IF NOT EXISTS \""uuid-ossp\"";"""
# Try to connect as USER to database DB_NAME through peewee
Populate Tables
# create Info table
# get class names for Category table
Faster anD available in Python 3.5 and above
# iterate through images in each class folder
killing this process after over 38 hours adding over 500k white-tailed deer crops from emammal
resuming for remaining classes
# get cropped image data for Image table
"if mean of each channel is about the same, image is likely grayscale"
# still have no info on these:
seq_id = CharField(null= True)                # sequence identifier for the original image
seq_num_frames = IntegerField(null = True)    # number of frames in sequence
frame_num = IntegerField(null = True)         # which frame number in sequence
location = CharField(null = True)             # location of camera trap
datetime = DateTimeField(null = True)
# store info about the detection corresponding to this image
# store info about the true labels for the detection
#  - for pretrain dataset this is the same as the detection_category if the detection categories
print(classes)
Connect to database and sample a dataset
Load the saved embedding model from the checkpoint
Update the dataset embedding
save the images
save the features
"with open(os.path.join(args.output_dir, 'lastlayer_features.mat'), 'wb') as f:"
"pickle.dump(sample_features, f)"
"with open(os.path.join(args.output_dir, 'labels.mat'), 'wb') as f:"
"pickle.dump(sample_labels, f)"
"parser.add_argument('--db_name', default='missouricameratraps', type=str, help='Name of the training (target) data Postgres DB.')"
"parser.add_argument('--db_user', default='user', type=str, help='Name of the user accessing the Postgres DB.')"
"parser.add_argument('--db_password', default='password', type=str, help='Password of the user accessing the Postgres DB.')"
"parser.add_argument('--base_model', type=str, help='Path to latest embedding model checkpoint.')"
"parser.add_argument('--output_dir', type=str, help='Output directory for subset of crops')"
Add json entry for this crop
Copy file for this crop to subset dataset crop dir
Copy file for its full-size source image to subset dataset image dir
Write crops.json to subset dataset crop dir
store info about the crops produced in a JSON file
------------------------------------------------------------------------------------------------------------#
COMMENT OUT IF NOT USING A SPECIFIC PROJECT WITHIN ROBERT LONG EMAMMAL DATASET
------------------------------------------------------------------------------------------------------------#
get some information about the source image
------------------------------------------------------------------------------------------------------------#
NOTE: EDIT THIS SECTION BASED ON DATASET SOURCE
get info about sequence the source image belongs to from path and directory
# missouricameratraps:
imgframenum = int(os.path.basename(imgfile).split('.JPG')[0].split('_')[-1])
imgseqid = int(os.path.split(os.path.dirname(imgfile))[-1])
"imgseqnumframes = len([name for name in os.listdir(os.path.dirname(imgfile)) if os.path.isfile(os.path.join(os.path.dirname(imgfile), name))])"
# emammal:
------------------------------------------------------------------------------------------------------------#
utility function
compute output
measure accuracy and record loss
switch to train mode
measure accuracy and record loss
compute output
measure accuracy and record loss
switch to evaluate mode
compute output
switch to evaluate mode
compute output
switch to evaluate mode
"print(self.labels_set, self.n_classes)"
Add all negatives for all positive pairs
print(triplets.shape[0])
constructor
update embedding values after a finetuning
select either the default or active pools
gather test set
gather train set
finetune the embedding model over the labeled pool
a utility function for saving the snapshot
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Variables for the hierarchical cluster
Variables for the AL algorithm
Data variables
"connectivity = kneighbors_graph(self.transformed_X,max_features)"
Fit cluster and update cluster variables
The sklearn hierarchical clustering algo numbers leaves which correspond
to actual datapoints 0 to n_points - 1 and all internal nodes have
ids greater than n_points - 1 with the root having the highest node id
"If no labels have been observed, simply return uniform distribution"
"If no observations, return worst possible upper lower bounds"
Loop through generations from bottom to top
Update admissible labels for node
Calculate score
Determine if node should be split
Make sure label set for node so that we can flow to children
if necessary
Only split if all ancestors are admissible nodes
This is part  of definition of admissible pruning
Check that pruning covers all leave nodes
Fill in labels
Observe labels for previously recommended batches
TODO(lishal): implement multiple selection methods
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
A list of initialized samplers is allowed as an input because
"for AL_methods that search over different mixtures, may want mixtures to"
have shared AL_methods so that initialization is only performed once for
computation intensive methods like HierarchicalClusteringAL and
states are shared between mixtures.
"If initialized samplers are not provided, initialize them ourselves."
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copy these objects to make sure they are not modified while simulating
trajectories as they are used later by the main run_experiment script.
Assumes that model has already by fit using all labeled data so
the probabilities can be used immediately to hallucinate labels
All models need to have predict_proba method
Hallucinate labels for selected datapoints to be label
using class probabilities from model
"Not saving already_selected here, if saving then should sort"
only for the input to fit but preserve ordering of indices in
already_selected
Useful to know how accuracy compares for model trained on hallucinated
labels vs trained on true labels.  But can remove this train to speed
up simulations.  Won't speed up significantly since many more models
are being trained inside the loop above.
Save trajectory for reference
Delete created copies
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER
Should check model but kernel_svm does not have coef_ so need better
handling here
Make sure that model object fed in did not change during simulations
Return indices based on return type specified
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Set gamma for gaussian kernel to be equal to 1/n_features
kneighbors graph is constructed using k=10
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of"
"another point, make it vice versa"
Graph edges are weighted by applying gaussian kernel to manhattan dist.
"By default, gamma for rbf kernel is equal to 1/n_features but may"
get better results if gamma is tuned.
Define graph density for an observation to be sum of weights for all
edges to the node representing the datapoint.  Normalize sum weights
by total number of neighbors.
"If a neighbor has already been sampled, reduce the graph density"
for its direct neighbors to promote diversity.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update observed reward and arm probabilities
Sample an arm
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
y only used for determining how many clusters there should be
probably not practical to assume we know # of classes before hand
should also probably scale with dimensionality of data
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
TODO(lishal): have MarginSampler and this share margin function
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update min_distances for all examples given new cluster center.
Assumes that the transform function takes in original data and not
flattened data.
Initialize centers with a randomly selected datapoint
New examples should not be in already selected since those points
should have min_distance of zero to a cluster center.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
This is uniform given the remaining pool but biased wrt the entire pool.
sample = [i for i in range(self.X.shape[0]) if i not in already_selected]
return sample[0:N]
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Fields for hierarchical clustering AL
Setting parent and storing nodes in dict for fast access
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Faster and available in Python 3.5 and above
transform_list.append(CenterCrop((processed_size)))
transform_list.append(Lambda(lambda X: normalize(X)))
"print(self.labels_set, self.n_classes)"
"from PyQt5 import QtCore, QtWidgets,QtGui"
from collections import deque
from peewee import *
from UIComponents.Tag import Tag
policy.setHeightForWidth(True)
"print(self.tab1.parentWidget(),self)"
self.tab4.add.clicked.connect(self.addSpecies)
self.tab4.update.clicked.connect(self.updateSpecies)
checkpoint= load_checkpoint('../merge/triplet_model_0054.tar')
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))"
"embedding_net = EmbeddingNet(checkpoint['arch'], checkpoint['feat_dim'])"
if checkpoint['loss_type'].lower()=='center':
"model = torch.nn.DataParallel(ClassificationNet(embedding_net, n_classes=14)).cuda()"
else:
model= torch.nn.DataParallel(embedding_net).cuda()
model.load_state_dict(checkpoint['state_dict'])
self.parentWidget().progressBar.setMaximum(len(run_dataset)//2048)
"e=Engine(model,None,None, verbose=True,progressBar= self.parentWidget().progressBar)"
label = [ x[1] for x in run_dataset.samples]
"print(indices,selected_set)"
print(query.sql())
src.delete().where(src.image_id<<rList))
"det= UserDetection.create(category_id=0, id=str(index+label[1][2]),image_id=final[0], bbox_X=label[1][0], bbox_Y=label[1][1], bbox_W=label[1][2], bbox_H=label[1][3])"
for x in self.tab1.grid.tags:
x.delete_instance()
db.create_tables([Detection])
This is simply to show the bar
"p = Process(target=ex.active, args=())"
p.start()
p.join()
ex.active()
ex.centralWidget().setCurrentIndex(1)
main()
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))"
print(row)
"unq_id= ""crops_""+str(uuid.uuid1())"
"print(line,imageWidth,imageHeight)"
"print(""%s,%s,%s,%0.6f,%0.6f,%0.6f,%0.6f,%0.6f""%(line[0], line[1],line[2],float(line[3]),topRel,leftRel,bottomRel,rightRel))"
if not os.path.exists(dest):
os.mkdir(dest)
raise
out.close()
"print length,(i-1)*length,i*length"
matplotlib.use('Agg')
get the inputs
zero the parameter gradients
forward + backward + optimize
print statistics
get the inputs
forward + backward + optimize
print statistics
get the inputs
zero the parameter gradients
forward + backward + optimize
get the inputs
forward + backward + optimize
print statistics
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
"conf.reset()"""""""
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
conf.reset()
for clf in classifiers:
"clf.fit(X_train, y_train)"
preds= clf.predict_proba(X)
uncertainty+= preds.max(axis=1)
print(uncertainty[ind])
print(uncertainty[ind])
true_labels = y[unlabeled_indices]
select up to 5 digit examples that the classifier is most uncertain about
remember best acc@1 and save checkpoint
"completeClassificationLoop(run_dataset, model,num_classes)"
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)"
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd)
labels = db.labels_
"mapp=(find_probablemap(label,labels, K=args.K))"
"print(""Clusters"")"
"for i,x in enumerate(labels):"
labels[i]= mapp[x]
print(np.sum(labels == label)/labels.size)
"print(""Confidence Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning(embd, label, idx)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(""Entropy Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning_entropy(embd, label, idx)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(idx,idx.shape)"
for i in idx:
print(paths[i])
"print(""Silohette active learning"")"
"idx= active_learning2(embd, 1000, args.num_clusters)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
embd= reduce_dimensionality(embd)#[0:10000])
labels= labels[0:10000]
label= label[0:10000]
paths= paths[0:10000]
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])"
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])"
plt.show()
"np.save(args.name_prefix+""_embeddings.npy"",embd)"
"np.save(args.name_prefix+""_labels.npy"",label)"
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")"
del output
define loss function (criterion) and optimizer
"optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay= args.weight_decay)"
optimizer.load_state_dict(checkpoint['optimizer'])
train for one epoch
evaluate on validation set
matplotlib.use('Agg')
selected_set.add(rand_ind[i])
"embedding_net = EmbeddingNet('resnet50', 256, True)"
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)"
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)"
print('Embedding Done')
sys.stdout.flush()
Random examples to start
"random_ids = noveltySamples(unlabeled_dataset.em, unlabeled_dataset.getIDs(), 1000)"
print(random_ids)
Move Records
Finetune the embedding model
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)"
unlabeled_dataset.updateEmbedding(model)
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})"
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()"
matplotlib.use('Agg')
"uncertainty= np.apply_along_axis(stats.entropy,1,probs) * (1 - probs.max(axis=1))"
"copy(paths[srt[i]], ""active"")"
"plot_together( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()), preds, base_ind, dataset.getpaths(), {})"
"return np.random.choice(range(0,prob_list[0].shape[0]), 100, replace=False).tolist()"
selected_set.add(rand_ind[i])
"query= Detection.update(kind = destKind.value).where(Detection.id.in_(rList), Detection.kind == srcKind.value)"
query.execute()
"embedding_net = EmbeddingNet('resnet50', 256, True)"
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)"
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)"
print('Embedding Done')
sys.stdout.flush()
"plot_embedding(dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})"
Random examples to start
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)"
print(random_ids)
Move Records
Finetune the embedding model
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)"
unlabeled_dataset.updateEmbedding(model)
"print(dataset.em[dataset.current_set].shape, np.asarray(dataset.getlabels()).shape, len(dataset.getpaths()))"
"plot_embedding( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})"
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})"
train_eval_classifier()
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()"
"names = [""Linear SVM"", ""RBF SVM"", ""Random Forest"", ""Neural Net"", ""Naive Bayes""]"
"classifiers = [SVC(kernel=""linear"", C=0.025, probability= True, class_weight='balanced'),"
"SVC(gamma=2, C=1, probability= True, class_weight='balanced'),"
"RandomForestClassifier(max_depth=None, n_estimators=100, class_weight='balanced'),"
"MLPClassifier(alpha=1),"
GaussianNB()]
estimators= []
"for name, clf in zip(names, classifiers):"
"estimators.append((name, clf))"
"eclf1 = VotingClassifier(estimators= estimators, voting='hard')"
"eclf2 = VotingClassifier(estimators= estimators, voting='soft')"
"names.append(""ensemble hard"")"
classifiers.append(eclf1)
"names.append(""ensemble soft"")"
classifiers.append(eclf2)
dataset.image_mode()
dataset.updateEmbedding(model)
y_pred= clf.predict(X_test)
"print(confusion_matrix(y_test, y_pred))"
paths= dataset.getpaths()
"for i, (yp, yt) in enumerate(zip(y_pred, y_test)):"
if yp != yt:
"copy(paths[i],""mistakes"")"
"print(yt, yp, paths[i],i)"
"clf_output= clf_e.embedding(eval_loader, dim=48)"
"self.train_transform = transforms.Compose([Resize(raw_size), CenterCrop((processed_size)), ToTensor(), Normalize(mean, std)])"
"print(self.labels_set, self.n_classes)"
"print(line,imageWidth,imageHeight)"
"print length,(i-1)*length,i*length"
#########################################################
## Configuration
#########################################################
## The actual code
Check arguments
Create output directories
Padding around the detected objects when cropping
1.3 for the cropping during test time and 1.3 for
the context that the CNN requires in the left-over
image
Load a (frozen) Tensorflow model into memory.
## Preparations: get all the output tensors
The following processing is only for single image
Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
Follow the convention by adding back the batch dimension
For all images in the image directoryig
Load image
Run inference
"all outputs are float32 numpy arrays, so convert types as appropriate"
Add detections to the collection
Get info about the image
Select detections with a confidence larger than DETECTION_CONFIDENCE
Get these boxes and convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes"
"However, we need to make sure that it box coordinates are still within the image"
"For each detected bounding box with high confidence, we will"
crop the image to the padded box and save it
generate a unique identifier for the detection
"bbox is the detected box, crop_box the padded / enlarged box"
Add numbering to the original file name if there are multiple boxes
The absolute file path where we will store the image
"if COCO_OUTPUT_DIR is set, then we will only use the shape"
of cropped_img in the following code. So instead of reading
cropped_img = np.array(Image.open(out_file))
we can speed everything up by reading only the size of the image
matplotlib.use('Agg')
conf= ConfusionMatrix(24)
get the inputs
zero the parameter gradients
forward + backward + optimize
print statistics
get the inputs
forward + backward + optimize
print statistics
get the inputs
zero the parameter gradients
forward + backward + optimize
get the inputs
forward + backward + optimize
print statistics
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
"conf.reset()"""""""
define loss function (criterion) and optimizer
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
conf.reset()
for clf in classifiers:
"clf.fit(X_train, y_train)"
preds= clf.predict_proba(X)
uncertainty+= preds.max(axis=1)
print(uncertainty[ind])
print(uncertainty[ind])
true_labels = y[unlabeled_indices]
select up to 5 digit examples that the classifier is most uncertain about
"print(indices,selected_set)"
print(query.sql())
remember best acc@1 and save checkpoint
"completeClassificationLoop(run_dataset, model,num_classes)"
"embd, label, paths = extract_embeddings(run_loader, model)"
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)"
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd)
labels = db.labels_
"mapp=(find_probablemap(label,labels, K=args.K))"
"print(""Clusters"")"
"for i,x in enumerate(labels):"
labels[i]= mapp[x]
print(np.sum(labels == label)/labels.size)
"print(""Confidence Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning(embd, label, idx)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(""Entropy Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning_entropy(embd, label, idx)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
"new_selected= selectSamples(embd,paths,3000)"
"print(idx,idx.shape)"
for i in idx:
print(paths[i])
"print(""Silohette active learning"")"
"idx= active_learning2(embd, 1000, args.num_clusters)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(""Random"")"
"idx = np.random.choice(np.arange(len(paths)), 1000, replace=False)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
embd= reduce_dimensionality(embd)#[0:10000])
labels= labels[0:10000]
label= label[0:10000]
paths= paths[0:10000]
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])"
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])"
plt.show()
"np.save(args.name_prefix+""_embeddings.npy"",embd)"
"np.save(args.name_prefix+""_labels.npy"",label)"
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")"
combo.setEnabled(not finalized)
print(self.bbox)
self.addWidget(self.child)
Moving container with arrows
Left - Bottom
Right-Bottom
Left-Top
Right-Top
Left - Bottom
Right - Bottom
Left - Top
Right - Top
check cursor horizontal position
check cursor vertical position
self.resizeEvent=self.onResize
"print(""Parent"", parent, parent.width(), parent.height())"
"self.setGeometry(0,0,410,307)"
"print(w,h,""w,h"")"
"print(""final"",tag.getFinal())"
pass
"self.tags.append(TContainer(self,Category.get(-1),[0,0,0.1,0.1],True, Qt.red))"
label= CharField()
fullname=str(self.model)
"self.name= (fullname[fullname.find("":"")+2:fullname.find("">"")].strip()+'_set').lower()"
"print(""Parent"", self.parentWidget().width(), self.parentWidget().height())"
"print(self.model,self.name,query.sql())"
self.tab4.speciesList.setModel(species)
"self.tab4.speciesList.setRowHidden(len(species.stringList())-1, True)"
self.speciesList.itemChanged.connect(self.itemChanged)
#####
""
video_utils.py
""
"Utilities for splitting, rendering, and assembling videos."
""
#####
"%% Constants, imports, environment"
from ai4eutils
%% Path utilities
%% Function for rendering frames to video and vice-versa
http://tsaith.github.io/combine-images-into-a-video-with-python-3-and-opencv-3.html
Determine the width and height from the first image
Define the codec and create VideoWriter object
"If we're not over-writing, check whether all frame images already exist"
"OpenCV seems to over-report the number of frames by 1 in some cases, or fails"
"to read the last frame; either way, I'm allowing one missing frame."
"print(""Rendering video {}, couldn't find frame {}"".format("
"input_video_file,missing_frame_number))"
...if we need to check whether to skip this video entirely
"for frame_number in tqdm(range(0,n_frames)):"
print('Skipping frame {}'.format(frame_filename))
Recursively enumerate video files
Create the target output folder
Render frames
input_video_file = input_fn_absolute; output_folder = output_folder_video
For each video
""
input_fn_relative = input_files_relative_paths[0]
"process_detection_with_options = partial(process_detection, options=options)"
zero-indexed
Load results
# Break into videos
im = images[0]
# For each video...
video_name = list(video_to_frames.keys())[0]
frame = frames[0]
At most one detection for each category for the whole video
category_id = list(detection_categories.keys())[0]
Find the nth-highest-confidence video to choose a confidence value
Prepare the output representation for this video
'max_detection_conf' is no longer included in output files by default
...for each video
Write the output file
%% Test driver
%% Constants
%% Split videos into frames
"%% List image files, break into folders"
Find unique folders
fn = frame_files[0]
%% Load detector output
%% Render detector frames
folder = list(folders)[0]
d = detection_results_this_folder[0]
...for each file in this folder
...for each folder
%% Render output videos
folder = list(folders)[0]
...for each video
""
run_inference_with_yolov5_val.py
""
Runs a folder of images through MegaDetector (or another YOLOv5 model) with YOLOv5's
"val.py, converting the output to the standard MD format.  The main goal is to leverage"
YOLO's test-time augmentation tools.
""
"YOLOv5's val.py uses each file's base name as a unique identifier, which doesn't work"
when you have typical camera trap images like:
""
a/b/c/RECONYX0001.JPG
d/e/f/RECONYX0001.JPG
""
...so this script jumps through a bunch of hoops to put a symlinks in a flat
"folder, run YOLOv5 on that folder, and map the results back to the real files."
""
"Currently requires the user to supply the path where a working YOLOv5 install lives,"
and assumes that the current conda environment is all set up for YOLOv5.
""
TODO:
""
* Figure out what happens when images are corrupted... right now this is the #1
"reason not to use this script, it may be the case that corrupted images look the"
same as empty images.
""
* Multiple GPU support
""
* Checkpointing
""
* Windows support (I have no idea what all the symlink operations will do on Windows)
""
"* Support alternative class names at the command line (currently defaults to MD classes,"
though other class names can be supplied programmatically)
""
%% Imports
%% Options class
# Required ##
# Optional ##
%% Main function
#%% Path handling
#%% Enumerate images
#%% Create symlinks to give a unique ID to each image
i_image = 0; image_fn = image_files_absolute[i_image]
...for each image
#%% Create the dataset file
Category IDs need to be continuous integers starting at 0
#%% Prepare YOLOv5 command
#%% Run YOLOv5 command
#%% Convert results to MD format
"We'll use the absolute path as a relative path, and pass '/'"
as the base path in this case.
#%% Clean up
...def run_inference_with_yolo_val()
%% Command-line driver
%% Scrap
%% Test driver (folder)
%% Test driver (file)
%% Preview results
options.sample_seed = 0
...for each prediction file
%% Compare results
Choose all pairwise combinations of the files in [filenames]
"%% Constants, imports, environment"
from multiprocessing.pool import ThreadPool as workerpool
Number of images to pre-fetch
Useful hack to force CPU inference.
""
"Need to do this before any PT/TF imports, which happen when we import"
from run_detector.
Numpy FutureWarnings from tensorflow import
%% Support functions for multiprocessing
TODO
""
The queue system is a little more elegant if we start one thread for reading and one
"for processing, and this works fine on Windows, but because we import TF at module load,"
"CUDA will only work in the main process, so currently the consumer function runs here."
""
"To enable proper multi-GPU support, we may need to move the TF import to a separate module"
that isn't loaded until very close to where inference actually happens.
%% Other support funtions
%% Image processing functions
%% Main function
Handle the case where image_file_names is not yet actually a list
"Find the images to score; images can be a directory, may need to recurse"
A json list of image paths
A single image file
Load the detector
Does not count those already processed
Will not add additional entries not in the starter checkpoint
Write a checkpoint if necessary
"Back up any previous checkpoints, to protect against crashes while we're writing"
the checkpoint file.
Write the new checkpoint
Remove the backup checkpoint if it exists
...if it's time to make a checkpoint
"When using multiprocessing, let the workers load the model"
"Results may have been modified in place, but we also return it for"
backwards-compatibility.
The typical case: we need to build the 'info' struct
"If the caller supplied the entire ""info"" struct"
"The 'max_detection_conf' field used to be included by default, and it caused all kinds"
"of headaches, so it's no longer included unless the user explicitly requests it."
%% Interactive driver
%%
%% Command-line driver
This is an experimental hack to allow the use of non-MD YOLOv5 models through
the same infrastructure; it disables the code that enforces MDv5-like class lists.
Load the checkpoint if available
""
Relative file names are only output at the end; all file paths in the checkpoint are
still full paths.
"Find the images to score; images can be a directory, may need to recurse"
A json list of image paths
A single image file
Test that we can write to the output_file's dir if checkpointing requested
"Don't overwrite existing checkpoint files, this is a sure-fire way to eventually"
erase someone's checkpoint.
"Commenting this out for now... the scenario where we are resuming from a checkpoint,"
then immediately overwrite that checkpoint with empty data is higher-risk than the
annoyance of crashing a few minutes after starting a job.
Confirm that we can write to the checkpoint path; this avoids issues where
we crash after several thousand images.
%% Imports
import pre- and post-processing functions from the YOLOv5 repo
scale_coords() became scale_boxes() in later YOLOv5 versions
%% Classes
padded resize
"Image size can be an int (which translates to a square target size) or (h,w)"
...if the caller has specified an image size
NMS
"As of v1.13.0.dev20220824, nms is not implemented for MPS."
""
Send predication back to the CPU to fix.
format detections/bounding boxes
"This is a loop over detection batches, which will always be length 1 in our case,"
since we're not doing batch inference.
Rescale boxes from img_size to im0 size
"normalized center-x, center-y, width and height"
"MegaDetector output format's categories start at 1, but the MD"
model's categories start at 0.
...for each detection in this batch
...if this is a non-empty batch
...for each detection batch
...try
for testing
"MegaDetector was trained with batch size of 1, and the resizing function is a part"
of the inference graph
"change from [y1, x1, y2, x2] to [x1, y1, width, height]"
convert numpy floats to Python floats
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size
"np_images = [np.asarray(image, np.uint8) for image in images]"
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)"
performs inference
our batch size is 1; need to loop the batch dim if supporting batch size > 1
"%% Constants, imports, environment"
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Numpy FutureWarnings from tensorflow import
Useful hack to force CPU inference
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
An enumeration of failure reasons
Number of decimal places to round to for confidence and bbox coordinates
Label mapping for MegaDetector
Should we allow classes that don't look anything like the MegaDetector classes?
"Each version of the detector is associated with some ""typical"" values"
"that are included in output files, so that downstream applications can"
use them as defaults.
%% Classes
Stick this into filenames before the extension for the rendered result
%% Utility functions
mps backend only available in torch >= 1.12.0
%% Main function
Dictionary mapping output file names to a collision-avoidance count.
""
"Since we'll be writing a bunch of files to the same folder, we rename"
as necessary to avoid collisions.
...def input_file_to_detection_file()
Image is modified in place
...for each image
...def load_and_run_detector()
%% Command-line driver
Must specify either an image file or a directory
"but for a single image, args.image_dir is also None"
%% Interactive driver
%%
#####
""
process_video.py
""
"Split a video (or folder of videos) into frames, run the frames through run_detector_batch.py,"
and optionally stitch together results into a new video with detection boxes.
""
TODO: allow video rendering when processing a whole folder
TODO: allow video rendering from existing results
""
#####
"%% Constants, imports, environment"
Only relevant if render_output_video is True
Folder to use for extracted frames
Folder to use for rendered frames (if rendering output video)
Should we render a video with detection boxes?
""
"Only supported when processing a single video, not a folder."
"If we are rendering boxes to a new video, should we keep the temporary"
rendered frames?
Should we keep the extracted frames?
%% Main functions
"TODO: keep track of whether we created this folder, delete if we're deleting the extracted"
"frames and we created the folder, and the output files aren't in the same folder.  For now,"
we're just deleting the extracted frames and leaving the empty folder around in this case.
Render detections to images
"TODO: keep track of whether we created this folder, delete if we're deleting the rendered"
"frames and we created the folder, and the output files aren't in the same folder.  For now,"
we're just deleting the rendered frames and leaving the empty folder around in this case.
Combine into a video
Delete the temporary directory we used for detection images
shutil.rmtree(rendering_output_dir)
(Optionally) delete the frames on which we ran MegaDetector
shutil.rmtree(frame_output_folder)
...process_video()
# Validate options
# Split every video into frames
# Run MegaDetector on the extracted frames
# (Optionally) delete the frames on which we ran MegaDetector
shutil.rmtree(frame_output_folder)
# Convert frame-level results to video-level results
...process_video_folder()
%% Interactive driver
%% Process a folder of videos
import clipboard; clipboard.copy(cmd)
%% Process a single video
import clipboard; clipboard.copy(cmd)
"%% Render a folder of videos, one file at a time"
import clipboard; clipboard.copy(s)
%% Command-line driver
Lint as: python3
Copyright 2020 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TPU is automatically inferred if tpu_name is None and
we are running under cloud ai-platform.
""
copy_checkpoints.py
""
Run this script with specified source_dir and target_dir while the model is training to make a copy
of every checkpoint (checkpoints are kept once an hour by default and is difficult to adjust)
""
do not copy event or evaluation results
input validation
plot the images
adjust the figure
"read in dataset CSV and create merged (dataset, location) col"
map label to label_index
load the splits
only weight the training set by detection confidence
TODO: consider weighting val and test set as well
isotonic regression calibration of MegaDetector confidence
treat each split separately
new_weight[i] = confidence[i] * (n / c) / total_confidence(i's label)
- n = # examples in split (weighted by confidence); c = # labels
- weight allocated to each label is n/c
"- within each label, weigh each example proportional to confidence"
- new weights sum to n
error checking
"maps output label name to set of (dataset, dataset_label) tuples"
find which other label (label_b) has intersection
input validation
create label index JSON
look into sklearn.preprocessing.MultiLabelBinarizer
Note: JSON always saves keys as strings!
"True for ground truth, False for MegaDetector"
always save as .jpg for consistency
get bounding boxes
convert from category ID to category name
"check if crops are already downloaded, and ignore bboxes below the"
confidence threshold
assign all images without location info to 'unknown_location'
remove images from labels that have fewer than min_locs locations
merge dataset and location into a single string '<dataset>/<location>'
"create DataFrame of counts. rows = locations, columns = labels"
label_count: label => number of examples
loc_count: label => number of locs containing that label
generate a new split
score the split
SSE for # of images per label (with 2x weight)
SSE for # of locs per label
label => list of datasets to prioritize for test and validation sets
"merge dataset and location into a tuple (dataset, location)"
sorted smallest to largest
greedily add to test set until it has >= 15% of images
sort the resulting locs
"modify loc_to_size in place, so copy its keys before iterating"
arguments relevant to both creating the dataset CSV and splits.json
arguments only relevant for creating the dataset CSV
arguments only relevant for creating the splits JSON
comment lines starting with '#' are allowed
""
prepare_classification_script.py
""
Notebook-y script used to prepare a series of shell commands to run a classifier
(other than MegaClassifier) on a MegaDetector result set.
""
Differs from prepare_classification_script_mc.py only in the final class mapping step.
""
%% Job options
%% Constants
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
%% Set up environment
commands.append('cd CameraTraps/classification\n')
commands.append('conda activate cameratraps-classifier\n')
%% Crop images
fn = input_files[0]
%% Run classifier
fn = input_files[0]
%% Merge classification and detection outputs
fn = input_files[0]
%% Write everything out
input validation
"Evaluating with accimage is much faster than Pillow or Pillow-SIMD, but accimage"
is Linux-only.
create output directory
override saved params with kwargs
"For now, we don't weight crops by detection confidence during"
evaluation. But consider changing this.
"create model, compile with TorchScript if given checkpoint is not compiled"
"verify that target names matches original ""label names"" from dataset"
"if the dataset does not already have a 'other' category, then the"
'other' category must come last in label_names to avoid conflicting
with an existing label_id
define loss function (criterion)
"this file ends up being huge, so we GZIP compress it"
double check that the accuracy metrics are computed properly
save the confusion matrices to .npz
save per-label statistics
set dropout and BN layers to eval mode
"even if batch contains sample weights, don't use them"
Do target mapping on the outputs (unnormalized logits) instead of
"the normalized (softmax) probabilities, because the loss function"
uses unnormalized logits. Summing probabilities is equivalent to
log-sum-exp of unnormalized logits.
"a confusion matrix C is such that C[i,j] is the # of observations known to"
be in group i and predicted to be in group j.
match pytorch EfficientNet model names
images dataset
"for smaller disk / memory usage, we cache the raw JPEG bytes instead"
of the decoded Tensor
convert JPEG bytes to a 3D uint8 Tensor
"keras EfficientNet already includes normalization from [0, 255] to [0, 1],"
so we don't need to do that here
labels dataset
img_files dataset
weights dataset
define the transforms
efficientnet data preprocessing:
- train:
"1) random crop: aspect_ratio_range=(0.75, 1.33), area_range=(0.08, 1.0)"
2) bicubic resize to img_size
3) random horizontal flip
- test:
1) center crop
2) bicubic resize to img_size
weights sums to:
- if weight_by_detection_conf: (# images in split - conf delta)
- otherwise: (# images in split)
"freeze the base model's weights, including BatchNorm statistics"
https://www.tensorflow.org/guide/keras/transfer_learning#fine-tuning
rebuild output
input validation
set seed
create logdir and save params
create dataloaders and log the index_to_label mapping
define loss function and optimizer
using EfficientNet training defaults
- batch norm momentum: 0.99
"- optimizer: RMSProp, decay 0.9 and momentum 0.9"
- epochs: 350
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs"
- weight decay: 1e-5
TODO: change weighted to False if oversampling minority classes
stop training after 8 epochs without improvement
log metrics
log confusion matrix
log tp/fp/fn images
"tf.summary.image requires input of shape [N, H, W, C]"
false positive for top3_pred[0]
false negative for label
"if evaluating or finetuning, set dropout & BN layers to eval mode"
"for each label, track 5 most-confident and least-confident examples"
"even if batch contains sample weights, don't use them"
we do not track L2-regularization loss in the loss metric
This dictionary will get written out at the end of this process; store
diagnostic variables here
error checking
refresh detection cache
save log of bad images
cache of Detector outputs: dataset name => {img_path => detection_dict}
img_path: <dataset-name>/<img-filename>
get SAS URL for images container
strip image paths of dataset name
save list of dataset names and task IDs for resuming
complete task name: 'detect_for_classifier_caltech_20200722_110816_task01'
HACK! Sleep for 10s between task submissions in the hopes that it
"decreases the chance of backend JSON ""database"" corruption"
task still running => continue
"task finished successfully, save response to disk"
error checking before we download and crop any images
convert from category ID to category name
we need the datasets table for getting SAS keys
"we already did all error checking above, so we don't do any here"
get ContainerClient
get bounding boxes
we must include the dataset <ds> in <crop_path_template> because
'{img_path}' actually gets populated with <img_file> in
load_and_crop()
"get the image, either from disk or from Blob Storage"
inelegant way to close the container_clients
""
prepare_classification_script_mc.py
""
Notebook-y script used to prepare a series of shell commands to run MegaClassifier
on a MegaDetector result set.
""
Differs from prepare_classification_script.py only in the final class mapping step.
""
%% Job options
%% Constants
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
%% Set up environment
commands.append('cd CameraTraps/classification\n')
commands.append('conda activate cameratraps-classifier\n')
%% Crop images
fn = input_files[0]
%% Run classifier
fn = input_files[0]
%% Remap classifier outputs
fn = input_files[0]
%% Merge classification and detection outputs
fn = input_files[0]
%% Write everything out
mean/std values from https://pytorch.org/docs/stable/torchvision/models.html
define the transforms
resizes smaller edge to img_size
weights sums to:
- if weight_by_detection_conf: (# images in split - conf delta)
- otherwise: # images in split
for normal (non-weighted) shuffling
set all parameters to not require gradients except final FC layer
replace final fully-connected layer (which has 1000 ImageNet classes)
"detect GPU, use all if available"
input validation
set seed
create logdir and save params
create dataloaders and log the index_to_label mapping
create model
define loss function and optimizer
using EfficientNet training defaults
- batch norm momentum: 0.99
"- optimizer: RMSProp, decay 0.9 and momentum 0.9"
- epochs: 350
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs"
- weight decay: 1e-5
stop training after 8 epochs without improvement
do a complete evaluation run
log metrics
log confusion matrix
log tp/fp/fn images
"for every image: undo normalization, clamp to [0, 1], CHW -> HWC"
"- cannot be in-place, because the HeapItem might be in multiple heaps"
writer.add_figure() has issues => using add_image() instead
"writer.add_figure(f'{label_name}/{tag}', fig, global_step=epoch)"
false positive for top3_pred[0]
false negative for label
"preds and labels both have shape [N, k]"
"if evaluating or finetuning, set dropout and BN layers to eval mode"
"for each label, track k_extreme most-confident and least-confident images"
"even if batch contains sample weights, don't use them"
lock before changing ImageFile.LOAD_TRUNCATED_IMAGES
input validation
use MegaDB to generate list of images
only keep images that:
"1) end in a supported file extension, and"
2) actually exist in Azure Blob Storage
3) belong to a label with at least min_locs locations
write out log of images / labels that were removed
"save label counts, pre-subsampling"
"save label counts, post-subsampling"
spec_dict['taxa']: list of dict
[
"{'level': 'family', 'name': 'cervidae', 'datasets': ['idfg']},"
"{'level': 'genus',  'name': 'meleagris'}"
]
"spec_dict['dataset_labels']: dict, dataset => list of dataset_label"
{
"""idfg"": [""deer"", ""elk"", ""prong""],"
"""idfg_swwlf_2019"": [""elk"", ""muledeer"", ""whitetaileddeer""]"
}
"maps output label name to set of (dataset, dataset_label) tuples"
"because MegaDB is organized by dataset, we do the same"
ds_to_labels = {
'dataset_name': {
"'dataset_label': [output_label1, output_label2]"
}
}
we need the datasets table for getting full image paths
The line
"[img.class[0], seq.class[0]][0] as class"
selects the image-level class label if available. Otherwise it selects the
"sequence-level class label. This line assumes the following conditions,"
expressed in the WHERE clause:
- at least one of the image or sequence class label is given
- the image and sequence class labels are arrays with length at most 1
- the image class label takes priority over the sequence class label
""
"In Azure Cosmos DB, if a field is not defined, then it is simply excluded"
"from the result. For example, on the following JSON object,"
{
"""dataset"": ""camera_traps"","
"""seq_id"": ""1234"","
"""location"": ""A1"","
"""images"": [{""file"": ""abcd.jpeg""}],"
"""class"": [""deer""],"
}
"the array [img.class[0], seq.class[0]] just gives ['deer'] because"
img.class is undefined and therefore excluded.
"if no path prefix, set it to the empty string '', because"
"os.path.join('', x, '') = '{x}/'"
result keys
"- already has: ['dataset', 'location', 'file', 'class', 'bbox']"
"- add ['label'], remove ['file']"
"if img is mislabeled, but we don't know the correct class, skip it"
"otherwise, update the img with the correct class, but skip the"
img if the correct class is not one we queried for
sort keys for determinism
we need the datasets table for getting SAS keys
strip leading '?' from SAS token
only check Azure Blob Storage
check local directory first before checking Azure Blob Storage
1st pass: populate label_to_locs
"label (tuple of str) => set of (dataset, location)"
2nd pass: eliminate bad images
prioritize is a list of prioritization levels
number of already matching images
main(
"label_spec_json_path='idfg_classes.json',"
"taxonomy_csv_path='../../camera-traps-private/camera_trap_taxonomy_mapping.csv',"
"output_dir='run_idfg',"
json_indent=4)
recursively find all files in cropped_images_dir
only find crops of images from detections JSON
resizes smaller edge to img_size
"Evaluating with accimage is much faster than Pillow or Pillow-SIMD, but accimage"
is Linux-only.
create dataset
create model
set dropout and BN layers to eval mode
load files
dataset => set of img_file
crop_path: <dataset>/<img_file>___cropXX_mdvY.Y.jpg
[----<img_path>----]
error checking
any row with 'correct_class' should be marked 'mislabeled'
filter to only the mislabeled rows
convert '\' to '/'
verify that overlapping indices are the same
"""add"" any new mislabelings"
write out results
error checking
load detections JSON
get detector version
convert from category ID to category name
copy keys to modify dict in-place
This will be removed later when we filter for animals
save log of bad images
"True for ground truth, False for MegaDetector"
always save as .jpg for consistency
"we already did all error checking above, so we don't do any here"
"get the image, either from disk or from Blob Storage"
inelegant way to close the container_client
"crop_path => normalized bbox coordinates [xmin, ymin, width, height]"
"only ground-truth bboxes do not have a ""confidence"" value"
try loading image from local directory
try to download image from Blob Storage
crop the image
"expand box width or height to be square, but limit to img size"
"Image.crop() takes box=[left, upper, right, lower]"
pad to square using 0s
Author: lukemelas (github username)
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch
With adjustments and added comments by workingcoder (github username).
Support the construction of 'efficientnet-l2' without pretrained weights
Expansion phase (Inverted Bottleneck)
"image_size = calculate_output_image_size(image_size, 1) <-- this wouldn't modify image_size"
Depthwise convolution phase
"Squeeze and Excitation layer, if desired"
Pointwise convolution phase
Expansion and Depthwise Convolution
Squeeze and Excitation
Pointwise Convolution
Skip connection and drop connect
The combination of skip connection and drop connect brings about stochastic depth.
Batch norm parameters
Get stem static or dynamic convolution depending on image size
Stem
Build blocks
Update block input and output filters based on depth multiplier.
The first block needs to take care of stride and filter size increase.
"image_size = calculate_output_image_size(image_size, block_args.stride)  # stride = 1"
Head
Final linear layer
Stem
Blocks
Head
Stem
Blocks
Head
Convolution layers
Pooling and final linear layer
Author: lukemelas (github username)
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch
With adjustments and added comments by workingcoder (github username).
###############################################################################
## Help functions for model architecture
###############################################################################
GlobalParams and BlockArgs: Two namedtuples
Swish and MemoryEfficientSwish: Two implementations of the method
round_filters and round_repeats:
Functions to calculate params for scaling model width and depth ! ! !
get_width_and_height_from_size and calculate_output_image_size
drop_connect: A structural design
get_same_padding_conv2d:
Conv2dDynamicSamePadding
Conv2dStaticSamePadding
get_same_padding_maxPool2d:
MaxPool2dDynamicSamePadding
MaxPool2dStaticSamePadding
"It's an additional function, not used in EfficientNet,"
but can be used in other model (such as EfficientDet).
"Parameters for the entire model (stem, all blocks, and head)"
Parameters for an individual model block
Set GlobalParams and BlockArgs's defaults
An ordinary implementation of Swish function
A memory-efficient implementation of Swish function
TODO: modify the params names.
"maybe the names (width_divisor,min_width)"
"are more suitable than (depth_divisor,min_depth)."
follow the formula transferred from official TensorFlow implementation
follow the formula transferred from official TensorFlow implementation
"generate binary_tensor mask according to probability (p for 0, 1-p for 1)"
Note:
The following 'SamePadding' functions make output size equal ceil(input size/stride).
"Only when stride equals 1, can the output size be the same as input size."
Don't be confused by their function names ! ! !
Tips for 'SAME' mode padding.
Given the following:
i: width or height
s: stride
k: kernel size
d: dilation
p: padding
Output after Conv2d:
o = floor((i+p-((k-1)*d+1))/s+1)
"If o equals i, i = floor((i+p-((k-1)*d+1))/s+1),"
=> p = (i-1)*s+((k-1)*d+1)-i
With the same calculation as Conv2dDynamicSamePadding
Calculate padding based on image size and save it
Calculate padding based on image size and save it
###############################################################################
## Helper functions for loading model params
###############################################################################
BlockDecoder: A Class for encoding and decoding BlockArgs
efficientnet_params: A function to query compound coefficient
get_model_params and efficientnet:
Functions to get BlockArgs and GlobalParams for efficientnet
url_map and url_map_advprop: Dicts of url_map for pretrained weights
load_pretrained_weights: A function to load pretrained weights
Check stride
"Coefficients:   width,depth,res,dropout"
Blocks args for the whole model(efficientnet-b0 by default)
It will be modified in the construction of EfficientNet Class according to model
note: all models have drop connect rate = 0.2
ValueError will be raised here if override_params has fields not included in global_params.
train with Standard methods
check more details in paper(EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks)
train with Adversarial Examples(AdvProp)
check more details in paper(Adversarial Examples Improve Image Recognition)
TODO: add the petrained weights url map of 'efficientnet-l2'
AutoAugment or Advprop (different preprocessing)
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils)
from data_management.megadb.schema import sequences_schema_check
resize is for displaying them more quickly
dataset and seq_id are required fields
sort the images in the sequence
"if no class label on the image, show class label on the sequence"
pool = ThreadPool()
"print('len of rendering_info', len(rendering_info))"
"tqdm(pool.imap_unordered(render_image_info_partial, rendering_info), total=len(rendering_info))"
options = write_html_image_list()
options['headerHtml']
print('Checking that the MegaDB entries conform to the schema...')
sequences_schema_check.sequences_schema_check(sequences)
#######
""
visualize_db.py
""
Outputs an HTML page visualizing annotations (class labels and/or bounding boxes)
on a sample of images in a database in the COCO Camera Traps format
""
#######
%% Imports
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils)
Assumes the cameratraps repo root is on the path
%% Settings
Set to None to visualize all images
Target size for rendering; set either dimension to -1 to preserve aspect ratio
"These are mutually exclusive; both are category names, not IDs"
"Special tag used to say ""show me all images with multiple categories"""
We sometimes flatten image directories by replacing a path separator with
another character.  Leave blank for the typical case where this isn't necessary.
Control rendering parallelization
Process-based parallelization in this function is currently unsupported
"due to pickling issues I didn't care to look at, but I'm going to just"
"flip this with a warning, since I intend to support it in the future."
%% Helper functions
"Translate the file name in an image entry in the json database to a path, possibly doing"
some manipulation of path separators
%% Core functions
"Optionally remove all images without bounding boxes, *before* sampling"
"Optionally include/remove images with specific labels, *before* sampling"
Put the annotations in a dataframe so we can select all annotations for a given image
Construct label map
Take a sample of images
Set of dicts representing inputs to render_db_bounding_boxes:
""
"bboxes, boxClasses, image_path"
All the class labels we've seen for this image (with out without bboxes)
Iterate over annotations for this image
iAnn = 0; anno = annos_i.iloc[iAnn]
...for each of this image's annotations
"We're adding html for an image before we render it, so it's possible this image will"
fail to render.  For applications where this script is being used to debua a database
"(the common case?), this is useful behavior, for other applications, this is annoying."
...for each image
...def render_image_info
def process_images(...)
%% Command-line driver
"Copy all fields from a Namespace (i.e., the output from parse_args) to an object."
""
Skips fields starting with _.  Does not check existence in the target object.
Convert to an options object
%% Interactive driver(s)
%%
os.startfile(htmlOutputFile)
%% Constants and imports
convert category ID from int to str
Retry on blob storage read failures
%% Functions
PIL.Image.convert() returns a converted copy of this image
Alter orientation as needed according to EXIF tag 0x112 (274) for Orientation
""
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28
https://www.media.mit.edu/pia/Research/deepview/exif.html
""
Null operation
Aspect ratio as width over height
ar = w / h
h = w / ar
w = ar * h
The following three functions are modified versions of those at:
""
https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py
Convert to pixels so we can use the PIL crop() function
PIL's crop() does surprising things if you provide values outside of
"the image, clip inputs"
...if this detection is above threshold
...for each detection
"list of lists, one list of strings for each bounding box (to accommodate multiple labels)"
for color selection
"Always render objects with a confidence of ""None"", this is typically used"
for ground truth data.
"{} is the default, which means ""show labels with no mapping"", so don't use ""if label_map"" here"
if label_map:
"To avoid duplicate colors with detection-only visualization, offset"
the classification class index by the number of detection classes
...for each classification
...if we have classification results
...if the confidence of this detection is above threshold
...for each detection
"print('Input must be of size [N, 4], but is ' + str(boxes_shape))"
Deliberately trimming to the width of the image only in the case where
"box expansion is turned on.  There's not an obvious correct behavior here,"
but the thinking is that if the caller provided an out-of-range bounding
"box, they meant to do that, but at least in the eyes of the person writing"
"this comment, if you expand a box for visualization reasons, you don't want"
to end up with part of a box.
""
A slightly more sophisticated might check whether it was in fact the expansion
"that made this box larger than the image, but this is the case 99.999% of the time"
"here, so that doesn't seem necessary."
...if we need to expand boxes
If the total height of the display strings added to the top of the bounding
"box exceeds the top of the image, stack the strings below the bounding box"
instead of above.
Each display_str has a top and bottom margin of 0.05x.
Reverse list and print from bottom to top.
Skip empty strings
"list of lists, one list of strings for each bounding box (to accommodate multiple labels)"
need to be a string here because PIL needs to iterate through chars
""
stacked bar charts are made with each segment starting from a y position
"To fit the legend in, shrink current axis by 20%"
Put a legend to the right of the current axis
####
""
visualize_incoming_annotations.py
""
Spot-check the annotations received from iMerit by visualizing annotated bounding
boxes on a sample of images and display them in HTML.
""
"Modified in 2021 March to use the new format (iMerit batch 12 onwards), which is a"
COCO formatted JSON with relative coordinates for the bbox.
""
from data_management.megadb.schema import sequences_schema_check
we used frame_num of 1 when sending out images to annotators when it is not explicitly stored (wcs esp.)
The file_name field in the incoming json looks like
alka_squirrels.seq2020_05_07_25C.frame119221.jpg
"we need to use the dataset, sequence and frame info to find the actual path in blob storage"
using the sequences
category_id 5 is No Object Visible
download the image
Write to HTML
allow forward references in typing annotations
class variables
instance variables
get path to root
taxonomy CSV rows without 'taxonomy_string' entries are excluded
"from the taxonomy graph, but can be included in a classification"
label specification JSON via the 'dataset_labels' key
use the lower parent
special cases
%% Imports
%% Taxnomy checking
taxonomy CSV rows without 'taxonomy_string' entries are excluded
"from the taxonomy graph, but can be included in a classification"
label specification JSON via the 'dataset_labels' key
...for each row in the taxnomy file
%% Command-line driver
%% Interactive driver
%%
which datasets are already processed?
"sequence-level query should be fairly fast, ~1 sec"
cases when the class field is on the image level (images in a sequence
"that had different class labels, 'caltech' dataset is like this)"
"this query may take a long time, >1hr"
"this query should be fairly fast, ~1 sec"
read species presence info from the JSON files for each dataset
has this class name appeared in a previous dataset?
columns to populate the spreadsheet
sort by descending species count
make the spreadsheet
hyperlink Bing search URLs
hyperlink example image SAS URLs
TODO hardcoded columns: change if # of examples or col_order changes
""
map_lila_taxonomy_to_wi_taxonomy.py
""
Loads the LILA category mapping (in which taxonomy information comes from an iNat taxonomy snapshot)
and tries to map each class to the Wildlife Insights taxonomy.
""
%% Constants and imports
Created by get_lila_category_list.py... contains counts for each category
This is a manually-curated file used to store mappings that had to be made manually
This is the main output file from this whole process
%% Load category and taxonomy files
%% Pull everything out of pandas
%% Cache WI taxonomy lookups
This is just a handy lookup table that we'll use to debug mismatches
taxon = wi_taxonomy[21653]; print(taxon)
Look for keywords that don't refer to specific taxa: blank/animal/unknown
Do we have a species name?
"If 'species' is populated, 'genus' should always be populated; one item currently breaks"
this rule.
...for each taxon
%% Find redundant taxa
%% Manual review of redundant taxa
%% Clean up redundant taxa
taxon_name = list(taxon_name_to_preferred_taxon_id.keys())[0]
"If we've gotten this far, we should be choosing from multiple taxa."
""
"This will become untrue if any of these are resolved later, at which point we shoudl"
remove them from taxon_name_to_preferred_id
Choose the preferred taxa
%% Read supplementary mappings
"Each line is [lila query],[WI taxon name],[notes]"
%% Map LILA categories to WI categories
Must be ordered from kingdom --> species
TODO:
"['subspecies','variety']"
i_taxon = 0; taxon = lila_taxonomy[i_taxon]; print(taxon)
"Go from kingdom --> species, choosing the lowest-level description as the query"
"E.g., 'car'"
"print('Made a supplementary mapping from {} to {}'.format(query,wi_taxon['taxon_name']))"
print('No match for {}'.format(query))
...for each LILA taxon
%% Manual mapping
%% Build a dictionary from LILA dataset names and categories to LILA taxa
i_d = 0; d = lila_taxonomy[i_d]
"%% Map LILA datasets to WI taxa, and count the number of each taxon available in each dataset"
dataset_name = list(lila_dataset_to_categories.keys())[0]
dataset_category = dataset_categories[0]
"Write out the dataset name, category name, WI GUID, WI scientific name, WI common name,"
and count
...for each category in this dataset
...for each dataset
...with open()
""
retrieve_sample_image.py
""
"Downloader that retrieves images from Google images, used for verifying taxonomy"
"lookups and looking for egregious mismappings (e.g., ""snake"" being mapped to a fish called"
"""snake"")."
""
"Simple wrapper around simple_image_download, but I've had to swap in and out the underlying"
downloader a few times.
""
%% Imports and environment
%%
%% Main entry point
%% Test driver
%%
""
"Using the taxonomy .csv file, map all LILA datasets to the standard taxonomy"
""
Does not currently produce results; this is just used to confirm that all category names
have mappings in the taxonomy file.
""
%% Constants and imports
Created by get_lila_category_list.py... contains counts for each category
%% Load category and taxonomy files
%% Map dataset names and category names to scientific names
i_row = 1; row = taxonomy_df.iloc[i_row]; row
"%% For each dataset, make sure we can map every category to the taxonomy"
dataset_name = list(lila_dataset_to_categories.keys())[0]
c = categories[0]
""
"Takes the megadb taxonomy mapping, extracts the rows that are relevant to"
"LILA, and does some cleanup."
""
%% Constants and imports
This is a partially-completed taxonomy file that was created from a different set of
"scripts, but covers *most* of LILA as of June 2022"
Created by get_lila_category_list.py
%% Read the input files
Get everything out of pandas
"%% Find all unique dataset names in the input list, compare them with data names from LILA"
d = input_taxonomy_rows[0]
The script that generates this dictionary creates a separate entry for bounding box
"metadata files, but those don't represent new dataset names"
%% Map input columns to output datasets
Make sure all of those datasets actually correspond to datasets on LILA
%% Re-write the input taxonomy file to refer to LILA datasets
Map the string datasetname:token to a taxonomic tree json
mapping = input_taxonomy_rows[0]
Make sure that all occurrences of this mapping_string give us the same output
assert taxonomy_string == taxonomy_mappings[mapping_string]
%% Re-write the input file in the target format
mapping_string = list(taxonomy_mappings.keys())[0]
""
prepare_lila_taxonomy_release.py
""
"Given the private intermediate taxonomy mapping, prepare the public (release)"
taxonomy mapping file.
""
%% Imports and constants
Created by get_lila_category_list.py... contains counts for each category
%% Find out which categories are actually used
dataset_name = datasets_to_map[0]
i_row = 0; row = df.iloc[i_row]; row
%% Generate the final output file
i_row = 0; row = df.iloc[i_row]; row
match_at_level = taxonomic_match[0]
i_row = 0; row = df.iloc[i_row]; row
"E.g.: (43117, 'genus', 'lepus', ['hares and jackrabbits']"
###############
---> CONSTANTS
###############
max_progressbar = count * (list(range(limit+1))[-1]+1)
"bar = progressbar.ProgressBar(maxval=max_progressbar,"
"widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()]).start()"
bar.update(bar.currval + 1)
bar.finish()
""
"Given a subset of LILA datasets, find all the categories, and start the taxonomy"
mapping process.
""
%% Constants and imports
Created by get_lila_category_list.py
'NACTI'
'Channel Islands Camera Traps'
%% Read the list of datasets
The script that generates this dictionary creates a separate entry for bounding box
"metadata files, but those don't represent new dataset names"
%% Find all categories
dataset_name = datasets_to_map[0]
%% Initialize taxonomic lookup
from taxonomy_mapping.species_lookup import (
"get_taxonomic_info, print_taxonomy_matche)"
%% Manual lookup
%%
q = 'white-throated monkey'
raise ValueError('')
%% Match every query against our taxonomies
mapping_string = category_mappings[1]; print(mapping_string)
...for each mapping
%% Write output rows
""
"Does some consistency-checking on the LILA taxonomy file, and generates"
an HTML preview page that we can use to determine whether the mappings
make sense.
""
%% Imports and constants
"lila_taxonomy_file = r""G:\git\agentmorrisprivate\lila-taxonomy\lila-taxonomy-mapping.csv"""
"lila_taxonomy_file = r""G:\temp\lila\lila_additions_2022.06.29.csv"""
%% Support functions
%% Read the taxonomy mapping file
%% Prepare taxonomy lookup
from taxonomy_mapping.species_lookup import (
"get_taxonomic_info, print_taxonomy_matche)"
%% Optionally remap all gbif-based mappings to inat (or vice-versa)
%%
i_row = 1; row = df.iloc[i_row]; row
This should be zero for the release .csv
%%
%% Check for mappings that disagree with the taxonomy string
Look for internal inconsistency
Look for outdated mappings
i_row = 0; row = df.iloc[i_row]
%% List null mappings
""
"These should all be things like ""unidentified"" and ""fire"""
""
i_row = 0; row = df.iloc[i_row]
%% List mappings with scientific names but no common names
%% List mappings that map to different things in different data sets
x = suppress_multiple_matches[-1]
...for each row where we saw this query
...for each row
"%% Verify that nothing ""unidentified"" maps to a species or subspecies"
"E.g., ""unidentified skunk"" should never map to a specific species of skunk"
%% Make sure there are valid source and level values for everything with a mapping
%% Find WCS mappings that aren't species or aren't the same as the input
"WCS used scientific names, so these remappings are slightly more controversial"
then the standard remappings.
row = df.iloc[-500]
"Anything marked ""species"" or ""unknown"" by definition doesn't map to a species,"
so ignore these.
print('WCS query {} ({}) remapped to {} ({})'.format(
"query,common_name,scientific_name,common_names_from_taxonomy))"
%% Download sample images for all scientific names
i_row = 0; row = df.iloc[i_row]
if s != 'mirafra':
continue
Check whether we already have enough images for this query
"print('Skipping query {}, already have {} images'.format(s,len(sizes_above_threshold)))"
Check whether we've already run this query for a previous row
...for each row in the mapping table
%% Rename .jpeg to .jpg
"print('Renaming {} to {}'.format(fn,new_fn))"
%% Choose representative images for each scientific name
s = list(scientific_name_to_paths.keys())[0]
Be suspicious of duplicate sizes
...for each scientific name
%% Delete unused images
%% Produce HTML preview
i_row = 2; row = df.iloc[i_row]
"image_paths = [os.path.relpath(p, output_base) for p in image_paths]"
...for each row
%% Open HTML preview
######
""
species_lookup.py
""
Look up species names (common or scientific) in the GBIF and iNaturalist
taxonomies.
""
Run initialize_taxonomy_lookup() before calling any other function.
""
######
%% Constants and imports
As of 2020.05.12:
""
"GBIF: ~777MB zipped, ~1.6GB taxonomy"
"iNat: ~2.2GB zipped, ~51MB taxonomy"
These are un-initialized globals that must be initialized by
the initialize_taxonomy_lookup() function below.
%% Functions
Initialization function
# Load serialized taxonomy info if we've already saved it
"# If we don't have serialized taxonomy info, create it from scratch."
Download and unzip taxonomy files
taxonomy_name = list(taxonomy_urls.items())[0][0]; zip_url = list(taxonomy_urls.items())[0][1]
Don't download the zipfile if we've already unzipped what we need
Bypasses download if the file exists already
Unzip the files we need
...for each file that we need from this zipfile
Remove the zipfile
os.remove(zipfile_path)
...for each taxonomy
"Create dataframes from each of the taxonomy files, and the GBIF common"
name file
Load iNat taxonomy
Load GBIF taxonomy
Remove questionable rows from the GBIF taxonomy
Load GBIF vernacular name mapping
Only keep English mappings
Convert everything to lowercase
"For each taxonomy table, create a mapping from taxon IDs to rows"
Create name mapping dictionaries
Build iNat dictionaries
row = inat_taxonomy.iloc[0]
Build GBIF dictionaries
"The canonical name is the Latin name; the ""scientific name"""
include the taxonomy name.
""
http://globalnames.org/docs/glossary/
This only seems to happen for really esoteric species that aren't
"likely to apply to our problems, but doing this for completeness."
Don't include taxon IDs that were removed from the master table
Save everything to file
...def initialize_taxonomy_lookup()
"list of dicts: {'source': source_name, 'taxonomy': match_details}"
i_match = 0
"list of (taxon_id, taxonRank, scientific name, [vernacular names])"
corresponding to an exact match and its parents
Walk taxonomy hierarchy
This can happen because we remove questionable rows from the
GBIF taxonomy
"print(f'Warning: no row exists for parent_taxon_id {parent_taxon_id},' + \"
"f'child taxon_id: {taxon_id}, query: {query}')"
The GBIF taxonomy contains unranked entries
...while there is taxonomy left to walk
...for each match
Remove redundant matches
i_tree_a = 0; tree_a = matching_trees[i_tree_a]
i_tree_b = 1; tree_b = matching_trees[i_tree_b]
"If tree a's primary taxon ID is inside tree b, discard tree a"
""
taxonomy_level_b = tree_b['taxonomy'][0]
...for each level in taxonomy B
...for each tree (inner)
...for each tree (outer)
...def traverse_taxonomy()
"print(""Finding taxonomy information for: {0}"".format(query))"
"In GBIF, some queries hit for both common and scientific, make sure we end"
up with unique inputs
"If the species is not found in either taxonomy, return None"
Both GBIF and iNat have a 1-to-1 mapping between taxon_id and row number
Walk both taxonomies
...def get_taxonomic_info()
m = matches[0]
"For example: [(9761484, 'species', 'anas platyrhynchos')]"
...for each taxonomy level
...for each match
...def print_taxonomy_matches()
%% Taxonomy functions that make subjective judgements
query = 'person'
Do we have an iNat match?
"print_taxonomy_matches(inat_matches, verbose=True)"
"print_taxonomy_matches(gbif_matches, verbose=True)"
print('Warning: multiple iNat matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple iNat common names for {query}')
Default to returning the query
"print(f'Matched iNat {query} to {scientific_name},{common_name}')"
...if we had iNat matches
If we either prefer GBIF or didn't have iNat matches
""
Code is deliberately redundant here; I'm expecting some subtleties in how
handle GBIF and iNat.
print('Warning: multiple GBIF matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple GBIF common names for {query}')
Default to returning the query
...if we needed to look in the GBIF taxonomy
...def _get_preferred_taxonomic_match()
%% Interactive drivers and debug
%% Initialization
%% Taxonomic lookup
query = 'lion'
print(matches)
Print the taxonomy in the taxonomy spreadsheet format
%% Directly access the taxonomy tables
%% Command-line driver
Read command line inputs (absolute path)
Read the tokens from the input text file
Loop through each token and get scientific name
""
process_species_by_dataset
""
We generated a list of all the annotations in our universe; this script is
used to (interactively) map them onto the GBIF and iNat taxonomies.  Don't
"try to run this script from top to bottom; it's used like a notebook, not like"
"a script, since manual review steps are required."
""
%% Imports
%autoreload 0
%autoreload -species_lookup
%% Constants
Input file
Output file after automatic remapping
File to which we manually copy that file and do all the manual review; this
should never be programmatically written to
The final output spreadsheet
HTML file generated to facilitate the identificaiton of egregious mismappings
%% Functions
Prefer iNat matches over GBIF matches
query = 'person'
Do we have an iNat match?
"print_taxonomy_matches(inat_matches, verbose=True)"
"print_taxonomy_matches(gbif_matches, verbose=True)"
print('Warning: multiple iNat matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple iNat common names for {query}')
Default to returning the query
"print(f'Matched iNat {query} to {scientific_name},{common_name}')"
...if we had iNat matches
If we either prefer GBIF or didn't have iNat matches
""
Code is deliberately redundant here; I'm expecting some subtleties in how
handle GBIF and iNat.
print('Warning: multiple GBIF matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple GBIF common names for {query}')
Default to returning the query
...if we needed to look in the GBIF taxonomy
...def get_preferred_taxonomic_match()
%% Initialization
%% Test single-query lookup
%%
%%
"q = ""grevy's zebra"""
%% Read the input data
%% Run all our taxonomic lookups
i_row = 0; row = df.iloc[i_row]
query = 'lion'
...for each query
Write to the excel file that we'll use for manual review
%% Download preview images for everything we successfully mapped
uncomment this to load saved output_file
"output_df = pd.read_excel(output_file, keep_default_na=False)"
i_row = 0; row = output_df.iloc[i_row]
...for each query
%% Write HTML file with representative images to scan for obvious mis-mappings
i_row = 0; row = output_df.iloc[i_row]
...for each row
%% Look for redundancy with the master table
Note: `master_table_file` is a CSV file that is the concatenation of the
"manually-remapped files (""manual_remapped.xlsx""), which are the output of"
this script run across from different groups of datasets. The concatenation
"should be done manually. If `master_table_file` doesn't exist yet, skip this"
"code cell. Then, after going through the manual steps below, set the final"
manually-remapped version to be the `master_table_file`.
%% Manual review
Copy the spreadsheet to another file; you're about to do a ton of manual
review work and you don't want that programmatically overwrriten.
""
See manual_review_xlsx above
%% Read back the results of the manual review process
%% Look for manual mapping errors
Manually inspect df for typos in 'dataset_names' and 'taxonomy_level' columns
Identify rows where:
""
- 'taxonomy_level' does not match level of 1st element in 'taxonomy_string'
- 'scientific_name' does not match name of 1st element in 'taxonomy_string'
""
...both of which typically represent manual mapping errors.
i_row = 0; row = df.iloc[i_row]
"I'm not sure why both of these checks are necessary, best guess is that"
the Excel parser was reading blanks as na on one OS/Excel version and as ''
on another.
The taxonomy_string column is a .json-formatted string; expand it into
an object via eval()
"%% Find scientific names that were added manually, and match them to taxonomies"
i_row = 0; row = df.iloc[i_row]
...for each query
%% Write out final version
""
"Generate read-only SAS URLs for all LILA containers, to facilitate partial downloads."
""
The results of this script end up here:
""
http://lila.science/wp-content/uploads/2020/03/lila_sas_urls.txt
""
"Update: that file is manually maintained now, it can't be programmatically generated"
""
%% Imports
Read-only
%% Enumerate containers
%% Generate SAS tokens
%% Generate SAS URLs
%% Write to output file
--------some stuff needed to get AJAX to work with bottle?--------#
-------------------------------------------------------------------------------- #
PREPARE TO QUEUE IMAGES FOR LABELING
-------------------------------------------------------------------------------- #
# Connect as USER to database DB_NAME through peewee and initialize database proxy
# Load embedding model
---------------------------------------------------------------------- #
CREATE QUEUE OF IMAGES TO LABEL
---------------------------------------------------------------------- #
Use classifier to generate predictions
# Update model predicted class in PostgreSQL database
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
-------------------------------------------------------------------------------- #
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI
-------------------------------------------------------------------------------- #
"# static routes (to serve CSS, etc.)"
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps"
"return bottle.static_file(filename, root='../../../../../../../../../../../.')"
# dynamic routes
if data['display_grayscale']:
indices_to_exclude.update(set(color_indices))
elif not data['display_grayscale']:
indices_to_exclude.update(set(grayscale_indices))
data['display_images'] = {}
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices]
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices]
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices]
data['display_images']['detection_categories'] = []
for i in indices:
if str(dataset.samples[i][1]) == 'None':
data['display_images']['detection_categories'].append('None')
else:
existing_category_entries = {cat.id: cat.name for cat in Category.select()}
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()"
data['display_images']['detection_categories'].append(cat_name)
Use image ids in images_to_label to get the corresponding dataset indices
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
print(set(dataset.set_indices[4]).update(set(indices_to_label)))
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Use image id images_to_label to get the corresponding dataset index
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Train on samples that have been labeled so far
dataset.set_kind(DetectionKind.UserDetection.value)
print(y_train)
Predict on the samples that have not been labeled
print(y_pred)
Update model predicted class in PostgreSQL database
timer = time.time()
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
print('Updating the database took %0.2f seconds'%(time.time() - timer))
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
"once the classifier has been trained the first time, switch to AL sampling"
""
make_oneclass_json.py
""
"Takes a coco-camera-traps .json database and collapses species classes to binary,"
optionally removing labels from empty images (to be detector-friendly) (depending on
"""experiment_type"")."
""
"Assumes that empty images are labeled as ""empty""."
""
%% Imports and environment
%% Core conversion function
"We're removing empty images from the annotation list, but not from"
"the ""images"" list; they'll still get used in detector training."
print('Ignoring empty annotation')
%% Interactive driver
%%
Load annotations
Convert from multi-class to one-class
Write out the one-class data
%% Command-line driver
""
plot_bounding_boxes.py
""
Takes a .json database containing bounding boxes and renders those boxes on the
source images.
""
"This assumes annotations in coco-camera-traps format, with absolute bbox"
coordinates.
""
%% Imports and environment
How many images should we process?  Set to -1 to process all images.
Should we randomize the image order?
%%  Read database and build up convenience mappings
Category ID to category name
Image ID to image info
Image ID to image path
Image ID to all annotations referring to this image
"%% Iterate over images, draw bounding boxes, write to file"
For each image
image = images[0]
Build up a list of bounding boxes to draw on this image
Load the image
Create figure and axes
Display the image
ann = imageAnnotations[0]
"For each annotation associated with this image, render bounding box and label"
"In the Rectangle() function, the first argument (""location"") is the bottom-left"
of the rectangle.
""
Origin is the upper-left of the image.
Add the patch to the Axes
Add a class label
This is magic goop that removes whitespace around image plots (sort of)
Write the output image
...for each image
""
plot_imerit_annotations.py
""
Takes a .json file full of bounding box annotations and renders those boxes on the
source images.
""
"This assumes annotations in the format we receive them, specifically:"
""
1) Relative bbox coordinates
"2) A list of .json objects, not a well-formatted .json file"
""
"I.e., don't use this on a COCO-style .json file.  See plot_bounding_boxes.py"
for the same operation performed on a proper COCO-camera-traps database.
""
%% Imports and environment
"os.makedirs(outputBase, exist_ok=True)"
%%  Read all source images and build up a hash table from image name to full path
"This spans training and validation directories, so it's not the same as"
just joining the image name to a base path
"%% Iterate over annotations, draw bounding boxes, write to file"
annData has keys:
""
"annotations, categories, images"
""
Each of these are lists of dictionaries
%% Render all annotations on each image in the sequence
%% Pull out image metadata
Build up a list of bounding boxes to draw on this image
Pull out just the image name from the filename
""
File names look like:
""
seq6efffac2-5567-11e8-b3fe-dca9047ef277.frame1.img59a94e52-23d2-11e8-a6a3-ec086b02610b.jpg
"m = re.findall(r'img(.*\.jpg)$', imgFileName, re.M|re.I)"
print(m)
assert(len(m) == 1)
queryFileName = m[0]
Map this image back to the original directory
"%% Loop over annotations, find annotations that match this image"
%%
"x,y,w,h"
""
"x,y is the bottom-left of the rectangle"
""
"x,y origin is the upper-left"
...for each annotation
%% Render with PIL (scrap)
%% Render with Matplotlib
Create figure and axes
Display the image
Location is the bottom-left of the rect
""
Origin is the upper-left
Add the patch to the Axes
This is magic goop that removes whitespace around image plots (sort of)
os.startfile(outputFileName)
%% Showing figures on-screen during debugging
plt.show()
Various (mostly unsuccessful) approaches to getting the plot window to show up
"in the foreground, which is a backend-specific operation..."
""
fig.canvas.manager.window.activateWindow()
fig.canvas.manager.window.raise_()
fm = plt.get_current_fig_manager()
"fm.window.attributes('-topmost', 1)"
"fm.window.attributes('-topmost', 0)"
""
# This is the one that I found to be most robust... at like 80% robust.
plt.get_current_fig_manager().window.raise_()
%%
...for each image
...for each file
""
convert_imerit_json_to_coco_json.py
""
"Takes a .json file with bounding boxes but no class labels, and a .json file containing the"
"class labels for those images, and creates a new json file with class labels and bounding"
boxes.
""
"The bounding box .json file is in the format returned by our annotators, which is not"
actually a fully-formed .json file; rather it's a series of .json objects
""
"Leaves filenames intact.  Stores imerit ""category IDs"" (empty/human/group/animal) in a new"
"field called ""annotation_type""."
""
%% Imports and constants
%% Configure files and paths
"For Snapshot Serengeti, we stored image IDs in our annotation files as:"
""
S1_B06_R1_PICT0008
""
...but the corresponding ID in the master database is actually:
""
S1\B06\R1\S1_B06_R1_PICT0008
""
"If this is ""True"", we'll expand the former to the latter"
Handling a one-off issue in which .'s were mysteriously replaced with -'s
"in our annotations.  This will be set dynamically, but I keep it here as"
a constant to remind me to remove this code when we clean this issue up.
Used in the (rare) case where a bounding box was added to an image that was originally
annotated as empty
Used in the (rare) case where we added bounding boxes to an image with multiple species
"Should we include ambiguous bounding boxes (with the ""ambiguous"" category label)"
in the output file?  Ambiguous boxes are boxes drawn on images with multiple species.
"%%  Read metadata from the master database, bounding boxes from the annotations file"
"The bounding box .json file is in the format returned by our annotators, which is not"
actually a fully-formed .json file; rather it's a series of .json objects
"Each element of annData is a dictionary corresponding to a single sequence, with keys:"
""
"annotations, categories, images"
sequence = annData[0]
%% Build convenience mappings
Image ID to images
Category ID to categories (referring to the database categories)
"Image ID to categories (i.e., species labels)"
Utility function we'll use to create annotations for images in empty
sequences (empty images in non-empty sequences already have annotations)
"%% Reformat annotations, grabbing category IDs from the master database (prep)"
iSequence = 0; sequence = annData[0]
"%% Reformat annotations, grabbing category IDs from the master database (loop)"
Make a copy here; we're going to manipulate the sequence annotations
when we need to add synthetic annotations for empty images
im = sequenceImages[0]
Are there any annotations in this sequence?
Which images in this sequence have annotations?
For each image in this sequence...
imeritImageID = im['id']
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG
Confirm that the file exists
Hande a one-off issue with our annotations
datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG
""
...had become:
""
datasetsnapshotserengeti.seqASG000001a-frame0.imgS1_B06_R1_PICT0008.JPG
Does it look like we encountered this issue?
Convert:
""
S1_B06_R1_PICT0008
""
...to:
""
S1/B06/B06_R1/S1_B06_R1_PICT0008
Make sure we only see each image once
Create empty annotations for empty images
""
Here we use the *unmodified* file name
Create an empty annotation for this image
Annotations still use the annotation filename (not database ID) at this point;
these will get converted to database IDs below when we process the
whole sequence.
Sanity-check image size
"print('Warning: img {} was listed in DB as {}x{}, annotated as {}x{}, actual size{}x{}'.format("
"old_id,new_im['width'],new_im['height'],im['width'],im['height'],imgObj.width,imgObj.height))"
...for each image in this sequence
For each annotation in this sequence...
ann = sequenceAnnotations[0]
Prepare an annotation using the category ID from the database and
the bounding box from the annotations file
Maintain iMerit's annotation category
Generate an (arbitrary) ID for this annotation; the COCO format has a concept
"of annotation ID, but our annotation files don't"
This was a one-off quirk with our file naming
We'll do special handling of images with multiple categories later
Store the annotation type (group/human/animal/empty)
This annotation has no bounding box but the image wasn't originally
annotated as empty
This annotation has a bounding box but the image was originally
annotated as empty
unnormalize the bbox
... for each annotation in this sequence
... for each sequence
%% Post-processing
Count empty images
...for each file
%% Sanity-check empty images
""
make_ss_annotation_image_folder.py
""
Take a directory full of images with the very long filenames we give annotators:
""
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension
""
"...along with a COCO-camera-traps database referring to those files, and:"
""
1) Creates a new COCO-camera-traps database with the original filenames in them
(copying the annotations)
""
2) Optionally creates a new directory with those images named according to the
"Snapshot Serengeti naming convention, including complete relative paths."
""
See convert_imerit_json_to_coco_json to see how we get from the original annotation
.json to a COCO-camera-traps database.
""
%% Constants and imports
%% Configure files/paths
%% Read the annotations (referring to the old filenames)
"%% Update filenames, optionally copying files"
im = data['images'][0]
For each image...
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG
"Find the image name, e.g. S1_B06_R1_PICT0008"
Convert:
""
S1_B06_R1_PICT0008.JPG
""
...to:
""
S1/B06/B06_R1/S1_B06_R1_PICT0008.JPG
...for each image
%% Write the revised database
""
get_annotation_tool_link.py
""
"Takes a COCO-camera-traps-style .json file with URLs already embedded, and prepares"
a link to the visipedia annotation tool that reviews a subset of those images.
""
""
create_new_annotation_json.py
""
"Creates a subset of a larger .json database, in this case specifically to pick some images"
from Snapshot Serengeti.
""
from utils import get_db_dicts
for seq in already_annotated:
seq_to_ims.pop(seq)
remove already annotated images
add lion images
lion_seqs = cat_to_seqs[cat_to_id['lionMale']] + cat_to_seqs[cat_to_id['lionFemale']]
#print(len(lion_seqs))
lion_seqs = [seq for seq in lion_seqs if seq not in already_annotated]
#print(len(lion_seqs))
"lion_seqs_to_annotate.extend(random.sample(lion_seqs, min(len(lion_seqs),num_lions)))"
print(len(lion_seqs_to_annotate))
num_elephants = 1000
elephant_seqs = cat_to_seqs[cat_to_id['elephant']]
#print(len(lion_seqs))
elephant_seqs = [seq for seq in elephant_seqs if seq not in already_annotated]
#print(len(lion_seqs))
"elephant_seqs_to_annotate = random.sample(elephant_seqs, num_elephants)"
num_empty = 10
empty_seqs_to_annotate = []
for loc in loc_to_seqs:
empty_seqs = cats_per_location[loc][cat_to_id['empty']]
empty_seqs = [seq for seq in empty_seqs if seq not in already_annotated]
empty_seqs = [seq for seq in empty_seqs if seq_to_season[seq] in seasons_to_keep]
"empty_seqs_to_annotate.extend(random.sample(empty_seqs, min(len(empty_seqs),num_empty)))"
ims_to_annotate.extend(empty_ims_to_annotate)
""
filter_database.py
""
"Look through a COCO-ct database and find images matching some crtieria, writing"
a subset of images and annotations to a new file.
""
%% Constants and imports
%% Configuration
%%  Read database and build up convenience mappings
Category ID to category name
Image ID to image info
Image ID to image path
%% Filter
ann = annotations[0]
Is this a tiny box or a group annotation?
"x,y,w,h"
All empty annotations should be classified as either empty or ambiguous
""
"The ambiguous cases are basically minor misses on the annotators' part,"
where two different small animals were present somewhere.
%% Write output file
""
find_images_for_classes.py
""
"Given a .json database, find images that are associated with one or more"
classes.
""
%% Constants and imports
%% Configuration
%%  Read database and build up convenience mappings
Category ID to category name
Image ID to image info
Image ID to image path
%% Look for target-class annotations
ann = annotations[0]
""
create_tfrecords_format.py
""
This script creates a tfrecords file from a dataset in VOTT format.
%% Imports and environment
set up the filenames and annotations
This loop reads the bboxes and corresponding labels and assigns them
the correct image. Kind of slow at the moment...
If needed: merging all classes
bbox_labels = ['Animal' for _ in bbox_labels]
BBox coords are stored in the format
"x_min (of width axis) y_min (of height axis), x_max, y_max"
Coordinate system starts in top left corner
"In this framework, we need ('ymin', 'xmin', 'ymax', 'xmax') format"
print out some stats
To make sure we loaded the bboxes correctly:
self.validate_bboxes()
For each image in the data set...
Make sure all are greater equal 0
%% Main tfrecord generation function
Propagate optional metadata to tfrecords
endfor each annotation for the current image
endfor each image
"Calculate number of shards to get the desired number of images per record,"
ensure it is evenly divisible by the number of threads
%% Command-line driver
%% Driver
""
make_tfrecords_cis_trans.py
""
Given a .json file that contains a three-element list (train/val/test) of image IDs and a .json database that contains
"those image IDs, generates tfrecords whose filenames include ""train""/""val""/""test"""
""
"dataset = json.load(open('/ai4efs/databases/snapshotserengeti/oneclass/SnapshotSerengeti_Seasons_1_to_4_tfrecord_format_valid_ims.json','r'))"
print('Creating trans_val tfrecords')
dataset = [im_id_to_im[idx] for idx in trans_val]
""
create_classification_tfrecords_from_json.py
""
Called from make_tfrecords_cis_trans.py if you're running a classification experiment.
""
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/'
datafile = 'eccv_18_annotation_files_oneclass/CaltechCameraTrapsECCV18'
image_file_root = datafolder+'eccv_18_all_images/'
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/'
datafolder = '/data/iwildcam/'
datafile = 'combined_iwildcam_annotations_oneclass/eccv_train_and_imerit_2'
need consecutive category ids
old_cat_id_to_new_cat_id = {categories[idx]['id']:idx+1 for idx in range(len(categories))}
print(old_cat_id_to_new_cat_id)
remove multiclass images
print(images[0])
print(vis_data[0])
""
make_tfrecords_from_json.py
""
"Given a coco-camera-traps .json file, creates tfrecords"
""
Thin wrapper for create_tfrecords_from_json.
""
%% Constants and imports
%% Constants and imports (interactive)
%%
%% Main tfrecord generation function
"check whether the input file has already been converted to the tfrecords format,"
"if not, convert"
"Calculate number of shards to get the desired number of images per record,"
ensure it is evenly divisible by the number of threads
%% Interactive driver
%%
%% Command-line driver
eMammal_make_splits.py
""
"Based on a tfrecords_format json file of the database, creates 3 splits according to"
the specified fractions based on location (images from the same location should be in
one split) or based on images.
""
"If a previous split is provided (append_to_previous_split is True), the entries in"
"each split will be preserved, and new entries will be appended, so that new models"
can warm start with a model trained on the original splits.
configurations and paths
approximate fraction for the new entries
read in the previous splits of image ID or location ID if available
"find new locations and assign them to a split, without reassigning any previous locations"
"find out which images are new, shuffle and split them"
do NOT sort the IDs to keep the shuffled order
export PYTHONPATH=$PYTHONPATH:tfmodels/research
add empty category
"add all images that don't have annotations, with cat empty"
print(detection_results.keys())
group the detections and gts by image id:
print(image_id)
print(len(scores))
print(len(labels))
recall_thresh = 0.9
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall])
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])"
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall])
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])"
exp_name = 'eccv_train'
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json'
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(len(detection_results['images']))
print(len(seqs))
"print(len(seqs[0]),len(seqs["
print(detection_results.keys())
group the detections by image id:
group the ground truth annotations by image id:
print(seq)
"for image_id, dets in per_image_detections.iteritems():"
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
print(num_gts)
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
if sum(detected_class_labels)>0:
print('detected animal box')
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])"
"print(scores, tp_fp_labels)"
num_total_gts += num_gts
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
print('valid box')
"print(best_im, best_score)"
exp_name = 'eccv_train'
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json'
export PYTHONPATH=$PYTHONPATH:tfmodels/research
need to loop over confidence values
"for each value, check if any detections on the image are > conf"
"If so, that image gets class ""animal"""
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0"
"for image_id, dets in per_image_detections.iteritems():"
"calc prec, rec for this confidence thresh"
group the ground truth annotations by image id:
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(detection_results.keys())
group the detections and gts by image id:
print(image_id)
print(len(scores))
print(len(labels))
recall_thresh = 0.9
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall])
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])"
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall])
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])"
print(sorted_ap)
plt.bar(sorted_ap)
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(detection_results.keys())
group the detections by image id:
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
print(gts)
print(num_gts)
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[:2])"
"print(scores, tp_fp_labels)"
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
export PYTHONPATH=$PYTHONPATH:tfmodels/research
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
print('detected animal box')
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])"
"print(scores, tp_fp_labels)"
num_total_gts += num_gts
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
print('valid box')
best_im = np.argmax(max_im_scores)
"print(best_im, best_score)"
"temp_labels = np.zeros(len(im_detection_labels),  dtype=np.int32)"
"temp_scores = np.zeros(len(im_detection_scores), dtype=np.float32)"
"for j in range(min(im_num_gts, len(im_detection_labels))):"
temp_labels[j] = True #TODO: this currently only works for oneclass?
temp_scores[j] = best_score
im_detection_labels = temp_labels
im_detection_scores = temp_scores
num_total_gts+=im_num_gts
"print(len(detection_scores), len(detection_scores[0]), len(detection_scores[1]))"
exp_name = 'small_balanced_cct'
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(seq)
"for image_id, dets in per_image_detections.iteritems():"
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
seq_num_gts.append(num_gts)
print(num_gts)
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
num_total_gts += 1
print('valid box')
"print(best_im, best_score)"
print('no valid box')
if sum(seq_num_gts)>0:
export PYTHONPATH=$PYTHONPATH:tfmodels/research
need to loop over confidence values
"for each value, check if any detections on the image are > conf"
"If so, that image gets class ""animal"""
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0"
"calc prec, rec for this confidence thresh"
export PYTHONPATH=$PYTHONPATH:tfmodels/research
group the detections by image id:
group the ground truth annotations by image id:
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
print('detected animal box')
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])"
"print(scores, tp_fp_labels)"
num_total_gts += num_gts
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
print('valid box')
"print(best_im, best_score)"
""
evaluate_detections.py
""
Adapted from analyze_detection.py which is now archived.
""
%% Imports and constants
%% Functions
"labels input to compute_object_detection_metrics() needs to start at 0, not 1"
num_detections = len(dets['boxes'])
to prevent 'Invalid dimensions for box data.' error
this box will not match any detections
compute one-class precision/recall/average precision (if every box is just of an object class)
%% Command-line driver
""
detection_eval_utils.py
""
Utility functions used in evaluate_detections.py
""
group the ground truth annotations by image id
#####
""
detect_and_predict_image.py
""
"Functions to load a TensorFlow detection and a classification model, run inference,"
"render bounding boxes on images, and write out the resulting"
images (with bounding boxes and classes).
""
"See the ""test driver"" cell for example invocation."
""
""
#####
"%% Constants, imports, environment"
Minimum detection confidence for showing a bounding box on the output image
Stick this into filenames before the extension for the rendered result
Number of top-scoring classes to show at each bounding box
%% Core detection functions
Load images if they're not already numpy arrays
iImage = 0; image = images[iImage]
"Load the image as an nparray of size h,w,nChannels"
"There was a time when I was loading with PIL and switched to mpimg,"
"but I can't remember why, and converting to RGB is a very good reason"
"to load with PIL, since mpimg doesn't give any indication of color"
"order, which basically breaks all .png files."
""
"So if you find a bug related to using PIL, update this comment"
"to indicate what it was, but also disable .png support."
image = mpimg.imread(image)
This shouldn't be necessarily when loading with PIL and converting to RGB
Actual detection
...for each image
"Currently ""boxes"" is a list of length nImages, where each element is shaped as"
""
"1,nDetections,4"
""
"This implicitly banks on TF giving us back a fixed number of boxes, let's assert on this"
to make sure this doesn't silently break in the future.
iBox = 0; box = boxes[iBox]
"""scores"" is a length-nImages list of elements with size 1,nDetections"
"""classes"" is a length-nImages list of elements with size 1,nDetections"
""
"Still as floats, but really representing ints"
Squeeze out the empty axis
boxes is nImages x nDetections x 4
scores and classes are both nImages x nDetections
Get input and output tensors of classification model
"imsize = cur_image['width'], cur_image['height']"
Select detections with a confidence larger 0.5
Get these boxes and convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes"
"However, we need to make sure that it box coordinates are still within the image"
For convenience:
Create an array with contains the index of the corresponding crop_box for each selected box
i.e. [False False 0 False 1 2 3 False False]
For each box
If this box should be classified
Run inference
if box should not be classified
...for each box
species_scores should have shape len(images) x len(boxes) x num_species
...for each image
...with tf.Session
with classification_graph
species_scores should have shape len(images) x len(boxes) x num_species
%% Rendering functions
Display the image
plt.show()
"top, left, bottom, right"
""
"x,y origin is the upper-left"
Generate bounding box text
Choose color based on class
Location is the bottom-left of the rect
""
Origin is the upper-left
iRight = x + w
iTop = y + h
Add the patch to the Axes
Add class description
First determine best location by finding the corner that is closest to the image center
relative corner coordinates
relative coordinates of image center
Compute pair-wise squared distance and get the index of the one with minimal distance
Get the corresponding coordinates ...
... and alignment for the text box
Plot the text box with background
...for each box
This is magic goop that removes whitespace around image plots (sort of)
"plt.savefig(outputFileName, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)"
os.startfile(outputFileName)
...for each image
...def render_bounding_boxes
Load and run detector on target images
Read the name of all classes
remove empty lines
%% Interactive driver
%%
%%
%% File helper functions
%% Command-line driver
Hack to avoid running on already-detected images
""
"Script for evaluating a frozen graph given a COCO-style dataset, which was generated by"
the ../data_management/databases/classification/make_classification_dataset.py script.
This is useful for debugging as the accuracy reported by this script should match the
accuracy reported by the Tensorflow training.
""
Check that all files exists for easier debugging
Load frozen graph
Collect tensors for input and output
Read image
Run inference
Check if correct
""
Script for evaluating precision/recall on a two-class problem given
"a frozen graph and a COCO-style dataset, which was generated by"
the ../data_management/databases/classification/make_classification_dataset.py script.
We assume the positive class is at index 0 (with a zero-based indexing)
""
Check that all files exists for easier debugging
Load frozen graph
Collect tensors for input and output
Read image
"with open(image_path, 'rb') as fi:"
"image =  sess.run(tf.image.decode_jpeg(fi.read(), channels=3))"
image = image / 255.
Run inference
predicted_class = np.argmax(predictions)
Check if correct
if coco.imgToAnns[image_id][0]['category_id'] == predicted_class:
correct = correct + 1
"In matplotlib < 1.5, plt.fill_between does not have a 'step' argument"
""
Script for selecting testing images of a COCO-style dataset generated by the script
../data_management/databases/classification/make_classification_dataset.py in a consistent
manner and predicting the class for it.
""
Mandatory parameters
Optional parameters
Validate parameters
Derived parameters
"We assume that the dataset was generated with the make_classification_dataset.py script,"
hence the images should be located in the same folder as the json
Make seletion deterministic
Load frozen graph
Get dataset information
Get classes
...and the class list corresponding to the model outputs by assuming
that they are in order of their ids
Get images of each class
Shuffle the image list
Start the image sampling
"Set of avaiable class IDs, will be filled below"
If there are still images left for that class
"Get image for the sampled class, we already shuffled the class images before so"
we can simply pop()
Start prediction
Collect tensors for input and output
Read image
Run inference
Print output to log file
#####
""
api_apply_classifier_single_node.py
""
Takes the JSON file produced by the detection API and
classifies all boxes above a confidence threshold.
""
#####
"%% Constants, imports, environment"
Assumes that the root of the CameraTraps repo is on the PYTHONPATH
Minimum detection confidence for classifying an object
Number of top-scoring classes to show at each bounding box
Enlargment factor applied to boxes before passing them to the classifier
""
Provides more context and can lead to better results
List of detection categories for which we will run the classification
""
"Currently there are {""1"": ""animal"", ""2"": ""person"", ""4"": ""vehicle""}"
""
Should be a list of string-formatted ints.
Number of significant float digits in JSON output
%% Core detection functions
Read the name of all classes
remove empty lines
Create field with name *classification_categories*
Add classes using 0-based indexing
def add_classification_categories
Make sure we have the right json object
Get input and output tensors of classification model
For each image
Read image
"Scale pixel values to [0,1]"
For each box
Skip detections with low confidence
Skip if detection category is not in whitelist
Skip if already classified
"Get current box in relative coordinates and format [x_min, y_min, width_of_box, height_of_box]"
"Convert to [ymin, xmin, ymax, xmax] and store it as 1x4 numpy array so we can"
re-use the generic multi-box padding code
Convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes."
""
"However, we need to make sure that it box coordinates are still within the image."
Get the first (and only) row as our bbox to classify
Get the image data for that box
Run inference
Add an empty list to the json for our predictions
Add the *num_annotated_classes* top scoring classes
...for each box
...for each image
...with tf.Session
with classification_graph
def classify_boxes
Load classification model
Load detector json
Add classes to detector_json
"Run classifier on all images, changes will be writting directly to the json"
Write output json
def load_and_run_classifier
%% Command-line driver
""
Script for selecting testing images of a COCO-style dataset generated by the script
../data_management/databases/classification/make_classification_dataset.py in a consistent
manner and predicting the class for it.
""
Assumes the cameratraps repo root is on the path
Make seletion deterministic
Mandatory parameters
Optional parameters
Validate parameters
Tranfer parameters to post-processing format
Load frozen graph
Reading image list
Reading class list
Image sampling
Start prediction
Collect tensors for input and output
Array for collecting infos for rendering the html
Read image
"Scale pixel values to [0,1]"
Run inference
Add links to all available classes
""
predict_image.py
""
"Given a pointer to a frozen detection graph, runs inference on a single image,"
printing the top classes to the console
""
%% Imports
%% Command-line processing
Check that all files exist for easier debugging
%% Inference
Load frozen graph
Load class list
Remove empty lines
Collect tensors for input and output
Read image
Run inference
Print output
""
Mostly unmodified script for freezing a model
Added for convenience and for possible future optimizations
""
Copyright 2015 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
'input_checkpoint' may be a prefix if we're using Saver V2 format
Remove all the explicit device specifications for this node. This helps to
make the graph more portable.
List of all partition variables. Because the condition is heuristic
"based, the list could include false positives."
This tensor doesn't exist in the graph (for example it's
'global_step' or a similar housekeeping element) so skip it.
`var_list` is required to be a map of variable names to Variable
tensors. Partition variables are Identity tensors that cannot be
handled by Saver.
Models that have been frozen previously do not contain Variables.
optimized_output_graph_def = optimize_for_inference_lib.optimize_for_inference(
"output_graph_def,"
"input_node_names.replace("" "", """").split("",""),"
"output_node_names.replace("" "", """").split("",""),"
tf.float32.as_datatype_enum)
Write GraphDef to file if output path has been given.
""
"Creates a graph description, which is required to create a frozen graph."
Adapted from from ./tf-slim/export_inference_graph.py
Added preprocessing to the definition for easier handling
""
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
tf.app.flags.DEFINE_integer(
"'batch_size', None,"
"'Batch size for the exported model. Defaulted to ""None"" so batch size can '"
'be specified at model runtime.')
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
#####################
Optimization Flags #
#####################
######################
Learning Rate Flags #
######################
######################
Dataset Flags #
######################
####################
Fine-Tuning Flags #
####################
"Note: when num_clones is > 1, this will actually have each clone to go"
over each epoch FLAGS.num_epochs_per_decay times. This is different
behavior from sync replicas and is expected to produce different results.
Warn the user if a checkpoint exists in the train_dir. Then we'll be
ignoring the checkpoint anyway.
TODO(sguada) variables.filter_variables()
######################
Config model_deploy #
######################
Create global_step
#####################
Select the dataset #
#####################
#####################
Select the network #
#####################
####################################
Select the preprocessing function #
####################################
#############################################################
Create a dataset provider that loads data from the dataset #
#############################################################
###################
Define the model #
###################
############################
Specify the loss function #
############################
Gather initial summaries.
"Gather update_ops from the first clone. These contain, for example,"
the updates for the batch_norm variables created by network_fn.
Add summaries for end_points.
Add summaries for losses.
Add summaries for variables.
################################
Configure the moving averages #
################################
########################################
Configure the optimization procedure. #
########################################
"If sync_replicas is enabled, the averaging will be done in the chief"
queue runner.
Update ops executed locally by trainer.
Variables to train.
and returns a train_tensor and summary_op
Add total_loss to summary.
Create gradient updates.
Add the summaries from the first clone. These contain the summaries
created by model_fn and either optimize_clones() or _gather_clone_loss().
Merge all summaries together.
##########################
Kicks off the training. #
##########################
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
#####################
Select the dataset #
#####################
###################
Select the model #
###################
#############################################################
Create a dataset provider that loads data from the dataset #
#############################################################
####################################
Select the preprocessing function #
####################################
###################
Define the model #
###################
Define the metrics:
Print the summaries to screen.
TODO(sguada) use num_epochs=1
This ensures that we make a single pass over all of the data.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Transform the image to floats.
"Randomly crop a [height, width] section of the image."
Randomly flip the image horizontally.
"Because these operations are not commutative, consider randomizing"
the order their operation.
Subtract off the mean and divide by the variance of the pixels.
Transform the image to floats.
Resize and crop if needed.
Subtract off the mean and divide by the variance of the pixels.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Use tf.slice instead of crop_to_bounding box as it accepts tensors to
define the crop size.
Compute the rank assertions.
Create a random bounding box.
""
Use tf.random_uniform and not numpy.random.rand as doing the former would
"generate random numbers at graph eval time, unlike the latter which"
generates random numbers at graph definition time.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Pass the real x only to one of the func calls.
The random_* ops do not necessarily clamp.
"Each bounding box has shape [1, num_boxes, box coords] and"
"the coordinates are ordered [ymin, xmin, ymax, xmax]."
A large fraction of image datasets contain a human-annotated bounding
box delineating the region of the image containing the object of interest.
We choose to create a new bounding box for the object which is a randomly
distorted version of the human-annotated bounding box that obeys an
"allowed range of aspect ratios, sizes and overlap with the human-annotated"
"bounding box. If no box is supplied, then we assume the bounding box is"
the entire image.
Crop the image to the specified bounding box.
"Each bounding box has shape [1, num_boxes, box coords] and"
"the coordinates are ordered [ymin, xmin, ymax, xmax]."
Restore the shape since the dynamic slice based upon the bbox_size loses
the third dimension.
This resizing operation may distort the images because the aspect
ratio is not respected. We select a resize method in a round robin
fashion based on the thread number.
Note that ResizeMethod contains 4 enumerated resizing methods.
We select only 1 case for fast_mode bilinear.
Randomly flip the image horizontally.
Randomly distort the colors. There are 1 or 4 ways to do it.
Crop the central region of the image with an area containing 87.5% of
the original image.
Resize the image to the specified height and width.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(sguada) docstring paragraph by (a) motivating the need for the file and
(b) defining clones.
TODO(sguada) describe the high-level components of model deployment.
"E.g. ""each model deployment is composed of several parts: a DeploymentConfig,"
"which captures A, B and C, an input_fn which loads data.. etc"
Set up DeploymentConfig
Create the global step on the device storing the variables.
Define the inputs
Define the optimizer.
Define the model including the loss.
Run training.
Namedtuple used to represent a clone during deployment.
"Namedtuple used to represent a DeployedModel, returned by deploy()."
Default parameters for DeploymentConfig
Create clones.
The return value.
Individual components of the loss that will need summaries.
Compute and aggregate losses on the clone device.
Add the summaries out of the clone device block.
Only use regularization_losses for the first clone
Compute the total_loss summing all the clones_losses.
Sum the gradients across clones.
Gather initial summaries.
Create Clones.
"Gather update_ops from the first clone. These contain, for example,"
the updates for the batch_norm variables created by model_fn.
Place the global step on the device storing the variables.
Compute the gradients for the clones.
Add summaries to the gradients.
Create gradient updates.
Only use regularization_losses for the first clone
Add the summaries from the first clone. These contain the summaries
created by model_fn and either optimize_clones() or _gather_clone_loss().
Add total_loss to summary.
Merge all summaries together.
Note that each grad_and_vars looks like the following:
"((grad_var0_clone0, var0), ... (grad_varN_cloneN, varN))"
!/usr/bin/python
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Failed to find ""index"" occurrence of item."
pylint: disable=broad-except
pylint: enable=broad-except
Grab the 'index' annotation.
Some images contain bounding box annotations that
"extend outside of the supplied image. See, e.g."
n03127925/n03127925_147.xml
"Additionally, for some bounding boxes, the min > max"
or the box is entirely outside of the image.
Example: <...>/n06470073/n00141669_6790.xml
Determine if the annotation is from an ImageNet Challenge label.
Note: There is a slight bug in the bounding box annotation data.
Many of the dog labels have the human label 'Scottish_deerhound'
instead of the synset ID 'n02092002' in the bbox.label field. As a
"simple hack to overcome this issue, we only exclude bbox labels"
*which are synset ID's* that do not match original synset label for
the XML file.
Guard against improperly specified boxes.
Note bbox.filename occasionally contains '%s' in the name. This is
data set noise that is fixed by just using the basename of the XML file.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The URLs where the MNIST data can be downloaded.
The names of the classes.
"First, process the training data:"
"Next, process the testing data:"
"Finally, write the labels file:"
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The URL where the Flowers data can be downloaded.
The number of images in the validation set.
Seed for repeatability.
The number of shards per dataset split.
Initializes function that decodes RGB JPEG data.
Read the filename:
Divide into train and test:
"First, convert the training and validation sets."
"Finally, write the labels file:"
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(nsilberman): Add tfrecord file type once the script is updated.
"If set to false, will not try to set label_to_names in dataset"
by reading them from labels.txt or github.
n01440764
n01443537
n02119247    black fox
n02119359    silver fox
pylint: disable=g-line-too-long
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The URL where the CIFAR data can be downloaded.
The number of training files.
The height and width of each image.
The names of the classes.
"First, process the training data:"
"Next, process the testing data:"
"Finally, write the labels file:"
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The labels file contains a list of valid labels are held in this file.
Assumes that the file contains entries as such:
n01440764
n01443537
n01484850
where each line corresponds to a label expressed as a synset. We map
each synset contained in the file to an integer (based on the alphabetical
ordering). See below for details.
This file containing mapping from synset to human-readable label.
Assumes each line of the file looks like:
""
n02119247    black fox
n02119359    silver fox
"n02119477    red fox, Vulpes fulva"
""
where each line corresponds to a unique mapping. Note that each line is
formatted as <synset>\t<human readable label>.
This file is the output of process_bounding_box.py
Assumes each line of the file looks like:
""
"n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940"
""
where each line corresponds to one bounding box annotation associated
with an image. Each line can be parsed as:
""
"<JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>"
""
Note that there might exist mulitple bounding box annotations associated
with an image file.
pylint: disable=expression-not-assigned
pylint: enable=expression-not-assigned
Create a single Session to run all image coding calls.
Initializes function that converts PNG to JPEG data.
Initializes function that converts CMYK JPEG data to RGB JPEG data.
Initializes function that decodes RGB JPEG data.
File list from:
https://groups.google.com/forum/embed/?place=forum/torch7#!topic/torch7/fOSTXHIESSU
File list from:
https://github.com/cytsai/ilsvrc-cmyk-image-list
Read the image file.
Clean the dirty data.
1 image is a PNG.
22 JPEG images are in CMYK colorspace.
Decode the RGB JPEG.
Check that image converted to RGB
Each thread produces N shards where N = int(num_shards / num_threads).
"For instance, if num_shards = 128, and the num_threads = 2, then the first"
"thread would produce shards [0, 64)."
"Generate a sharded version of the file name, e.g. 'train-00002-of-00010'"
"Break all images into batches with a [ranges[i][0], ranges[i][1]]."
Launch a thread for each batch.
Create a mechanism for monitoring when all threads are finished.
Create a generic TensorFlow-based utility for converting all image codings.
Wait for all the threads to terminate.
Leave label index 0 empty as a background class.
Construct the list of JPEG files and labels.
Shuffle the ordering of all image files in order to guarantee
random ordering of the images with respect to label in the
saved TFRecord files. Make the randomization repeatable.
Build a map from synset to human-readable label.
Run it!
Allowing None in the signature so that dataset_factory can use the default.
!/usr/bin/python
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Read in the 50000 synsets associated with the validation data set.
Make all sub-directories in the validation data dir.
Move all of the image to the appropriate sub-directory.
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
"The dataset has classes with no images (empty and human), which have ID 0 and 1,"
so we need to specify 49 here despite having only 47 classes with images
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Final pooling and prediction
Global average pooling.
Pooling with a fixed kernel size.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The current_stride variable keeps track of the effective stride of the
activations. This allows us to invoke atrous convolution whenever applying
the next residual unit would result in the activations having stride larger
than the target output_stride.
The atrous convolution rate parameter.
Move stride from the block's last unit to the end of the block.
"If we have reached the target output_stride, then we need to employ"
atrous convolution with stride=1 and multiply the atrous rate by the
current unit's stride for use in subsequent layers.
Collect activations at the block's end before performing subsampling.
Subsampling of the block's output activations.
"The following implies padding='SAME' for pool1, which makes feature"
alignment easier for dense prediction tasks. This is also used in
https://github.com/facebook/fb.resnet.torch. However the accompanying
code of 'Deep Residual Learning for Image Recognition' uses
padding='VALID' for pool1. You can switch to that choice by setting
"slim.arg_scope([slim.max_pool2d], padding='VALID')."
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Collect outputs for conv2d, fully_connected and max_pool2d"
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"inputs has shape [batch, 224, 224, 3]"
"inputs has shape [batch, 513, 513, 3]"
We do not include batch normalization or activation functions in
conv1 because the first ResNet unit will perform these. Cf.
Appendix of [2].
This is needed because the pre-activation variant does not have batch
normalization or activation functions in the residual unit output. See
Appendix of [2].
Convert end_points_collection into a dictionary of end_points.
Global average pooling.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
=============================================================================
"These parameters come from the online port, which don't necessarily match"
those in the paper.
TODO(nsilberman): confirm these values with Philip.
##########
Encoder #
##########
"No normalizer for the first encoder layers as per 'Image-to-Image',"
Section 5.1.1
First layer doesn't use normalizer_fn
Last layer doesn't use activation_fn nor normalizer_fn
##########
Decoder #
##########
"Dropout is used at both train and test time as per 'Image-to-Image',"
Section 2.1 (last paragraph).
The Relu comes BEFORE the upsample op:
Explicitly set the normalizer_fn to None to override any default value
"that may come from an arg_scope, such as pix2pix_arg_scope."
No normalization on the input layer.
Stride 1 on the last layer.
"1-dim logits, stride 1, no activation, no normalization."
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
Alias
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
pylint: disable=unused-import
pylint: enable=unused-import
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
If we are fine tuning a checkpoint we need to start at a lower learning
rate since we are farther along on training.
We can start quantizing immediately if we are finetuning.
We need to wait for the model to train a bit before we quantize if we are
training from scratch.
Call rewriter to produce graph with fake quant ops and folded batch norms
"quant_delay delays start of quantization till quant_delay steps, allowing"
for better model accuracy.
Configure the learning rate using an exponential decay.
"When restoring from a floating point model, the min/max values for"
quantized weights and activations are not present.
We instruct slim to ignore variables that are missing during restoration
by setting ignore_missing_vars=True
"If we are restoring from a floating point model, we need to initialize"
the global step to zero for the exponential decay to result in
reasonable learning rates.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Orignaly, arg_scope = slim.arg_scope and layers = slim, now switch to more"
update-to-date tf.contrib.* API.
Decay for the moving averages.
epsilon to prevent 0s in variance.
Turns off fused batch norm.
collection containing the moving mean and moving variance.
"For inverted pyramid models, we start with gating switched off."
batch_size x 32 x 112 x 112 x 64
Separable conv is slow when used at first conv layer.
batch_size x 32 x 56 x 56 x 64
batch_size x 32 x 56 x 56 x 64
batch_size x 32 x 56 x 56 x 192
batch_size x 32 x 28 x 28 x 192
batch_size x 32 x 28 x 28 x 256
batch_size x 16 x 14 x 14 x 512
batch_size x 16 x 14 x 14 x 512
batch_size x 16 x 14 x 14 x 512
batch_size x 16 x 14 x 14 x 528
batch_size x 16 x 14 x 14 x 832
batch_size x 8 x 7 x 7 x 832
batch_size x 8 x 7 x 7 x 1024
Final pooling and prediction
Temporal average pooling.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
299 x 299 x 3
149 x 149 x 32
147 x 147 x 32
147 x 147 x 64
73 x 73 x 160
71 x 71 x 192
35 x 35 x 384
4 x Inception-A blocks
35 x 35 x 384
Reduction-A block
17 x 17 x 1024
7 x Inception-B blocks
17 x 17 x 1024
Reduction-B block
8 x 8 x 1536
3 x Inception-C blocks
Auxiliary Head logits
17 x 17 x 1024
Final pooling and prediction
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which"
can be set to False to disable pooling here (as in resnet_*()).
8 x 8 x 1536
1 x 1 x 1536
1536
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN
setups need the gradient of gradient FusedBatchNormGrad.
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN
setups need the gradient of gradient FusedBatchNormGrad.
First upscaling is different because it takes the input vector.
Last layer has different normalizer and activation.
Convert to proper channels.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Use clip_by_value to simulate bandpass activation.
Use clip_by_value to simulate bandpass activation.
Use clip_by_value to simulate bandpass activation.
149 x 149 x 32
147 x 147 x 32
147 x 147 x 64
73 x 73 x 64
73 x 73 x 80
71 x 71 x 192
35 x 35 x 192
35 x 35 x 320
TODO(alemi): Register intermediate endpoints
"17 x 17 x 1088 if output_stride == 8,"
33 x 33 x 1088 if output_stride == 16
TODO(alemi): register intermediate endpoints
TODO(gpapan): Properly support output_stride for the rest of the net.
8 x 8 x 2080
TODO(alemi): register intermediate endpoints
8 x 8 x 1536
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which"
can be set to False to disable pooling here (as in resnet_*()).
Set weight_decay for weights in conv2d and fully_connected layers.
Set activation_fn and parameters for batch_norm.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Decay for the moving averages.
epsilon to prevent 0s in variance.
collection containing update_ops.
use fused batch norm if possible.
Set weight_decay for weights in Conv and FC layers.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"inputs has shape [batch, 224, 224, 3]"
"inputs has shape [batch, 513, 513, 3]"
Use clip_by_value to simulate bandpass activation.
Convert end_points_collection into a dictionary of end_points.
Global average pooling.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"end_points will collect relevant activations for external use, for example"
summaries or losses.
299 x 299 x 3
149 x 149 x 32
147 x 147 x 32
147 x 147 x 64
73 x 73 x 64
73 x 73 x 80.
71 x 71 x 192.
35 x 35 x 192.
Inception blocks
mixed: 35 x 35 x 256.
mixed_1: 35 x 35 x 288.
mixed_2: 35 x 35 x 288.
mixed_3: 17 x 17 x 768.
mixed4: 17 x 17 x 768.
mixed_5: 17 x 17 x 768.
mixed_6: 17 x 17 x 768.
mixed_7: 17 x 17 x 768.
mixed_8: 8 x 8 x 1280.
mixed_9: 8 x 8 x 2048.
mixed_10: 8 x 8 x 2048.
Auxiliary Head logits
Shape of feature map before the final layer.
Final pooling and prediction
Global average pooling.
Pooling with a fixed kernel size.
1 x 1 x 2048
2048
1000
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Reflection pad by 1 in spatial dimensions (axes 1, 2 = h, w) to make a 3x3"
'valid' convolution produce an output with the same dimension as the
input.
This corrects 1 pixel offset for images with even width and height.
conv2d is left aligned and conv2d_transpose is right aligned for even
sized images (while doing 'SAME' padding).
Note: This doesn't reflect actual model in paper.
Neither dropout nor batch norm -> dont need is_training
##########
Encoder #
##########
7x7 input stage
##################
Residual Blocks #
##################
##########
Decoder #
##########
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"end_points will collect relevant activations for external use, for example"
summaries or losses.
Used to find thinned depths for each layer.
Note that sizes in the comments below assume an input spatial size of
"224x224, however, the inputs can be of any size greater 32x32."
224 x 224 x 3
depthwise_multiplier here is different from depth_multiplier.
depthwise_multiplier determines the output channels of the initial
"depthwise conv (see docs for tf.nn.separable_conv2d), while"
depth_multiplier controls the # channels of the subsequent 1x1
convolution. Must have
in_channels * depthwise_multipler <= out_channels
so that the separable convolution is not overparameterized.
Use a normal convolution instead of a separable convolution.
112 x 112 x 64
56 x 56 x 64
56 x 56 x 64
56 x 56 x 192
28 x 28 x 192
Inception module.
28 x 28 x 256
28 x 28 x 320
14 x 14 x 576
14 x 14 x 576
14 x 14 x 576
14 x 14 x 576
14 x 14 x 576
7 x 7 x 1024
7 x 7 x 1024
Final pooling and prediction
Global average pooling.
Pooling with a fixed kernel size.
1 x 1 x 1024
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
=============================================================================
Tensorflow mandates these.
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture
Conv defines 3x3 convolution layers
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.
stride is the stride of the convolution
depth is the number of channels or filters in a layer
MOBILENETV1_CONV_DEFS specifies the MobileNet body
Used to find thinned depths for each layer.
The current_stride variable keeps track of the output stride of the
"activations, i.e., the running product of convolution strides up to the"
current network layer. This allows us to invoke atrous convolution
whenever applying the next convolution would result in the activations
having output stride larger than the target output_stride.
The atrous convolution rate parameter.
"If we have reached the target output_stride, then we need to employ"
atrous convolution with stride=1 and multiply the atrous rate by the
current unit's stride for use in subsequent layers.
By passing filters=None
separable_conv2d produces only a depthwise convolution layer
Global average pooling.
Pooling with a fixed kernel size.
1 x 1 x 1024
Set weight_decay for weights in Conv and DepthSepConv layers.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Decay for the moving averages.
epsilon to prevent 0s in variance.
Turns off fused batch norm.
collection containing the moving mean and moving variance.
Final pooling and prediction
Temporal average pooling.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Orignaly, add_arg_scope = slim.add_arg_scope and layers = slim, now switch to"
more update-to-date tf.contrib.* API.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Make sure that round down does not go down by more than 10%.
"Note: want to round down, we adjust each split to match the total."
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts."
which provide numbered scopes.
b1 -> b2 * r -> b2
i -> (o * r) (bottleneck) -> o
"Note in contrast with expansion, we always have"
projection to produce the desired output size.
stride check enforces that we don't add residuals when spatial
dimensions are None
Depth matches
Don't do any splitting if we end up with less than 8 filters
on either side.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Make sure that round down does not go down by more than 10%.
Set conv defs defaults and overrides.
a) Set the tensorflow scope
b) set padding to default: note we might consider removing this
since it is also set by mobilenet_scope
c) set all defaults
d) set all extra overrides.
The current_stride variable keeps track of the output stride of the
"activations, i.e., the running product of convolution strides up to the"
current network layer. This allows us to invoke atrous convolution
whenever applying the next convolution would result in the activations
having output stride larger than the target output_stride.
The atrous convolution rate parameter.
Insert default parameters before the base scope which includes
any custom overrides set in mobilenet.
"If we have reached the target output_stride, then we need to employ"
atrous convolution with stride=1 and multiply the atrous rate by the
current unit's stride for use in subsequent layers.
Update params.
Only insert rate to params if rate > 1.
Set padding
Add all tensors that end with 'output' to
endpoints
1 x 1 x num_classes
Note: legacy scope name.
"Recover output shape, for unknown shape."
the network created will be trainble with dropout/batch norm
initialized appropriately.
Note: do not introduce parameters that would change the inference
"model here (for example whether to use bias), modify conv_def instead."
Set weight_decay for weights in Conv and FC layers.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
pyformat: disable
Architecture: https://arxiv.org/abs/1801.04381
Note: these parameters of batch norm affect the architecture
that's why they are here and not in training_scope.
pyformat: enable
NB: do not set depth_args unless they are provided to avoid overriding
whatever default depth_multiplier might have thanks to arg_scope.
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that
"'finegrain_classification_mode' is set to True, which means the embedding"
layer will not be shrinked when given a depth-multiplier < 1.0.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Notes for training NASNet Cifar Model
-------------------------------------
batch_size: 32
learning rate: 0.025
cosine (single period) learning rate decay
auxiliary head loss weighting: 0.4
clip global norm of all gradients by 5
600 epochs with a batch size of 32
This is used for the drop path probabilities since it needs to increase
the drop out probability over the course of training.
Notes for training large NASNet model on ImageNet
-------------------------------------
batch size (per replica): 16
learning rate: 0.015 * 100
learning rate decay factor: 0.97
num epochs per decay: 2.4
sync sgd with 100 replicas
auxiliary head loss weighting: 0.4
label smoothing: 0.1
clip global norm of all gradients by 10
Notes for training the mobile NASNet ImageNet model
-------------------------------------
batch size (per replica): 32
learning rate: 0.04 * 50
learning rate scaling factor: 0.97
num epochs per decay: 2.4
sync sgd with 50 replicas
auxiliary head weighting: 0.4
label smoothing: 0.1
clip global norm of all gradients by 10
Decay for the moving averages.
epsilon to prevent 0s in variance.
Decay for the moving averages.
epsilon to prevent 0s in variance.
Decay for the moving averages.
epsilon to prevent 0s in variance.
Shape of feature map before the final layer.
149 x 149 x 32
Run the reduction cells
Calculate the total number of cells in the network
Add 2 for the reduction cells
Calculate the total number of cells in the network
Add 2 for the reduction cells
"If ImageNet, then add an additional two for the stem cells"
Calculate the total number of cells in the network
Add 2 for the reduction cells
"If ImageNet, then add an additional two for the stem cells"
Find where to place the reduction cells or stride normal cells
Setup for building in the auxiliary head.
Run the cells
true_cell_num accounts for the stem cells
Final softmax layer
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Find where to place the reduction cells or stride normal cells
pylint: disable=protected-access
pylint: enable=protected-access
Setup for building in the auxiliary head.
Run the cells
true_cell_num accounts for the stem cells
pylint: disable=protected-access
pylint: enable=protected-access
Final softmax layer
pylint: disable=protected-access
pylint: enable=protected-access
Calculate the total number of cells in the network.
There is no distinction between reduction and normal cells in PNAS so the
total number of cells is equal to the number normal cells plus the number
of stem cells (two by default).
pylint: disable=protected-access
pylint: enable=protected-access
Calculate the total number of cells in the network.
There is no distinction between reduction and normal cells in PNAS so the
total number of cells is equal to the number normal cells plus the number
of stem cells (two by default).
Configuration for the PNASNet-5 model.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"The cap for tf.clip_by_value, it's hinted from the activation distribution"
"that the majority of activation values are in the range [-6, 6]."
Skip path 1
Skip path 2
"First pad with 0's on the right and bottom, then shift the filter to"
include those 0's that were added.
"If odd number of filters, add an additional one to the second path."
Concat and apply BN
Set the prev layer to the current layer if it is none
Check to be sure prev layer stuff is setup correctly
num_or_size_splits=1
Apply conv operations
Combine hidden states using 'add'.
Add hiddenstate to the list of hiddenstates we can choose from
Dont stride if this is not one of the original hiddenstates
"Check if a stride is needed, then use a strided 1x1 here"
Determine if a reduction should be applied to make the number of
filters match.
Return the concat of all the states
Scale keep prob by layer number
The added 2 is for the reduction cells
Decrease the keep probability over time
""
Script for generating a two-class dataset in COCO format for training an obscured image classifier
""
Requires Python >= 3.6 because of the glob ** expression
""
Collect images and labels
"Labels: clean = 0, obscured = 1"
""
top_folders_to_bottom.py
""
Given a base folder with files like:
""
A/1/2/a.jpg
B/3/4/b.jpg
""
"...moves the top-level folders to the bottom in a new output folder, i.e., creates:"
""
1/2/A/a.jpg
3/4/B/b.jpg
""
"In practice, this is used to make this:"
""
animal/camera01/image01.jpg
""
...look like:
""
camera01/animal/image01.jpg
""
%% Constants and imports
%% Support functions
%% Main functions
Find top-level folder
Find file/folder names
Move or copy
...def process_file()
Enumerate input folder
Convert absolute paths to relative paths
Standardize delimiters
Make sure each input file maps to a unique output file
relative_filename = relative_files[0]
Loop
...def top_folders_to_bottom()
%% Interactive driver
%%
%%
%% Command-line driver
"python top_folders_to_bottom.py ""g:\temp\separated_images"" ""g:\temp\separated_images_inverted"" --n_threads 100"
Convert to an options object
%% Constants and imports
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils)
Assumes the cameratraps repo root is on the path
%% Options
"Make sure there is no overlap between the two sets, because this will cause"
issues in the code
## Required inputs
## Options
Can be a folder or a SAS URL
# These apply only when we're doing ground-truth comparisons
Classes we'll treat as negative
""
"Include the token ""#NO_LABELS#"" to indicate that an image with no annotations"
should be considered empty.
Classes we'll treat as neither positive nor negative
"A list of output sets that we should count, but not render images for."
""
"Typically used to preview sets with lots of empties, where you don't want to"
"subset but also don't want to render 100,000 empty images."
""
"detections, non_detections"
"detections_animal, detections_person, detections_vehicle"
"By default, choose a confidence threshold based on the detector version"
Used for summary statistics only
"Number of images to sample, -1 for ""all images"""
"Random seed for sampling, or None"
Optionally separate detections into categories (animal/vehicle/human)
""
Currently only supported when ground truth is unavailable
Optionally replace one or more strings in filenames with other strings;
useful for taking a set of results generated for one folder structure
and applying them to a slightly different folder structure.
Allow bypassing API output loading when operating on previously-loaded
results
Should we also split out a separate report about the detections that were
just below our main confidence threshold?
""
Currently only supported when ground truth is unavailable
Control rendering parallelization
...PostProcessingOptions
#%% Helper classes and functions
Anything greater than this isn't clearly positive or negative
image has annotations suggesting both negative and positive
"image is not annotated or is annotated with 'unknown', 'unlabeled', ETC."
image has not yet been assigned a state
"In some analyses, we add an additional class that lets us look at"
detections just below our main confidence threshold
count the # of images with each type of DetectionStatus
Check whether this image has:
- unknown / unassigned-type labels
- negative-type labels
"- positive labels (i.e., labels that are neither unknown nor negative)"
"assert has_unknown_labels is False, '{} has unknown labels'.format(annotations)"
If there are no image annotations...
n_negative += 1
im['_detection_status'] = DetectionStatus.DS_NEGATIVE
"If the image has more than one type of labels, it's ambiguous"
"note: bools are automatically converted to 0/1, so we can sum"
"After the check above, we can be sure it's only one of positive,"
"negative, or unknown."
""
Important: do not merge the following 'unknown' branch with the first
"'unknown' branch above, where we tested 'if len(categories) == 0'"
""
If the image has only unknown labels
If the image has only negative labels
If the images has only positive labels
"Annotate the category, if it is unambiguous"
...for each image
...mark_detection_status()
"Leaving code in place for reading from blob storage, may support this"
in the future.
resize is to display them in this notebook or in the HTML more quickly
os.path.isfile() is slow when mounting remote directories; much faster
to just try/except on the image open.
return ''
Render images to a flat folder... we can use os.sep here because we've
already normalized paths
"Create class labels like ""gt_1"" or ""gt_27"""
"for i_box,box in enumerate(ground_truth_boxes):"
gt_classes.append('_' + str(box[-1]))
"errno.ENAMETOOLONG doesn't get thrown properly on Windows, so"
we awkwardly check against a hard-coded limit
Use slashes regardless of os
Optionally add links back to the original images
...render_bounding_boxes
Count items in each category
Optionally sort by filename before writing to html
Write the individual HTML files
...prepare_html_subpages()
Get unique categories above the threshold for this image
Render an image (with no ground truth information)
"This is a list of [class,confidence] pairs, sorted by confidence"
"If we either don't have a confidence threshold, or we've met our"
confidence threshold
...if this detection has classification info
...for each detection
...def render_image_no_gt()
This should already have been normalized to either '/' or '\'
...def render_image_with_gt()
%% Main function
#%% Expand some options for convenience
#%% Prepare output dir
#%% Load ground truth if available
Mark images in the ground truth as positive or negative
#%% Load detection (and possibly classification) results
"If the caller hasn't supplied results, load them"
Determine confidence thresholds if necessary
Remove failed rows
Convert keys and values to lowercase
"Add column 'pred_detection_label' to indicate predicted detection status,"
not separating out the classes
#%% Pull out descriptive metadata
This is rare; it only happens during debugging when the caller
is supplying already-loaded API results.
"#%% If we have ground truth, remove images we can't match to ground truth"
#%% Sample images for visualization
#%% Fork here depending on whether or not ground truth is available
"If we have ground truth, we'll compute precision/recall and sample tp/fp/tn/fn."
""
Otherwise we'll just visualize detections/non-detections.
#%% Detection evaluation: compute precision/recall
numpy array of detection probabilities
"numpy array of bools (0.0/1.0), and -1 as null value"
Don't include ambiguous/unknown ground truth in precision/recall analysis
"For completeness, include the result at a confidence threshold of 1.0"
Compute and print summary statistics
Thresholds go up throughout precisions/recalls/thresholds; find the last
value where recall is at or above target.  That's our precision @ target recall.
"np.where returns a tuple of arrays, but in this syntax where we're"
"comparing an array with a scalar, there will only be one element."
Convert back to a list
Flatten the confusion matrix
"#%% Collect classification results, if they exist"
Mapping of classnames to idx for the confusion matrix.
""
"The lambda is actually kind of a hack, because we use assume that"
the following code does not reassign classname_to_idx
Confusion matrix as defaultdict of defaultdict
""
"Rows / first index is ground truth, columns / second index is predicted category"
iDetection = 0; fn = detector_files[iDetection]; print(fn)
"If this image has classification predictions, and an unambiguous class"
"annotated, and is a positive image..."
"The unambiguous category, we make this a set for easier handling afterward"
"Compute the accuracy as intersection of union,"
i.e. (# of categories in both prediciton and GT)
divided by (# of categories in either prediction or GT
""
"In case of only one GT category, the result will be 1.0, if"
prediction is one category and this category matches GT
""
"It is 1.0/(# of predicted top-1 categories), if the GT is"
one of the predicted top-1 categories.
""
"It is 0.0, if none of the predicted categories is correct"
Distribute this accuracy across all predicted categories in the
confusion matrix
...for each file in the detection results
If we have classification results
Build confusion matrix as array from classifier_cm
Print some statistics
Prepare confusion matrix output
Get confusion matrix as string
Get fixed-size classname for each idx
Prepend class name on each line and add to the top
Print formatted confusion matrix
"Actually don't, this gets really messy in all but the widest consoles"
Plot confusion matrix
To manually add more space at bottom: plt.rcParams['figure.subplot.bottom'] = 0.1
""
"Add 0.5 to figsize for every class. For two classes, this will result in"
"fig = plt.figure(figsize=[4,4])"
...if we have classification results
#%% Render output
Write p/r table to .csv file in output directory
Write precision/recall plot to .png file in output directory
#%% Sampling
Sample true/false positives/negatives with correct/incorrect top-1
classification and render to html
Accumulate html image structs (in the format expected by write_html_image_lists)
"for each category, e.g. 'tp', 'fp', ..., 'class_bird', ..."
Add default entries by accessing them for the first time
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]"
"Each element will be a three-tuple with elements file,max_conf,detections"
"Assemble the information we need for rendering, so we can parallelize without"
dealing with Pandas
i_row = 0; row = images_to_visualize.iloc[0]
Filenames should already have been normalized to either '/' or '\'
Map all the rendering results in the list rendering_results into the
dictionary images_html
Prepare the individual html image files
Write index.html
Show links to each GT class
""
We could do this without classification results; currently we don't.
Add links to all available classes
Close body and html tags
...for each image
"#%% Otherwise, if we don't have ground truth..."
#%% Sample detections/non-detections
Accumulate html image structs (in the format expected by write_html_image_list)
for each category
Add default entries by accessing them for the first time
"Maps detection categories - e.g. ""human"" - to result set names, e.g."
"""detections_human"""
Add a set of results for each category and combination of categories
Create output directories
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]"
"list of 3-tuples with elements (file, max_conf, detections)"
"Assemble the information we need for rendering, so we can parallelize without"
dealing with Pandas
i_row = 0; row = images_to_visualize.iloc[0]
Filenames should already have been normalized to either '/' or '\'
"render_image_no_gt(file_info,detection_categories_to_results_name,"
"detection_categories,classification_categories)"
Map all the rendering results in the list rendering_results into the
dictionary images_html
Prepare the individual html image files
Write index.html
"We can't just sum these, because image_counts includes images in both their"
detection and classification classes
total_images = sum(image_counts.values())
Don't print classification classes here; we'll do that later with a slightly
different structure
Add links to all available classes
os.startfile(output_html_file)
...if we do/don't have ground truth
...process_batch_results
%% Interactive driver(s)
%%
options.unlabeled_classes = ['human']
os.start(ppresults.output_html_file)
%% Command-line driver
""
load_api_results.py
""
Loads the output of the batch processing API (json) into a pandas dataframe.
""
Also functions to group entries by seq_id.
""
Includes the deprecated functions that worked with the old CSV API output format.
""
%% Constants and imports
%% Functions for grouping by sequence_id
example
input 'file': 'SER/S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG'
output 'id': 'S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG'
%% Functions for loading the result as a Pandas DataFrame
Validate that this is really a detector output file
Fields in the API output json other than 'images'
Normalize paths to simplify comparisons later
"image['file'] = image['file'].replace('\\','/')"
Replace some path tokens to match local paths to original blob structure
"If this is a newer file that doesn't include maximum detection confidence values,"
"add them, because our unofficial internal dataframe format includes this."
Pack the json output into a Pandas DataFrame
Sanity-check that this is really a detector output file
Normalize paths to simplify comparisons later
De-serialize detections
Optionally replace some path tokens to match local paths to the original blob structure
string_to_replace = list(options.detector_output_filename_replacements.keys())[0]
"TODO: hit some silly issues with vectorized str() and escaped characters, vectorize"
this later.
""
"detection_results['image_path'].str.replace(string_to_replace,replacement_string)"
iRow = 0
%% Imports
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils)
%% Constants and support classes
We will confirm that this matches what we load from each file
Process-based parallelization isn't supported yet
%% Main function
"Warn the user if some ""detections"" might not get rendered"
#%% Validate inputs
#%% Load both result sets
assert results_a['detection_categories'] == default_detection_categories
assert results_b['detection_categories'] == default_detection_categories
#%% Make sure they represent the same set of images
#%% Find differences
"Each of these maps a filename to a two-element list (the image in set A, the image in set B)"
""
"Right now, we only handle a very simple notion of class transition, where the detection"
of maximum confidence changes class *and* both images have an above-threshold detection.
fn = filenames_a[0]
We shouldn't have gotten this far if error_on_non_matching_lists is set
det = im_a['detections'][0]
...for each filename
#%% Sample and plot differences
"Render two sets of results (i.e., a comparison) for a single"
image.
...def render_image_pair()
fn = image_filenames[0]
...def render_detection_comparisons()
"For each category, generate comparison images and the"
comparison HTML page.
""
category = 'common_detections'
Choose detection pairs we're going to render for this category
...for each category
#%% Write the top-level HTML file content
...def compare_batch_results()
%% Interactive driver
%% KRU
%% Command-line driver
# TODO
""
"Merge high-confidence detections from one results file into another file,"
when the target file does not detect anything on an image.
""
Does not currently attempt to merge every detection based on whether individual
detections are missing; only merges detections into images that would otherwise
be considered blank.
""
"If you want to literally merge two .json files, see combine_api_outputs.py."
""
%% Constants and imports
%% Structs
Don't bother merging into target images where the max detection is already
higher than this threshold
"If you want to merge only certain categories, specify one"
(but not both) of these.
%% Main function
im = output_data['images'][0]
"Determine whether we should be processing all categories, or just a subset"
of categories.
i_source_file = 0; source_file = source_files[i_source_file]
source_im = source_data['images'][0]
detection_category = list(detection_categories)[0]
"This is already a detection, no need to proceed looking for detections to"
transfer
Boxes are x/y/w/h
source_sizes = [det['bbox'][2]*det['bbox'][3] for det in source_detections_this_category_raw]
Only look at boxes below the size threshold
...for each detection category
"print('Adding {} detections to image {}'.format(len(detections_to_transfer),image_filename))"
Update the max_detection_conf field (if present)
...for each image
...for each source file
%% Test driver
%%
%% Command-line driver (TODO)
""
separate_detections_into_folders.py
""
## Overview
""
"Given a .json file with batch processing results, separate the files in that"
"set of results into folders that contain animals/people/vehicles/nothing,"
according to per-class thresholds.
""
"Image files are copied, not moved."
""
""
## Output structure
""
Preserves relative paths within each of those folders; cannot be used with .json
files that have absolute paths in them.
""
"For example, if your .json file has these images:"
""
a/b/c/1.jpg
a/b/d/2.jpg
a/b/e/3.jpg
a/b/f/4.jpg
a/x/y/5.jpg
""
And let's say:
""
"* The results say that the first three images are empty/person/vehicle, respectively"
"* The fourth image is above threshold for ""animal"" and ""person"""
* The fifth image contains an animal
""
* You specify an output base folder of c:\out
""
You will get the following files:
""
c:\out\empty\a\b\c\1.jpg
c:\out\people\a\b\d\2.jpg
c:\out\vehicles\a\b\e\3.jpg
c:\out\animal_person\a\b\f\4.jpg
c:\out\animals\a\x\y\5.jpg
""
## Rendering bounding boxes
""
"By default, images are just copied to the target output folder.  If you specify --render_boxes,"
bounding boxes will be rendered on the output images.  Because this is no longer strictly
"a copy operation, this may result in the loss of metadata.  More accurately, this *may*"
result in the loss of some EXIF metadata; this *will* result in the loss of IPTC/XMP metadata.
""
Rendering boxes also makes this script a lot slower.
""
## Classification-based separation
""
"If you have a results file with classification data, you can also specify classes to put"
"in their own folders, within the ""animals"" folder, like this:"
""
"--classification_thresholds ""deer=0.75,cow=0.75"""
""
"So, e.g., you might get:"
""
c:\out\animals\deer\a\x\y\5.jpg
""
"In this scenario, the folders within ""animals"" will be:"
""
"deer, cow, multiple, unclassified"
""
"""multiple"" in this case only means ""deer and cow""; if an image is classified as containing a"
"bird and a bear, that would end up in ""unclassified"", since the folder separation is based only"
on the categories you provide at the command line.
""
"No classification-based separation is done within the animal_person, animal_vehicle, or"
animal_person_vehicle folders.
""
%% Constants and imports
Occasionally we have near-zero confidence detections associated with COCO classes that
didn't quite get squeezed out of the model in training.  As long as they're near zero
"confidence, we just ignore them."
%% Options class
"Dictionary mapping categories (plus combinations of categories, and 'empty') to output folders"
Populated only when using classification results
"Originally specified as a string, converted to a dict mapping name:threshold"
...__init__()
...class SeparateDetectionsIntoFoldersOptions
%% Support functions
Find the maximum confidence for each category
""
det = detections[0]
"For zero-confidence detections, we occasionally have leftover goop"
from COCO classes
assert det['conf'] < invalid_category_epsilon
...for each detection on this image
Count the number of thresholds exceeded
...for each category
If this is above multiple thresholds
"TODO: handle species-based separation in, e.g., the animal_person case"
"Are we making species classification folders, and is this an animal?"
Do we need to put this into a specific species folder?
Find the animal-class detections that are above threshold
Count the number of classification categories that are above threshold for at
least one detection
d = valid_animal_detections[0]
classification = d['classifications'][0]
"Do we have a threshold for this category, and if so, is"
this classification above threshold?
...for each classification
...for each detection
...if we have to deal with classification subfolders
...if we have 0/1/more categories above threshold
...if this is/isn't a failure case
Skip this image if it's empty and we're not processing empty images
"At this point, this image is getting copied; we may or may not also need to"
draw bounding boxes.
Do a simple copy operation if we don't need to render any boxes
Open the source image
"Render bounding boxes for each category separately, beacuse"
we allow different thresholds for each category.
"When we're not using classification folders, remove classification"
information to maintain standard detection colors.
...for each category
Read EXIF metadata
"Write output with EXIF metadata if available, and quality='keep' if this is a JPEG"
"image.  Unfortunately, neither parameter likes ""None"", so we get a slightly"
icky cascade of if's here.
Also see:
""
https://discuss.dizzycoding.com/determining-jpg-quality-in-python-pil/
""
...for more ways to preserve jpeg quality if quality='keep' doesn't do the trick.
...if we don't/do need to render boxes
...def process_detections()
%% Main function
Create output folder if necessary
Load detection results
Map class names to output folders
Create all combinations of categories
category_name = category_names[0]
Do we have a custom threshold for this category?
Create folder mappings for each category
Create the actual folders
"Handle species classification thresholds, if specified"
"E.g. deer=0.75,cow=0.75"
token = tokens[0]
...for each token
...if classification thresholds are still in string format
Validate the classes in the threshold list
...if we need to deal with classification categories
i_image = 14; im = images[i_image]; im
...for each image
...def separate_detections_into_folders
%% Interactive driver
%%
%%
%%
%% Testing various command-line invocations
"With boxes, no classification"
"No boxes, no classification (default)"
"With boxes, with classification"
"No boxes, with classification"
%% Command-line driver
"python api\batch_processing\postprocessing\separate_detections_into_folders.py ""d:\temp\rspb_mini.json"" ""d:\temp\demo_images\rspb_2018_2019_mini"" ""d:\temp\separation_test"" --nthreads 2"
Convert to an options object
"print('{} {}'.format(v,name))"
List of category numbers to use in separation; uses all categories if None
"Can be ""size"", ""width"", or ""height"""
For each image...
""
im = images[0]
d = im['detections'][0]
Are there really any detections here?
Is this a category we're supposed to process?
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing#detector-outputs
...for each detection
...for each image
...def categorize_detections_by_size()
""
add_max_conf.py
""
"The MD output format included a ""max_detection_conf"" field with each image"
up to and including version 1.2; it was removed as of version 1.3 (it's
redundant with the individual detection confidence values).
""
"Just in case someone took a dependency on that field, this script allows you"
to add it back to an existing .json file.
""
%% Imports and constants
%% Main function
%% Driver
#######
""
remove_repeat_detections.py
""
"Used after running find_repeat_detections, then manually filtering the results,"
to create a final filtered output file.
""
"If you want to use this script, we recommend that you read the user's guide:"
""
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms
""
#######
%% Constants and imports
%% Main function
%% Interactive driver
%%
%% Command-line driver
#######
""
repeat_detections_core.py
""
Core utilities shared by find_repeat_detections and remove_repeat_detections.
""
#######
%% Imports and environment
"from ai4eutils; this is assumed to be on the path, as per repo convention"
"""PIL cannot read EXIF metainfo for the images"""
"""Metadata Warning, tag 256 had too many entries: 42, expected 1"""
%% Constants
%% Classes
Relevant for rendering the folder of images for filtering
""
"imageBase can also be a SAS URL, in which case some error-checking is"
disabled.
Don't consider detections with confidence lower than this as suspicious
Don't consider detections with confidence higher than this as suspicious
What's the IOU threshold for considering two boxes the same?
How many occurrences of a single location (as defined by the IOU threshold)
are required before we declare it suspicious?
"Ignore ""suspicious"" detections larger than some size; these are often animals"
taking up the whole image.  This is expressed as a fraction of the image size.
"Ignore ""suspicious"" detections smaller than some size"
Ignore folders with more than this many images in them
A list of classes we don't want to treat as suspicious. Each element is an int.
Load detections from a filter file rather than finding them from the detector output
".json file containing detections, should be called detectionIndex.json in the filtering_* folder"
produced in the first pass
(optional) List of filenames remaining after deletion of identified
repeated detections that are actually animals.  This should be a flat
"text file, one relative filename per line.  See enumerate_images()."
Turn on/off optional outputs
Determines whether bounding-box rendering errors (typically network errors) should
be treated as failures
Box rendering options
State variables
"Replace filename tokens after reading, useful when the directory structure"
has changed relative to the structure the detector saw
How many folders up from the leaf nodes should we be going to aggregate images?
An optional function that takes a string (an image file name) and returns
"a string (the corresponding  folder ID), typically used when multiple folders"
actually correspond to the same camera in a manufacturer-specific way (e.g.
a/b/c/RECONYX100 and a/b/c/RECONYX101 may really be the same camera).
Include/exclude specific folders... only one of these may be
"specified; ""including"" folders includes *only* those folders."
"Optionally show *other* detections (i.e., detections other than the"
one the user is evaluating) in a light gray
"If bRenderOtherDetections is True, what color should we use to render the"
(hopefully pretty subtle) non-target detections?
""
"In theory I'd like these ""other detection"" rectangles to be partially"
"transparent, but this is not straightforward, and the alpha is ignored"
"here.  But maybe if I leave it here and wish hard enough, someday it"
will work.
""
otherDetectionsColors = ['dimgray']
Sort detections within a directory so nearby detections are adjacent
"in the list, for faster review."
""
"Can be None, 'xsort', or 'clustersort'"
""
* None sorts detections chronologically by first occurrence
* 'xsort' sorts detections from left to right
* 'clustersort' clusters detections and sorts by cluster
Only relevant if smartSort == 'clustersort'
"The data table (Pandas DataFrame), as loaded from the input json file via"
load_api_results()
"The other fields in the input json file, loaded via load_api_results()"
The data table after modification
dict mapping folder names to whole rows from the data table
dict mapping filenames to rows in the master table
"An array of length nDirs, where each element is a list of DetectionLocation"
objects for that directory that have been flagged as suspicious
"This is a bit of a hack right now, but for future-proofing, I don't want to call this"
"to retrieve anything other than the highest-confidence detection, and I'm assuming this"
"is already sorted, so assert() that."
It's not clear whether it's better to use instances[0].bbox or self.bbox
"here... they should be very similar, unless iouThreshold is very low."
self.bbox is a better representation of the overal DetectionLocation.
%% Helper functions
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t"
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t"
%% Sort a list of candidate detections to make them visually easier to review
Just sort by the X location of each box
"Prepare a list of points to represent each box,"
that's what we'll use for clustering
Upper-left
"points.append([det.bbox[0],det.bbox[1]])"
Center
"Labels *could* be any unique labels according to the docs, but in practice"
they are unique integers from 0:nClusters
Make sure the labels are unique incrementing integers
Store the label assigned to each cluster
"Now sort the clusters by their x coordinate, and re-assign labels"
so the labels are sortable
"Compute the centroid for debugging, but we're only going to use the x"
"coordinate.  This is the centroid of points used to represent detections,"
which may be box centers or box corners.
old_cluster_label_to_new_cluster_label[old_cluster_label] =\
new_cluster_labels[old_cluster_label]
%% Look for matches (one directory)
List of DetectionLocations
candidateDetections = []
Create a tree to store candidate detections
For each image in this directory
""
iDirectoryRow = 0; row = rows.iloc[iDirectoryRow]
""
"iDirectoryRow is a pandas index, so it may not start from zero;"
"for debugging, we maintain i_iteration as a loop index."
print('Searching row {} of {} (index {}) in dir {}'.\
"format(i_iteration,len(rows),iDirectoryRow,dirName))"
Don't bother checking images with no detections above threshold
"Array of dicts, where each element is"
{
"'category': '1',  # str value, category ID"
"'conf': 0.926,  # confidence of this detections"
""
"# (x_min, y_min) is upper-left, all in relative coordinates"
"'bbox': [x_min, y_min, width_of_box, height_of_box]"
""
}
For each detection in this image
"This is no longer strictly true; I sometimes run RDE in stages, so"
some probabilities have already been made negative
""
assert confidence >= 0.0 and confidence <= 1.0
Optionally exclude some classes from consideration as suspicious
Is this detection too big to be suspicious?
print('Illegal zero-size bounding box on image {}'.format(filename))
These are relative coordinates
print('Ignoring very small detection with area {}'.format(area))
print('Ignoring very large detection with area {}'.format(area))
This will return candidates of all classes
For each detection in our candidate list
Don't match across categories
Is this a match?
"If so, add this example to the list for this detection"
We *don't* break here; we allow this instance to possibly
match multiple candidates.  There isn't an obvious right or
wrong here.
...for each detection on our candidate list
"If we found no matches, add this to the candidate list"
candidateDetections.append(candidate)
pyqtree
...for each detection
...for each row
Get all candidate detections
print('Found {} candidate detections for folder {}'.format(
"len(candidateDetections),dirName))"
"For debugging only, it's convenient to have these sorted"
as if they had never gone into a tree structure.  Typically
this is in practce a sort by filename.
...def find_matches_in_directory(dirName)
"%% Update the detection table based on suspicious results, write .csv output"
"An array of length nDirs, where each element is a list of DetectionLocation"
objects for that directory that have been flagged as suspicious
For each directory
For each suspicious detection group in this directory
For each instance of this suspicious detection
This should match the bbox for the detection event
The bbox for this instance should be almost the same as the bbox
"for this detection group, where ""almost"" is defined by the IOU"
threshold.
if iou < options.iouThreshold:
"print('IOU warning: {},{}'.format(iou,options.iouThreshold))"
Make sure the bounding box matches
"Make the probability negative, if it hasn't been switched by"
another bounding box
...for each instance
...for each detection
...for each directory
Update maximum probabilities
For each row...
No longer strictly true; sometimes I run RDE on RDE output
assert maxPOriginal >= 0
We should only be making detections *less* likely in this process
"If there was a meaningful change, count it"
"Negative probabilities should be the only reason maxP changed, so"
we should have found at least one negative value if we reached
this point.
...if there was a meaningful change to the max probability for this row
...for each row
If we're also writing output...
"...def update_detection_table(RepeatDetectionResults,options)"
%% Main function
#%% Input handling
Validate some options
Load the filtering file
Load the same options we used when finding repeat detections
...except for things that explicitly tell this function not to
find repeat detections.
...if we're loading from an existing filtering file
Check early to avoid problems with the output folder
"Load file to a pandas dataframe.  Also populates 'max_detection_conf', even if it's"
not present in the .json file.
detectionResults[detectionResults['failure'].notna()]
"Before doing any real work, make sure we can *probably* access images"
"This is just a cursory check on the first image, but it heads off most"
"problems related to incorrect mount points, etc.  Better to do this before"
spending 20 minutes finding repeat detections.
#%% Separate files into directories
This will be a map from a directory name to smaller data frames
This is a mapping back into the rows of the original table
iRow = 0; row = detectionResults.iloc[0]
Create a new DataFrame with just this row
rowsByDirectory[dirName] = pd.DataFrame(row)
...for each unique detection
Convert lists of rows to proper DataFrames
#% Look for matches (or load them from file)
"Are we actually looking for matches, or just loading from a file?"
length-nDirs list of lists of DetectionLocation objects
We're actually looking for matches...
"We get slightly nicer progress bar behavior using threads, by passing a pbar"
object and letting it get updated.  We can't serialize this object across
processes.
#%% Find suspicious locations based on match results
For each directory
""
iDir = 51
A list of DetectionLocation objects
A list of DetectionLocation objects
occurrenceList is a list of file/detection pairs
Sort the above-threshold detections for easier review
...for each directory
If we're just loading detections from a file...
"We're skipping detection-finding, but to see which images are actually legit false"
"positives, we may be looking for physical files or loading from a text file."
For each directory
iDir = 0; detections = suspiciousDetections[0]
""
"suspiciousDetections is an array of DetectionLocation objects,"
one per directory.
For each detection that was present before filtering
iDetection = 0; detection = detections[iDetection]
Are we checking the directory to see whether detections were actually false
"positives, or reading from a list?"
Is the image still there?
"If not, remove this from the list of suspicious detections"
...for each detection
...for each directory
...if we are/aren't finding detections (vs. loading from file)
Create filtering directory
iDir = 0; suspiciousDetectionsThisDir = suspiciousDetections[iDir]
suspiciousDetectionsThisDir is a list of DetectionLocation objects
iDetection = 0; detection = suspiciousDetectionsThisDir[0]
Sort instances in descending order by confidence
Choose the highest-confidence index
Should we render (typically in a very light color) detections
*other* than the one we're highlighting here?
Render other detections first (typically in a thin+light box)
Now render the example detection (on top of at least one
of the other detections)
This converts the *first* instance to an API standard detection;
"because we just sorted this list in descending order by confidence,"
this is the highest-confidence detection.
...if we are/aren't rendering other bounding boxes
...for each detection in this folder
...for each folder
Write out the detection index
...if we're writing filtering info
...find_repeat_detections()
#######
""
find_repeat_detections.py
""
"If you want to use this script, we recommend that you read the user's guide:"
""
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms
""
"Really, don't try to run this script without reading the user's guide, you'll think"
it's more magical than it is.
""
"This script looks through a sequence of detections in the API output json file, and finds"
"candidates that might be ""repeated false positives"", i.e. that random branch that the"
detector thinks is an animal/person/vehicle.
""
"Typically after running this script, you would do a manual step to remove"
"true positives, then run remove_repeat_detections to produce a final output file."
""
There's no way that statement was self-explanatory; see the user's guide.
""
#######
%% Constants and imports
%% Interactive driver
%%
%% Command-line driver
Convert to an options object
""
If a request has been sent to AML for batch scoring but the monitoring thread of the API was
"interrupted (uncaught exception or having to re-start the API container), we could manually"
"aggregate results from each shard using this script, assuming all jobs submitted to AML have finished."
""
Need to have set environment variables STORAGE_ACCOUNT_NAME and STORAGE_ACCOUNT_KEY to those of the
"storage account backing the API. Also need to adjust the INTERNAL_CONTAINER, AML_CONTAINER and"
AML_CONFIG fields in api_core/orchestrator_api/api_config.py to match the instance of the API that this
request was submitted to.
""
May need to change the import statement in api_core/orchestrator_api/orchestrator.py
"""from sas_blob_utils import SasBlob"" to"
"""from .sas_blob_utils import SasBlob"" to not confuse with the module in AI4Eutils;"
"and change ""import api_config"" to"
"""from api.batch_processing.api_core.orchestrator_api import api_config"""
Execute this script from the root of the repository. You may need to add the repository to PYTHONPATH.
"list_jobs_submitted cannot be serialized (""can't pickle _thread.RLock objects ""), but"
do not need it for aggregating results
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%% helper classes and functions
TODO log exception when we have more telemetry
TODO check that the expiry date of input_container_sas is at least a month
into the future
"if no permission specified explicitly but has an access policy, assumes okay"
TODO - check based on access policy as well
return current UTC time as a string in the ISO 8601 format (so we can query by
timestamp in the Cosmos DB job status table.
example: '2021-02-08T20:02:05.699689Z'
"image_paths will have length at least 1, otherwise would have ended before this step"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
a job moves from created to running/problem after the Batch Job has been submitted
"job_id should be unique across all instances, and is also the partition key"
TODO do not read the entry first to get the call_params when the Cosmos SDK add a
patching functionality:
https://feedback.azure.com/forums/263030-azure-cosmos-db/suggestions/6693091-be-able-to-do-partial-updates-on-document
need to retain other fields in 'status' to be able to restart monitoring thread
retain existing fields; update as needed
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
sentinel should change if new configurations are available
configs have not changed
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Gunicorn logger handler will get attached if needed in server.py
set for all tasks in the job
cannot execute the scoring script that is in the mounted directory; has to be copied to cwd
not luck giving the commandline arguments via formatted string - set as env vars instead
form shards of images and assign each shard to a Task
for persisting stdout and stderr
persist stdout and stderr (will be removed when node removed)
paths are relative to the Task working directory
can also just upload on failure
first try submitting Tasks
retry submitting Tasks
Change the Job's on_all_tasks_complete option to 'terminateJob' so the Job's status changes automatically
after all submitted tasks are done
This is so that we do not take up the quota for active Jobs in the Batch account.
return type: TaskAddCollectionResult
actually we should probably only re-submit if it's a server_error
docs: # https://docs.microsoft.com/en-us/rest/api/batchservice/odata-filters-in-batch#list-tasks
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%% Flask app
reference: https://trstringer.com/logging-flask-gunicorn-the-manageable-way/
%% Helper classes
%% Flask endpoints
required params
can be an URL to a file not hosted in an Azure blob storage container
"if use_url, then images_requested_json_sas is required"
optional params
check model_version is among the available model versions
check request_name has only allowed characters
optional params for telemetry collection - logged to status table for now as part of call_params
All API instances / node pools share a quota on total number of active Jobs;
we cannot accept new Job submissions if we are at the quota
required fields
request_status is either completed or failed
the create_batch_job thread will stop when it wakes up the next time
"Fix for Zooniverse - deleting any ""-"" characters in the job_id"
"If the status is running, it could be a Job submitted before the last restart of this"
"API instance. If that is the case, we should start to monitor its progress again."
WARNING model_version could be wrong (a newer version number gets written to the output file) around
"the time that  the model is updated, if this request was submitted before the model update"
and the API restart; this should be quite rare
conform to previous schemes
%% undocumented endpoints
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Gunicorn logger handler will get attached if needed in server.py
request_name and request_submission_timestamp are for appending to
output file names
image_paths can be a list of strings (Azure blob names or public URLs)
"or a list of length-2 lists where each is a [image_id, metadata] pair"
Case 1: listing all images in the container
- not possible to have attached metadata if listing images in a blob
list all images to process
+ 1 so if the number of images listed > MAX_NUMBER_IMAGES_ACCEPTED_PER_JOB
we will know and not proceed
Case 2: user supplied a list of images to process; can include metadata
filter down to those conforming to the provided prefix and accepted suffixes (image file types)
prefix is case-sensitive; suffix is not
"Although urlparse(p).path preserves the extension on local paths, it will not work for"
"blob file names that contains ""#"", which will be treated as indication of a query."
"If the URL is generated via Azure Blob Storage, the ""#"" char will be properly encoded"
apply the first_n and sample_n filters
OK if first_n > total number of images
sample by shuffling image paths and take the first sample_n images
"upload the image list to the container, which is also mounted on all nodes"
all sharding and scoring use the uploaded list
now request_status moves from created to running
an extra field to allow the monitoring thread to restart after an API restart: total number of tasks
also record the number of images to process for reporting
start the monitor thread with the same name
"both succeeded and failed tasks are marked ""completed"" on Batch"
"preserving format from before, but SAS URL to 'failed_images' and 'images' are no longer provided"
"failures should be contained in the output entries, indicated by an 'error' field"
"when people download this, the timestamp will have : replaced by _"
check if the result blob has already been written (could be another instance of the API / worker thread)
"and if so, skip aggregating and uploading the results, and just generate the SAS URL, which"
could be needed still if the previous request_status was `problem`.
upload the output JSON to the Job folder
%% Helper functions *copied* from ct_utils.py and visualization/visualization_utils.py
"Determine the factor, which shifts the decimal point of x"
just behind the last significant digit
"Shift decimal point by multiplicatipon with factor, flooring, and"
division by factor
PIL.Image.convert() returns a converted copy of this image
alter orientation as needed according to EXIF tag 0x112 (274) for Orientation
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28
https://www.media.mit.edu/pia/Research/deepview/exif.html
"%% TFDetector class, an unmodified *copy* of the class in detection/tf_detector.py,"
so we do not have to import the packages required by run_detector.py
Number of decimal places to round to for confidence and bbox coordinates
"MegaDetector was trained with batch size of 1, and the resizing function is a part"
of the inference graph
An enumeration of failure reasons
"change from [y1, x1, y2, x2] to [x1, y1, width, height]"
convert numpy floats to Python floats
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size
"np_images = [np.asarray(image, np.uint8) for image in images]"
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)"
performs inference
our batch size is 1; need to loop the batch dim if supporting batch size > 1
%% Scoring script
determine if there is metadata attached to each image_id
information to determine input and output locations
other parameters for the task
test that we can write to output path; also in case there is no image to process
list images to process
"items in this list can be strings or [image_id, metadata]"
model path
"Path to .pb TensorFlow detector model file, relative to the"
models/megadetector_copies folder in mounted container
score the images
""
manage_video_batch.py
""
Notebook-esque script to manage the process of running a local batch of videos
through MD.  Defers most of the heavy lifting to manage_local_batch.py .
""
%% Imports and constants
%% Split videos into frames
"%% List frame files, break into folders"
Find unique (relative) folders
fn = frame_files[0]
%% List videos
%% Check for videos that are missing entirely
list(folder_to_frame_files.keys())[0]
video_filenames[0]
fn = video_filenames[0]
%% Check for videos with very few frames
%% Print the list of videos that are problematic
%% Process images like we would for any other camera trap job
"...typically using manage_local_batch.py, but do this however you like, as long"
as you get a results file at the end.
""
"If you do RDE, remember to use the second folder from the bottom, rather than the"
bottom-most folder.
%% Convert frame results to video results
%% Confirm that the videos in the .json file are what we expect them to be
%% Scrap
%% Test a possibly-broken video
%% List videos in a folder
%% Imports
%% Constants
%% Classes
class variables
"instance variables, in order of when they are typically set"
Leaving this commented out to remind us that we don't want this check here; let
the API fail on these images.  It's a huge hassle to remove non-image
files.
""
for path_or_url in images_list:
if not is_image_file_or_url(path_or_url):
raise ValueError('{} is not an image'.format(path_or_url))
Commented out as a reminder: don't check task status (which is a rest API call)
in __repr__; require the caller to explicitly request status
"status=getattr(self, 'status', None))"
estimate # of failed images from failed shards
Download all three JSON urls to memory
Remove files that were submitted but don't appear to be images
assert all(is_image_file_or_url(s) for s in submitted_images)
Diff submitted and processed images
Confirm that the procesed images are a subset of the submitted images
https://www.geeksforgeeks.org/break-list-chunks-size-n-python/
%% Interactive driver
%%
%%
%%
%% Imports and constants
%% Constants I set per script
## Required
base_output_folder_name = os.path.expanduser('~/postprocessing/' + organization_name_short)
Shared Access Signature (SAS) tokens for the Azure Blob Storage container.
Leading question mark is optional.
""
The read-only token is used for accessing images; the write-enabled token is
used for writing file lists.
## Typically left as default
"Pre-pended to all folder names/prefixes, if they're defined below"
"This is how we break the container up into multiple taskgroups, e.g., for"
separate surveys. The typical case is to do the whole container as a single
taskgroup.
"If your ""folders"" are really logical folders corresponding to multiple folders,"
map them here
"A list of .json files to load images from, instead of enumerating.  Formatted as a"
"dictionary, like folder_prefixes."
This is only necessary if you will be performing postprocessing steps that
"don't yet support SAS URLs, specifically the ""subsetting"" step, or in some"
cases the splitting of files into multiple output directories for
empty/animal/vehicle/people.
""
"For those applications, you will need to mount the container to a local drive."
For this case I recommend using rclone whether you are on Windows or Linux;
rclone is much easier than blobfuse for transient mounting.
""
"But most of the time, you can ignore this."
"Supported model_versions: '4', '3', '4_prelim'"
""
Also available at the /supported_model_versions and /default_model_version
endpoints
""
"Unless you have any specific reason to set this to a non-default value, leave"
"it at the default, which as of 2020.04.28 is MegaDetector 4.1"
""
"additional_task_args = {""model_version"":""4_prelim""}"
""
"file_lists_by_folder will contain a list of local JSON file names,"
each JSON file contains a list of blob names corresponding to an API taskgroup
"%% Derived variables, path setup"
local folders
Turn warnings into errors if more than this many images are missing
%% Support functions
"scheme, netloc, path, query, fragment"
%% Read images from lists or enumerate blobs to files
folder_name = folder_names[0]
"Load file lists for this ""folder"""
""
file_list = input_file_lists[folder][0]
Write to file
A flat list of blob paths for each folder
folder_name = folder_names[0]
"If we don't/do have multiple prefixes to enumerate for this ""folder"""
"If this is intended to be a folder, it needs to end in '/', otherwise"
files that start with the same string will match too
...for each prefix
Write to file
...for each folder
%% Some just-to-be-safe double-checking around enumeration
Make sure each folder has at least one image matched; the opposite is usually a sign of a copy/paste issue
...for each image
...for each prefix
...for each folder
Make sure each image comes from one of our folders; the opposite is usually a sign of a bug up above
...for each prefix
...for each folder
...for each image
%% Divide images into chunks for each folder
The JSON file at folder_chunks[i][j] corresponds to task j of taskgroup i
list_file = file_lists_by_folder[0]
"%% Create taskgroups and tasks, and upload image lists to blob storage"
periods not allowed in task names
%% Generate API calls for each task
clipboard.copy(request_strings[0])
clipboard.copy('\n\n'.join(request_strings))
%% Run the tasks (don't run this cell unless you are absolutely sure!)
I really want to make sure I'm sure...
%% Estimate total time
Around 0.8s/image on 16 GPUs
%% Manually create task groups if we ran the tasks manually
%%
"%% Write task information out to disk, in case we need to resume"
%% Status check
print(task.id)
%% Resume jobs if this notebook closes
%% For multiple tasks (use this only when we're merging with another job)
%% For just the one task
%% Load into separate taskgroups
p = task_cache_paths[0]
%% Typically merge everything into one taskgroup
"%% Look for failed shards or missing images, start new tasks if necessary"
List of lists of paths
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup];
"Make a copy, because we append to taskgroup"
i_task = 0; task = tasks[i_task]
Each taskgroup corresponds to one of our folders
Check that we have (almost) all the images
Now look for failed images
Write it out as a flat list as well (without explanation of failures)
...for each task
...for each task group
%% Pull results
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup]; task = taskgroup[0]
Each taskgroup corresponds to one of our folders
...for each task
...for each task group
%% Combine results from task groups into final output files
Check that we have (almost) all the images
The only reason we should ever have a repeated request is the case where an
"image was missing and we reprocessed it, or where it failed and later succeeded"
"There may be non-image files in the request list, ignore those"
Something has gone bonkers if there are images in the results that
aren't in the request
...for each folder
%% Post-processing (no ground truth)
i_folder = 0; folder_name_raw = folder_names[i_folder]
"Omit some pages from the output, useful when animals are rare"
%% Manual processing follows
""
"Everything after this should be considered mostly manual, and no longer includes"
looping over folders.
""
%% RDE (sample directory collapsing)
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images"
"%% Repeat detection elimination, phase 1"
"Deliberately leaving these imports here, rather than at the top, because this cell is not"
typically executed
To invoke custom collapsing of folders for a particular manufacturer's naming scheme
options.customDirNameFunction = remove_overflow_folders
Exclude people and vehicles from RDE
"options.excludeClasses = [2,3]"
%% Manual RDE step
# DELETE THE VALID DETECTIONS ##
%% Re-filtering
%% Post-processing (post-RDE)
i_folder = 0; folder_name_raw = folder_names[i_folder]
"Omit some pages from the output, useful when animals are rare"
"print('No RDE file available for {}, skipping'.format(folder_name))"
continue
%% Create a new category for large boxes
%% Subsetting
i_folder = 0; folder_name = folders[i_folder]
%% String replacement
%% Folder splitting
i_folder = 0; folder_name_raw = folder_names[i_folder]
api_output_file = folder_name_to_combined_output_file[folder_name]
%% Imports and constants
from ai4eutils
To specify a non-default confidence threshold for including detections in the .json file
Turn warnings into errors if more than this many images are missing
Only relevant when we're using a single GPU
"Specify a target image size when running MD... strongly recommended to leave this at ""None"""
Only relevant when running on CPU
OS-specific script line continuation character
OS-specific script comment character
"Prefer threads on Windows, processes on Linux"
"This is for things like image rendering, not for MegaDetector"
Should we use YOLOv5's val.py instead of run_detector_batch.py?
Directory in which to run val.py.  Only relevant if use_yolo_inference_scripts is True.
Should we remove intermediate files used for running YOLOv5's val.py?
""
Only relevant if use_yolo_inference_scripts is True.
Should we apply YOLOv5's augmentation?  Only allowed when use_yolo_inference_scripts
is True.
%% Constants I set per script
Optional descriptor
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v5.0.0/md_v5b.0.0.pt')
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v4.1.0/md_v4.1.0.pb')
"Number of jobs to split data into, typically equal to the number of available GPUs"
Only used to print out a time estimate
"%% Derived variables, constant validation, path setup"
%% Enumerate files
%% Load files from prior enumeration
%% Divide images into chunks
%% Estimate total time
%% Write file lists
%% Generate commands
"A list of the scripts tied to each GPU, as absolute paths.  We'll write this out at"
the end so each GPU's list of commands can be run at once.  Generally only used when
"running lots of small batches via YOLOv5's val.py, which doesn't support checkpointing."
i_task = 0; task = task_info[i_task]
Generate the script to run MD
Check whether this output file exists
Generate the script to resume from the checkpoint (only supported with MD inference code)
...for each task
Write out a script for each GPU that runs all of the commands associated with
that GPU.  Typically only used when running lots of little scripts in lieu
of checkpointing.
...for each GPU
%% Run the tasks
%%% Run the tasks (commented out)
i_task = 0; task = task_info[i_task]
"This will write absolute paths to the file, we'll fix this later"
...for each chunk
...if False
"%% Load results, look for failed or missing images in each task"
i_task = 0; task = task_info[i_task]
im = task_results['images'][0]
...for each task
%% Merge results files and make images relative
im = combined_results['images'][0]
%% Post-processing (pre-RDE)
options.sample_seed = 0
"Omit some pages from the output, useful when animals are rare"
%% RDE (sample directory collapsing)
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images"
%%
%%
%%
relativePath = image_filenames[0]
"%% Repeat detection elimination, phase 1"
"Deliberately leaving these imports here, rather than at the top, because this"
cell is not typically executed
options.minSuspiciousDetectionSize = 0.05
This will cause a very light gray box to get drawn around all the detections
we're *not* considering as suspicious.
options.lineThickness = 5
options.boxExpansion = 8
To invoke custom collapsing of folders for a particular manufacturer's naming scheme
options.customDirNameFunction = remove_overflow_folders
Exclude people and vehicles from RDE
"options.excludeClasses = [2,3]"
options.maxImagesPerFolder = 50000
options.includeFolders = ['a/b/c']
options.excludeFolder = ['a/b/c']
"Can be None, 'xsort', or 'clustersort'"
import clipboard; clipboard.copy(os.path.dirname(suspiciousDetectionResults.filterFile))
path_utils.open_file(os.path.dirname(suspiciousDetectionResults.filterFile))
%% Manual RDE step
# DELETE THE VALID DETECTIONS ##
%% Re-filtering
%% Post-processing (post-RDE)
options.sample_seed = 0
"Omit some pages from the output, useful when animals are rare"
"%% Run MegaClassifier (actually, write out a script that runs MegaClassifier)"
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
#%% Set up environment
commands.append('cd CameraTraps/classification\n')
commands.append('conda activate cameratraps-classifier\n')
#%% Crop images
fn = input_files[0]
#%% Run classifier
fn = input_files[0]
#%% Remap classifier outputs
fn = input_files[0]
#%% Merge classification and detection outputs
fn = input_files[0]
#%% Write  out classification script
"%% Run a non-MegaClassifier classifier (i.e., a classifier with no output mapping)"
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
#%% Set up environment
#%% Crop images
fn = input_files[0]
#%% Run classifier
fn = input_files[0]
#%% Merge classification and detection outputs
fn = input_files[0]
#%% Write everything out
%% Run the classifier(s) via the .sh script(s) or batch file(s) we just wrote
...
%% Within-image classification smoothing
""
Only count detections with a classification confidence threshold above
"*classification_confidence_threshold*, which in practice means we're only"
looking at one category per detection.
""
If an image has at least *min_detections_above_threshold* such detections
"in the most common category, and no more than *max_detections_secondary_class*"
"in the second-most-common category, flip all detections to the most common"
category.
""
"Optionally treat some classes as particularly unreliable, typically used to overwrite an"
"""other"" class."
""
This cell also removes everything but the non-dominant classification for each detection.
""
How many detections do we need above the classification threshold to determine a dominant category
for an image?
"Even if we have a dominant class, if a non-dominant class has at least this many classifications"
"in an image, leave them alone."
"If the dominant class has at least this many classifications, overwrite ""other"" classifications"
What confidence threshold should we use for assessing the dominant category in an image?
Which classifications should we even bother over-writing?
Detection confidence threshold for things we count when determining a dominant class
Which detections should we even bother over-writing?
"Before we do anything else, get rid of everything but the top classification"
for each detection.
...for each detection in this image
...for each image
im = d['images'][0]
...for each classification
...if there are classifications for this detection
...for each detection
"Handle a quirky special case: if the most common category is ""other"" and"
"it's ""tied"" with the second-most-common category, swap them"
secondary_count = category_to_count[keys[1]]
The 'secondary count' is the most common non-other class
If we have at least *min_detections_to_overwrite_other* in a category that isn't
"""other"", change all ""other"" classifications to that category"
...for each classification
...if there are classifications for this detection
...for each detection
"...if we should overwrite all ""other"" classifications"
"At this point, we know we have a dominant category; change all other above-threshold"
"classifications to that category.  That category may have been ""other"", in which"
case we may have already made the relevant changes.
det = detections[0]
...for each classification
...if there are classifications for this detection
...for each detection
...for each image
...for each file we want to smooth
"%% Post-processing (post-classification, post-within-image-smoothing)"
classification_detection_file = classification_detection_files[1]
%% Read EXIF data from all images
%% Prepare COCO-camera-traps-compatible image objects for EXIF results
import dateutil
"This is a standard format for EXIF datetime, and dateutil.parser"
doesn't handle it correctly.
return dateutil.parser.parse(s)
exif_result = exif_results[0]
Currently we assume that each leaf-node folder is a location
"We collected this image this century, but not today, make sure the parsed datetime"
jives with that.
""
The latter check is to make sure we don't repeat a particular pathological approach
"to datetime parsing, where dateutil parses time correctly, but swaps in the current"
date when it's not sure where the date is.
...for each exif image result
%% Assemble into sequences
Make a list of images appearing at each location
im = image_info[0]
%% Load classification results
Map each filename to classification results for that file
%% Smooth classification results over sequences (prep)
These are the only classes to which we're going to switch other classifications
Only switch classifications to the dominant class if we see the dominant class at least
this many times
"If we see more than this many of a class that are above threshold, don't switch those"
classifications to the dominant class.
"If the ratio between a dominant class and a secondary class count is greater than this,"
"regardless of the secondary class count, switch those classificaitons (i.e., ignore"
max_secondary_class_classifications_above_threshold_for_class_smoothing).
""
"This may be different for different dominant classes, e.g. if we see lots of cows, they really"
"tend to be cows.  Less so for canids, so we set a higher ""override ratio"" for canids."
"If there are at least this many classifications for the dominant class in a sequence,"
"regardless of what that class is, convert all 'other' classifications (regardless of"
confidence) to that class.
"If there are at least this many classifications for the dominant class in a sequence,"
"regardless of what that class is, classify all previously-unclassified detections"
as that class.
Only count classifications above this confidence level when determining the dominant
"class, and when deciding whether to switch other classifications."
Confidence values to use when we change a detection's classification (the
original confidence value is irrelevant at that point)
%% Smooth classification results over sequences (supporting functions)
im = images_this_sequence[0]
det = results_this_image['detections'][0]
Only process animal detections
Only process detections with classification information
"We only care about top-1 classifications, remove everything else"
Make sure the list of classifications is already sorted by confidence
...and just keep the first one
"Confidence values should be sorted within a detection; verify this, and ignore"
...for each detection in this image
...for each image in this sequence
...top_classifications_for_sequence()
Count above-threshold classifications in this sequence
Sort the dictionary in descending order by count
"Handle a quirky special case: if the most common category is ""other"" and"
"it's ""tied"" with the second-most-common category, swap them."
...def count_above_threshold_classifications()
%% Smooth classifications at the sequence level (main loop)
Break if this token is contained in a filename (set to None for normal operation)
i_sequence = 0; seq_id = all_sequences[i_sequence]
Count top-1 classifications in this sequence (regardless of confidence)
Handy debugging code for looking at the numbers for a particular sequence
Count above-threshold classifications for each category
"If our dominant category ID isn't something we want to smooth to, don't mess around with this sequence"
"# Smooth ""other"" classifications ##"
"By not re-computing ""max_count"" here, we are making a decision that the count used"
"to decide whether a class should overwrite another class does not include any ""other"""
classifications we changed to be the dominant class.  If we wanted to include those...
""
sorted_category_to_count = count_above_threshold_classifications(classifications_this_sequence)
max_count = get_first_value_from_sorted_dictionary(sorted_category_to_count)
assert dominant_category_id == get_first_key_from_sorted_dictionary(sorted_category_to_count)
# Smooth non-dominant classes ##
Don't flip classes to the dominant class if they have a large number of classifications
"Don't smooth over this class if there are a bunch of them, and the ratio"
if primary to secondary class count isn't too large
Default ratio
Does this dominant class have a custom ratio?
# Smooth unclassified detections ##
...for each sequence
%% Write smoothed classification results
"%% Post-processing (post-classification, post-within-image-and-within-sequence-smoothing)"
%% Zip .json files
%% 99.9% of jobs end here
Everything after this is run ad hoc and/or requires some manual editing.
%% Compare results files for different model versions (or before/after RDE)
Choose all pairwise combinations of the files in [filenames]
%% Merge in high-confidence detections from another results file
%% Create a new category for large boxes
"This is a size threshold, not a confidence threshold"
size_options.categories_to_separate = [3]
%% Preview large boxes
%% .json splitting
options.query = None
options.replacement = None
Reminder: 'n_from_bottom' with a parameter of zero is the same as 'bottom'
%% Custom splitting/subsetting
i_folder = 0; folder_name = folders[i_folder]
"This doesn't do anything in this case, since we're not splitting folders"
options.make_folder_relative = True
%% String replacement
%% Splitting images into folders
%% Generate commands for a subset of tasks
i_task = 8
...for each task
%% End notebook: turn this script into a notebook (how meta!)
Exclude everything before the first cell
Remove the first [first_non_empty_lines] from the list
Add the last cell
""
xmp_integration.py
""
"Tools for loading MegaDetector batch API results into XMP metadata, specifically"
for consumption in digiKam:
""
https://cran.r-project.org/web/packages/camtrapR/vignettes/camtrapr2.html
""
%% Imports and constants
%% Class definitions
Folder where images are stored
.json file containing MegaDetector output
"String to remove from all path names, typically representing a"
prefix that was added during MegaDetector processing
Optionally *rename* (not copy) all images that have no detections
above [rename_conf] for the categories in rename_cats from x.jpg to
x.check.jpg
"Comma-deleted list of category names (or ""all"") to apply the rename_conf"
behavior to.
"Minimum detection threshold (applies to all classes, defaults to None,"
i.e. 0.0
%% Functions
Relative image path
Absolute image path
List of categories to write to XMP metadata
Categories with above-threshold detections present for
this image
Maximum confidence for each category
Have we already added this to the list of categories to
write out to this image?
If we're supposed to compare to a threshold...
Else we treat *any* detection as valid...
Keep track of the highest-confidence detection for this class
If we're doing the rename/.check behavior...
Legacy code to rename files where XMP writing failed
%% Interactive/test driver
%%
%% Command-line driver
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Cosmos DB `batch-api-jobs` table for job status
"aggregate the number of images, country and organization names info from each job"
submitted during yesterday (UTC time)
create the card
""
api_frontend.py
""
"Defines the Flask app, which takes requests (one or more images) from"
"remote callers and pushes the images onto the shared Redis queue, to be processed"
by the main service in api_backend.py .
""
%% Imports
%% Initialization
%% Support functions
Make a dict that the request_processing_function can return to the endpoint
function to notify it of an error
Verify that the content uploaded is not too big
""
request.content_length is the length of the total payload
Verify that the number of images is acceptable
...def check_posted_data(request)
%% Main loop
Check whether the request_processing_function had an error
Write images to temporary files
""
TODO: read from memory rather than using intermediate files
"Submit the image(s) for processing by api_backend.py, who is waiting on this queue"
TODO: convert to a blocking read and eliminate the sleep() statement in this loop
"image = Image.open(os.path.join(temp_direc, image_name))"
...if we do/don't have a request available on the queue
...while(True)
...def detect_sync()
%% Command-line driver
use --non-docker if you are testing without Docker
""
python api_frontend.py --non-docker
# Camera trap real-time API configuration
"Full path to the temporary folder for image storage, only meaningful"
within the Docker container
Upper limit on total content length (all images and parameters)
Minimum confidence threshold for detections
Minimum confidence threshold for showing a bounding box on the output image
Use this when testing without Docker
""
api_backend.py
""
"Defines the model execution service, which pulls requests (one or more images)"
"from the shared Redis queue, and runs them through the TF model."
""
%% Imports
%% Initialization
%% Main loop
TODO: convert to a blocking read and eliminate the sleep() statement in this loop
Filter the detections by the confidence threshold
""
"Each result is [ymin, xmin, ymax, xmax, confidence, category]"
""
"Coordinates are relative, with the origin in the upper-left"
...if serialized_entry
...while(True)
...def detect_process()
%% Command-line driver
use --non-docker if you are testing without Docker
""
python api_frontend.py --non-docker
run detections on a test image to load the model
%%
Importing libraries
%%
%%
%%
app = typer.Typer()
%%
@app.command()
GPU configuration: set up GPUs based on availability and user specification
Environment variable setup for numpy multi-threading
Load and set configurations from the YAML file
Set a global seed for reproducibility
"If the annotation directory does not have a data split, split the data first"
Replace annotation dir from config with the directory containing the split files
Split the data according to the split type
Get the path to the annotation files
Split training data
Split validation and test data
Dataset and algorithm loading based on the configuration
Logger setup based on the specified logger type
Callbacks for model checkpointing and learning rate monitoring
Trainer configuration in PyTorch Lightning
"Training, validation, or evaluation execution based on the mode"
%%
%%
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
PyTorch imports
"Importing the model, dataset, transformations and utility functions from PytorchWildlife"
Importing the utility function for saving cropped images
Setting the device to use for computations ('cuda' indicates GPU)
Initializing the MegaDetectorV5 model for image detection
Creating a dataset of images with the specified transform
Creating a DataLoader for batching and parallel processing of the images
Performing batch detection on the images
Saving the detected objects as cropped images
%%
Read the original CSV file
Prepare a list to store new records for the new CSV
Process the data if the name of the file is in the dataframe
Save the crop into a new csv
Add record to the new CSV data
Create a DataFrame from the new records
Define the path for the new CSV file
Save the new DataFrame to CSV
# DATA SPLITTING
Load the data from the csv file
Separate the features and the targets
First split to separate out the test set
Adjust val_size to account for the initial split
Second split to separate out the validation set
"Combine features, labels, and classification back into dataframes"
Create the output directory in case that it does not exist
Save the splits to new CSV files
Return the dataframes
Load the data from the csv file
Calculate train size based on val and test size
Get unique locations
Split locations into train and temp (temporary holding for val and test)
Adjust the proportions for val and test based on the remaining locations
"Allocate images to train, validation, and test sets based on their location"
Save the datasets to CSV files
Return the split datasets
Load the data from the csv file
Convert 'Photo_Time' from string to datetime
Calculate train size based on val and test size
Sort by 'Photo_Time' to ensure chronological order
Group photos into sequences based on a 30-second interval
Assign unique sequence IDs to each group
Get unique sequence IDs
Split sequence IDs into train and temp (temporary holding for val and test)
Adjust the proportions for val and test based on the remaining sequences
"Allocate images to train, validation, and test sets based on their sequence ID"
Save the datasets to CSV files
Return the split datasets
Exportable class names for external use
Applying the ResNet layers and operations
Initialize the network with the specified settings
Selecting the appropriate ResNet architecture and pre-trained weights
self.pretrained_weights = ResNet18_Weights.IMAGENET1K_V1
self.pretrained_weights = ResNet50_Weights.IMAGENET1K_V1
Constructing the feature extractor and classifier
Criterion for binary classification
Load pre-trained weights and adjust for the current model
init_weights = self.pretrained_weights.get_state_dict(progress=True)
Load the weights into the feature extractor
Identify missing and unused keys in the loaded weights
Import necessary libraries
Exportable class names for external use
Define normalization mean and standard deviation for image preprocessing
Define data transformations for training and validation datasets
Load data for prediction
Load data for training/validation
"Load datasets for different modes (training, validation, testing, prediction)"
Calculate class counts and label mappings
Define parameters for the optimizer
Optimizer parameters for feature extraction
Optimizer parameters for the classifier
Setup optimizer and optimizer scheduler
Forward pass
Calculate loss
Forward pass
Forward pass
Concatenate outputs from all test steps
Calculate the metrics and save the output
Forward pass
Concatenate outputs from all predict steps
Compute the confusion matrix from true labels and predictions
Calculate class-wise accuracy (accuracy for each class)
Calculate micro accuracy (overall accuracy)
Calculate macro accuracy (mean of class-wise accuracies)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Importing necessary basic libraries and modules
%%
PyTorch imports for tensor operations
%%
"Importing the models, transformations, and utility functions from PytorchWildlife"
%%
Setting the device to use for computations ('cuda' indicates GPU)
%%
Initializing the model for image detection
%%
Initializing the model for image classification
%%
Defining transformations for detection and classification
%%
Initializing a box annotator for visualizing detections
Processing the video and saving the result with annotated detections and classifications
%%
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Importing basic libraries
%%
%%
"Importing the models, dataset, transformations, and utility functions from PytorchWildlife"
%%
Setting the device to use for computations ('cuda' indicates GPU)
Initializing a supervision box annotator for visualizing detections
Create a temp folder
Initializing the detection and classification models
Defining transformations for detection and classification
%% Defining functions for different detection scenarios
Only run classifier when detection class is animal
%% Building Gradio UI
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Importing necessary basic libraries and modules
%%
PyTorch imports
%%
"Importing the model, dataset, transformations and utility functions from PytorchWildlife"
%%
Setting the device to use for computations ('cuda' indicates GPU)
%%
Initializing the MegaDetectorV5 model for image detection
%% Single image detection
Specifying the path to the target image TODO: Allow argparsing
Opening and converting the image to RGB format
Initializing the Yolo-specific transform for the image
Performing the detection on the single image
Saving the detection results
%% Batch detection
Specifying the folder path containing multiple images for batch detection
Creating a dataset of images with the specified transform
Creating a DataLoader for batching and parallel processing of the images
Performing batch detection on the images
%% Output to annotated images
Saving the batch detection results as annotated images
%% Output to cropped images
Saving the detected objects as cropped images
%% Output to JSON results
Saving the detection results in JSON format
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Making the DetectionImageFolder class available for import from this module
Listing and sorting all image files in the specified directory
Get image filename and path
Load and convert image to RGB
Apply transformation if specified
Only run recognition on animal detections
Get image path and corresponding bbox xyxy for cropping
Load and crop image with supervision
Apply transformation if specified
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
from yolov5.utils.augmentations import letterbox
Making the provided classes available for import from this module
Convert PIL Image to Torch Tensor
Original shape
New shape
Scale ratio (new / old) and compute padding
Resize image
Pad image
Convert the image to a PyTorch tensor and normalize it
Resize and pad the image using a customized letterbox function.
Normalization constants
Define the sequence of transformations
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
!!! Output paths need to be optimized !!!
!!! Output paths need to be optimized !!!
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Importing basic libraries
Placeholder class-level attributes to be defined in derived classes
"If there are size differences in the input images, use a for loop instead of matrix processing for scaling"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Image size for the Opossum classifier
Class names for prediction
"If pretrained, use the provided URL to fetch the weights"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Making the PlainResNetInference class available for import from this module
Following the ResNet structure to extract features
Initialize the network and weights
... [Missing weight URL definition for ResNet18]
... [Missing weight URL definition for ResNet50]
Print missing and unused keys for debugging purposes
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Image size for the Opossum classifier
Class names for prediction
"If pretrained, use the provided URL to fetch the weights"
Configuration file for the Sphinx documentation builder.
""
"For the full list of built-in configuration values, see the documentation:"
https://www.sphinx-doc.org/en/master/usage/configuration.html
-- Project information -----------------------------------------------------
https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information
-- General configuration ---------------------------------------------------
https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration
-- Options for HTML output -------------------------------------------------
https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output
-- Options for todo extension ----------------------------------------------
https://www.sphinx-doc.org/en/master/usage/extensions/todo.html#configuration
"Determine the factor, which shifts the decimal point of x"
just behind the last significant digit
"Shift decimal point by multiplicatipon with factor, flooring, and"
division by factor
_ = pretty_print_object(obj)
Sloppy that I'm making a module-wide change here...
Determine the coordinates of the intersection rectangle
The intersection of two axis-aligned bounding boxes is always an
axis-aligned bounding box
Compute the area of both AABBs
Compute the intersection over union by taking the intersection
area and dividing it by the sum of prediction + ground-truth
areas - the intersection area.
%% Functions for running commands as subprocesses
%%
%% Test driver for execute_and_print
%% Parallel test driver for execute_command_and_print
Should we use threads (vs. processes) for parallelization?
"Only relevant if n_workers == 1, i.e. if we're not parallelizing"
%% Constants and imports
%% Classes
Collect all names
Make names unique and sort
cast location to string as the entries in locations are strings
Convert classnames to lowercase to simplify comparisons later
Normalize paths to simplify comparisons later
"Make custom replacements in filenames, typically used to"
accommodate changes in root paths after DB construction
## Build useful mappings to facilitate working with the DB
Category ID <--> name
Image filename --> ID
Image ID --> image object
Image ID --> annotations
"Each image can potentially multiple annotations, hence using lists"
...__init__
...class IndexedJsonDb
%% Functions
Find all unique locations
i_location = 0; location = locations[i_location]
Sorting datetimes fails when there are None's in the list.  So instead of sorting datetimes
"directly, sort tuples with a boolean for none-ness, then the datetime itself."
""
https://stackoverflow.com/questions/18411560/sort-list-while-pushing-none-values-to-the-end
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_location[1]
"Start a new sequence if necessary, including the case where this datetime is invalid"
"If this was an invalid datetime, this will record the previous datetime"
"as None, which will force the next image to start a new sequence."
...for each image in this location
Fill in seq_num_frames
...for each location
...create_sequences()
""
cct_to_md.py
""
"""Converts"" a COCO Camera Traps file to a MD results file.  Currently ignores"
"non-bounding-box annotations, and gives all annotations a confidence of 1.0."
""
The only reason to do this is if you are going to add information to an existing
"CCT-formatted dataset, and want to do that in Timelapse."
""
"Currently assumes that width and height are present in the input data, does not"
read them from images.
""
%% Constants and imports
%% Functions
# Validate input
# Read input
# Prepare metadata
ann = d['annotations'][0]
# Process images
im = d['images'][0]
"MegaDetector: [x,y,width,height] (normalized, origin upper-left)"
"CCT: [x,y,width,height] (absolute, origin upper-left)"
...if there's a bounding box
...for each annotation
This field is no longer included in MD output files by default
im_out['max_detection_conf'] = max_detection_conf
...for each image
# Write output
...cct_to_md()
%% Command-line driver
TODO
%% Interactive driver
%%
%%
""
cct_json_to_filename_json.py
""
"Given a .json file in COCO Camera Traps format, outputs a .json-formatted list of"
relative file names present in the CCT file.
""
%% Constants and environment
%% Main function
"json.dump(s,open(outputFilename,'w'))"
%% Command-line driver
%% Interactive driver
%%
""
cct_to_csv.py
""
"""Converts"" a COCO Camera Traps .json file to .csv, in quotes because"
"all kinds of assumptions are made here, and if you have a particular .csv"
"format in mind, YMMV.  Most notably, does not include any bounding box information"
or any non-standard fields that may be present in the .json file.  Does not
propagate information about sequence-level vs. image-level annotations.
""
"Does not assume access to the images, therefore does not open .jpg files to find"
"datetime information if it's not in the metadata, just writes datetime as 'unknown'."
""
%% Imports
%% Main function
#%% Read input
#%% Build internal mappings
annotation = annotations[0]
#%% Write output file
im = images[0]
Write out one line per class:
...for each class name
...for each image
...with open(output_file)
...def cct_to_csv
%% Interactive driver
%%
%% Command-line driver
""
remove_exif.py
""
"Removes all EXIF/IPTC/XMP metadata from a folder of images, without making"
"backup copies, using pyexiv2."
""
%% Imports and constants
%% List files
%% Remove EXIF data (support)
PYEXIV2 IS NOT THREAD SAFE; DO NOT CALL THIS IN PARALLEL FROM A SINGLE PROCESS
data = img.read_exif(); print(data)
%% Debug
%%
%%
%% Remove EXIF data (execution)
fn = image_files[0]
"joblib.Parallel defaults to a process-based backend, but let's be sure"
"results = Parallel(n_jobs=n_exif_threads,verbose=2,prefer='processes')(delayed(remove_exif)(fn) for fn in image_files[0:10])"
""
yolo_to_coco.py
""
Converts a YOLO-formatted dataset to a COCO-formatted dataset.
""
"Currently supports only a single folder (i.e., no recursion).  Treats images without"
corresponding .txt files as empty.
""
%% Imports and constants
from ai4eutils
%% Support functions
Validate input
Class names
Blank lines should only appear at the end
Enumerate images
fn = image_files[0]
Create the image object for this image
Is there an annotation file for this image?
"This is an image with no annotations, currently don't do anything special"
here
s = lines[0]
"COCO: [x_min, y_min, width, height] in absolute coordinates"
"YOLO: [class, x_center, y_center, width, height] in normalized coordinates"
...for each annotation
...if this image has annotations
...for each image
...def yolo_to_coco()
%% Interactive driver
%% Convert YOLO folders to COCO
%% Check DB integrity
%% Preview some images
%% Command-line driver
TODO
""
read_exif.py
""
"Given a folder of images, read relevant metadata (EXIF/IPTC/XMP) fields from all images,"
and write them to  a .json or .csv file.
""
This module can use either PIL (which can only reliably read EXIF data) or exiftool (which
can read everything).  The latter approach expects that exiftool is available on the system
path.  No attempt is made to be consistent in format across the two approaches.
""
%% Imports and constants
From ai4eutils
%% Options
Number of concurrent workers
Should we use threads (vs. processes) for parallelization?
""
Not relevant if n_workers is 1.
Should we use exiftool or pil?
%% Functions
exif_tags = img.info['exif'] if ('exif' in img.info) else None
print('Warning: unrecognized EXIF tag: {}'.format(k))
"-G means ""Print group name for each tag"", e.g. print:"
""
[File]          Bits Per Sample                 : 8
""
...instead of:
""
Bits Per Sample                 : 8
A list of three-element lists (type/tag/value)
line_raw = exif_lines[0]
A typical line:
""
[ExifTool]      ExifTool Version Number         : 12.13
"Split on the first occurrence of "":"""
...for each output line
...which processing library are we using?
...read_exif_tags_for_image()
...populate_exif_data()
Enumerate *relative* paths
Find all EXIF tags that exist in any image
...for each tag in this image
...for each image
Write header
...for each key that *might* be present in this image
...for each image
...with open()
...if we're writing to .json/.csv
https://stackoverflow.com/questions/11210104/check-if-a-program-exists-from-a-python-script
%% Interactive driver
%%
output_file = os.path.expanduser('~/data/test-exif.csv')
options.processing_library = 'pil'
"file_path = os.path.join(input_folder,'KRU_S1_11_R1_IMAG0148.JPG')"
%%
%% Command-line driver
""
"Given a json-formatted list of image filenames, retrieve the width and height of every image."
""
%% Constants and imports
%% Processing functions
Is this image on disk?
"all_results = list(tqdm(pool.imap(process_image, filenames), total=len(filenames)))"
%% Interactive driver
%%
List images in a test folder
%%
"process_list_file(image_list_file,image_size_file,image_prefix=base_dir)"
%% Command-line driver
"python sanity_check_json_db.py ""e:\wildlife_data\wellington_data\wellington_camera_traps.json"" --baseDir ""e:\wildlife_data\wellington_data\images"" --bFindUnusedImages --bCheckImageSizes"
"python sanity_check_json_db.py ""D:/wildlife_data/mcgill_test/mcgill_test.json"" --baseDir ""D:/wildlife_data/mcgill_test"" --bFindUnusedImages --bCheckImageSizes"
"Here the '-u' prevents buffering, which makes tee happier"
""
python -u sanity_check_json_db.py '/datadrive1/nacti_metadata.json' --baseDir '/datadrive1/nactiUnzip/' --bFindUnusedImages --bCheckImageSizes | tee ~/nactiTest.out
""
Partially-formed stub to get from MegaDetector output files to COCO Camera Traps data.
""
"Was actually written to convert *many* MD .json files to a single CCT file, hence"
the loop over .json files.
""
"THIS CODE HAS NEVER BEEN RUN, it was added as a demonstration of how to do this.  YMMV."
""
"You may find a more polished, command-line-ready version of this code at:"
""
https://github.com/StewartWILDlab/mdtools
""
%% Constants and imports
"Images sizes are required to convert between absolute and relative coordinates,"
so we need to read the images.
Only required if you want to write a database preview
%% Create CCT dictionaries
image_ids_to_images = {}
Force the empty category to be ID 0
Load .json annotations for this data set
i_entry = 0; entry = data['images'][i_entry]
""
"PERF: Not exactly trivially parallelizable, but about 100% of the"
time here is spent reading image sizes (which we need to do to get from
"absolute to relative coordinates), so worth parallelizing."
Generate a unique ID from the path
detection = detections[0]
Have we seen this category before?
Create an annotation
"MegaDetector: [x,y,width,height] (normalized, origin upper-left)"
"CCT: [x,y,width,height] (absolute, origin upper-left)"
...for each detection
...for each image
Remove non-reviewed images and associated annotations
%% Create info struct
%% Write .json output
%% Clean start
## Everything after this should work from a clean start ###
%% Validate output
%% Preview animal labels
%% Preview empty labels
"viz_options.classes_to_exclude = ['empty','human']"
""
generate_crops_from_cct.py
""
"Given a .json file in COCO Camera Traps format, create a cropped image for"
each bounding box.
""
%% Imports and constants
%% Functions
# Read and validate input
# Find annotations for each image
"This actually maps image IDs to annotations, but only to annotations"
containing boxes
# Generate crops
TODO: parallelize this loop
im = d['images'][0]
Load the image
Generate crops
i_ann = 0; ann = annotations_this_image[i_ann]
"x/y/w/h, origin at the upper-left"
...for each box
...for each image
...generate_crops_from_cct()
%% Interactive driver
%%
%%
%%
%% Command-line driver
TODO
%% Scrap
%%
""
coco_to_yolo.py
""
Converts a COCO-formatted dataset to a YOLO-formatted dataset.
""
"If the input and output folders are the same, writes .txt files to the input folder,"
and neither moves nor modifies images.
""
"Currently ignores segmentation masks, and errors if an annotation has a"
segmentation polygon but no bbox
""
Has only been tested on a handful of COCO Camera Traps data sets; if you
"use it for more general COCO conversion, YMMV."
""
%% Imports and constants
%% Support functions
Validate input
Read input data
Parse annotations
i_ann = 0; ann = data['annotations'][0]
Make sure no annotations have *only* segmentation data
Re-map class IDs to make sure they run from 0...n-classes-1
""
"TODO: this allows unused categories in the output data set, which I *think* is OK,"
but I'm only 81% sure.
Process images (everything but I/O)
"List of dictionaries with keys 'source_image','dest_image','bboxes','dest_txt'"
i_image = 0; im = data['images'][i_image]
"assert os.path.isfile(source_image), 'Could not find image {}'.format(source_image)"
If this annotation has no bounding boxes...
"This is not entirely clear from the COCO spec, but it seems to be consensus"
"that if you want to specify an image with no objects, you don't include any"
annotations for that image.
We allow empty bbox lists in COCO camera traps; this is typically a negative
"example in a dataset that has bounding boxes, and 0 is typically the empty"
category.
...if this is an empty annotation
"COCO: [x_min, y_min, width, height] in absolute coordinates"
"YOLO: [class, x_center, y_center, width, height] in normalized coordinates"
Convert from COCO coordinates to YOLO coordinates
...for each annotation
...if this image has annotations
...for each image
Write output
Category IDs should range from 0..N-1
TODO: parallelize this loop
""
output_info = images_to_copy[0]
Only write an annotation file if there are bounding boxes.  Images with
"no .txt files are treated as hard negatives, at least by YOLOv5:"
""
https://github.com/ultralytics/yolov5/issues/3218
""
"I think this is also true for images with empty annotation files, but"
"I'm using the convention suggested on that issue, i.e. hard negatives"
are expressed as images without .txt files.
bbox = bboxes[0]
...for each image
...def coco_to_yolo()
%% Interactive driver
%% CCT data
"If preview_export is True, I'm exporting to preview these with BoundingBoxEditor:"
""
https://github.com/mfl28/BoundingBoxEditor
""
"This export will be compatible, other than the fact that you need to move"
"""object.data"" into the ""labels"" folder."
""
"Otherwise I'm exporting for training, in the YOLOv5 flat format."
%% Command-line driver
TODO
""
cct_to_wi.py
""
Converts COCO Camera Traps .json files to the Wildlife Insights
batch upload format
""
Also see:
""
https://github.com/ConservationInternational/Wildlife-Insights----Data-Migration
""
https://data.naturalsciences.org/wildlife-insights/taxonomy/search
""
%% Imports
%% Paths
A COCO camera traps file with information about this dataset
A .json dictionary mapping common names in this dataset to dictionaries with the
"WI taxonomy fields: common_name, wi_taxon_id, class, orer, family, genus, species"
%% Constants
%% Project information
%% Read templates
%% Compare dictionary to template lists
Write the header
Write values
%% Project file
%% Camera file
%% Deployment file
%% Images file
Read .json file with image information
Read taxonomy dictionary
Populate output information
df = pd.DataFrame(columns = images_fields)
annotation = annotations[0]
im = input_data['images'][0]
"We don't have counts, but we can differentiate between zero and 1"
This is the label mapping used for our incoming iMerit annotations
"Only used to parse the incoming annotations. In our database, the string name is used to avoid confusion"
MegaDetector outputs
""
add_bounding_boxes_to_megadb.py
""
"Given COCO-formatted JSONs containing manually labeled bounding box annotations, add them to"
"MegaDB sequence entries, which can then be ingested into MegaDB."
the category map that comes in the COCO JSONs for iMerit batch 12 - to check that each
JSON
"dataset name : (seq_id, frame_num) : [bbox, bbox]"
where bbox is a dict with str 'category' and list 'bbox'
iterate over image_id_to_image rather than image_id_to_annotations so we include
the confirmed empty images
The file_name field in the incoming json looks like
alka_squirrels.seq2020_05_07_25C.frame119221.jpg
there seems to be a bug in the annotations where sometimes there's a
non-empty label along with a label of category_id 5
ignore the empty label (they seem to be actually non-empty)
""
add_bounding_boxes_to_json.py
""
This script takes a image database in the COCO Camera Traps format and merges in a set of bounding
box annotations in the format that iMerit uses (a .json where actually only each row is a valid json).
""
"If you need to update an existing bbox database, please get all the original annotation files and"
re-generate from scratch
""
%% Imports
%% Configurations and paths
images database
output bboxes database
annotation files (pseudo json) obtained from our annotation vendor that contain annotations for this dataset
None or a string or tuple of strings that is the prefix to all file_name of interest / in this dataset in the annotation files
functions for mapping the image_id in the annotation files (pseudo jsons) to the image_id used in the image DB
our img_id doesn't contain frame info
"batch3 - ""file_name"":""ASG0000019_0_S1_B06_R1_PICT0007.JPG"""
"batch5 and 7 - ""file_name"":""datasetsnapshotserengeti.seqASG000002m-frame0.imgS1_B06_R1_PICT0056.JPG"""
sometimes - 'datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG'
"id in DB (old_token): 'S6/J01/J01_R1/S6_J01_R1_IMAG0001', 'S1/B05/B05_R1/S1_B05_R1_PICT0036'"
specify which one to use for your dataset here
%% Load the image database and fill in DB info for the output bbox database
load the images database
%% Find the height and width of images from the annotation files
""
...if they are not available in the images DB
each row in this pseudo-json is a COCO formatted entry for an image sequence
%% Other functions required by specific datasets
the IDFG image database does not include images from unlabeled folders that were annotated with bounding boxes
%% Create the bbox database from all annotation files pertaining to this dataset
"for the incoming annotations, look up by category name (common) and convert to the numerical id used in our databases"
"for each annotation pseudo-json, check that the image it refers to exists in the original database"
each row in this pseudo-json is a COCO formatted entry for an image sequence
check that entry is for this dataset
category map for this entry in the annotation file - usually the same across all entries but just in case
rspb_add_image_entry(img_id)
use the image length and width in the image DB
"[top left x, top left y, width, height] in relative coordinates"
"add all images that have been sent to annotation, some of which may be empty of bounding boxes"
rspb_add_image_entry(db_image_id)
%% Common queries
This query is used when preparing tfrecords for object detector training.
We do not want to get the whole seq obj where at least one image has bbox because
some images in that sequence will not be bbox labeled so will be confusing.
Include images with bbox length 0 - these are confirmed empty by bbox annotators.
"If frame_num is not available, it will not be a field in the result iterable."
"Note that the seq_id is the Cosmos DB assigned ID for that sequence, not the"
"seq_id field, which may contain ""/"" characters."
"For public datasets to be converted to the CCT format, we get the whole seq object because"
sequence level attributes need to be included too. megadb/converters/megadb_to_cct.py handles
the case of bbox-only JSONs with the flag exclude_species_class
Getting all sequences in a dataset - for updating or deleting entries which need the id field
%% Parameters
Use None if querying across all partitions
"The `sequences` table has the `dataset` as the partition key, so if only querying"
"entries from one dataset, set the dataset name here."
"e.g. {'name': '@top_n', 'value': 100} - see query_and_upsert_examples/query_for_data.ipynb"
Use False if do not want all results stored in a single JSON.
%% Script
execute the query
loop through and save the results
MODIFY HERE depending on the query
wiitigers Unicode issue - no good mapping from DB file names to file names in blob URL
build filename
if need to re-download a dataset's images in case of corruption
entries_to_download = {
"filename: entry for filename, entry in entries_to_download.items()"
if entry['dataset'] == DATASET
}
input validation
"existing files, with paths relative to <store_dir>"
parse JSON or TXT file
"create a new storage container client for this dataset,"
and cache it
schema already checks that the min possible value of frame_num is 0
"if there are more than one image item, each needs a frame_num"
checks across all sequence items
per sequence item checks
load the schema
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory
load the schema
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory
checks across all datasets items
check for expiry date of the SAS keys
update the sequences
"`id` is from the database, as well as all attributes starting with _"
"if valuable sequence information is available, add them to the image"
required fields for an image object
add seq-level class labels for this image
add other sequence-level properties to each image too
add other image-level properties
... for im in seq['images']
... for seq in mega_db
consolidate categories
some property names have changed in the new schema
a dummy sequence ID will be generated if the image entry does not have a seq_id field
seq_id only needs to be unique within this dataset; MegaDB does not rely on it as the _id field
"""annotations"" fields are opened and have its sub-field surfaced one level up"
set the `dataset` property on each sequence to the provided dataset_name
check that the location field is the same for all images in a sequence
check which fields in a CCT image entry are sequence-level
image-level properties that really should be sequence-level
need to add (misidentified) seq properties not present for each image in a sequence to img_level_properties
"(some properties act like flags - all have the same value, but not present on each img)"
add the sequence-level properties to the sequence objects
not every sequence have to have all the seq_level_properties
get the value of this sequence-level property from the first image entry
check which fields are really dataset-level and should be included in the dataset table instead.
delete sequence-level properties that should be dataset-level
make all `class` fields lower-case; cast `seq_id` to type string in case they're integers
turn all float NaN values into None so it gets converted to null when serialized
this was an issue in the Snapshot Safari datasets
%% validation
"at first a dict of image_id: image_obj with annotations embedded, then it becomes"
an array of image objects
%% integrate the image DB
takes in image entries and species and other annotations in the image DB
convert the species category to explicit string name
there may be other fields in the annotation object
these fields should already be gotten from the image object
%% integrate the bbox DB
add any images that are not in the image DB
also add any fields in the image object that are not present already
add bbox to the annotations field
for any newly added images
"'bbox_abs': bbox_anno['bbox'],"
not keeping height and width
""
jb_csv_to_json.py
""
Convert a particular .csv file to CCT format.  Images were not available at
"the time I wrote this script, so this is much shorter than other scripts"
in this folder.
""
%% Constants and environment
%% Read source data
%% Confirm filename uniqueness (this data set has one label per image)
"Build up a map from filenames to a list of rows, checking image existence as we go"
%% Create CCT dictionaries
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
imageName = imageFilenames[0]
"As per above, this is convenient and appears to be true; asserting to be safe"
"Filenames look like ""290716114012001a1116.jpg"""
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
%% Create info struct
%% Write output
%% Sanity-check
%% Imports and constants
%% Load data
"with open(image_json,'r') as f:"
data = json.load(f)
%% Sanity-check data
%% Label previews
%% Collect images to annotate
%% Sort by sequence and frame
%% Copy to a folder by GUID
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension
im = images_to_annotate[0]
...for each image
%% Write out the annotation list
%% Constants and imports
%% Enumerate files
edited_image_folder = edited_image_folders[0]
fn = edited_image_files[0]
%% Read metadata and capture location information
i_row = 0; row = df.iloc[i_row]
Sometimes '2017' was just '17' in the date column
%% Read the .json files and build output dictionaries
json_fn = json_files[0]
if 'partial' in json_fn:
continue
line = lines[0]
if image_fn == 'SD1_238_6_26_17_16_76.73.jpg':
asdfad
SD29_079_5_14_2018_17_52.85.jpg
Re-write two-digit years as four-digit years
Sometimes the year was written with two digits instead of 4
assert len(tokens[4]) == 4 and tokens[4].startswith('20')
Have we seen this location already?
"Not a typo, it's actually ""formateddata"""
An image shouldn't be annotated as both empty and non-empty
An image shouldn't be annotated as both empty and non-empty
box = formatteddata[0]
"open_file(os.path.join(base_folder,jpeg_image_folder,image_fn))"
...for each box
...if there are boxes on this image
...for each line
...with open()
...for each json file
%% Prepare the output .json
%% Check DB integrity
%% Print unique locations
SD12_202_6_23_2017_1_31.85.jpg
%% Preview some images
%% Statistics
""
"Import a Snapshot Safari project (one project, one season)"
""
Before running this script:
""
"* Mount the blob container where the images live, or copy the"
images to local storage
""
What this script does:
""
* Creates a .json file
* Creates zip archives of the season without humans.
* Copies animals and humans to separate folders
""
After running this script:
""
* Create or update LILA page
* Push zipfile and unzipped images to LILA
* Push unzipped humans to wildlifeblobssc
* Delete images from UMN uplaod storage
""
"Snapshot Serengeti is handled specially, because we're dealing with bounding"
boxes too.  See snapshot_serengeti_lila.py.
""
%% Imports
From ai4eutils
From CameraTraps
%% Constants
project_name = 'XXX'; season_name = 'S1'; project_friendly_name = 'Snapshot Unknown'
project_name = 'SER'; season_name = 'S1-11'; project_friendly_name = 'Snapshot Serengeti'
project_name = 'KRU'; season_name = 'S1'; project_friendly_name = 'Snapshot Kruger'
project_name = 'CDB'; season_name = 'S1'; project_friendly_name = 'Snapshot Camdeboo'
project_name = 'MTZ'; season_name = 'S1'; project_friendly_name = 'Snapshot Mountain Zebra'
project_name = 'ENO'; season_name = 'S1'; project_friendly_name = 'Snapshot Enonkishu'
project_name = 'KAR'; season_name = 'S1'; project_friendly_name = 'Snapshot Karoo'
project_name = 'KGA'; season_name = 'S1'; project_friendly_name = 'Snapshot Kgalagadi'
%% Folder/file creation
E.g. KRU_S1
E.g. Z:\KRU
E.g. Z:\KRU\KRU_S1
Contains annotations for each capture event (sequence)
Maps image IDs to filenames; each line looks like:
""
"KRU_S1#1#1#2,3,KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0004.JPG"
"Total number of each answer to each question, e.g. total number of times each species was identified"
""
Not used here
Create output folders
Images will be placed in a season-specific folder inside this (the source data includes
this in path names)
%% Load metadata files
%% Convert to dictionaries (prep)
%% Convert to dictionaries (loops)
"TODO: iterrows() is a terrible way to do this, but this is one of those days"
"where I want to get this done, not get better at Python."
irow = 0; row = image_table.iloc[0]
"Loaded as an int64, converting to int here"
...for each row in the image table
Make sure image IDs are what we think they are
...for each row in the annotation table
%% Take a look at categories (just sanity-checking)
print('\nCategories by species:')
pp.pprint(categories_by_species)
%% Fill in some image fields we didn't have when we created the image table
"width, height, corrupt, seq_num_frames, location, datetime"
Every annotation in this list should have the same sequence ID
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;"
there's no reason to do this redundantly for every image
Every image in this sequence should point back to the same equence
Every annotation in this list should have the same location
Every annotation in this list should have the same datetime
Is this image on disk?
iImage = 0; im = images[0]
...for each image
"images_processed = pool.map(process_image, images)"
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))"
%% Count missing/corrupted images
%% Print distribution of sequence lengths (sanity-check)
%% Replicate annotations across images
iAnn = 0; ann = annotations[iAnn]
%% See what files are on disk but not annotated
%% Sanity-check image and annotation uniqueness
%% Minor updates to fields
%% Write .json file
%% Create a list of human files
ann = annotations[0]
%% Create public archive and public/private folders
im = images[0]
E.g. KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0001.JPG
Copy to private output folder
Add to zipfile
Possibly start a new archive
Copy to public output folder
...for each image
%% Sanity-check .json file
"This will produce some validation errors, because this zipfile doesn't include humans"
%% Zip up .json and .csv files
%% When I skip to this part (using a pre-rendered .json file)
%%
%%
ann = annotations[0]
%% Summary prep for LILA
"%% Generate preview, sanity-check labels"
"viz_options.classes_to_include = ['jackalblackbacked','bustardkori']"
%% Scrap
%% Find annotations for a particular image
%% Write a list of missing images
""
save_the_elephants_survey_A.py
""
Convert the .csv file provided for the Save the Elephants Survey A data set to a
COCO-camera-traps .json file
""
%% Constants and environment
Handle all unstructured fields in the source data as extra fields in the annotations
"photo_type really should be an image property, but there are a few conflicts"
that forced me to handle it as an annotation proprerty
%% Read source data
"%% Map filenames to rows, verify image existence"
Maps relative paths to row indices in input_metadata
"Build up a map from filenames to a list of rows, checking image existence as we go"
Ignore directories
%% Make sure the multiple-annotation cases make sense
%%
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
Force the empty category to be ID 0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image]
Example filename:
""
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG'
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG'
i_row = row_indices[0]
There are a small handful of datetime mismatches across annotations
for the same image
assert im['datetime'] == timestamp
Special cases based on the 'photo type' field
Various spellings of 'community'
Have we seen this category before?
Create an annotation
fieldname = list(mapped_fields.keys())[0]
...for each row
...for each image
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
%% Scrap
%% Find unique photo types
##
""
timelapse_csv_set_to_json.py
""
Given a directory full of reasonably-consistent Timelapse-exported
".csvs, assemble a CCT .json."
""
"Assumes that you have a list of all files in the directory tree, including"
image and .csv files.
""
##
%% Constants and imports
Text file with relative paths to all files (images and .csv files)
"%% Read file list, make a list of all image files and all .csv files"
"%% Verify column consistency, create a giant array with all rows from all .csv files"
i_csv = 0; csv_filename = csv_files[0]
%% Prepare some data structures we'll need for mapping image rows in .csv files to actual image files
Enumerate all folders containing image files
"In this data set, a site folder looks like:"
""
Processed Images\\site_name
%% Map .csv files to candidate camera folders
fn = valid_csv_files[0]
"Some site folders appear as ""XXNNNN"", some appear as ""XXNNNN_complete"""
...for each .csv file
%% Map camera folders to candidate image folders
%% Main loop over labels (prep)
Force the empty category to be ID 0
Images that are marked empty and also have a species label
%% Main loop over labels (loop)
i_row = 0; row = input_metadata.iloc[i_row]
"for i_row,row in input_metadata.iterrows():"
"Usually this is just a single folder name, sometimes it's a full path,"
which we don't want
Check whether this file exists on disk
Retrieve image width and height
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each row in the big table of concatenated .csv files
%% Fix cases where an image was annotated as 'unlabeled' and as something else
This annotation is 'unlabeled'
Was there another category associated with this image?
%% Check for un-annnotated images
Enumerate all images
list(relative_path_to_image.keys())[0]
%% Create info struct
%% Write output
%% Sanity-check the database's integrity
%% Render a bunch of images to make sure the labels got carried along correctly
""
auckland_doc_to_json.py
""
Convert Auckland DOC data set to COCO camera traps format.  This was
for a training data set where class names were encoded in path names.
""
%% Constants and imports
Filenames will be stored in the output .json relative to this base dir
%% Enumerate files
%% Assemble dictionaries
Force the empty category to be ID 0
fn = image_files[0]; print(fn)
Typically y:\Maukahuka_Auckland_Island\1_Training\Winter_Trial_2019\cat\cat\eat\20190903_IDdY_34_E3_tmp_201908240051.JPG
"This data set has two top-level folders, ""1_Training"" (which has class names encoded"
"in paths) and ""2_Testing"" (which has no class information)."
...for each image
%% Write output .json
%% Write train/test .jsons
%% Validate .json files
%% Preview labels
checkpoint
""
awc_to_json.py
""
Convert a particular .csv file to CCT format.
""
%% Constants and environment
%% Read source data
%% Main loop over labels
Force the empty category to be ID 0
iRow = 0; row = input_metadata.iloc[iRow]
"ImageID,FileName,FilePath,SpeciesID,CommonName"
"This makes an assumption of one annotation per image, which happens to be"
true in this data set.
Retrieve image width and height
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
%% Check for images that aren't included in the metadata file
Enumerate all images
list(relativePathToImage.keys())[0]
%% Create info struct
%% Write output
%% Sanity-check the database's integrity
%% Render a bunch of images to make sure the labels got carried along correctly
""
idaho-camera-traps.py
""
Prepare the Idaho Camera Traps dataset for release on LILA.
""
%% Imports and constants
Multi-threading for .csv file comparison and image existence validation
"We are going to map the original filenames/locations to obfuscated strings, but once"
"we've done that, we will re-use the mappings every time we run this script."
This is the file to which mappings get saved
The maximum time (in seconds) between images within which two images are considered the
same sequence.
"This is a two-column file, where each line is [string in the original metadata],[category name we want to map it to]"
"The output file, using the original strings"
"The output file, using obfuscated strings for everything but filenamed"
"The output file, using obfuscated strings and obfuscated filenames"
"One time only, I ran MegaDetector on the whole dataset..."
...then set aside any images that *may* have contained humans that had not already been
annotated as such.  Those went in this folder...
...and the ones that *actually* had humans (identified via manual review) got
copied to this folder...
"...which was enumerated to this text file, which is a manually-curated list of"
images that were flagged as human.
Unopinionated .json conversion of the .csv metadata
%% List files (images + .csv)
Ignore .csv files in folders with multiple .csv files
...which would require some extra work to decipher.
fn = csv_files[0]
%% Parse each .csv file into sequences (function)
csv_file = csv_files[-1]
os.startfile(csv_file_absolute)
survey = csv_file.split('\\')[0]
Sample paths from which we need to derive locations:
""
St.Joe_elk\AM99\Trip 1\100RECNX\TimelapseData.csv
Beaverhead_elk\AM34\Trip 1\100RECNX\TimelapseData.csv
""
ClearCreek_mustelids\Winter2015-16\FS-001-P\FS-001-P.csv
ClearCreek_mustelids\Summer2015\FS-001\FS-001.csv
ClearCreek_mustelids\Summer2016\IDFG-016\IDFG-016.csv
""
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017b
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017a
Load .csv file
Validate the opstate column
# Create datetimes
print('Creating datetimes')
i_row = 0; row = df.iloc[i_row]
Make sure data are sorted chronologically
""
"In odd circumstances, they are not... so sort them first, but warn"
Debugging when I was trying to see what was up with the unsorted dates
# Parse into sequences
print('Creating sequences')
i_row = 0; row = df.iloc[i_row]
Start a new sequence if:
""
* This image has no timestamp
* This image has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each row
# Parse labels for each sequence
sequence_id = location_sequences[0]
Row indices in a sequence should be adjacent
sequence_df = df[df['seq_id']==sequence_id]
# Determine what's present
Be conservative; assume humans are present in all maintenance images
The presence columns are *almost* always identical for all images in a sequence
assert single_presence_value
print('Warning: presence value for {} is inconsistent for {}'.format(
"presence_column,sequence_id))"
...for each presence column
Tally up the standard (survey) species
"If no presence columns are marked, all counts should be zero"
count_column = count_columns[0]
Occasionally a count gets entered (correctly) without the presence column being marked
"assert len(values) == 1 and values[0] == 0, 'Non-zero counts with no presence"
columns marked for sequence {}'.format(sequence_id)
"Handle this by virtually checking the ""right"" box"
Make sure we found a match
Handle 'other' tags
column_name = otherpresent_columns[0]
print('Found non-survey counted species column: {}'.format(column_name))
...for each non-empty presence column
"Very rarely, the ""otherpresent"" column is checked, but no more detail is available"
...handling non-survey species
Build the sequence data
i_row = 0; row = sequence_df.iloc[i_row]
Only one folder used a single .csv file for two subfolders
...for each sequence
...def csv_to_sequences()
%% Parse each .csv file into sequences (loop)
%%
%%
i_file = -1; csv_file = csv_files[i_file]
%% Save sequence data
%% Load sequence data
%%
%% Validate file mapping (based on the existing enumeration)
sequences = sequences_by_file[0]
sequence = sequences[0]
"Actually, one folder has relative paths"
assert '\\' not in image_file_relative and '/' not in image_file_relative
os.startfile(csv_folder)
assert os.path.isfile(image_file_absolute)
found_file = os.path.isfile(image_file_absolute)
...for each image
...for each sequence
...for each .csv file
%% Load manual category mappings
The second column is blank when the first column already represents the category name
%% Convert to CCT .json (original strings)
Force the empty category to be ID 0
For each .csv file...
""
sequences = sequences_by_file[0]
For each sequence...
""
sequence = sequences[0]
Find categories for this image
"When 'unknown' is used in combination with another label, use that"
"label; the ""unknown"" here doesn't mean ""another unknown species"", it means"
there is some other unknown property about the main species.
category_name_string = species_present[0]
"This piece of text had a lot of complicated syntax in it, and it would have"
been too complicated to handle in a general way
print('Ignoring category {}'.format(category_name_string))
Don't process redundant labels
category_name = category_names[0]
If we've seen this category before...
If this is a new category...
print('Adding new category for {}'.format(category_name))
...for each category (inner)
...for each category (outer)
...if we do/don't have species in this sequence
"We should have at least one category assigned (which may be ""empty"" or ""unknown"")"
assert len(sequence_category_ids) > 0
Was any image in this sequence manually flagged as human?
print('Flagging sequence {} as human based on manual review'.format(sequence_id))
For each image in this sequence...
""
i_image = 0; im = images[i_image]
Create annotations for this image
...for each image in this sequence
...for each sequence
...for each .csv file
Verify that all images have annotations
ann = ict_data['annotations'][0]
For debugging only
%% Create output (original strings)
%% Validate .json file
%% Preview labels
%% Look for humans that were found by MegaDetector that haven't already been identified as human
This whole step only needed to get run once
%%
Load MD results
Get a list of filenames that MD tagged as human
im = md_results['images'][0]
...for each detection
...for each image
Map images to annotations in ICT
ann = ict_data['annotations'][0]
For every image
im = ict_data['images'][0]
Does this image already have a human annotation?
...for each annotation
...for each image
%% Copy images for review to a new folder
fn = missing_human_images[0]
%% Manual step...
Copy any images from that list that have humans in them to...
%% Create a list of the images we just manually flagged
fn = human_tagged_filenames[0]
E.g. '0000_Beaverhead_elk~AM174~Trip 1~100RECNX~IMG_1397.JPG'
"%% Translate location, image, sequence IDs"
Load mappings if available
Generate mappings
If we've seen this location before...
Otherwise assign a string-formatted int as the ID
If we've seen this sequence before...
Otherwise assign a string-formatted int as the ID
Assign an image ID
...for each image
Assign annotation mappings
Save mappings
"Back this file up, lest we should accidentally re-run this script"
with force_generate_mappings = True and overwrite the mappings we used.
...if we are/aren't re-generating mappings
%% Apply mappings
"%% Write new dictionaries (modified strings, original files)"
"%% Validate .json file (modified strings, original files)"
%% Preview labels (original files)
"viz_options.classes_to_exclude = ['empty','deer','elk']"
viz_options.classes_to_include = ['bobcat']
%% Copy images to final output folder (prep)
ann = d['annotations'][0]
Is this a public or private image?
Generate absolute path
Copy to output
Update the filename reference
...def process_image(im)
%% Copy images to final output folder (execution)
For each image
im = images[0]
Write output .json
%% Make sure the right number of images got there
%% Validate .json file (final filenames)
%% Preview labels (final filenames)
"viz_options.classes_to_exclude = ['empty','deer','elk']"
viz_options.classes_to_include = ['horse']
viz_options.classes_to_include = [viz_options.multiple_categories_tag]
"viz_options.classes_to_include = ['human','vehicle','domestic dog']"
%% Create zipfiles
%% List public files
%% Find the size of each file
fn = all_public_output_files[0]
%% Split into chunks of approximately-equal size
...for each file
%% Create a zipfile for each chunk
...for each filename
with ZipFile()
...def create_zipfile()
i_file_list = 0; file_list = file_lists[i_file_list]
"....if __name__ == ""__main__"""
""
wellington_to_json.py
""
Convert the .csv file provided for the Wellington data set to a
COCO-camera-traps .json file
""
%% Constants and environment
%% Read source data
"Filenames were provided as *.jpg, but images were *.JPG, converting here"
"%% Map filenames to rows, verify image existence"
"Takes ~30 seconds, since it's checking the existence of ~270k images"
"Build up a map from filenames to a list of rows, checking image existence as we go"
"I didn't expect this to be true a priori, but it appears to be true, and"
it saves us the trouble of checking consistency across multiple occurrences
of an image.
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
imageName = imageFilenames[0]
"As per above, this is convenient and appears to be true; asserting to be safe"
"Filenames look like ""290716114012001a1116.jpg"""
This gets imported as an int64
"These appear as ""image1"", ""image2"", etc."
"In the form ""001a"""
Can be in the form '111' or 's46'
"In the form ""7/29/2016 11:40"""
Check image height and width
"Use 'empty', to be consistent with other data on lila"
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
""
mcgill_to_json.py
""
Convert the .csv file provided for the McGill test data set to a
COCO-camera-traps .json file
""
%% Constants and environment
%% Read source data
"%% Map filenames to rows, verify image existence"
Create an additional column for concatenated filenames
Maps relative filenames to rows
"Build up a map from filenames to a list of rows, checking image existence as we go"
row = input_metadata.iloc[0]
"I didn't expect this to be true a priori, but it appears to be true, and"
it saves us the trouble of checking consistency across multiple occurrences
of an image.
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
row = input_metadata.iloc[0]
"Filenames look like ""290716114012001a1116.jpg"""
"In the form ""001a"""
Can be in the form '111' or 's46'
"In the form ""7/29/2016 11:40"""
Check image height and width
NaN is the only thing we should see that's not a string
NaN is the only thing we should see that's not a string
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
""
bellevue_to_json.py
""
"""Bellevue Camera Traps"" is the rather unremarkable camera trap data set"
used by one of the repo's maintainers for testing.  It's organized as:
""
approximate_date/[loose_camera_specifier/]/species
""
E.g.:
""
"""2018.03.30\coyote\DSCF0091.JPG"""
"""2018.07.18\oldcam\empty\DSCF0001.JPG"""
""
%% Constants and imports
from the ai4eutils repo
Filenames will be stored in the output .json relative to this base dir
%% Exif functions
"%% Enumerate files, create image/annotation/category info"
Force the empty category to be ID 0
Keep track of unique camera folders
Each element will be a dictionary with fields:
""
"relative_path, width, height, datetime"
fname = image_files[0]
Corrupt or not an image
Store file info
E.g. 2018.03.30/coyote/DSCF0091.JPG
...for each image file
%% Synthesize sequence information
Sort images by time within each folder
camera_path = camera_folders[0]
previous_datetime = sorted_images_this_camera[0]['datetime']
im = sorted_images_this_camera[1]
Start a new sequence if necessary
...for each image in this camera
...for each camera
Fill in seq_num_frames
%% A little cleanup
%% Write output .json
%% Sanity-check data
%% Label previews
""
snapshot_safar_importer_reprise.py
""
This is a 2023 update to snapshot_safari_importer.py.  We do a bunch of things now that
we didn't do the last time we imported Snapshot data (like updating the big taxonomy)
"file, and we skip a bunch of things now that we used to do (like generating massive"
"zipfiles).  So, new year, new importer."
""
%% Constants and imports
%% List files
"Do a one-time enumeration of the entire drive; this will take a long time,"
but will save a lot of hassle later.
%% Create derived lists
Takes about 60 seconds
CSV files are one of:
""
"_report_lila.csv (this is the one we want to use, with the species/count/etc. for each sequence)"
_report_lila_image_inventory.csv (maps captures to images)
_report_lila_overview.csv (distrubution of species)
%% List project folders
Project folders look like one of these:
""
APN
Snapshot Cameo/DEB
%% Map report and inventory files to codes
fn = csv_files[0]
%% Make sure that every report has a corresponding inventory file
%% Count species based on overview and report files
%% Print counts
%% Make sure that capture IDs in the reports/inventory files match
...and that all the images in the inventory tables are actually present on disk.
assert image_path_relative in all_files_relative_set
Make sure this isn't just a case issue
...for each report on this project
...for each project
"%% For all the files we have on disk, see which are and aren't in the inventory files"
"There aren't any capital-P .PNG files, but if I don't include that"
"in this list, I'll look at this in a year and wonder whether I forgot"
to include it.
fn = all_files_relative[0]
print('Skipping project {}'.format(project_code))
""
plot_wni_giraffes.py
""
Plot keypoints on a random sample of images from the wni-giraffes data set.
""
%% Constants and imports
%% Load and select data
%% Support functions
https://stackoverflow.com/questions/32504246/draw-ellipse-in-python-pil-with-line-thickness
Use a single channel image (mode='L') as mask.
The size of the mask can be increased relative to the imput image
to get smoother looking results.
draw outer shape in white (color) and inner shape in black (transparent)
downsample the mask using PIL.Image.LANCZOS
(a high-quality downsampling filter).
paste outline color to input image through the mask
%% Plot some images
ann = annotations_to_plot[0]
i_tool = 0; tool_name = short_tool_names[i_tool]
Don't plot tools that don't have a consensus annotation
...for each tool
...for each annotation
""
idfg_iwildcam_lila_prep.py
""
Adding class labels (from the private test .csv) to the iWildCam 2019 IDFG
"test set, in preparation for release on LILA."
""
This version works with the public iWildCam release images.
""
"%% ############ Take one, from iWildCam .json files ############"
%% Imports and constants
%% Read input files
Remove the header line
%% Parse annotations
Lines look like:
""
"b005e5b2-2c0b-11e9-bcad-06f1011196c4,1,Private"
%% Minor cleanup re: images
%% Create annotations
%% Prepare info
%% Minor adjustments to categories
Remove unused categories
Name adjustments
%% Create output
%% Write output
%% Validate .json file
%% Preview labels
viz_options.classes_to_exclude = ['test']
"%% ############ Take two, from pre-iWildCam .json files created from IDFG .csv files ############"
%% Imports and constants
%% One-time line break addition
%% Read input files
%% Prepare info
%% Minor adjustments to categories
%% Minor adjustments to annotations
%% Create output
%% Write output
%% Validate .json file
%% Preview labels
viz_options.classes_to_exclude = ['test']
""
missouri_to_json.py
""
Create .json files from the original source files for the Missouri Camera Traps
data set.  Metadata was provided here in two formats:
""
"1) In one subset of the data, folder names indicated species names.  In Set 1,"
there are no empty sequences.  Set 1 has a metadata file to indicate image-level
bounding boxes.
""
2) A subset of the data (overlapping with (1)) was annotated with bounding
"boxes, specified in a whitespace-delimited text file.  In set 2, there are"
"some sequences omitted from the metadata file, which implied emptiness."
""
"In the end, set 2 labels were not reliable enough to publish, so LILA includes only set 1."
""
%% Constants and imports
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Filenames will be stored in the output .json relative to this base dir
This will be a list of filenames that need re-annotation due to redundant boxes
"%% Enumerate files, read image sizes (both sets)"
"Takes a few minutes, since we're reading image sizes."
Each element will be a list of relative path/full path/width/height
"Only process leaf nodes corresponding to sequences, which look like:"
""
E:\wildlife_data\missouri_camera_traps\Set1\1.02-Agouti\SEQ75583
E:\wildlife_data\missouri_camera_traps\Set2\p1d101
""
assert len(files) <= 2
Read the image
Not an image...
Store file info
"...if we didn't hit the max file limit, keep going"
...for each file
%% Add sequence lengths (both sets)
%% Load the set 1 metadata file
"List of lists, length varies according to number of bounding boxes"
""
Preserves original ordering
iLine = 0; line = metadataSet1Lines[0]
"Lines should be filename, number of bounding boxes, boxes (four values per box)"
Make sure we have image info for this image
%% Print missing files from Set 1 metadata
Manual changes I made to the metadata file:
""
'IMG' --> 'IMG_'
Red_Brocket_Deer --> Red_Deer
European-Hare --> European_Hare
Wood-Mouse --> Wood_Mouse
Coiban-Agouti --> Coiban_Agouti
%% Load the set 2 metadata file
"This metadata file contains most (but not all) images, and a class label (person/animal/empty)"
"for each, plus bounding boxes."
"List of lists, length varies according to number of bounding boxes"
""
Preserves original ordering
"Create class IDs for each *sequence*, which we'll use to attach classes to"
images for which we don't have metadata
""
This only contains mappings for sequences that appear in the metadata.
iLine = 0; line = metadataSet2Lines[0]
"Lines should be filename, number of bounding boxes, labeled boxes (five values per box)"
""
Empty images look like filename\t0\t0
E.g. 'Set2\\p1d101\\p1d101s100i10.JPG'
Make sure we don't have mixed classes within an image
"Figure out what class this *sequence* is, so we know how to handle unlabeled"
images from this sequence
Can't un-do a mixed sequence
Previously-empty sequences get the image class label
"If the sequence has a non-empty class, possibly change it"
Make sure we have image info for this image
...for each line in the set 2 metadata file
%% What Set 2 images do I not have metadata for?
These are *mostly* empty images
iImage = 0; imageID = set2ImageIDs[iImage]
%% Create categories and annotations for set 1
"Though we have no empty sequences, we do have empty images in this set"
For each image
""
iImage = 0; imageID = set1ImageIDs[iImage]
E.g. Set1\\1.80-Coiban_Agouti\\SEQ83155\\SEQ83155_IMG_0010.JPG
Find the species name
This image may still be empty...
category['count'] = category['count'] + 1
"If we have bounding boxes, create image-level annotations"
"filename, number of bounding boxes, boxes (four values per box)"
"Make sure the relative filename matches, allowing for the fact that"
some of the filenames in the metadata aren't quite right
This image is non-empty
"Some redundant bounding boxes crept in, don't add them twice"
Check this bbox against previous bboxes
""
Inefficient?  Yes.  In an important way?  No.
"Bounding box values are in absolute coordinates, with the origin"
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1]."
""
"Convert to floats and to x/y/w/h, as per CCT standard"
...for each box
if we do/don't have boxes for this image
Else create a sequence-level annotation
...for each image
%% Write out the list of images with redundant boxes
%% Create categories and annotations for set 2
For each image
""
iImage = 0; imageID = set2ImageIDs[iImage]
E.g. 'Set2\\p1d100\\p1d100s10i1.JPG'
"Find the sequence ID, sanity check filename against what we stored"
"If we have bounding boxes or an explicit empty label, create image-level annotations"
"filename, number of bounding boxes, labeled boxes (five values per box)"
"Make sure the relative filename matches, allowing for the fact that"
some of the filenames in the metadata aren't quite right
"Bounding box values are in absolute coordinates, with the origin"
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1]."
""
Convert to floats and to x/y/w/h
...for each box
...if we do/don't have boxes for this image
Else create a sequence-level annotation
...for each image
"%% The 'count' field isn't really meaningful, delete it"
"It's really the count of image-level annotations, not total images assigned to a class"
%% Write output .json files
%% Sanity-check final set 1 .json file
"python sanity_check_json_db.py --bCheckImageSizes --baseDir ""E:\wildlife_data\missouri_camera_traps"" ""E:\wildlife_data\missouri_camera_traps\missouri_camera_traps_set1.json"""
%% Generate previews
""
filenames_to_json.py
""
Take a directory of images in which species labels are encoded by folder
"names, and produces a COCO-style .json file"
""
%% Constants and imports
from the ai4eutils repo
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Filenames will be stored in the output .json relative to this base dir
"rawClassListFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_classes.csv')"
"classMappingsFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_class_mapping.csv')"
"%% Enumerate files, read image sizes"
Each element will be a list of relative path/full path/width/height
Read the image
Corrupt or not an image
Store file info
Write to output file
...for each image file
...csv file output
...if the file list is/isn't available
%% Enumerate classes
Maps classes to counts
We like 'empty' to be class 0
%% Assemble dictionaries
...for each category
Each element is a list of relative path/full path/width/height/className
...for each image
%% External class mapping
%% Write raw class table
cat = categories[0]
%% Read the mapped class table
"id, source, count, target"
"%% Make classMappings contain *all* classes, not just remapped classes"
cat = categories[0]
%% Create new class list
"Start at 1, explicitly assign 0 to ""empty"""
One-off issue with character encoding
%% Re-map annotations
ann = annotations[0]
%% Write output .json
%% Utilities
%%
Find images with a particular tag
%% Randomly sample annotations
""
umn_to_json.py
""
Prepare images and metadata for the Orinoquía Camera Traps dataset.
""
%% Imports and constants
"For two deployments, we're only processing imagse in the ""detections"" subfolder"
%% Enumerate deployment folders
%% Load ground truth
i_row = 0; row = ground_truth_df.iloc[i_row]
%% Create relative paths for ground truth data
"Some deployment folders have no subfolders, e.g. this is a valid file name:"
""
M00/01010132.JPG
""
"But some deployment folders have subfolders, e.g. this is also a valid file name:"
""
N17/100EK113/07160020.JPG
""
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly"
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths."
deployment_name = list(deployment_folders)[0]
Enumerate all files in this folder
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))"
filename = files[100]
...for each file in this deployment
...for each deployment
%% Add relative paths to our ground truth table
i_row = 0; row = ground_truth_df.iloc[i_row]
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's"
"just the deployment ID and the image name, separated by a slash."
Find the relative path for this image
Make sure this image file exists
...for each row in the ground truth table
%% Take everything out of Pandas
%% Convert string timestamps to Python datetimes
im = ground_truth_dicts[0]
"Filenames look like, e.g., N36/100EK113/06040726.JPG"
%% Synthesize sequence information
Sort images by time within each location
i_location=0; location = locations[i_location]
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_camera[1]
"Timestamp for this image, may be None"
Start a new sequence if:
""
* This image has no timestamp
* This iamge has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each image in this location
...for each location
%% Create category dict and category IDs
im = images[0]
%% Imports and constants (.json generation)
%% Count frames in each sequence
%% Double check images with multiple annotations
im = images[0]
%% Assemble dictionaries
Force the empty category to be ID 0
input_im = images[0]
...for each image
%% Write output .json
%% Validate .json file
%% Map relative paths to annotation categories
ann = data['annotations'][0]
%% Copy images to output
EXCLUDE HUMAN AND MISSING
im = data['images'][0]
im = images[0]
%% Preview labels
viz_options.classes_to_exclude = ['test']
""
nacti_fieldname_adjustments.py
""
"NACTI metadata was posted with ""filename"" in images instead of ""file_name"", and"
"used string (rather than int) category IDs (in categories, but not in annotations)."
""
This script fixes those issues and rev's the version number.
""
%% Constants and environment
%% Read .json file
"%% Rev version number, update field names and types"
%% Write json file
""
cct_field_adjustments.py
""
CCT metadata was posted with int locations instead of strings.
""
This script fixes those issues and rev's the version number.
""
%% Constants and environment
%% Read .json file
"%% Rev version number, update field names and types"
%% Write json file
%% Check output data file
""
carrizo_shrubfree_2018.py
""
Convert the .csv file provided for the Carrizo Mojave data set to a
COCO-camera-traps .json file
""
%% Constants and environment
%% Read source data
Original .csv file had superfluous spaces in column names
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
58 missing files (of 17652)
%% Check for images that aren't included in the metadata file
3012 of 20606 files are not in metadata
%% Create CCT dictionaries
Map categories to integer IDs
""
The category '0' is reserved for 'empty'
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
Each filename should just match one row
Don't include images that don't exist on disk
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
""
snapshot_serengeti_lila.py
""
Create zipfiles of Snapshot Serengeti S1-S11.
""
"Create a metadata file for S1-S10, plus separate metadata files"
"for S1-S11.  At the time this code was written, S11 was under embargo."
""
Create zip archives of each season without humans.
""
Create a human zip archive.
""
%% Constants and imports
import sys; sys.path.append(r'c:\git\ai4eutils')
import sys; sys.path.append(r'c:\git\cameratraps')
assert(os.path.isdir(metadata_base))
"There are two redundant categories, and we re-map ""blank"" to ""empty"" as per CCT convention"
"%% Load metadata files, concatenate into a single table"
iSeason = 1
%% Convert to dictionaries (prep)
%% Convert to dictionaries (loops)
"TODO: iterrows() is a terrible way to do this, but this is one of those days"
"where I want to get this done, not get better at Python."
irow = 0; row = image_table.iloc[0]
"Loaded as an int64, converting to int here"
...for each row in the image table
Make sure image IDs are what we think they are
...for each row in the annotation table
%% Load previously-saved dictionaries when re-starting mid-script
%%
%% Take a look at categories (just sanity-checking)
%%
%% Fill in some image fields we didn't have when we created the image table
"width, height, corrupt, seq_num_frames, location, datetime"
Every annotation in this list should have the same sequence ID
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;"
there's no reason to do this redundantly for every image
Every image in this sequence should point back to the same equence
Every annotation in this list should have the same location
Every annotation in this list should have the same datetime
Is this image on disk?
iImage = 0; im = images[0]
...for each image
"images_processed = pool.map(process_image, images)"
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))"
%%
%% Count missing/corrupted images
%% Print distribution of sequence lengths (sanity-check)
%% Replicate annotations across images
iAnn = 0; ann = annotations[iAnn]
%% See what files are on disk but not annotated (~15 mins)
247370 files not in the database (of 7425810)
%% Load old image database
%% Look for old images not in the new DB and vice-versa
"At the time this was written, ""old"" was S1-S6"
old_im = cct_old['images'][0]
new_im = images[0]
4 old images not in new db
12 new images not in old db
%% Save our work
%% Load our work
%%
%% Examine size mismatches
i_mismatch = -1; old_im = size_mismatches[i_mismatch]
%% Sanity-check image and annotation uniqueness
"%% Split data by seasons, create master list for public seasons"
ann = annotations[0]
%% Minor updates to fields
"%% Write master .json out for S1-10, write individual season .jsons (including S11)"
"Loop over all seasons, plus one iteration for the ""all public data"" iteration, and"
"one for the ""all data"" iteration"
%% Find categories that only exist in S11
List of categories in each season
Category 55 (fire) only in S11
Category 56 (hyenabrown) only in S11
Category 57 (wilddog) only in S11
Category 58 (kudu) only in S11
Category 59 (pangolin) only in S11
Category 60 (lioncub) only in S11
%% Prepare season-specific .csv files
iSeason = 1
%% Create a list of human files
ann = annotations[0]
%% Save our work
%% Load our work
%%
"%% Create archives (human, per-season) (prep)"
im = images[0]
im = images[0]
Don't include humans
Only include files from this season
Possibly start a new archive
...for each image
i_season = 0
"for i_season in range(0,nSeasons):"
create_season_archive(i_season)
%% Create archives (loop)
pool = ThreadPool(nSeasons+1)
"n_images = pool.map(create_archive, range(-1,nSeasons))"
"seasons_to_zip = range(-1,nSeasons)"
...for each season
%% Sanity-check .json files
%logstart -o r'E:\snapshot_temp\python.txt'
%% Zip up .json and .csv files
pool = ThreadPool(len(files_to_zip))
"pool.map(zip_single_file, files_to_zip)"
%% Super-sanity-check that S11 info isn't leaking
im = data_public['images'][0]
ann = data_public['annotations'][0]
iRow = 0; row = annotation_df.iloc[iRow]
iRow = 0; row = image_df.iloc[iRow]
%% Create bounding box archive
i_image = 0; im = data['images'][0]
i_box = 0; boxann = bbox_data['annotations'][0]
%% Sanity-check a few files to make sure bounding boxes are still sensible
import sys; sys.path.append(r'C:\git\CameraTraps')
%% Check categories
%% Summary prep for LILA
""
wi_to_json
""
Prepares CCT-formatted metadata based on a Wildlife Insights data export.
""
"Mostly assumes you have the images also, for validation/QA."
""
%% Imports and constants
"Because WI filenames are GUIDs, it's not practical to page through sequences in an"
image viewer.  So we're going to (optionally) create a copy of the data set where
images are ordered.
%% Load ground truth
%% Take everything out of Pandas
%% Synthesize common names when they're not available
"Blank rows should always have ""Blank"" as the common name"
"print('Warning: missing common name for row {} ({})'.format(i_row,row['filename']))"
%% Convert string timestamps to Python datetimes
im = ground_truth_dicts[0]
"The field called ""location"" in the WI .csv file is a URL, we want to reclaim"
"the ""location"" keyword for CCT output"
"Filenames look like, e.g., N36/100EK113/06040726.JPG"
%% Synthesize sequence information
Sort images by time within each location
i_location=0; location = locations[i_location]
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_camera[1]
"Timestamp for this image, may be None"
Start a new sequence if:
""
* This image has no timestamp
* This iamge has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each image in this location
...for each location
%% Create category dict and category IDs
im = images[0]
%% Count frames in each sequence
%% Build relative paths
im = images[0]
Sample URL:
""
gs://project-asfasdfd/deployment/21444549/asdfasdfd-616a-4d10-a921-45ac456c568a.jpg'
%% Double check images with multiple annotations
im = images[0]
%% Assemble dictionaries
Force the empty category to be ID 0
input_im = images[0]
"print('Warning: image ID {} ({}) has multiple annotations'.format(im['id'],im['id'].replace('_','/')))"
...for each image
%% Write output .json
%% Validate .json file
%% Preview labels
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))"
%%
%% Create ordered dataset
"Because WI filenames are GUIDs, it's not practical to page through sequences in an"
image viewer.  So we're going to create a copy of the data set where images are
ordered.
im = images_out[0]; im
%% Create ordered .json
%% Copy files to their new locations
im = ordered_images[0]
im = data_ordered['images'][0]
%% Preview labels in the ordered dataset
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))"
%% Open an ordered filename from the unordered filename
""
rspb_to_json.py
""
Convert the .csv file provided for the RSPB data set to a
COCO-camera-traps .json file
""
%% Constants and environment
[location] is an obfuscation
%% Create info struct
%% Read source data
metadataTable.columns.values
""
"array(['Project', 'inDir', 'FileName', 'Station', 'Camera',"
"'StationCameraFileName', 'DateTimeOriginal', 'DateReadable',"
"'outDir', 'filename_new', 'fileExistsAlready', 'CopyStatus',"
"'Species'], dtype=object)"
We'll populate these later
keys should be lowercase
"%% Enumerate images, confirm filename uniqueness"
"%% Update metadata filenames to include site and camera folders, check existence"
""
Takes ~1min
iRow = 0; row = metadataTable.iloc[iRow]
There's a bug in the metadata; the 'camera' column isn't correct.
camera = row['Camera']
"These appear as, e.g., '3.22e12'"
camera = str(int(float(camera)))
Let's pull this out of the file name instead
""
Filenames look like one of the following:
""
A1__03224850850507__2015-11-28__10-45-04(1).JPG
Bayama2PH__C05__NA(NA).JPG
assert(os.path.isfile(fullPath))
metadataTable.iloc[iRow] = row
Re-assemble into an updated table
%% Check for images that aren't included in the metadata file
Enumerate all images
Write to a text file
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
iRow = 0; row = metadataTable.iloc[iRow]
A1__03224850850507__2015-11-28__10-45-04(1).JPG
'A1\\03224850850507\\A1__03224850850507__2015-11-28__10-45-04(1).JPG'
Not currently populated
"Often -1, sometimes a semi-meaningful int"
A1
03224850850507
"In variable form, but sometimes '28/11/2015 10:45'"
Check image height and width
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Write output
%% Check database integrity
%% Preview a few images to make sure labels were passed along sensibly
%% One-time processing step: copy images to a flat directory for annotation
%%
""
pc_to_json.py
""
Convert a particular collection of .csv files to CCT format.
""
%% Constants and environment
%% Read and concatenate source data
List files
"List of dataframes, one per .csv file; we'll concatenate later"
i_file = 87; fn = input_files[i_file]
Concatenate into a giant data frame
%% List files
%% Main loop over labels (prep)
Force the empty category to be ID 0
%% Main loop over labels (loop)
iRow = 0; row = input_metadata.iloc[iRow]
"ImageID,FileName,FilePath,SpeciesID,CommonName"
assert os.path.isfile(full_path)
Retrieve image width and height
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
%% See what's up with missing files
s = list(image_relative_paths)[0]
s = missing_files[0]
%% Check for images that aren't included in the metadata file
%% Sample the database
%%
Collect the images we want
%% Create info struct
%% Write output
%% Sanity-check the database's integrity
%% Render a bunch of images to make sure the labels got carried along correctly
options.classes_to_exclude = ['unlabeled']
%% Write out a list of files to annotate
""
save_the_elephants_survey_B.py
""
Convert the .csv file provided for the Save the Elephants Survey B data set to a
COCO-camera-traps .json file
""
%% Constants and environment
input_base = r'/mnt/blobfuse/wildlifeblobssc/ste_2019_08_drop'
output_base = r'/home/gramener/survey_b'
Handle all unstructured fields in the source data as extra fields in the annotations
"photo_type really should be an image property, but there are a few conflicts"
that forced me to handle it as an annotation proprerty
%% Read source data
"%% Map filenames to rows, verify image existence"
"%% Map filenames to rows, verify image existence"
Maps relative paths to row indices in input_metadata
"Build up a map from filenames to a list of rows, checking image existence as we go"
Ignore directories
%% Make sure the multiple-annotation cases make sense
%%
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
Force the empty category to be ID 0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image]
Example filename:
""
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG'
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG'
i_row = row_indices[0]
timestamp = row['Date']
There are a small handful of datetime mismatches across annotations
for the same image
assert im['datetime'] == timestamp
Special cases based on the 'photo type' field
Various spellings of 'community'
Have we seen this category before?
Create an annotation
fieldname = list(mapped_fields.keys())[0]
...for each row
...for each image
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
%% Scrap
%% Find unique photo types
""
ubc_to_json.py
""
Convert the .csv file provided for the UBC data set to a
COCO-camera-traps .json file
""
"Images were provided in eight folders, each of which contained a .csv"
file with annotations.  Those annotations came in two slightly different
"formats, the two formats corresponding to folders starting with ""SC_"" and"
otherwise.
""
%% Constants and environment
Map Excel column names - which vary a little across spreadsheets - to a common set of names
%% Enumerate images
Load from file if we've already enumerated
%% Create CCT dictionaries
Force the empty category to be ID 0
To simplify debugging of the loop below
#%% Create CCT dictionaries (loop)
#%%
Read source data for this folder
Rename columns
Folder name is the first two characters of the filename
""
Create relative path names from the filename itself
Folder name is the camera name
""
Create relative path names from camera name and filename
Which of our images are in the spreadsheet?
i_row = 0; fn = input_metadata['image_relative_path'][i_row]
#%% Check for images that aren't included in the metadata file
Find all the images in this folder
Which of these aren't in the spreadsheet?
#%% Create entries in CCT dictionaries
Only process images we have on disk
"This is redundant, but doing this for clarity, at basically no performance"
cost since we need to *read* the images below to check validity.
i_row = row_indices[0]
"These generally represent zero-byte images in this data set, don't try"
to find the very small handful that might be other kinds of failures we
might want to keep around.
print('Error opening image {}'.format(image_relative_path))
If we've seen this category before...
...make sure it used the same latin --> common mapping
""
"If the previous instance had no mapping, use the new one."
assert common_name == category['common_name']
Create an annotation
...for each annotation we found for this image
...for each image
...for each dataset
Print all of our species mappings
"%% Copy images for which we actually have annotations to a new folder, lowercase everything"
im = images[0]
%% Create info struct
"%% Convert image IDs to lowercase in annotations, tag as sequence level"
"While there isn't any sequence information, the nature of false positives"
"here leads me to believe the images were labeled at the sequence level, so"
we should trust labels more when positives are verified.  Overall false
positive rate looks to be between 1% and 5%.
%% Write output
%% Validate output
%% Preview labels
""
helena_to_cct.py
""
Convert the Helena Detections data set to a COCO-camera-traps .json file
""
%% Constants and environment
This is one time process
%% Create Filenames and timestamps mapping CSV
import pdb;pdb.set_trace()
%% To create CCT JSON for RSPB dataset
%% Read source data
Original Excel file had timestamp in different columns
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
%% Skipping this check because one image has multiple species
assert len(duplicate_rows) == 0
%% Check for images that aren't included in the metadata file
%% Create CCT dictionaries
Map categories to integer IDs
""
The category '0' is reserved for 'empty'
For each image
Don't include images that don't exist on disk
Some filenames will match to multiple rows
assert(len(rows) == 1)
iRow = rows[0]
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
ann['datetime'] = row['datetime']
ann['site'] = row['site']
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
%% Imports and constants
from github.com/microsoft/ai4eutils
from github.com/ecologize/CameraTraps
A list of files in the lilablobssc container for this data set
The raw detection files provided by NOAA
A version of the above with filename columns added
%% Read input .csv
%% Read list of files
%% Convert paths to full paths
i_row = 0; row = df.iloc[i_row]
assert ir_image_path in all_files
...for each row
%% Write results
"%% Load output file, just to be sure"
%% Render annotations on an image
i_image = 2004
%% Download the image
%% Find all the rows (detections) associated with this image
"as l,r,t,b"
%% Render the detections on the image(s)
In pixel coordinates
In pixel coordinates
%% Save images
%% Clean up
""
carrizo_trail_cam_2017.py
""
"Convert the .csv files provided for the ""Trail Cam Carrizo"" 2017 data set to"
a COCO-camera-traps .json file.
""
%% Constants and environment
%% Read source data
Removing the empty records
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
908 missing files (of 60562)
%% Check for images that aren't included in the metadata file
105329 of 164983 files are not in metadata
%% Create CCT dictionaries
Map categories to integer IDs
""
The category '0' is reserved for 'empty'
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
Each filename should just match one row
Don't include images that don't exist on disk
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
""
channel_islands_to_cct.py
""
Convert the Channel Islands data set to a COCO-camera-traps .json file
""
"Uses the command-line tool ExifTool (exiftool.org) to pull EXIF tags from images,"
"because every Python package we tried failed to pull the ""Maker Notes"" field properly."
""
"%% Imports, constants, paths"
# Imports ##
# Constants ##
# Paths ##
Confirm that exiftool is available
"assert which(exiftool_command_name) is not None, 'Could not locate the ExifTool executable'"
%% Load information from every .json file
"Ignore the sample file... actually, first make sure there is a sample file"
...and now ignore that sample file.
json_file = json_files[0]
ann = annotations[0]
...for each annotation in this file
...for each .json file
"%% Confirm URL uniqueness, handle redundant tags"
Have we already added this image?
"One .json file was basically duplicated, but as:"
""
Ellie_2016-2017 SC12.json
Ellie_2016-2017-SC12.json
"If the new image has no output, just leave the old one there"
"If the old image has no output, and the new one has output, default to the one with output"
Don't worry about the cases where someone tagged 'fox' and someone tagged 'fox_partial'
...for each image we've already added
...if this URL is/isn't in the list of URLs we've already processed
...for each image
%% Save progress
%%
%%
%% Download files (functions)
https://www.quickprogrammingtips.com/python/how-to-download-multiple-files-concurrently-in-python.html
"This is returned with a leading slash, remove it"
%% Download files (execution)
%% Read required fields from EXIF data (functions)
"-G means ""Print group name for each tag"", e.g. print:"
""
[File]          Bits Per Sample                 : 8
""
...instead of:
""
Bits Per Sample                 : 8
"If we don't get any EXIF information, this probably isn't an image"
line_raw = exif_lines[0]
"Split on the first occurrence of "":"""
Typically:
""
"'[MakerNotes]    Sequence                        ', '1 of 3']"
Not a typo; we are using serial number as a location
"If there are multiple timestamps, make sure they're *almost* the same"
"If there are multiple timestamps, make sure they're *almost* the same"
...for each line in the exiftool output
"This isn't directly related to the lack of maker notes, but it happens that files that are missing"
maker notes also happen to be missing EXIF date information
...process_exif()
"This is returned with a leading slash, remove it"
Ignore non-image files
%% Read EXIF data (execution)
ann = images[0]
%% Save progress
Use default=str to handle datetime objects
%%
%%
"Not deserializing datetimes yet, will do this if I actually need to run this"
%% Check for EXIF read errors
%% Remove junk
Ignore non-image files
%% Fill in some None values
"...so we can sort by datetime later, and let None's be sorted arbitrarily"
%% Find unique locations
%% Synthesize sequence information
Sort images by time within each location
i_location=0; location = locations[i_location]
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_camera[1]
"Timestamp for this image, may be None"
Start a new sequence if:
""
* This image has no timestamp
* This image has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each image in this location
...for each location
%% Count frames in each sequence
images_this_sequence = [im for im in images if im['seq_id'] == seq_id]
"%% Create output filenames for each image, store original filenames"
i_location = 0; location = locations[i_location]
i_image = 0; im = sorted_images_this_location[i_image]
%% Save progress
Use default=str to handle datetime objects
%%
%%
%% Copy images to their output files (functions)
%% Copy images to output files (execution)
%% Rename the main image list for consistency with other scripts
%% Create CCT dictionaries
Make sure this is really a box
Transform to CCT format
Force the empty category to be ID 0
i_image = 0; input_im = all_image_info[0]
"This issue only impacted one image that wasn't a real image, it was just a screenshot"
"showing ""no images available for this camera"""
Convert datetime if necessary
Process temperature if available
Read width and height if necessary
I don't know what this field is; confirming that it's always None
Process object and bbox
os.startfile(output_image_full_path)
"Zero is hard-coded as the empty category, but check to be safe"
"I can't figure out the 'index' field, but I'm not losing sleep about it"
assert input_annotation['index'] == 1+i_ann
"Some annotators (but not all) included ""_partial"" when animals were partially obscured"
"Annotators *mostly* used 'none', but sometimes 'empty'.  'empty' is CCT-correct."
If we've seen this category before...
If this is a new category...
...if this is an empty/non-empty annotation
Create an annotation
...for each annotation on this image
...for each image
%% Change *two* annotations on images that I discovered contains a human after running MDv4
%% Move human images
ann = annotations[0]
%% Count images by location
%% Write output
%% Validate output
%% Preview labels
viz_options.classes_to_exclude = [0]
""
ena24_to_json_2017.py
""
Convert the ENA24 data set to a COCO-camera-traps .json file
""
%% Constants and environment
Temporary folders for human and non-human images
Clean existing output folders/zipfiles
%% Support functions
%% Read source data
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
Check image height and width
"Each row is category, [box coordinates]"
"If there's just one row, loadtxt reads it as a 1d array; make it a 2d array"
with one row
Each row is a bounding box
Have we seen this category before?
Create an annotation
...for each bounding box
"This was here for debugging; nearly every instance is Human+Horse, Human+Vehicle,"
"or Human+Dog, but there is one Rabbit+Opossium, and a few Deer+Chicken!"
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Create ZIP files for human and non human
%% Validate output
%% Preview labels
%% Imports and constants
configurations and paths
%% Helper functions
"dest_path = copy(source_path, dest_folder)"
num_workers = multiprocessing.cpu_count()
pool = ThreadPool(num_workers)
"results = pool.starmap(_copy_unzip, zip(sources, itertools.repeat(dest_folder)))"
""
print('Waiting for processes to finish...')
pool.close()
pool.join()
sequential
%% Command-line driver
if the blob container is already mounted on the VM
or you can download them using the storage Python SDK
store storage account key in environment variable AZ_STORAGE_KEY
""
eMammal_helpers.py
""
Support functions for processing eMammal metadata
""
%% Constants and imports
%% Support functions
"pad to a total of 3 digits if < 1000, or 4 digits otherwise"
img_frame is a string from the xml tree
"length 4 frame order is returned as is, others are left padded to be 3 digit long"
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels
pad to a total of 4 digits
img_frame is a string from the xml tree
"length 4 frame order is returned as is, others are left padded to be 3 digit long"
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels
others column
summer day hours: 6am - 7pm
others day hours: 7am - 6pm
""
make_eMammal_json.py
""
"Produces the COCO-formatted json database for an eMammal dataset, i.e. a"
"collection of folders, each of which contains a deployment_manifest.xml file."
""
"In this process, each image needs to be loaded to size it."
""
"To add bounding box annotations to the resulting database, use"
add_annotations_to_eMammal_json.py.
""
%% Constants and imports
"Either add the eMammal directory to your path, or run from there"
os.chdir(r'd:\git\CameraTraps\database_tools\eMammal')
import warnings
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
warnings.filterwarnings('ignore')
Should we run the image size retrieval in parallel?
%% Support functions
%% Main loop (metadata processing; image sizes are retrieved later)
deployment = folders[0]
sequence = image_sequences[0]
get species info for this sequence
add each image's info to database
img = images[0]
"some manifests don't have the ImageOrder info, but the info is in the file name"
full_img_id has no frame info
""
frame number only used in requests to iMerit for ordering
...for each image
...for each sequence
...for each deployment
%% Get image sizes
"'tasks' is currently a list of 2-tuples, with each entry as [image dictionary,path]."
""
"Go through that and copy just the image dictionaries to 'db_images', adding size"
information to each entry.  Takes a couple hours.
opening each image seems too fast for this multi-threaded version to be faster than sequential code.
%% Assemble top-level dictionaries
%% Write out .json
""
make_full_SS_json.py
""
Create a COCO-camera-traps .json file for Snapshot Serengeti data from
the original .csv files provided on Dryad.
""
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no"
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated
Snapshot Safari preparation process.
""
%% Imports and constants
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays"
Count the number of images with multiple species
"%% Read image .csv file, format into a dictionary mapping images to capture events"
%% Create CCT-style .json
still need image width and height
...for each image
%% Write output files
""
make_per_season_SS_json.py
""
Create a COCO-camera-traps .json file for each Snapshot Serengeti season from
the original .csv files provided on Dryad.
""
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no"
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated
Snapshot Safari preparation process.
""
%% Imports and constants
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays"
"%% Read image .csv file, format into a dictionary mapping images to capture events"
%% Create CCT-style .json
still need image width and height
...for each image ID
%% Write output files
...for each season
""
generate_lila_per_image_labels.py
""
"Generate a .csv file with one row per annotation, containing full URLs to every"
"camera trap image on LILA, with taxonomically expanded labels."
""
"Typically there will be one row per image, though images with multiple annotations"
will have multiple rows.
""
"Some images may not physically exist, particularly images that are labeled as ""human""."
This script does not validate image URLs.
""
Does not include bounding box annotations.
""
%% Constants and imports
"We'll write images, metadata downloads, and temporary files here"
"Some datasets don't have ""sequence_level_annotation"" fields populated, but we know their"
annotation level
%% Download and parse the metadata file
To select an individual data set for debugging
%% Download and extract metadata for the datasets we're interested in
%% Load taxonomy data
"%% Build a dictionary that maps each [dataset,query] pair to the full taxonomic label set"
i_row = 0; row = taxonomy_df.iloc[i_row]
%% Process annotations for each dataset
ds_name = list(metadata_table.keys())[0]
"Go through annotations, marking each image with the categories that are present"
""
ann = annotations[0]
im = images[10]
This field name was only used for Caltech Camera Traps
raise ValueError('Suspicious date parsing result')
Special case we don't want to print a warning about
"Location, sequence, and image IDs are only guaranteed to be unique within"
"a dataset, so for the output .csv file, include both"
category_name = list(categories_this_image)[0]
Only print a warning the first time we see an unmapped label
...for each category that was applied at least once to this image
...for each image in this dataset
print('Warning: no date information available for this dataset')
print('Warning: no location information available for this dataset')
...for each dataset
...with open()
%% Read the .csv back
%% Do some post-hoc integrity checking
"-1 isn't *really* valid, but we use it sometimes for sequences of unknown length"
i_row = 0; row = df.iloc[i_row]
%% Preview constants
%% Choose images to download
ds_name = list(metadata_table.keys())[2]
Find all rows for this dataset
...for each dataset
%% Download images
i_image = 0; image = images_to_download[i_image]
%% Write preview HTML
im = images_to_download[0]
""
Common constants and functions related to LILA data management/retrieval.
""
%% Imports and constants
LILA camera trap master metadata file
"wildlife_insights_taxonomy_url = 'https://api.wildlifeinsights.org/api/v1/taxonomy?fields=class,order,family,genus,species,authority,taxonomyType,uniqueIdentifier,commonNameEnglish&page[size]={}'.format(wildlife_insights_page_size)"
from ai4eutils
%% Common functions
"We haven't implemented paging, make sure that's not an issue"
d['data'] is a list of items that look like:
Put the master metadata file in the same folder where we're putting images
Read lines from the master metadata file
Parse those lines into a table
Each line in this file is name/sas_url/json_url/[bbox_json_url]
Create a separate entry for bounding boxes if they exist
Unzip if necessary
""
get_lila_category_list.py
""
Generates a .json-formatted dictionary mapping each LILA dataset to all categories
"that exist for that dataset, with counts for the number of occurrences of each category"
"(the number of *annotations* for each category, not the number of *images*)."
""
"Also loads the taxonomy mapping file, to include scientific names for each category."
""
get_lila_category_counts counts the number of *images* for each category in each dataset.
""
%% Constants and imports
array to fill for output
"We'll write images, metadata downloads, and temporary files here"
Created by get_lila_category_list.py... contains counts for each category
%% Load category and taxonomy files
%% Map dataset names and category names to scientific names
i_row = 1; row = taxonomy_df.iloc[i_row]; row
%% Download and parse the metadata file
%% Download and extract metadata for the datasets we're interested in
%% Get category names and counts for each dataset
ds_name = 'NACTI'
Open the metadata file
Collect list of categories and mappings to category name
ann = annotations[0]
c = categories[0]
"Don't do taxonomy mapping for bbox data sets, which are sometimes just binary and are"
always redundant with the class-level data sets.
"As of right now, this is the only quirky case"
...for each dataset
%% Save dict
%% Print the results
ds_name = list(dataset_to_categories.keys())[0]
...for each dataset
""
download_lila_subset.py
""
"Example of how to download a list of files from LILA, e.g. all the files"
in a data set corresponding to a particular species.
""
%% Constants and imports
LILA camera trap master metadata file
"In this example, we're using the Missouri Camera Traps data set and the Caltech Camera Traps dataset"
All lower-case; we'll convert category names to lower-case when comparing
"We'll write images, metadata downloads, and temporary files here"
"We will demonstrate two approaches to downloading, one that loops over files"
"and downloads directly in Python, another that uses AzCopy."
""
AzCopy will generally be more performant and supports resuming if the
transfers are interrupted.  This script assumes that azcopy is on the system path.
Number of concurrent download threads (when not using AzCopy) (AzCopy does its
own magical parallelism)
%% Support functions
remove the leading '/'
%% Download and parse the metadata file
Put the master metadata file in the same folder where we're putting images
Read lines from the master metadata file
Parse those lines into a table
Each line in this file is name/base_url/json_url/[box_url]
%% Download and extract metadata for the datasets we're interested in
Unzip if necessary
...for each dataset of interest
%% List of files we're going to download (for all data sets)
"Flat list or URLS, for use with direct Python downloads"
For use with azcopy
This may or may not be a SAS URL
# Open the metadata file
# Build a list of image files (relative path names) that match the target species
Retrieve all the images that match that category
Retrieve image file names
Convert to URLs
...for each dataset
%% Download those image files
ds_name = 'Caltech Camera Traps'
ds_name = 'SWG Camera Traps'
We want to use the whole relative path for this script (relative to the base of the container)
"to build the output filename, to make sure that different data sets end up in different folders."
This may or may not be a SAS URL
For example:
""
caltech-unzipped/cct_images
swg-camera-traps
Check whether the URL includes a folder
E.g. caltech-unzipped
E.g. cct_images
E.g. swg-camera-traps
"Write out a list of files, and use the azcopy ""list-of-files"" option to download those files."
""
This azcopy feature is unofficially documented at:
""
https://github.com/Azure/azure-storage-azcopy/wiki/Listing-specific-files-to-transfer
""
import clipboard; clipboard.copy(cmd)
Loop over files
""
get_lila_category_counts.py
""
Count the number of images and bounding boxes with each label in one or more LILA datasets.
""
"This script doesn't write these counts out anywhere other than the console, it's just intended"
as a template for doing operations like this on LILA data.  get_lila_category_list.py writes
"information out to a .json file, but it counts *annotations*, not *images*, for each category."
""
%% Constants and imports
"If None, will use all datasets"
"We'll write images, metadata downloads, and temporary files here"
%% Download and parse the metadata file
%% Download and extract metadata for the datasets we're interested in
%% Count categories
ds_name = datasets_of_interest[0]
"Go through annotations, marking each image with the categories that are present"
""
ann = annotations[0]
Now go through images and count categories
im = images[0]
...for each dataset
%% Print the results
...for each dataset
""
add_url_to_database.py
""
"Adds a ""url"" field to a coco-camera-traps .json database, specifically to allow the db to"
be reviewed in the Visipedia annotation tool.
""
""
remove_corrupted_images_from_database.py
""
"Given a coco-camera-traps .json file, checks all images for TF-friendliness and generates"
a new .json file that only contains the non-corrupted images.
""
%% Imports and constants
import multiprocessing
"I leave this at an annoying low number, since by definition weird stuff will"
"be happening in the TF kernel, and it's useful to keep having content in the console."
%% Function definitions
"I sometimes pass in a list of images, sometimes a dict with a single"
element mapping a job ID to the list of images
"We're about to start a lot of TF sessions, and we don't want gobs"
of debugging information printing out for every session.
At some point we were creating a single session and looping over images
"within that session, but the only way I found to reliably not run out"
of GPU memory was to create a session per image and gc.collect() after
each session.
Map Image IDs to boolean (should I keep this image?)
"Convert to lists, append job numbers to the image lists"
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)"
Merge results
%% Interactive driver
%%
base_dir = r'D:\temp\snapshot_serengeti_tfrecord_generation'
Load annotations
Check for corruption
Write out only the uncorrupted data
%% Command-line driver
#######
""
integrity_check_json_db.py
""
"Does some integrity-checking and computes basic statistics on a db, specifically:"
""
* Verifies that required fields are present and have the right types
* Verifies that annotations refer to valid images
* Verifies that annotations refer to valid categories
"* Verifies that image, category, and annotation IDs are unique"
""
* Optionally checks file existence
""
* Finds un-annotated images
* Finds unused categories
""
* Prints a list of categories sorted by count
""
#######
%% Constants and environment
%% Functions
"If baseDir is non-empty, checks image existence"
This is used in a medium-hacky way to share modified options across threads
print('Image path {} does not exist'.format(filePath))
"#%% Read .json file if necessary, integrity-check fields"
info = data['info']
"#%% Build dictionaries, checking ID uniqueness and internal validity as we go"
Confirm that required fields are present
Confirm ID uniqueness
...for each category
image = images[0]
Confirm that required fields are present
Confirm ID uniqueness
We previously supported ints here; this should be strings now
"assert isinstance(image['location'], str) or isinstance(image['location'], int), \"
'Illegal image location type'
Are we checking for unused images?
Recursively enumerate images
print('Image {} is unused'.format(p))
Are we checking file existence and/or image size?
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)"
...for each image
Confirm that required fields are present
Confirm ID uniqueness
Confirm validity
...for each annotation
#%% Print statistics
Find un-annotated images and multi-annotation images
Find unused categories
Prints a list of categories sorted by count
""
https://stackoverflow.com/questions/72899/how-do-i-sort-a-list-of-dictionaries-by-a-value-of-the-dictionary
...def integrity_check_json_db()
%% Command-line driver
%% Interactive driver(s)
%%
Integrity-check .json files for LILA
options.iMaxNumImages = 10
""
combine_two_json_files.py
""
"Merges two coco-camera-traps .json files. In particular, categories are combined and re-numbered."
""
Combined Info
Combined Images
Combined Categories
## categories to merge
Combined Annotations
""
make_detection_db_for_viewing.py
""
"Given a .json file with ground truth bounding boxes, and a .p file containing detections for the same images,"
"creates a new .json file with separate classes for ground truth and detection, suitable for viewing in the Visipedia"
annotation tool.
""
%% Imports and constants
%% Main function
im_id_to_im = {im['id']:im for im in images}
make new categories to distinguish between ground truth and detections
"update all gt annotations to be class ""gt"""
collect all detections by image
keep any detection with score above det_thresh
"need to convert bbox from [x1,y1,x2,y2] to [x,y,w,h]"
"add ""info"" and ""licenses"" for annotation tools to function"
create new db
%% Command-line handling
""
analyze_json_database.py
""
Plots location/class/etc. distributions for classes in a coco-camera-traps .json file.
""
Currently includes some one-off code for specific species.
""
%% Constants and imports
%% Path configuration
%% Load source data
%% Build image/category dictionaries
%% Make plot of category distribution
%% make plots of location distribution
"plt.title('Number of images per location, by category')"
"plt.tight_layout(rect=[0,0,1,0.9])"
#make plot of images per season
%% Make plot of lions per location
%% Make plot of elephants per location
for loc in sorted_by_total[:25]:
"print('Location:' + loc[0] +', Lions: ' + str(len(loc_to_lion_ims[loc[0]])) + ', Elephants: ' + str(len(loc_to_elephant_ims[loc[0]])) + ', Total ims: ' + str(len(loc_to_ims[loc[0]])))"
""
subset_json_db.py
""
Select a subset of images (and associated annotations) from a .json file
in COCO Camera Traps format.
""
"To subset the .json files produced by our batch processing API, see"
subset_json_detector_output.py
""
Sample invocation:
""
"python subset_json_db.py ""E:\Statewide_wolf_container\idfg_20190409.json"" ""E:\Statewide_wolf_container\idfg_20190409_clearcreek.json"" ""clearcreek"" --ignore_case"
""
%% Constants and imports
%% Functions
Load the input file if necessary
Find images matching the query
Find annotations referring to those images
Write the output file if requested
%% Interactive driver
%%
%% Command-line driver
%% Constants and imports
%% Merge functions
i_input_dict = 0; input_dict = input_dicts[i_input_dict]
We will prepend an index to every ID to guarantee uniqueness
Map detection categories from the original data set into the merged data set
...for each category
Merge original image list into the merged data set
Create a unique ID
...for each image
Same for annotations
...for each annotation
"Merge info dicts, don't check completion time fields"
...for each dictionary
Convert merged image dictionaries to a sorted list
%% Driver
""
add_width_and_height_to_database.py
""
Grabs width and height from actual image files for a .json database that is missing w/h.
""
Originally used when we created a .json file for snapshot serengeti from .csv.
""
"This file converts the JSON output of the batch processing API to a pickle file, which can be used by the"
script ./make_classification_dataset.py
"Convert boxes from JSON   [x_min, y_min, width_of_box, height_of_box]"
"to PICKLE [ymin,  xmin,  ymax,         xmax]"
Write detections to file with pickle
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the"
script ./make_classification_dataset.py
Write detections to file with pickle
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the"
script ./make_classification_dataset.py
Assumes that the root of the CameraTrap repo is in the PYTHONPATH
Minimum threshold to put a detection into the output JSON file
Parameter check
Load detections from input
Load COCO style annotations
Build output JSON in format version 1.0
Adding the only known metadata info
"The pickle file does not contain category information, so we assume the default"
For each image with detections
for each detection
"Convert boxes from [ymin, xmin, ymax, xmax] format to"
"[x_min, y_min, width_of_box, height_of_box]"
Write output json
global flag for whether or not we encounter missing images
"- will only print ""missing image"" warning once"
TFRecords variables
1.3 for the cropping during test time and 1.3 for the context that the
CNN requires in the left-over image
Create output directories
Load COCO style annotations from the input dataset
"Get all categories, their names, and create updated ID for the json file"
Prepare the coco-style json files
Split the dataset by locations
Load detections
Write out COCO-style json files to the output directory
Write detections to file with pickle
## Preparations: get all the output tensors
For all images listed in the annotations file
Skip the image if it is annotated with more than one category
"Get ""old"" and ""new"" category ID and category name for this image."
Skip if in excluded categories.
get path to image
"If we already have detection results, we can use them"
Otherwise run detector and add detections to the collection
Only select detections with confidence larger than DETECTION_THRESHOLD
Skip image if no detection selected
whether it belongs to a training or testing location
Skip images that we do not have available right now
- this is useful for processing parts of large datasets
Load image
Run inference
"remove batch dimension, and convert from float32 to appropriate type"
convert normalized bbox coordinates to pixel coordinates
Pad the detected animal to a square box and additionally by
"PADDING_FACTOR. The result will be in crop_boxes. However, we need to make"
sure that its box coordinates are still within the image.
"for each bounding box, crop the image to the padded box and save it"
"Create the file path as it will appear in the annotation json,"
adding the box number if there are multiple boxes
"if the cropped file already exists, verify its size"
Add annotations to the appropriate json
js_keys = ['/'.join(im['file_name'].split('/')[1:])[:-4] for im in js['images']]
for tk in js_keys:
"assert np.isclose(1, np.sum(detections[tk]['detection_scores'] > 0.5))"
""
separate_detections_by_size
""
Not-super-well-maintained script to break a list of API output files up
based on bounding box size.
""
%% Imports and constants
Folder with one or more .json files in it that we want to split up
Enumerate .json files
Define size thresholds and confidence thresholds
"Not used directly in this script, but useful if we want to generate previews"
%% Split by size
For each size threshold...
For each file...
fn = input_files[0]
Just double-checking; we already filtered this out above
Don't reprocess .json files we generated with this script
Load the input file
For each image...
1.1 is the same as infinity here; no box can be bigger than a whole image
What's the smallest detection above threshold?
"[x_min, y_min, width_of_box, height_of_box]"
""
size = w * h
...for each detection
Which list do we put this image on?
...for each image in this file
Make sure the number of images adds up
Write out all files
...for each size threshold
...for each file
""
tile_images.py
""
Split a folder of images into tiles.  Preserves relative folder structure in a
"new output folder, with a/b/c/d.jpg becoming, e.g.:"
""
a/b/c/d_row_0_col_0.jpg
a/b/c/d_row_0_col_1.jpg
""
%% Imports and constants
from ai4eutils
%% Main function
TODO: parallelization
""
i_fn = 2; relative_fn = image_relative_paths[i_fn]
Can we skip this image because we've already generated all the tiles?
TODO: super-sloppy that I'm pasting this code from below
From:
""
https://github.com/whiplashoo/split-image/blob/main/src/split_image/split.py
i_col = 0; i_row = 1
left/top/right/bottom
...for each row
...for each column
...for each image
%% Interactive driver
%%
%%
""
rde_debug.py
""
Some useful cells for comparing the outputs of the repeat detection
"elimination process, specifically to make sure that after optimizations,"
results are the same up to ordering.
""
%% Compare two RDE files
i_dir = 0
break
"Regardless of ordering within a directory, we should have the same"
number of unique detections
Re-sort
Make sure that we have the same number of instances for each detection
Make sure the box values match
""
aggregate_video.py
""
Aggregate results and render output video for a video that's already been run through MD
""
%% Constants
%% Processing
im = d['images'][0]
...for each detection
This is no longer included in output files by default
# Split into frames
# Render output video
## Render detections to images
## Combine into a video
""
ocr_sandbox.py
""
sandbox for experimenting with using OCR to pull metadata from camera trap images
""
The general approach is:
""
"* Crop a fixed percentage from the top and bottom of an image, slightly larger"
than the largest examples we've seen of how much space is used for metadata.
""
"* Refine that crop by blurring a little, then looking for huge peaks in the"
"color histogram suggesting a solid background, then finding rows that are"
mostly that color.
""
"* Crop to the refined crop, then run pytesseract to extract text"
""
"* Use regular expressions to find time and date, in the future can add, e.g.,"
"temperature (which is often present *only* in the images, unlike time/date which"
are also usually in EXIF but often wrong or lost in processing)
""
"The metadata extraction (EXIF, IPTC) here is just sample code that seemed to"
belong in this file.
""
Contact: Dan Morris (cameratraps@lila.science)
""
%% Constants and imports
pip install pytesseract
""
"Also intall tesseract from: https://github.com/UB-Mannheim/tesseract/wiki, and add"
"the installation dir to your path (on Windows, typically C:\Program Files (x86)\Tesseract-OCR)"
pip install IPTCInfo3
from the ai4eutils repo: https://github.com/Microsoft/ai4eutils
""
"Only used for writing out a summary, not important for the core metadata extraction"
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Using a semi-arbitrary metric of how much it feels like we found the
"text-containing region, discard regions that appear to be extraction failures"
Pad each crop with a few pixels to make tesseract happy
Discard text from the top
"When we're looking for pixels that match the background color, allow some"
tolerance around the dominant color
We need to see a consistent color in at least this fraction of pixels in our rough
crop to believe that we actually found a candidate metadata region.
"What fraction of the [top,bottom] of the image should we use for our rough crop?"
A row is considered a probable metadata row if it contains at least this fraction
"of the background color.  This is used only to find the top and bottom of the crop area,"
"so it's not that *every* row needs to hit this criteria, only the rows that are generally"
above and below the text.
%% Support functions
"%% Load some images, pull EXIF and IPTC data for fun"
%% Rough crop
"This will be an nImages x 1 list of 2 x 1 lists (image top, image bottom)"
image = images[0]
"l,t,r,b"
""
"0,0 is upper-left"
"%% Close-crop around the text, return a revised image and success metric"
Did we find a sensible mode that looks like a background value?
"This looks very scientific, right?  Definitely a probability?"
"print('Failed min background fraction test: {} of {}'.format(pBackGroundValue,minBackgroundFraction))"
"Notes to self, things I tried that didn't really go anywhere..."
"analysisImage = cv2.blur(analysisImage, (3,3))"
"analysisImage = cv2.medianBlur(analysisImage,5)"
"analysisImage = cv2.Canny(analysisImage,100,100)"
imagePil = Image.fromarray(analysisImage); imagePil
Use row heuristics to refine the crop
""
This egregious block of code makes me miss my fluency in Matlab.
"print('Cropping to {},{},{},{}'.format(x,y,w,h))"
Crop the image
"For some reason, tesseract doesn't like characters really close to the edge"
imagePil = Image.fromarray(croppedImage); imagePil
%% Go to OCR-town
"An nImages x 2 list of strings, extracted from the top and bottom of each image"
An nImages x 2 list of cropped images
iImage = 0; iRegion = 1; regionSet = imageRegions[iImage]; region = regionSet[iRegion]
text = pytesseract.image_to_string(region)
pil --> cv2
"image = cv2.medianBlur(image, 3)"
"image = cv2.erode(image, None, iterations=2)"
"image = cv2.dilate(image, None, iterations=4)"
"image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]"
"image = cv2.blur(image, (3,3))"
"image = cv2.copyMakeBorder(image,10,10,10,10,cv2.BORDER_CONSTANT,value=[0,0,0])"
"text = pytesseract.image_to_string(imagePil, lang='eng')"
https://github.com/tesseract-ocr/tesseract/wiki/Command-Line-Usage
"psm 6: ""assume a single uniform block of text"""
""
...for each cropped region
...for each image
%% Extract dates and times
s = '1:22 pm'
s = '1:23:44 pm'
%% Write results to a handy html file
Add image name and resized image
Add results and individual region images
"textStyle = ""font-family:calibri,verdana,arial;font-weight:bold;font-size:150%;text-align:left;margin-left:50px;"""
%% Scrap
Alternative approaches to finding the text/background  region
Using findCountours()
imagePil = Image.fromarray(analysisImage); imagePil
"analysisImage = cv2.erode(analysisImage, None, iterations=3)"
"analysisImage = cv2.dilate(analysisImage, None, iterations=3)"
"analysisImage = cv2.threshold(analysisImage, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]"
Find object with the biggest bounding box
Using connectedComponents()
analysisImage = image
print('Found {} components'.format(nb_components))
We just want the *background* image
""
"Takes a .json file with MD results for an individual video, and converts to a .csv that includes"
frame times.  This is very bespoke to animal detection and does not include other classes.
""
%% Imports and constants
Only necessary if you want to extract the sample rate from the video
%% Extract the sample rate if necessary
%% Load results
%% Convert to .csv
i_image = 0; im = results['images'][i_image]
""
umn-pr-analysis.py
""
Precision/recall analysis for UMN data
""
%% Imports and constants
results_file = results_file_filtered
"For two deployments, we're only processing imagse in the ""detections"" subfolder"
String to remove from MegaDetector results
%% Enumerate deployment folders
%% Load MD results
im = md_results['images'][0]
%% Load ground truth
i_row = 0; row = ground_truth_df.iloc[i_row]
%% Create relative paths for ground truth data
"Some deployment folders have no subfolders, e.g. this is a valid file name:"
""
M00/01010132.JPG
""
"But some deployment folders have subfolders, e.g. this is also a valid file name:"
""
N17/100EK113/07160020.JPG
""
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly"
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths."
deployment_name = list(deployment_folders)[0]
Enumerate all files in this folder
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))"
filename = files[100]
...for each file in this deployment
...for each deployment
%% Map relative paths to MD results
%% Add relative paths to our ground truth table
i_row = 0; row = ground_truth_df.iloc[i_row]
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's"
"just the deployment ID and the image name, separated by a slash."
Find the relative path for this image
Make sure we have MegaDetector results for this file
Make sure this image file exists
...for each row in the ground truth table
%% Take everything out of Pandas
%% Some additional error-checking of the ground truth
An early version of the data required consistency between common_name and is_blank
%% Combine MD and ground truth results
d = ground_truth_dicts[0]
Find the maximum confidence for each category
...for each detection
...for each image
%% Precision/recall analysis
...for each image
Thresholds go up throughout precisions/recalls/thresholds; find the last
value where recall is at or above target.  That's our precision @ target recall.
Flatten the confusion matrix
Write precision/recall plot to .png file in output directory
pr_figure_relative_filename = 'prec_recall.png'
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)"
plt.show(block=False)
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')"
%% Find and manually review all images of humans
%%
"...if this image is annotated as ""human"""
...for each image
%% Find and manually review all MegaDetector animal misses
%%
im = merged_images[0]
GT says this is not an animal
GT says this is an animal
%% Convert .json to .csv
%%
""
kga-pr-analysis.py
""
Precision/recall analysis for KGA data
""
%% Imports and constants
%% Load and filter MD results
%% Load and filter ground truth
%% Map images to image-level results
%% Map sequence IDs to images and annotations to images
Verify consistency of annotation within a sequence
TODO
%% Find max confidence values for each category for each sequence
seq_id = list(sequence_id_to_images.keys())[1000]
im = images_this_sequence[0]
det = md_result['detections'][]
...for each detection
...for each image in this sequence
...for each sequence
%% Prepare for precision/recall analysis
seq_id = list(sequence_id_to_images.keys())[1000]
cat_id = list(category_ids_this_sequence)[0]
...for each category in this sequence
...for each sequence
%% Precision/recall analysis
"Confirm that thresholds are increasing, recall is decreasing"
This is not necessarily true
assert np.all(precisions[:-1] <= precisions[1:])
Thresholds go up throughout precisions/recalls/thresholds; find the max
value where recall is at or above target.  That's our precision @ target recall.
"This is very slightly optimistic in its handling of non-monotonic recall curves,"
but is an easy scheme to deal with.
Flatten the confusion matrix
Write precision/recall plot to .png file in output directory
pr_figure_relative_filename = 'prec_recall.png'
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)"
plt.show(block=False)
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')"
%% Scrap
%% Find and manually review all sequence-level MegaDetector animal misses
input_base = '/media/user/lila-01/lila/snapshot-safari/KGA/KGA_public'
i_sequence = 0; seq_id = pr_sequence_ids[i_sequence]
i_seq = 0; seq_id = false_negative_sequences[i_seq]
"sequence_folder = os.path.join(sequence_preview_dir,'seq_{}'.format(str(i_seq).zfill(3)))"
fn = image_files[0]
"print('Copying {} to {}'.format(input_path,output_path))"
...for each file in this sequence.
...for each sequence
%% Image-level postprocessing
parse arguments
check if a GPU is available
load a pretrained embedding model
setup experiment
load the embedding model
setup the target dataset
setup finetuning criterion
setup an active learning environment
create a classifier
the main active learning loop
Active Learning
finetune the embedding model and load new embedding values
gather labeled pool and train the classifier
save a snapshot
Load a checkpoint if necessary
setup the training dataset and the validation dataset
setup data loaders
check if a GPU is available
create a model
setup loss criterion
define optimizer
load a checkpoint if provided
setup a deep learning engine and start running
train the model
train for one epoch
evaluate on validation set
save a checkpoint
open the file
read it
do the substitution
matplotlib.use('Agg')
from UIComponents.DBObjects import *
Initialize Database
# database connection credentials
# try to connect as USER to database DB_NAME through peewee
Load the saved embedding model
dataset_query = Detection.select().limit(5)
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})"
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getalllabels()) , dataset.getallpaths(), {})"
Random examples to start
"random_ids = np.random.choice(dataset.current_set, 1000, replace=False).tolist()"
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)"
print(random_ids)
Move Records
"moveRecords(dataset, DetectionKind.ModelDetection.value, DetectionKind.UserDetection.value, random_ids)"
#print([len(x) for x in dataset.set_indices])
# Finetune the embedding model
#dataset.set_kind(DetectionKind.UserDetection.value)
#dataset.train()
"#train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)"
"#finetune_embedding(model, checkpoint['loss_type'], dataset, 32, 4, 100)"
#save_checkpoint({
"#        'arch': model.arch,"
"#        'state_dict': model.state_dict(),"
"#        'optimizer' : optimizer.state_dict(),"
"#        'loss_type' : loss_type,"
"#        }, False, ""%s%s_%s_%04d.tar""%('finetuned', loss_type, model.arch, len(dataset.set_indices[DetectionKind.UserDetection.value])))"
Get indices of samples to get user to label
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
"kwargs[""already_selected""].extend(indices)"
Train on samples that have been labeled so far
Test on the samples that have not been labeled
"'optimizer' : optimizer.state_dict(),"
num_classes= len(train_dataset.getClassesInfo()[0])
"print(""Num Classes= ""+str(num_classes))"
define loss function (criterion) and optimizer
"optimizer = torch.optim.SGD(params, momentum = 0.9, lr = args.lr, weight_decay = args.weight_decay)"
optimizer.load_state_dict(checkpoint['optimizer'])
train for one epoch
"adjust_lr(optimizer,epoch)"
if epoch % 1 == 0 and epoch > 0:
"a, b, c = e.predict(train_embd_loader, load_info = True, dim = args.feat_dim)"
"plot_embedding(reduce_dimensionality(a), b, c, {})"
evaluate on validation set
--------some stuff needed to get AJAX to work with bottle?--------#
-------------------------------------------------------------------------------- #
PREPARE TO QUEUE IMAGES FOR LABELING
-------------------------------------------------------------------------------- #
# Connect as USER to database DB_NAME through peewee and initialize database proxy
# Load embedding model
---------------------------------------------------------------------- #
CREATE QUEUE OF IMAGES TO LABEL
---------------------------------------------------------------------- #
Use classifier to generate predictions
# Update model predicted class in PostgreSQL database
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
-------------------------------------------------------------------------------- #
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI
-------------------------------------------------------------------------------- #
"# static routes (to serve CSS, etc.)"
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps"
"return bottle.static_file(filename, root='../../../../../../../../../../../.')"
# dynamic routes
if data['display_grayscale']:
indices_to_exclude.update(set(color_indices))
elif not data['display_grayscale']:
indices_to_exclude.update(set(grayscale_indices))
data['display_images'] = {}
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices]
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices]
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices]
data['display_images']['detection_categories'] = []
for i in indices:
if str(dataset.samples[i][1]) == 'None':
data['display_images']['detection_categories'].append('None')
else:
existing_category_entries = {cat.id: cat.name for cat in Category.select()}
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()"
data['display_images']['detection_categories'].append(cat_name)
Use image ids in images_to_label to get the corresponding dataset indices
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
print(set(dataset.set_indices[4]).update(set(indices_to_label)))
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Use image id images_to_label to get the corresponding dataset index
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Train on samples that have been labeled so far
dataset.set_kind(DetectionKind.UserDetection.value)
print(y_train)
Predict on the samples that have not been labeled
print(y_pred)
Update model predicted class in PostgreSQL database
timer = time.time()
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
print('Updating the database took %0.2f seconds'%(time.time() - timer))
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
"once the classifier has been trained the first time, switch to AL sampling"
# sourceMappingURL=bootstrap.bundle.min.js.map
"print(ap_distances.size(),an_distances.size())"
losses = -(((-ap_distances)/128)+1+1e-16).log() - (((-(128-an_distances))/128)+1+1e-16).log()
import pdb
pdb.set_trace()
losses = ap_distances - an_distances + self.margin
print(losses.size())
from UIComponents.DBObjects import *
TODO: should this also change self.kind?
get the embedding representations for all samples (i.e. set current_set to all indices)
"return PILImage.open(os.path.join(self.img_base,path)).convert('RGB')"
"print(self.labels_set, self.n_classes)"
from sklearn.manifold import TSNE
embedding= TSNE(n_components=2).fit_transform(X)
embedding= PCA(n_components=2).fit_transform(X)
return X
"print(dir(event), type(sc))"
"print(label,bgcolor)"
"patches.append(mpatches.Patch(color=indexcolors[i], label=label))"
"plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,"
"ncol=12, mode=""expand"", borderaxespad=0., handles=patches)"
plt.legend(handles=patches)
"plt.xlabel('Dim 1', fontsize=12)"
"plt.ylabel('Dim 2', fontsize=12)"
plt.grid(True)
"plt.xlabel('Dim 1', fontsize=12)"
"plt.ylabel('Dim 2', fontsize=12)"
plt.grid(True)
print(thumb)
"img.thumbnail((16, 12), PILImage.ANTIALIAS)"
plt.show()
Add all negatives for all positive pairs
print(triplets.shape[0])
compute output
measure accuracy
compute loss on this batch
"train on a batch, record loss, and measure accuracy (if calc_accuracy)"
compute output
measure accuracy and record loss
switch to evaluate mode
compute output
switch to evaluate mode
"self.fc13 = nn.Linear(128, 64)"
self.bn2 = nn.BatchNorm1d(64)
x = F.relu(self.fc12(x))
x = F.relu(self.bn1(self.fc13(x)))
x = F.relu(self.fc13(x))
"x = F.dropout(x, training=self.training)"
save features last FC layer
x = F.relu(x)
save features last FC layer
import matplotlib.pyplot as plt
plt.switch_backend('agg')
class EmbeddingNet(nn.Module):
"def __init__(self, architecture, feat_dim, use_pretrained=False):"
"super(EmbeddingNet, self).__init__()"
self.feat_dim= feat_dim
self.inner_model = models.__dict__[architecture](pretrained=use_pretrained)
if architecture.startswith('resnet'):
in_feats= self.inner_model.fc.in_features
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)"
elif architecture.startswith('inception'):
in_feats= self.inner_model.fc.in_features
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)"
if architecture.startswith('densenet'):
in_feats= self.inner_model.classifier.in_features
"self.inner_model.classifier = nn.Linear(in_feats, feat_dim)"
if architecture.startswith('vgg'):
in_feats= self.inner_model.classifier._modules['6'].in_features
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)"
if architecture.startswith('alexnet'):
in_feats= self.inner_model.classifier._modules['6'].in_features
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)"
"def forward(self, x):"
return self.inner_model.forward(x)
class NormalizedEmbeddingNet(EmbeddingNet):
"def __init__(self, architecture, feat_dim, use_pretrained=False):"
"EmbeddingNet.__init__(self, architecture, feat_dim, use_pretrained = use_pretrained)"
"def forward(self, x):"
embedding =  F.normalize(self.inner_model.forward(x))*10.0
"return embedding, embedding"
"def get_random_images(num, image_dir, test_transforms):"
"data = datasets.ImageFolder(image_dir, transform=test_transforms) # slight abuse; this expects subfolders corresponding to classes but we have no classes here"
indices = list(range(len(data)))
np.random.shuffle(indices)
idx = indices[:num]
from torch.utils.data.sampler import SubsetRandomSampler
sampler = SubsetRandomSampler(idx)
"loader = torch.utils.data.DataLoader(data,"
"sampler=sampler, batch_size=num)"
dataiter = iter(loader)
"images, labels = dataiter.next()"
"return images, labels"
"def predict_image(image, model, test_transforms):"
"device = torch.device(""cuda"" if torch.cuda.is_available()"
"else ""cpu"")"
image_tensor = test_transforms(image).float()
image_tensor = image_tensor.unsqueeze_(0)
input = Variable(image_tensor)
input = input.to(device)
output = model(input)[0]
return output.data.cpu().numpy()
Connect to database and initialize db_proxy
# database connection credentials
# load the dataset
Load the saved embedding model from the checkpoint
# update the dataset embedding
# Create a folder for saving embedding visualizations with this model checkpoint
model_emb_dirname = os.path.basename(args.base_model).split('.')[0]
"os.makedirs(model_emb_dirname, exist_ok=True)"
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')"
dataset.embedding_mode()
"assert 2==3, 'break'"
datasetindices = list(range(len(dataset)))
np.random.shuffle(datasetindices)
random_indices = datasetindices[:args.num]
print(random_indices)
"selected_sample_features = np.array([]).reshape(0, 256)"
selected_sample_labels = []
for idx in random_indices:
"selected_sample_features = np.vstack([selected_sample_features, X_train[idx]])"
selected_sample_labels.append(y_train[idx])
img_path = imagepaths[idx].split('.JPG')[0]
image = dataset.loader(img_path)
selected_sample_images.append(image)
# TRY NEAREST NEIGHBORS WALK THROUGH EMBEDDING
nbrs = NearestNeighbors(n_neighbors=args.num).fit(selected_sample_features)
"distances, indices = nbrs.kneighbors(selected_sample_features)"
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')"
"idx_w_closest_nbr = np.where(distances[:,1] == min(distances[:,1]))[0][0]"
order = [idx_w_closest_nbr]
for ii in range(len(distances)):
"distances[ii, 0] = np.inf"
while len(order)<args.num:
curr_idx = order[-1]
curr_neighbors = indices[curr_idx]
curr_dists = list(distances[curr_idx])
# print(min(curr_dists))
next_closest_pos = curr_dists.index(min(curr_dists))
next_closest = curr_neighbors[next_closest_pos]
order.append(next_closest)
# make sure you can't revisit past nodes
for vi in order:
vi_pos = list(indices[next_closest]).index(vi)
"distances[next_closest, vi_pos] = np.inf"
for ii in range(len(order)):
imgidx = order[ii]
image = selected_sample_images[imgidx]
"image.save(model_emb_dirname+""/img""+str(ii)+""_""+str(selected_sample_labels[imgidx])+"".png"")"
# Specify the transformations on the input images before inference
"# test_transforms = transforms.Compose([transforms.Resize([224, 224]), transforms.ToTensor()])"
"test_transforms = transforms.Compose([transforms.Resize([256, 256]), transforms.RandomCrop([224, 224]), transforms.RandomHorizontalFlip(), transforms.ColorJitter(), transforms.ToTensor(), transforms.Normalize([0.407328, 0.407328, 0.407328], [0.118641, 0.118641, 0.118641])])"
"images, labels = get_random_images(num, image_dir, test_transforms)"
"all_features = np.array([]).reshape(0, 256)"
for ii in range(len(images)):
image = to_pil(images[ii])
"features = predict_image(image, model, test_transforms)"
"all_features = np.vstack([all_features, features])"
# for ii in range(len(images)):
#     image = to_pil(images[ii])
"#     image.save(""img""+str(ii)+"".png"")"
# TRY CLUSTERING
kmeans1 = KMeans(n_clusters=5).fit(StandardScaler().fit_transform(all_features))
print(kmeans1.labels_)
for ii in range(len(images)):
image = to_pil(images[ii])
"filename = str(kmeans1.labels_[ii])+""/img""+str(ii)+"".png"""
if not os.path.exists(os.path.dirname(filename)):
os.makedirs(os.path.dirname(filename))
image.save(filename)
class SaveFeatures():
"def __init__(self, module):"
self.hook = module.register_forward_hook(self.hook_fn)
"def hook_fn(self, module, input, output):"
"self.features = torch.tensor(output, requires_grad=True).cuda()"
def close(self):
self.hook.remove()
Load the saved embedding model from the checkpoint
"Get a sample from the database, with eval transforms applied, etc."
Connect to database and sample a dataset
output = model.forward(sample_image.unsqueeze(0))
print(output)
with torch.no_grad():
sample_image_input = sample_image.cuda(non_blocking=True)
"_, output = model(sample_image_input) # compute output"
print(output)
sample_image = PILImage.open(sample_image_path).convert('RGB')
"sample_image = transforms.Compose([Resize([256, 256]), CenterCrop(([[224,224]])), ToTensor(), Normalize([0.369875, 0.388726, 0.347536], [0.136821, 0.143952, 0.145229])])(sample_image)"
print(list(model_inner_resnet.children()))
print(model_inner_resnet.fc)
print(model_inner_resnet.fc0)
# print(model_inner_resnet.layer4[0].conv2)
# print(type(model))
# print(len(list(model_inner_resnet.children())))
# print(list(model.children()))
# print(list(list(model.children())[0].children()))
"img = np.uint8(np.random.uniform(150, 180, (56, 56, 3)))/255"
"img_tensor = torch.unsqueeze(torch.from_numpy(img), 0)"
full_out = model_inner_resnet.forward(img_tensor)
print(full_out)
model(img_tensor)
activations = SaveFeatures(model_inner_resnet.layer4[0].conv2)
print(activations.features)
print(type(activations.features))
activations.close()
Connect to database and sample a dataset
Load the saved embedding model from the checkpoint
Update the dataset embedding
Get a random query image
# # # IMAGES IN THE SAME SEQUENCE # # # #
"assert 2==3, 'break'"
# # # CLOSEST IN (EMBEDDING) FEATURE SPACE # # # #
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Variables for the hierarchical cluster
Variables for the AL algorithm
Data variables
"connectivity = kneighbors_graph(self.transformed_X,max_features)"
Fit cluster and update cluster variables
The sklearn hierarchical clustering algo numbers leaves which correspond
to actual datapoints 0 to n_points - 1 and all internal nodes have
ids greater than n_points - 1 with the root having the highest node id
"If no labels have been observed, simply return uniform distribution"
"If no observations, return worst possible upper lower bounds"
Loop through generations from bottom to top
Update admissible labels for node
Calculate score
Determine if node should be split
Make sure label set for node so that we can flow to children
if necessary
Only split if all ancestors are admissible nodes
This is part  of definition of admissible pruning
Check that pruning covers all leave nodes
Fill in labels
Observe labels for previously recommended batches
TODO(lishal): implement multiple selection methods
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
A list of initialized samplers is allowed as an input because
"for AL_methods that search over different mixtures, may want mixtures to"
have shared AL_methods so that initialization is only performed once for
computation intensive methods like HierarchicalClusteringAL and
states are shared between mixtures.
"If initialized samplers are not provided, initialize them ourselves."
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copy these objects to make sure they are not modified while simulating
trajectories as they are used later by the main run_experiment script.
Assumes that model has already by fit using all labeled data so
the probabilities can be used immediately to hallucinate labels
All models need to have predict_proba method
Hallucinate labels for selected datapoints to be label
using class probabilities from model
"Not saving already_selected here, if saving then should sort"
only for the input to fit but preserve ordering of indices in
already_selected
Useful to know how accuracy compares for model trained on hallucinated
labels vs trained on true labels.  But can remove this train to speed
up simulations.  Won't speed up significantly since many more models
are being trained inside the loop above.
Save trajectory for reference
Delete created copies
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER
Should check model but kernel_svm does not have coef_ so need better
handling here
Make sure that model object fed in did not change during simulations
Return indices based on return type specified
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Set gamma for gaussian kernel to be equal to 1/n_features
kneighbors graph is constructed using k=10
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of"
"another point, make it vice versa"
Graph edges are weighted by applying gaussian kernel to manhattan dist.
"By default, gamma for rbf kernel is equal to 1/n_features but may"
get better results if gamma is tuned.
Define graph density for an observation to be sum of weights for all
edges to the node representing the datapoint.  Normalize sum weights
by total number of neighbors.
"If a neighbor has already been sampled, reduce the graph density"
for its direct neighbors to promote diversity.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update observed reward and arm probabilities
Sample an arm
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
y only used for determining how many clusters there should be
probably not practical to assume we know # of classes before hand
should also probably scale with dimensionality of data
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
TODO(lishal): have MarginSampler and this share margin function
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update min_distances for all examples given new cluster center.
Assumes that the transform function takes in original data and not
flattened data.
Initialize centers with a randomly selected datapoint
New examples should not be in already selected since those points
should have min_distance of zero to a cluster center.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
This is uniform given the remaining pool but biased wrt the entire pool.
sample = [i for i in range(self.X.shape[0]) if i not in already_selected]
return sample[0:N]
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Fields for hierarchical clustering AL
Setting parent and storing nodes in dict for fast access
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Connect to database DB_NAME as USER and initialize tables
Populate Info table
Populate Category table
"# For now, we have a predefined list of species we expect to see in the camera trap database (e.g. maybe from a quick look through the images)"
# TODO: allow user to update the class list through the labeling tool UI as they see different species
Populate Image and Detection tables
Image entry data
Detection entry data
Connect to database DB_NAME as USER and initialize tables
Populate Info table
Populate Category table
# Create category records from species present in the COCO camera trap classification dataset (COCO classes minus any excluded classes)
# TODO: allow user to update the class list through the labeling tool UI as they see different species
Populate Image and Detection tables
"with open(os.path.join(args.crop_dir,'crops.json'), 'r') as infile:"
crops_json = json.load(infile)
counter = 0
timer = time.time()
num_detections = len(crops_json)
for detectionid in crops_json:
counter += 1
detection_data = crops_json[detectionid]
# Image entry data
existing_image_entries = Image.select().where((Image.file_name == detection_data['file_name']))
try:
existing_image_entry = existing_image_entries.get()
except:
"image_entry = Image.create(id=detectionid, file_name=detection_data['file_name'], width=detection_data['width'], height=detection_data['height'], grayscale=detection_data['grayscale'],"
"source_file_name=detection_data['source_file_name'], relative_size=detection_data['relative_size'],"
"seq_id=detection_data['seq_id'], seq_num_frames=detection_data['seq_num_frames'], frame_num=detection_data['frame_num'])"
image_entry.save()
# Detection entry data
"detection_entry = Detection.create(id=detectionid, image=detectionid, bbox_confidence=detection_data['bbox_confidence'],"
"bbox_X1=detection_data['bbox_X1'], bbox_Y1=detection_data['bbox_Y1'], bbox_X2=detection_data['bbox_X2'], bbox_Y2=detection_data['bbox_Y2'],"
kind=DetectionKind.ModelDetection.value)
detection_entry.save()
if counter%100 == 0:
"print('Updated database with Image and Detection table entries for %d out of %d crops in %0.2f seconds'%(counter, num_detections, time.time() - timer))"
# data related to original image
Get class names from .txt list
Initialize Oracle table
Map filenames to classes (NOTE: we assume a single image does not contain more than one class)
"For each detection, use source image path to get class"
TODO update: Assumes that crops have already
been generated for the images using make_active_learning_classification_dataset.py. The created DB contains tables:
- info: information about the dataset
- image: images present in the dataset
- detections: crops of images with detections with confidence greater than a specified threshold
Initialize Database
# database connection credentials
HOST = 'localhost'
PORT = 5432
"# first, make sure the (user, password) has been created"
"# sudo -u postgres psql -c ""CREATE USER <db_user> WITH PASSWORD <db_password>;"""
"# sudo -u postgres psql -c ""CREATE DATABASE <db_name> WITH OWNER <db_user> CONNECTION LIMIT -1;"""
"# sudo -u postgres psql -c ""GRANT CONNECT ON DATABASE <db_name> TO <db_user>;"""
"# sudo -u postgres psql -d <db_name> -c ""CREATE EXTENSION IF NOT EXISTS \""uuid-ossp\"";"""
# Try to connect as USER to database DB_NAME through peewee
Populate Tables
# create Info table
# get class names for Category table
Faster anD available in Python 3.5 and above
# iterate through images in each class folder
killing this process after over 38 hours adding over 500k white-tailed deer crops from emammal
resuming for remaining classes
# get cropped image data for Image table
"if mean of each channel is about the same, image is likely grayscale"
# still have no info on these:
seq_id = CharField(null= True)                # sequence identifier for the original image
seq_num_frames = IntegerField(null = True)    # number of frames in sequence
frame_num = IntegerField(null = True)         # which frame number in sequence
location = CharField(null = True)             # location of camera trap
datetime = DateTimeField(null = True)
# store info about the detection corresponding to this image
# store info about the true labels for the detection
#  - for pretrain dataset this is the same as the detection_category if the detection categories
print(classes)
Connect to database and sample a dataset
Load the saved embedding model from the checkpoint
Update the dataset embedding
save the images
save the features
"with open(os.path.join(args.output_dir, 'lastlayer_features.mat'), 'wb') as f:"
"pickle.dump(sample_features, f)"
"with open(os.path.join(args.output_dir, 'labels.mat'), 'wb') as f:"
"pickle.dump(sample_labels, f)"
"parser.add_argument('--db_name', default='missouricameratraps', type=str, help='Name of the training (target) data Postgres DB.')"
"parser.add_argument('--db_user', default='user', type=str, help='Name of the user accessing the Postgres DB.')"
"parser.add_argument('--db_password', default='password', type=str, help='Password of the user accessing the Postgres DB.')"
"parser.add_argument('--base_model', type=str, help='Path to latest embedding model checkpoint.')"
"parser.add_argument('--output_dir', type=str, help='Output directory for subset of crops')"
Add json entry for this crop
Copy file for this crop to subset dataset crop dir
Copy file for its full-size source image to subset dataset image dir
Write crops.json to subset dataset crop dir
store info about the crops produced in a JSON file
------------------------------------------------------------------------------------------------------------#
COMMENT OUT IF NOT USING A SPECIFIC PROJECT WITHIN ROBERT LONG EMAMMAL DATASET
------------------------------------------------------------------------------------------------------------#
get some information about the source image
------------------------------------------------------------------------------------------------------------#
NOTE: EDIT THIS SECTION BASED ON DATASET SOURCE
get info about sequence the source image belongs to from path and directory
# missouricameratraps:
imgframenum = int(os.path.basename(imgfile).split('.JPG')[0].split('_')[-1])
imgseqid = int(os.path.split(os.path.dirname(imgfile))[-1])
"imgseqnumframes = len([name for name in os.listdir(os.path.dirname(imgfile)) if os.path.isfile(os.path.join(os.path.dirname(imgfile), name))])"
# emammal:
------------------------------------------------------------------------------------------------------------#
utility function
compute output
measure accuracy and record loss
switch to train mode
measure accuracy and record loss
compute output
measure accuracy and record loss
switch to evaluate mode
compute output
switch to evaluate mode
compute output
switch to evaluate mode
"print(self.labels_set, self.n_classes)"
Add all negatives for all positive pairs
print(triplets.shape[0])
constructor
update embedding values after a finetuning
select either the default or active pools
gather test set
gather train set
finetune the embedding model over the labeled pool
a utility function for saving the snapshot
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Variables for the hierarchical cluster
Variables for the AL algorithm
Data variables
"connectivity = kneighbors_graph(self.transformed_X,max_features)"
Fit cluster and update cluster variables
The sklearn hierarchical clustering algo numbers leaves which correspond
to actual datapoints 0 to n_points - 1 and all internal nodes have
ids greater than n_points - 1 with the root having the highest node id
"If no labels have been observed, simply return uniform distribution"
"If no observations, return worst possible upper lower bounds"
Loop through generations from bottom to top
Update admissible labels for node
Calculate score
Determine if node should be split
Make sure label set for node so that we can flow to children
if necessary
Only split if all ancestors are admissible nodes
This is part  of definition of admissible pruning
Check that pruning covers all leave nodes
Fill in labels
Observe labels for previously recommended batches
TODO(lishal): implement multiple selection methods
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
A list of initialized samplers is allowed as an input because
"for AL_methods that search over different mixtures, may want mixtures to"
have shared AL_methods so that initialization is only performed once for
computation intensive methods like HierarchicalClusteringAL and
states are shared between mixtures.
"If initialized samplers are not provided, initialize them ourselves."
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copy these objects to make sure they are not modified while simulating
trajectories as they are used later by the main run_experiment script.
Assumes that model has already by fit using all labeled data so
the probabilities can be used immediately to hallucinate labels
All models need to have predict_proba method
Hallucinate labels for selected datapoints to be label
using class probabilities from model
"Not saving already_selected here, if saving then should sort"
only for the input to fit but preserve ordering of indices in
already_selected
Useful to know how accuracy compares for model trained on hallucinated
labels vs trained on true labels.  But can remove this train to speed
up simulations.  Won't speed up significantly since many more models
are being trained inside the loop above.
Save trajectory for reference
Delete created copies
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER
Should check model but kernel_svm does not have coef_ so need better
handling here
Make sure that model object fed in did not change during simulations
Return indices based on return type specified
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Set gamma for gaussian kernel to be equal to 1/n_features
kneighbors graph is constructed using k=10
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of"
"another point, make it vice versa"
Graph edges are weighted by applying gaussian kernel to manhattan dist.
"By default, gamma for rbf kernel is equal to 1/n_features but may"
get better results if gamma is tuned.
Define graph density for an observation to be sum of weights for all
edges to the node representing the datapoint.  Normalize sum weights
by total number of neighbors.
"If a neighbor has already been sampled, reduce the graph density"
for its direct neighbors to promote diversity.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update observed reward and arm probabilities
Sample an arm
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
y only used for determining how many clusters there should be
probably not practical to assume we know # of classes before hand
should also probably scale with dimensionality of data
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
TODO(lishal): have MarginSampler and this share margin function
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update min_distances for all examples given new cluster center.
Assumes that the transform function takes in original data and not
flattened data.
Initialize centers with a randomly selected datapoint
New examples should not be in already selected since those points
should have min_distance of zero to a cluster center.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
This is uniform given the remaining pool but biased wrt the entire pool.
sample = [i for i in range(self.X.shape[0]) if i not in already_selected]
return sample[0:N]
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Fields for hierarchical clustering AL
Setting parent and storing nodes in dict for fast access
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Faster and available in Python 3.5 and above
transform_list.append(CenterCrop((processed_size)))
transform_list.append(Lambda(lambda X: normalize(X)))
"print(self.labels_set, self.n_classes)"
"from PyQt5 import QtCore, QtWidgets,QtGui"
from collections import deque
from peewee import *
from UIComponents.Tag import Tag
policy.setHeightForWidth(True)
"print(self.tab1.parentWidget(),self)"
self.tab4.add.clicked.connect(self.addSpecies)
self.tab4.update.clicked.connect(self.updateSpecies)
checkpoint= load_checkpoint('../merge/triplet_model_0054.tar')
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))"
"embedding_net = EmbeddingNet(checkpoint['arch'], checkpoint['feat_dim'])"
if checkpoint['loss_type'].lower()=='center':
"model = torch.nn.DataParallel(ClassificationNet(embedding_net, n_classes=14)).cuda()"
else:
model= torch.nn.DataParallel(embedding_net).cuda()
model.load_state_dict(checkpoint['state_dict'])
self.parentWidget().progressBar.setMaximum(len(run_dataset)//2048)
"e=Engine(model,None,None, verbose=True,progressBar= self.parentWidget().progressBar)"
label = [ x[1] for x in run_dataset.samples]
"print(indices,selected_set)"
print(query.sql())
src.delete().where(src.image_id<<rList))
"det= UserDetection.create(category_id=0, id=str(index+label[1][2]),image_id=final[0], bbox_X=label[1][0], bbox_Y=label[1][1], bbox_W=label[1][2], bbox_H=label[1][3])"
for x in self.tab1.grid.tags:
x.delete_instance()
db.create_tables([Detection])
This is simply to show the bar
"p = Process(target=ex.active, args=())"
p.start()
p.join()
ex.active()
ex.centralWidget().setCurrentIndex(1)
main()
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))"
print(row)
"unq_id= ""crops_""+str(uuid.uuid1())"
"print(line,imageWidth,imageHeight)"
"print(""%s,%s,%s,%0.6f,%0.6f,%0.6f,%0.6f,%0.6f""%(line[0], line[1],line[2],float(line[3]),topRel,leftRel,bottomRel,rightRel))"
if not os.path.exists(dest):
os.mkdir(dest)
raise
out.close()
"print length,(i-1)*length,i*length"
matplotlib.use('Agg')
get the inputs
zero the parameter gradients
forward + backward + optimize
print statistics
get the inputs
forward + backward + optimize
print statistics
get the inputs
zero the parameter gradients
forward + backward + optimize
get the inputs
forward + backward + optimize
print statistics
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
"conf.reset()"""""""
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
conf.reset()
for clf in classifiers:
"clf.fit(X_train, y_train)"
preds= clf.predict_proba(X)
uncertainty+= preds.max(axis=1)
print(uncertainty[ind])
print(uncertainty[ind])
true_labels = y[unlabeled_indices]
select up to 5 digit examples that the classifier is most uncertain about
remember best acc@1 and save checkpoint
"completeClassificationLoop(run_dataset, model,num_classes)"
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)"
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd)
labels = db.labels_
"mapp=(find_probablemap(label,labels, K=args.K))"
"print(""Clusters"")"
"for i,x in enumerate(labels):"
labels[i]= mapp[x]
print(np.sum(labels == label)/labels.size)
"print(""Confidence Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning(embd, label, idx)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(""Entropy Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning_entropy(embd, label, idx)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(idx,idx.shape)"
for i in idx:
print(paths[i])
"print(""Silohette active learning"")"
"idx= active_learning2(embd, 1000, args.num_clusters)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
embd= reduce_dimensionality(embd)#[0:10000])
labels= labels[0:10000]
label= label[0:10000]
paths= paths[0:10000]
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])"
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])"
plt.show()
"np.save(args.name_prefix+""_embeddings.npy"",embd)"
"np.save(args.name_prefix+""_labels.npy"",label)"
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")"
del output
define loss function (criterion) and optimizer
"optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay= args.weight_decay)"
optimizer.load_state_dict(checkpoint['optimizer'])
train for one epoch
evaluate on validation set
matplotlib.use('Agg')
selected_set.add(rand_ind[i])
"embedding_net = EmbeddingNet('resnet50', 256, True)"
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)"
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)"
print('Embedding Done')
sys.stdout.flush()
Random examples to start
"random_ids = noveltySamples(unlabeled_dataset.em, unlabeled_dataset.getIDs(), 1000)"
print(random_ids)
Move Records
Finetune the embedding model
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)"
unlabeled_dataset.updateEmbedding(model)
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})"
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()"
matplotlib.use('Agg')
"uncertainty= np.apply_along_axis(stats.entropy,1,probs) * (1 - probs.max(axis=1))"
"copy(paths[srt[i]], ""active"")"
"plot_together( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()), preds, base_ind, dataset.getpaths(), {})"
"return np.random.choice(range(0,prob_list[0].shape[0]), 100, replace=False).tolist()"
selected_set.add(rand_ind[i])
"query= Detection.update(kind = destKind.value).where(Detection.id.in_(rList), Detection.kind == srcKind.value)"
query.execute()
"embedding_net = EmbeddingNet('resnet50', 256, True)"
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)"
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)"
print('Embedding Done')
sys.stdout.flush()
"plot_embedding(dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})"
Random examples to start
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)"
print(random_ids)
Move Records
Finetune the embedding model
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)"
unlabeled_dataset.updateEmbedding(model)
"print(dataset.em[dataset.current_set].shape, np.asarray(dataset.getlabels()).shape, len(dataset.getpaths()))"
"plot_embedding( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})"
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})"
train_eval_classifier()
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()"
"names = [""Linear SVM"", ""RBF SVM"", ""Random Forest"", ""Neural Net"", ""Naive Bayes""]"
"classifiers = [SVC(kernel=""linear"", C=0.025, probability= True, class_weight='balanced'),"
"SVC(gamma=2, C=1, probability= True, class_weight='balanced'),"
"RandomForestClassifier(max_depth=None, n_estimators=100, class_weight='balanced'),"
"MLPClassifier(alpha=1),"
GaussianNB()]
estimators= []
"for name, clf in zip(names, classifiers):"
"estimators.append((name, clf))"
"eclf1 = VotingClassifier(estimators= estimators, voting='hard')"
"eclf2 = VotingClassifier(estimators= estimators, voting='soft')"
"names.append(""ensemble hard"")"
classifiers.append(eclf1)
"names.append(""ensemble soft"")"
classifiers.append(eclf2)
dataset.image_mode()
dataset.updateEmbedding(model)
y_pred= clf.predict(X_test)
"print(confusion_matrix(y_test, y_pred))"
paths= dataset.getpaths()
"for i, (yp, yt) in enumerate(zip(y_pred, y_test)):"
if yp != yt:
"copy(paths[i],""mistakes"")"
"print(yt, yp, paths[i],i)"
"clf_output= clf_e.embedding(eval_loader, dim=48)"
"self.train_transform = transforms.Compose([Resize(raw_size), CenterCrop((processed_size)), ToTensor(), Normalize(mean, std)])"
"print(self.labels_set, self.n_classes)"
"print(line,imageWidth,imageHeight)"
"print length,(i-1)*length,i*length"
#########################################################
## Configuration
#########################################################
## The actual code
Check arguments
Create output directories
Padding around the detected objects when cropping
1.3 for the cropping during test time and 1.3 for
the context that the CNN requires in the left-over
image
Load a (frozen) Tensorflow model into memory.
## Preparations: get all the output tensors
The following processing is only for single image
Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
Follow the convention by adding back the batch dimension
For all images in the image directoryig
Load image
Run inference
"all outputs are float32 numpy arrays, so convert types as appropriate"
Add detections to the collection
Get info about the image
Select detections with a confidence larger than DETECTION_CONFIDENCE
Get these boxes and convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes"
"However, we need to make sure that it box coordinates are still within the image"
"For each detected bounding box with high confidence, we will"
crop the image to the padded box and save it
generate a unique identifier for the detection
"bbox is the detected box, crop_box the padded / enlarged box"
Add numbering to the original file name if there are multiple boxes
The absolute file path where we will store the image
"if COCO_OUTPUT_DIR is set, then we will only use the shape"
of cropped_img in the following code. So instead of reading
cropped_img = np.array(Image.open(out_file))
we can speed everything up by reading only the size of the image
matplotlib.use('Agg')
conf= ConfusionMatrix(24)
get the inputs
zero the parameter gradients
forward + backward + optimize
print statistics
get the inputs
forward + backward + optimize
print statistics
get the inputs
zero the parameter gradients
forward + backward + optimize
get the inputs
forward + backward + optimize
print statistics
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
"conf.reset()"""""""
define loss function (criterion) and optimizer
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
conf.reset()
for clf in classifiers:
"clf.fit(X_train, y_train)"
preds= clf.predict_proba(X)
uncertainty+= preds.max(axis=1)
print(uncertainty[ind])
print(uncertainty[ind])
true_labels = y[unlabeled_indices]
select up to 5 digit examples that the classifier is most uncertain about
"print(indices,selected_set)"
print(query.sql())
remember best acc@1 and save checkpoint
"completeClassificationLoop(run_dataset, model,num_classes)"
"embd, label, paths = extract_embeddings(run_loader, model)"
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)"
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd)
labels = db.labels_
"mapp=(find_probablemap(label,labels, K=args.K))"
"print(""Clusters"")"
"for i,x in enumerate(labels):"
labels[i]= mapp[x]
print(np.sum(labels == label)/labels.size)
"print(""Confidence Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning(embd, label, idx)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(""Entropy Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning_entropy(embd, label, idx)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
"new_selected= selectSamples(embd,paths,3000)"
"print(idx,idx.shape)"
for i in idx:
print(paths[i])
"print(""Silohette active learning"")"
"idx= active_learning2(embd, 1000, args.num_clusters)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(""Random"")"
"idx = np.random.choice(np.arange(len(paths)), 1000, replace=False)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
embd= reduce_dimensionality(embd)#[0:10000])
labels= labels[0:10000]
label= label[0:10000]
paths= paths[0:10000]
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])"
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])"
plt.show()
"np.save(args.name_prefix+""_embeddings.npy"",embd)"
"np.save(args.name_prefix+""_labels.npy"",label)"
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")"
combo.setEnabled(not finalized)
print(self.bbox)
self.addWidget(self.child)
Moving container with arrows
Left - Bottom
Right-Bottom
Left-Top
Right-Top
Left - Bottom
Right - Bottom
Left - Top
Right - Top
check cursor horizontal position
check cursor vertical position
self.resizeEvent=self.onResize
"print(""Parent"", parent, parent.width(), parent.height())"
"self.setGeometry(0,0,410,307)"
"print(w,h,""w,h"")"
"print(""final"",tag.getFinal())"
pass
"self.tags.append(TContainer(self,Category.get(-1),[0,0,0.1,0.1],True, Qt.red))"
label= CharField()
fullname=str(self.model)
"self.name= (fullname[fullname.find("":"")+2:fullname.find("">"")].strip()+'_set').lower()"
"print(""Parent"", self.parentWidget().width(), self.parentWidget().height())"
"print(self.model,self.name,query.sql())"
self.tab4.speciesList.setModel(species)
"self.tab4.speciesList.setRowHidden(len(species.stringList())-1, True)"
self.speciesList.itemChanged.connect(self.itemChanged)
#####
""
video_utils.py
""
"Utilities for splitting, rendering, and assembling videos."
""
#####
"%% Constants, imports, environment"
from ai4eutils
%% Path utilities
%% Function for rendering frames to video and vice-versa
http://tsaith.github.io/combine-images-into-a-video-with-python-3-and-opencv-3.html
Determine the width and height from the first image
Define the codec and create VideoWriter object
"If we're not over-writing, check whether all frame images already exist"
"OpenCV seems to over-report the number of frames by 1 in some cases, or fails"
"to read the last frame; either way, I'm allowing one missing frame."
"print(""Rendering video {}, couldn't find frame {}"".format("
"input_video_file,missing_frame_number))"
...if we need to check whether to skip this video entirely
"for frame_number in tqdm(range(0,n_frames)):"
print('Skipping frame {}'.format(frame_filename))
Recursively enumerate video files
Create the target output folder
Render frames
input_video_file = input_fn_absolute; output_folder = output_folder_video
For each video
""
input_fn_relative = input_files_relative_paths[0]
"process_detection_with_options = partial(process_detection, options=options)"
zero-indexed
Load results
# Break into videos
im = images[0]
# For each video...
video_name = list(video_to_frames.keys())[0]
frame = frames[0]
At most one detection for each category for the whole video
category_id = list(detection_categories.keys())[0]
Find the nth-highest-confidence video to choose a confidence value
Prepare the output representation for this video
'max_detection_conf' is no longer included in output files by default
...for each video
Write the output file
%% Test driver
%% Constants
%% Split videos into frames
"%% List image files, break into folders"
Find unique folders
fn = frame_files[0]
%% Load detector output
%% Render detector frames
folder = list(folders)[0]
d = detection_results_this_folder[0]
...for each file in this folder
...for each folder
%% Render output videos
folder = list(folders)[0]
...for each video
""
run_inference_with_yolov5_val.py
""
Runs a folder of images through MegaDetector (or another YOLOv5 model) with YOLOv5's
"val.py, converting the output to the standard MD format.  The main goal is to leverage"
YOLO's test-time augmentation tools.
""
"YOLOv5's val.py uses each file's base name as a unique identifier, which doesn't work"
when you have typical camera trap images like:
""
a/b/c/RECONYX0001.JPG
d/e/f/RECONYX0001.JPG
""
...so this script jumps through a bunch of hoops to put a symlinks in a flat
"folder, run YOLOv5 on that folder, and map the results back to the real files."
""
"Currently requires the user to supply the path where a working YOLOv5 install lives,"
and assumes that the current conda environment is all set up for YOLOv5.
""
TODO:
""
* Figure out what happens when images are corrupted... right now this is the #1
"reason not to use this script, it may be the case that corrupted images look the"
same as empty images.
""
* Multiple GPU support
""
* Checkpointing
""
* Windows support (I have no idea what all the symlink operations will do on Windows)
""
"* Support alternative class names at the command line (currently defaults to MD classes,"
though other class names can be supplied programmatically)
""
%% Imports
%% Options class
# Required ##
# Optional ##
%% Main function
#%% Path handling
#%% Enumerate images
#%% Create symlinks to give a unique ID to each image
i_image = 0; image_fn = image_files_absolute[i_image]
...for each image
#%% Create the dataset file
Category IDs need to be continuous integers starting at 0
#%% Prepare YOLOv5 command
#%% Run YOLOv5 command
#%% Convert results to MD format
"We'll use the absolute path as a relative path, and pass '/'"
as the base path in this case.
#%% Clean up
...def run_inference_with_yolo_val()
%% Command-line driver
%% Scrap
%% Test driver (folder)
%% Test driver (file)
%% Preview results
options.sample_seed = 0
...for each prediction file
%% Compare results
Choose all pairwise combinations of the files in [filenames]
"%% Constants, imports, environment"
from multiprocessing.pool import ThreadPool as workerpool
Number of images to pre-fetch
Useful hack to force CPU inference.
""
"Need to do this before any PT/TF imports, which happen when we import"
from run_detector.
Numpy FutureWarnings from tensorflow import
%% Support functions for multiprocessing
TODO
""
The queue system is a little more elegant if we start one thread for reading and one
"for processing, and this works fine on Windows, but because we import TF at module load,"
"CUDA will only work in the main process, so currently the consumer function runs here."
""
"To enable proper multi-GPU support, we may need to move the TF import to a separate module"
that isn't loaded until very close to where inference actually happens.
%% Other support funtions
%% Image processing functions
%% Main function
Handle the case where image_file_names is not yet actually a list
"Find the images to score; images can be a directory, may need to recurse"
A json list of image paths
A single image file
Load the detector
Does not count those already processed
Will not add additional entries not in the starter checkpoint
Write a checkpoint if necessary
"Back up any previous checkpoints, to protect against crashes while we're writing"
the checkpoint file.
Write the new checkpoint
Remove the backup checkpoint if it exists
...if it's time to make a checkpoint
"When using multiprocessing, let the workers load the model"
"Results may have been modified in place, but we also return it for"
backwards-compatibility.
The typical case: we need to build the 'info' struct
"If the caller supplied the entire ""info"" struct"
"The 'max_detection_conf' field used to be included by default, and it caused all kinds"
"of headaches, so it's no longer included unless the user explicitly requests it."
%% Interactive driver
%%
%% Command-line driver
This is an experimental hack to allow the use of non-MD YOLOv5 models through
the same infrastructure; it disables the code that enforces MDv5-like class lists.
Load the checkpoint if available
""
Relative file names are only output at the end; all file paths in the checkpoint are
still full paths.
"Find the images to score; images can be a directory, may need to recurse"
A json list of image paths
A single image file
Test that we can write to the output_file's dir if checkpointing requested
"Don't overwrite existing checkpoint files, this is a sure-fire way to eventually"
erase someone's checkpoint.
"Commenting this out for now... the scenario where we are resuming from a checkpoint,"
then immediately overwrite that checkpoint with empty data is higher-risk than the
annoyance of crashing a few minutes after starting a job.
Confirm that we can write to the checkpoint path; this avoids issues where
we crash after several thousand images.
%% Imports
import pre- and post-processing functions from the YOLOv5 repo
scale_coords() became scale_boxes() in later YOLOv5 versions
%% Classes
padded resize
"Image size can be an int (which translates to a square target size) or (h,w)"
...if the caller has specified an image size
NMS
"As of v1.13.0.dev20220824, nms is not implemented for MPS."
""
Send predication back to the CPU to fix.
format detections/bounding boxes
"This is a loop over detection batches, which will always be length 1 in our case,"
since we're not doing batch inference.
Rescale boxes from img_size to im0 size
"normalized center-x, center-y, width and height"
"MegaDetector output format's categories start at 1, but the MD"
model's categories start at 0.
...for each detection in this batch
...if this is a non-empty batch
...for each detection batch
...try
for testing
"MegaDetector was trained with batch size of 1, and the resizing function is a part"
of the inference graph
"change from [y1, x1, y2, x2] to [x1, y1, width, height]"
convert numpy floats to Python floats
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size
"np_images = [np.asarray(image, np.uint8) for image in images]"
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)"
performs inference
our batch size is 1; need to loop the batch dim if supporting batch size > 1
"%% Constants, imports, environment"
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Numpy FutureWarnings from tensorflow import
Useful hack to force CPU inference
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
An enumeration of failure reasons
Number of decimal places to round to for confidence and bbox coordinates
Label mapping for MegaDetector
Should we allow classes that don't look anything like the MegaDetector classes?
"Each version of the detector is associated with some ""typical"" values"
"that are included in output files, so that downstream applications can"
use them as defaults.
%% Classes
Stick this into filenames before the extension for the rendered result
%% Utility functions
mps backend only available in torch >= 1.12.0
%% Main function
Dictionary mapping output file names to a collision-avoidance count.
""
"Since we'll be writing a bunch of files to the same folder, we rename"
as necessary to avoid collisions.
...def input_file_to_detection_file()
Image is modified in place
...for each image
...def load_and_run_detector()
%% Command-line driver
Must specify either an image file or a directory
"but for a single image, args.image_dir is also None"
%% Interactive driver
%%
#####
""
process_video.py
""
"Split a video (or folder of videos) into frames, run the frames through run_detector_batch.py,"
and optionally stitch together results into a new video with detection boxes.
""
TODO: allow video rendering when processing a whole folder
TODO: allow video rendering from existing results
""
#####
"%% Constants, imports, environment"
Only relevant if render_output_video is True
Folder to use for extracted frames
Folder to use for rendered frames (if rendering output video)
Should we render a video with detection boxes?
""
"Only supported when processing a single video, not a folder."
"If we are rendering boxes to a new video, should we keep the temporary"
rendered frames?
Should we keep the extracted frames?
%% Main functions
"TODO: keep track of whether we created this folder, delete if we're deleting the extracted"
"frames and we created the folder, and the output files aren't in the same folder.  For now,"
we're just deleting the extracted frames and leaving the empty folder around in this case.
Render detections to images
"TODO: keep track of whether we created this folder, delete if we're deleting the rendered"
"frames and we created the folder, and the output files aren't in the same folder.  For now,"
we're just deleting the rendered frames and leaving the empty folder around in this case.
Combine into a video
Delete the temporary directory we used for detection images
shutil.rmtree(rendering_output_dir)
(Optionally) delete the frames on which we ran MegaDetector
shutil.rmtree(frame_output_folder)
...process_video()
# Validate options
# Split every video into frames
# Run MegaDetector on the extracted frames
# (Optionally) delete the frames on which we ran MegaDetector
shutil.rmtree(frame_output_folder)
# Convert frame-level results to video-level results
...process_video_folder()
%% Interactive driver
%% Process a folder of videos
import clipboard; clipboard.copy(cmd)
%% Process a single video
import clipboard; clipboard.copy(cmd)
"%% Render a folder of videos, one file at a time"
import clipboard; clipboard.copy(s)
%% Command-line driver
Lint as: python3
Copyright 2020 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TPU is automatically inferred if tpu_name is None and
we are running under cloud ai-platform.
""
copy_checkpoints.py
""
Run this script with specified source_dir and target_dir while the model is training to make a copy
of every checkpoint (checkpoints are kept once an hour by default and is difficult to adjust)
""
do not copy event or evaluation results
input validation
plot the images
adjust the figure
"read in dataset CSV and create merged (dataset, location) col"
map label to label_index
load the splits
only weight the training set by detection confidence
TODO: consider weighting val and test set as well
isotonic regression calibration of MegaDetector confidence
treat each split separately
new_weight[i] = confidence[i] * (n / c) / total_confidence(i's label)
- n = # examples in split (weighted by confidence); c = # labels
- weight allocated to each label is n/c
"- within each label, weigh each example proportional to confidence"
- new weights sum to n
error checking
"maps output label name to set of (dataset, dataset_label) tuples"
find which other label (label_b) has intersection
input validation
create label index JSON
look into sklearn.preprocessing.MultiLabelBinarizer
Note: JSON always saves keys as strings!
"True for ground truth, False for MegaDetector"
always save as .jpg for consistency
get bounding boxes
convert from category ID to category name
"check if crops are already downloaded, and ignore bboxes below the"
confidence threshold
assign all images without location info to 'unknown_location'
remove images from labels that have fewer than min_locs locations
merge dataset and location into a single string '<dataset>/<location>'
"create DataFrame of counts. rows = locations, columns = labels"
label_count: label => number of examples
loc_count: label => number of locs containing that label
generate a new split
score the split
SSE for # of images per label (with 2x weight)
SSE for # of locs per label
label => list of datasets to prioritize for test and validation sets
"merge dataset and location into a tuple (dataset, location)"
sorted smallest to largest
greedily add to test set until it has >= 15% of images
sort the resulting locs
"modify loc_to_size in place, so copy its keys before iterating"
arguments relevant to both creating the dataset CSV and splits.json
arguments only relevant for creating the dataset CSV
arguments only relevant for creating the splits JSON
comment lines starting with '#' are allowed
""
prepare_classification_script.py
""
Notebook-y script used to prepare a series of shell commands to run a classifier
(other than MegaClassifier) on a MegaDetector result set.
""
Differs from prepare_classification_script_mc.py only in the final class mapping step.
""
%% Job options
%% Constants
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
%% Set up environment
commands.append('cd CameraTraps/classification\n')
commands.append('conda activate cameratraps-classifier\n')
%% Crop images
fn = input_files[0]
%% Run classifier
fn = input_files[0]
%% Merge classification and detection outputs
fn = input_files[0]
%% Write everything out
input validation
"Evaluating with accimage is much faster than Pillow or Pillow-SIMD, but accimage"
is Linux-only.
create output directory
override saved params with kwargs
"For now, we don't weight crops by detection confidence during"
evaluation. But consider changing this.
"create model, compile with TorchScript if given checkpoint is not compiled"
"verify that target names matches original ""label names"" from dataset"
"if the dataset does not already have a 'other' category, then the"
'other' category must come last in label_names to avoid conflicting
with an existing label_id
define loss function (criterion)
"this file ends up being huge, so we GZIP compress it"
double check that the accuracy metrics are computed properly
save the confusion matrices to .npz
save per-label statistics
set dropout and BN layers to eval mode
"even if batch contains sample weights, don't use them"
Do target mapping on the outputs (unnormalized logits) instead of
"the normalized (softmax) probabilities, because the loss function"
uses unnormalized logits. Summing probabilities is equivalent to
log-sum-exp of unnormalized logits.
"a confusion matrix C is such that C[i,j] is the # of observations known to"
be in group i and predicted to be in group j.
match pytorch EfficientNet model names
images dataset
"for smaller disk / memory usage, we cache the raw JPEG bytes instead"
of the decoded Tensor
convert JPEG bytes to a 3D uint8 Tensor
"keras EfficientNet already includes normalization from [0, 255] to [0, 1],"
so we don't need to do that here
labels dataset
img_files dataset
weights dataset
define the transforms
efficientnet data preprocessing:
- train:
"1) random crop: aspect_ratio_range=(0.75, 1.33), area_range=(0.08, 1.0)"
2) bicubic resize to img_size
3) random horizontal flip
- test:
1) center crop
2) bicubic resize to img_size
weights sums to:
- if weight_by_detection_conf: (# images in split - conf delta)
- otherwise: (# images in split)
"freeze the base model's weights, including BatchNorm statistics"
https://www.tensorflow.org/guide/keras/transfer_learning#fine-tuning
rebuild output
input validation
set seed
create logdir and save params
create dataloaders and log the index_to_label mapping
define loss function and optimizer
using EfficientNet training defaults
- batch norm momentum: 0.99
"- optimizer: RMSProp, decay 0.9 and momentum 0.9"
- epochs: 350
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs"
- weight decay: 1e-5
TODO: change weighted to False if oversampling minority classes
stop training after 8 epochs without improvement
log metrics
log confusion matrix
log tp/fp/fn images
"tf.summary.image requires input of shape [N, H, W, C]"
false positive for top3_pred[0]
false negative for label
"if evaluating or finetuning, set dropout & BN layers to eval mode"
"for each label, track 5 most-confident and least-confident examples"
"even if batch contains sample weights, don't use them"
we do not track L2-regularization loss in the loss metric
This dictionary will get written out at the end of this process; store
diagnostic variables here
error checking
refresh detection cache
save log of bad images
cache of Detector outputs: dataset name => {img_path => detection_dict}
img_path: <dataset-name>/<img-filename>
get SAS URL for images container
strip image paths of dataset name
save list of dataset names and task IDs for resuming
complete task name: 'detect_for_classifier_caltech_20200722_110816_task01'
HACK! Sleep for 10s between task submissions in the hopes that it
"decreases the chance of backend JSON ""database"" corruption"
task still running => continue
"task finished successfully, save response to disk"
error checking before we download and crop any images
convert from category ID to category name
we need the datasets table for getting SAS keys
"we already did all error checking above, so we don't do any here"
get ContainerClient
get bounding boxes
we must include the dataset <ds> in <crop_path_template> because
'{img_path}' actually gets populated with <img_file> in
load_and_crop()
"get the image, either from disk or from Blob Storage"
inelegant way to close the container_clients
""
prepare_classification_script_mc.py
""
Notebook-y script used to prepare a series of shell commands to run MegaClassifier
on a MegaDetector result set.
""
Differs from prepare_classification_script.py only in the final class mapping step.
""
%% Job options
%% Constants
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
%% Set up environment
commands.append('cd CameraTraps/classification\n')
commands.append('conda activate cameratraps-classifier\n')
%% Crop images
fn = input_files[0]
%% Run classifier
fn = input_files[0]
%% Remap classifier outputs
fn = input_files[0]
%% Merge classification and detection outputs
fn = input_files[0]
%% Write everything out
mean/std values from https://pytorch.org/docs/stable/torchvision/models.html
define the transforms
resizes smaller edge to img_size
weights sums to:
- if weight_by_detection_conf: (# images in split - conf delta)
- otherwise: # images in split
for normal (non-weighted) shuffling
set all parameters to not require gradients except final FC layer
replace final fully-connected layer (which has 1000 ImageNet classes)
"detect GPU, use all if available"
input validation
set seed
create logdir and save params
create dataloaders and log the index_to_label mapping
create model
define loss function and optimizer
using EfficientNet training defaults
- batch norm momentum: 0.99
"- optimizer: RMSProp, decay 0.9 and momentum 0.9"
- epochs: 350
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs"
- weight decay: 1e-5
stop training after 8 epochs without improvement
do a complete evaluation run
log metrics
log confusion matrix
log tp/fp/fn images
"for every image: undo normalization, clamp to [0, 1], CHW -> HWC"
"- cannot be in-place, because the HeapItem might be in multiple heaps"
writer.add_figure() has issues => using add_image() instead
"writer.add_figure(f'{label_name}/{tag}', fig, global_step=epoch)"
false positive for top3_pred[0]
false negative for label
"preds and labels both have shape [N, k]"
"if evaluating or finetuning, set dropout and BN layers to eval mode"
"for each label, track k_extreme most-confident and least-confident images"
"even if batch contains sample weights, don't use them"
lock before changing ImageFile.LOAD_TRUNCATED_IMAGES
input validation
use MegaDB to generate list of images
only keep images that:
"1) end in a supported file extension, and"
2) actually exist in Azure Blob Storage
3) belong to a label with at least min_locs locations
write out log of images / labels that were removed
"save label counts, pre-subsampling"
"save label counts, post-subsampling"
spec_dict['taxa']: list of dict
[
"{'level': 'family', 'name': 'cervidae', 'datasets': ['idfg']},"
"{'level': 'genus',  'name': 'meleagris'}"
]
"spec_dict['dataset_labels']: dict, dataset => list of dataset_label"
{
"""idfg"": [""deer"", ""elk"", ""prong""],"
"""idfg_swwlf_2019"": [""elk"", ""muledeer"", ""whitetaileddeer""]"
}
"maps output label name to set of (dataset, dataset_label) tuples"
"because MegaDB is organized by dataset, we do the same"
ds_to_labels = {
'dataset_name': {
"'dataset_label': [output_label1, output_label2]"
}
}
we need the datasets table for getting full image paths
The line
"[img.class[0], seq.class[0]][0] as class"
selects the image-level class label if available. Otherwise it selects the
"sequence-level class label. This line assumes the following conditions,"
expressed in the WHERE clause:
- at least one of the image or sequence class label is given
- the image and sequence class labels are arrays with length at most 1
- the image class label takes priority over the sequence class label
""
"In Azure Cosmos DB, if a field is not defined, then it is simply excluded"
"from the result. For example, on the following JSON object,"
{
"""dataset"": ""camera_traps"","
"""seq_id"": ""1234"","
"""location"": ""A1"","
"""images"": [{""file"": ""abcd.jpeg""}],"
"""class"": [""deer""],"
}
"the array [img.class[0], seq.class[0]] just gives ['deer'] because"
img.class is undefined and therefore excluded.
"if no path prefix, set it to the empty string '', because"
"os.path.join('', x, '') = '{x}/'"
result keys
"- already has: ['dataset', 'location', 'file', 'class', 'bbox']"
"- add ['label'], remove ['file']"
"if img is mislabeled, but we don't know the correct class, skip it"
"otherwise, update the img with the correct class, but skip the"
img if the correct class is not one we queried for
sort keys for determinism
we need the datasets table for getting SAS keys
strip leading '?' from SAS token
only check Azure Blob Storage
check local directory first before checking Azure Blob Storage
1st pass: populate label_to_locs
"label (tuple of str) => set of (dataset, location)"
2nd pass: eliminate bad images
prioritize is a list of prioritization levels
number of already matching images
main(
"label_spec_json_path='idfg_classes.json',"
"taxonomy_csv_path='../../camera-traps-private/camera_trap_taxonomy_mapping.csv',"
"output_dir='run_idfg',"
json_indent=4)
recursively find all files in cropped_images_dir
only find crops of images from detections JSON
resizes smaller edge to img_size
"Evaluating with accimage is much faster than Pillow or Pillow-SIMD, but accimage"
is Linux-only.
create dataset
create model
set dropout and BN layers to eval mode
load files
dataset => set of img_file
crop_path: <dataset>/<img_file>___cropXX_mdvY.Y.jpg
[----<img_path>----]
error checking
any row with 'correct_class' should be marked 'mislabeled'
filter to only the mislabeled rows
convert '\' to '/'
verify that overlapping indices are the same
"""add"" any new mislabelings"
write out results
error checking
load detections JSON
get detector version
convert from category ID to category name
copy keys to modify dict in-place
This will be removed later when we filter for animals
save log of bad images
"True for ground truth, False for MegaDetector"
always save as .jpg for consistency
"we already did all error checking above, so we don't do any here"
"get the image, either from disk or from Blob Storage"
inelegant way to close the container_client
"crop_path => normalized bbox coordinates [xmin, ymin, width, height]"
"only ground-truth bboxes do not have a ""confidence"" value"
try loading image from local directory
try to download image from Blob Storage
crop the image
"expand box width or height to be square, but limit to img size"
"Image.crop() takes box=[left, upper, right, lower]"
pad to square using 0s
Author: lukemelas (github username)
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch
With adjustments and added comments by workingcoder (github username).
Support the construction of 'efficientnet-l2' without pretrained weights
Expansion phase (Inverted Bottleneck)
"image_size = calculate_output_image_size(image_size, 1) <-- this wouldn't modify image_size"
Depthwise convolution phase
"Squeeze and Excitation layer, if desired"
Pointwise convolution phase
Expansion and Depthwise Convolution
Squeeze and Excitation
Pointwise Convolution
Skip connection and drop connect
The combination of skip connection and drop connect brings about stochastic depth.
Batch norm parameters
Get stem static or dynamic convolution depending on image size
Stem
Build blocks
Update block input and output filters based on depth multiplier.
The first block needs to take care of stride and filter size increase.
"image_size = calculate_output_image_size(image_size, block_args.stride)  # stride = 1"
Head
Final linear layer
Stem
Blocks
Head
Stem
Blocks
Head
Convolution layers
Pooling and final linear layer
Author: lukemelas (github username)
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch
With adjustments and added comments by workingcoder (github username).
###############################################################################
## Help functions for model architecture
###############################################################################
GlobalParams and BlockArgs: Two namedtuples
Swish and MemoryEfficientSwish: Two implementations of the method
round_filters and round_repeats:
Functions to calculate params for scaling model width and depth ! ! !
get_width_and_height_from_size and calculate_output_image_size
drop_connect: A structural design
get_same_padding_conv2d:
Conv2dDynamicSamePadding
Conv2dStaticSamePadding
get_same_padding_maxPool2d:
MaxPool2dDynamicSamePadding
MaxPool2dStaticSamePadding
"It's an additional function, not used in EfficientNet,"
but can be used in other model (such as EfficientDet).
"Parameters for the entire model (stem, all blocks, and head)"
Parameters for an individual model block
Set GlobalParams and BlockArgs's defaults
An ordinary implementation of Swish function
A memory-efficient implementation of Swish function
TODO: modify the params names.
"maybe the names (width_divisor,min_width)"
"are more suitable than (depth_divisor,min_depth)."
follow the formula transferred from official TensorFlow implementation
follow the formula transferred from official TensorFlow implementation
"generate binary_tensor mask according to probability (p for 0, 1-p for 1)"
Note:
The following 'SamePadding' functions make output size equal ceil(input size/stride).
"Only when stride equals 1, can the output size be the same as input size."
Don't be confused by their function names ! ! !
Tips for 'SAME' mode padding.
Given the following:
i: width or height
s: stride
k: kernel size
d: dilation
p: padding
Output after Conv2d:
o = floor((i+p-((k-1)*d+1))/s+1)
"If o equals i, i = floor((i+p-((k-1)*d+1))/s+1),"
=> p = (i-1)*s+((k-1)*d+1)-i
With the same calculation as Conv2dDynamicSamePadding
Calculate padding based on image size and save it
Calculate padding based on image size and save it
###############################################################################
## Helper functions for loading model params
###############################################################################
BlockDecoder: A Class for encoding and decoding BlockArgs
efficientnet_params: A function to query compound coefficient
get_model_params and efficientnet:
Functions to get BlockArgs and GlobalParams for efficientnet
url_map and url_map_advprop: Dicts of url_map for pretrained weights
load_pretrained_weights: A function to load pretrained weights
Check stride
"Coefficients:   width,depth,res,dropout"
Blocks args for the whole model(efficientnet-b0 by default)
It will be modified in the construction of EfficientNet Class according to model
note: all models have drop connect rate = 0.2
ValueError will be raised here if override_params has fields not included in global_params.
train with Standard methods
check more details in paper(EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks)
train with Adversarial Examples(AdvProp)
check more details in paper(Adversarial Examples Improve Image Recognition)
TODO: add the petrained weights url map of 'efficientnet-l2'
AutoAugment or Advprop (different preprocessing)
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils)
from data_management.megadb.schema import sequences_schema_check
resize is for displaying them more quickly
dataset and seq_id are required fields
sort the images in the sequence
"if no class label on the image, show class label on the sequence"
pool = ThreadPool()
"print('len of rendering_info', len(rendering_info))"
"tqdm(pool.imap_unordered(render_image_info_partial, rendering_info), total=len(rendering_info))"
options = write_html_image_list()
options['headerHtml']
print('Checking that the MegaDB entries conform to the schema...')
sequences_schema_check.sequences_schema_check(sequences)
#######
""
visualize_db.py
""
Outputs an HTML page visualizing annotations (class labels and/or bounding boxes)
on a sample of images in a database in the COCO Camera Traps format
""
#######
%% Imports
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils)
Assumes the cameratraps repo root is on the path
%% Settings
Set to None to visualize all images
Target size for rendering; set either dimension to -1 to preserve aspect ratio
"These are mutually exclusive; both are category names, not IDs"
"Special tag used to say ""show me all images with multiple categories"""
We sometimes flatten image directories by replacing a path separator with
another character.  Leave blank for the typical case where this isn't necessary.
Control rendering parallelization
Process-based parallelization in this function is currently unsupported
"due to pickling issues I didn't care to look at, but I'm going to just"
"flip this with a warning, since I intend to support it in the future."
%% Helper functions
"Translate the file name in an image entry in the json database to a path, possibly doing"
some manipulation of path separators
%% Core functions
"Optionally remove all images without bounding boxes, *before* sampling"
"Optionally include/remove images with specific labels, *before* sampling"
Put the annotations in a dataframe so we can select all annotations for a given image
Construct label map
Take a sample of images
Set of dicts representing inputs to render_db_bounding_boxes:
""
"bboxes, boxClasses, image_path"
All the class labels we've seen for this image (with out without bboxes)
Iterate over annotations for this image
iAnn = 0; anno = annos_i.iloc[iAnn]
...for each of this image's annotations
"We're adding html for an image before we render it, so it's possible this image will"
fail to render.  For applications where this script is being used to debua a database
"(the common case?), this is useful behavior, for other applications, this is annoying."
...for each image
...def render_image_info
def process_images(...)
%% Command-line driver
"Copy all fields from a Namespace (i.e., the output from parse_args) to an object."
""
Skips fields starting with _.  Does not check existence in the target object.
Convert to an options object
%% Interactive driver(s)
%%
os.startfile(htmlOutputFile)
%% Constants and imports
convert category ID from int to str
Retry on blob storage read failures
%% Functions
PIL.Image.convert() returns a converted copy of this image
Alter orientation as needed according to EXIF tag 0x112 (274) for Orientation
""
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28
https://www.media.mit.edu/pia/Research/deepview/exif.html
""
Null operation
Aspect ratio as width over height
ar = w / h
h = w / ar
w = ar * h
The following three functions are modified versions of those at:
""
https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py
Convert to pixels so we can use the PIL crop() function
PIL's crop() does surprising things if you provide values outside of
"the image, clip inputs"
...if this detection is above threshold
...for each detection
"list of lists, one list of strings for each bounding box (to accommodate multiple labels)"
for color selection
"Always render objects with a confidence of ""None"", this is typically used"
for ground truth data.
"{} is the default, which means ""show labels with no mapping"", so don't use ""if label_map"" here"
if label_map:
"To avoid duplicate colors with detection-only visualization, offset"
the classification class index by the number of detection classes
...for each classification
...if we have classification results
...if the confidence of this detection is above threshold
...for each detection
"print('Input must be of size [N, 4], but is ' + str(boxes_shape))"
Deliberately trimming to the width of the image only in the case where
"box expansion is turned on.  There's not an obvious correct behavior here,"
but the thinking is that if the caller provided an out-of-range bounding
"box, they meant to do that, but at least in the eyes of the person writing"
"this comment, if you expand a box for visualization reasons, you don't want"
to end up with part of a box.
""
A slightly more sophisticated might check whether it was in fact the expansion
"that made this box larger than the image, but this is the case 99.999% of the time"
"here, so that doesn't seem necessary."
...if we need to expand boxes
If the total height of the display strings added to the top of the bounding
"box exceeds the top of the image, stack the strings below the bounding box"
instead of above.
Each display_str has a top and bottom margin of 0.05x.
Reverse list and print from bottom to top.
Skip empty strings
"list of lists, one list of strings for each bounding box (to accommodate multiple labels)"
need to be a string here because PIL needs to iterate through chars
""
stacked bar charts are made with each segment starting from a y position
"To fit the legend in, shrink current axis by 20%"
Put a legend to the right of the current axis
####
""
visualize_incoming_annotations.py
""
Spot-check the annotations received from iMerit by visualizing annotated bounding
boxes on a sample of images and display them in HTML.
""
"Modified in 2021 March to use the new format (iMerit batch 12 onwards), which is a"
COCO formatted JSON with relative coordinates for the bbox.
""
from data_management.megadb.schema import sequences_schema_check
we used frame_num of 1 when sending out images to annotators when it is not explicitly stored (wcs esp.)
The file_name field in the incoming json looks like
alka_squirrels.seq2020_05_07_25C.frame119221.jpg
"we need to use the dataset, sequence and frame info to find the actual path in blob storage"
using the sequences
category_id 5 is No Object Visible
download the image
Write to HTML
allow forward references in typing annotations
class variables
instance variables
get path to root
taxonomy CSV rows without 'taxonomy_string' entries are excluded
"from the taxonomy graph, but can be included in a classification"
label specification JSON via the 'dataset_labels' key
use the lower parent
special cases
%% Imports
%% Taxnomy checking
taxonomy CSV rows without 'taxonomy_string' entries are excluded
"from the taxonomy graph, but can be included in a classification"
label specification JSON via the 'dataset_labels' key
...for each row in the taxnomy file
%% Command-line driver
%% Interactive driver
%%
which datasets are already processed?
"sequence-level query should be fairly fast, ~1 sec"
cases when the class field is on the image level (images in a sequence
"that had different class labels, 'caltech' dataset is like this)"
"this query may take a long time, >1hr"
"this query should be fairly fast, ~1 sec"
read species presence info from the JSON files for each dataset
has this class name appeared in a previous dataset?
columns to populate the spreadsheet
sort by descending species count
make the spreadsheet
hyperlink Bing search URLs
hyperlink example image SAS URLs
TODO hardcoded columns: change if # of examples or col_order changes
""
map_lila_taxonomy_to_wi_taxonomy.py
""
Loads the LILA category mapping (in which taxonomy information comes from an iNat taxonomy snapshot)
and tries to map each class to the Wildlife Insights taxonomy.
""
%% Constants and imports
Created by get_lila_category_list.py... contains counts for each category
This is a manually-curated file used to store mappings that had to be made manually
This is the main output file from this whole process
%% Load category and taxonomy files
%% Pull everything out of pandas
%% Cache WI taxonomy lookups
This is just a handy lookup table that we'll use to debug mismatches
taxon = wi_taxonomy[21653]; print(taxon)
Look for keywords that don't refer to specific taxa: blank/animal/unknown
Do we have a species name?
"If 'species' is populated, 'genus' should always be populated; one item currently breaks"
this rule.
...for each taxon
%% Find redundant taxa
%% Manual review of redundant taxa
%% Clean up redundant taxa
taxon_name = list(taxon_name_to_preferred_taxon_id.keys())[0]
"If we've gotten this far, we should be choosing from multiple taxa."
""
"This will become untrue if any of these are resolved later, at which point we shoudl"
remove them from taxon_name_to_preferred_id
Choose the preferred taxa
%% Read supplementary mappings
"Each line is [lila query],[WI taxon name],[notes]"
%% Map LILA categories to WI categories
Must be ordered from kingdom --> species
TODO:
"['subspecies','variety']"
i_taxon = 0; taxon = lila_taxonomy[i_taxon]; print(taxon)
"Go from kingdom --> species, choosing the lowest-level description as the query"
"E.g., 'car'"
"print('Made a supplementary mapping from {} to {}'.format(query,wi_taxon['taxon_name']))"
print('No match for {}'.format(query))
...for each LILA taxon
%% Manual mapping
%% Build a dictionary from LILA dataset names and categories to LILA taxa
i_d = 0; d = lila_taxonomy[i_d]
"%% Map LILA datasets to WI taxa, and count the number of each taxon available in each dataset"
dataset_name = list(lila_dataset_to_categories.keys())[0]
dataset_category = dataset_categories[0]
"Write out the dataset name, category name, WI GUID, WI scientific name, WI common name,"
and count
...for each category in this dataset
...for each dataset
...with open()
""
retrieve_sample_image.py
""
"Downloader that retrieves images from Google images, used for verifying taxonomy"
"lookups and looking for egregious mismappings (e.g., ""snake"" being mapped to a fish called"
"""snake"")."
""
"Simple wrapper around simple_image_download, but I've had to swap in and out the underlying"
downloader a few times.
""
%% Imports and environment
%%
%% Main entry point
%% Test driver
%%
""
"Using the taxonomy .csv file, map all LILA datasets to the standard taxonomy"
""
Does not currently produce results; this is just used to confirm that all category names
have mappings in the taxonomy file.
""
%% Constants and imports
Created by get_lila_category_list.py... contains counts for each category
%% Load category and taxonomy files
%% Map dataset names and category names to scientific names
i_row = 1; row = taxonomy_df.iloc[i_row]; row
"%% For each dataset, make sure we can map every category to the taxonomy"
dataset_name = list(lila_dataset_to_categories.keys())[0]
c = categories[0]
""
"Takes the megadb taxonomy mapping, extracts the rows that are relevant to"
"LILA, and does some cleanup."
""
%% Constants and imports
This is a partially-completed taxonomy file that was created from a different set of
"scripts, but covers *most* of LILA as of June 2022"
Created by get_lila_category_list.py
%% Read the input files
Get everything out of pandas
"%% Find all unique dataset names in the input list, compare them with data names from LILA"
d = input_taxonomy_rows[0]
The script that generates this dictionary creates a separate entry for bounding box
"metadata files, but those don't represent new dataset names"
%% Map input columns to output datasets
Make sure all of those datasets actually correspond to datasets on LILA
%% Re-write the input taxonomy file to refer to LILA datasets
Map the string datasetname:token to a taxonomic tree json
mapping = input_taxonomy_rows[0]
Make sure that all occurrences of this mapping_string give us the same output
assert taxonomy_string == taxonomy_mappings[mapping_string]
%% Re-write the input file in the target format
mapping_string = list(taxonomy_mappings.keys())[0]
""
prepare_lila_taxonomy_release.py
""
"Given the private intermediate taxonomy mapping, prepare the public (release)"
taxonomy mapping file.
""
%% Imports and constants
Created by get_lila_category_list.py... contains counts for each category
%% Find out which categories are actually used
dataset_name = datasets_to_map[0]
i_row = 0; row = df.iloc[i_row]; row
%% Generate the final output file
i_row = 0; row = df.iloc[i_row]; row
match_at_level = taxonomic_match[0]
i_row = 0; row = df.iloc[i_row]; row
"E.g.: (43117, 'genus', 'lepus', ['hares and jackrabbits']"
###############
---> CONSTANTS
###############
max_progressbar = count * (list(range(limit+1))[-1]+1)
"bar = progressbar.ProgressBar(maxval=max_progressbar,"
"widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()]).start()"
bar.update(bar.currval + 1)
bar.finish()
""
"Given a subset of LILA datasets, find all the categories, and start the taxonomy"
mapping process.
""
%% Constants and imports
Created by get_lila_category_list.py
'NACTI'
'Channel Islands Camera Traps'
%% Read the list of datasets
The script that generates this dictionary creates a separate entry for bounding box
"metadata files, but those don't represent new dataset names"
%% Find all categories
dataset_name = datasets_to_map[0]
%% Initialize taxonomic lookup
from taxonomy_mapping.species_lookup import (
"get_taxonomic_info, print_taxonomy_matche)"
%% Manual lookup
%%
q = 'white-throated monkey'
raise ValueError('')
%% Match every query against our taxonomies
mapping_string = category_mappings[1]; print(mapping_string)
...for each mapping
%% Write output rows
""
"Does some consistency-checking on the LILA taxonomy file, and generates"
an HTML preview page that we can use to determine whether the mappings
make sense.
""
%% Imports and constants
"lila_taxonomy_file = r""G:\git\agentmorrisprivate\lila-taxonomy\lila-taxonomy-mapping.csv"""
"lila_taxonomy_file = r""G:\temp\lila\lila_additions_2022.06.29.csv"""
%% Support functions
%% Read the taxonomy mapping file
%% Prepare taxonomy lookup
from taxonomy_mapping.species_lookup import (
"get_taxonomic_info, print_taxonomy_matche)"
%% Optionally remap all gbif-based mappings to inat (or vice-versa)
%%
i_row = 1; row = df.iloc[i_row]; row
This should be zero for the release .csv
%%
%% Check for mappings that disagree with the taxonomy string
Look for internal inconsistency
Look for outdated mappings
i_row = 0; row = df.iloc[i_row]
%% List null mappings
""
"These should all be things like ""unidentified"" and ""fire"""
""
i_row = 0; row = df.iloc[i_row]
%% List mappings with scientific names but no common names
%% List mappings that map to different things in different data sets
x = suppress_multiple_matches[-1]
...for each row where we saw this query
...for each row
"%% Verify that nothing ""unidentified"" maps to a species or subspecies"
"E.g., ""unidentified skunk"" should never map to a specific species of skunk"
%% Make sure there are valid source and level values for everything with a mapping
%% Find WCS mappings that aren't species or aren't the same as the input
"WCS used scientific names, so these remappings are slightly more controversial"
then the standard remappings.
row = df.iloc[-500]
"Anything marked ""species"" or ""unknown"" by definition doesn't map to a species,"
so ignore these.
print('WCS query {} ({}) remapped to {} ({})'.format(
"query,common_name,scientific_name,common_names_from_taxonomy))"
%% Download sample images for all scientific names
i_row = 0; row = df.iloc[i_row]
if s != 'mirafra':
continue
Check whether we already have enough images for this query
"print('Skipping query {}, already have {} images'.format(s,len(sizes_above_threshold)))"
Check whether we've already run this query for a previous row
...for each row in the mapping table
%% Rename .jpeg to .jpg
"print('Renaming {} to {}'.format(fn,new_fn))"
%% Choose representative images for each scientific name
s = list(scientific_name_to_paths.keys())[0]
Be suspicious of duplicate sizes
...for each scientific name
%% Delete unused images
%% Produce HTML preview
i_row = 2; row = df.iloc[i_row]
"image_paths = [os.path.relpath(p, output_base) for p in image_paths]"
...for each row
%% Open HTML preview
######
""
species_lookup.py
""
Look up species names (common or scientific) in the GBIF and iNaturalist
taxonomies.
""
Run initialize_taxonomy_lookup() before calling any other function.
""
######
%% Constants and imports
As of 2020.05.12:
""
"GBIF: ~777MB zipped, ~1.6GB taxonomy"
"iNat: ~2.2GB zipped, ~51MB taxonomy"
These are un-initialized globals that must be initialized by
the initialize_taxonomy_lookup() function below.
%% Functions
Initialization function
# Load serialized taxonomy info if we've already saved it
"# If we don't have serialized taxonomy info, create it from scratch."
Download and unzip taxonomy files
taxonomy_name = list(taxonomy_urls.items())[0][0]; zip_url = list(taxonomy_urls.items())[0][1]
Don't download the zipfile if we've already unzipped what we need
Bypasses download if the file exists already
Unzip the files we need
...for each file that we need from this zipfile
Remove the zipfile
os.remove(zipfile_path)
...for each taxonomy
"Create dataframes from each of the taxonomy files, and the GBIF common"
name file
Load iNat taxonomy
Load GBIF taxonomy
Remove questionable rows from the GBIF taxonomy
Load GBIF vernacular name mapping
Only keep English mappings
Convert everything to lowercase
"For each taxonomy table, create a mapping from taxon IDs to rows"
Create name mapping dictionaries
Build iNat dictionaries
row = inat_taxonomy.iloc[0]
Build GBIF dictionaries
"The canonical name is the Latin name; the ""scientific name"""
include the taxonomy name.
""
http://globalnames.org/docs/glossary/
This only seems to happen for really esoteric species that aren't
"likely to apply to our problems, but doing this for completeness."
Don't include taxon IDs that were removed from the master table
Save everything to file
...def initialize_taxonomy_lookup()
"list of dicts: {'source': source_name, 'taxonomy': match_details}"
i_match = 0
"list of (taxon_id, taxonRank, scientific name, [vernacular names])"
corresponding to an exact match and its parents
Walk taxonomy hierarchy
This can happen because we remove questionable rows from the
GBIF taxonomy
"print(f'Warning: no row exists for parent_taxon_id {parent_taxon_id},' + \"
"f'child taxon_id: {taxon_id}, query: {query}')"
The GBIF taxonomy contains unranked entries
...while there is taxonomy left to walk
...for each match
Remove redundant matches
i_tree_a = 0; tree_a = matching_trees[i_tree_a]
i_tree_b = 1; tree_b = matching_trees[i_tree_b]
"If tree a's primary taxon ID is inside tree b, discard tree a"
""
taxonomy_level_b = tree_b['taxonomy'][0]
...for each level in taxonomy B
...for each tree (inner)
...for each tree (outer)
...def traverse_taxonomy()
"print(""Finding taxonomy information for: {0}"".format(query))"
"In GBIF, some queries hit for both common and scientific, make sure we end"
up with unique inputs
"If the species is not found in either taxonomy, return None"
Both GBIF and iNat have a 1-to-1 mapping between taxon_id and row number
Walk both taxonomies
...def get_taxonomic_info()
m = matches[0]
"For example: [(9761484, 'species', 'anas platyrhynchos')]"
...for each taxonomy level
...for each match
...def print_taxonomy_matches()
%% Taxonomy functions that make subjective judgements
query = 'person'
Do we have an iNat match?
"print_taxonomy_matches(inat_matches, verbose=True)"
"print_taxonomy_matches(gbif_matches, verbose=True)"
print('Warning: multiple iNat matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple iNat common names for {query}')
Default to returning the query
"print(f'Matched iNat {query} to {scientific_name},{common_name}')"
...if we had iNat matches
If we either prefer GBIF or didn't have iNat matches
""
Code is deliberately redundant here; I'm expecting some subtleties in how
handle GBIF and iNat.
print('Warning: multiple GBIF matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple GBIF common names for {query}')
Default to returning the query
...if we needed to look in the GBIF taxonomy
...def _get_preferred_taxonomic_match()
%% Interactive drivers and debug
%% Initialization
%% Taxonomic lookup
query = 'lion'
print(matches)
Print the taxonomy in the taxonomy spreadsheet format
%% Directly access the taxonomy tables
%% Command-line driver
Read command line inputs (absolute path)
Read the tokens from the input text file
Loop through each token and get scientific name
""
process_species_by_dataset
""
We generated a list of all the annotations in our universe; this script is
used to (interactively) map them onto the GBIF and iNat taxonomies.  Don't
"try to run this script from top to bottom; it's used like a notebook, not like"
"a script, since manual review steps are required."
""
%% Imports
%autoreload 0
%autoreload -species_lookup
%% Constants
Input file
Output file after automatic remapping
File to which we manually copy that file and do all the manual review; this
should never be programmatically written to
The final output spreadsheet
HTML file generated to facilitate the identificaiton of egregious mismappings
%% Functions
Prefer iNat matches over GBIF matches
query = 'person'
Do we have an iNat match?
"print_taxonomy_matches(inat_matches, verbose=True)"
"print_taxonomy_matches(gbif_matches, verbose=True)"
print('Warning: multiple iNat matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple iNat common names for {query}')
Default to returning the query
"print(f'Matched iNat {query} to {scientific_name},{common_name}')"
...if we had iNat matches
If we either prefer GBIF or didn't have iNat matches
""
Code is deliberately redundant here; I'm expecting some subtleties in how
handle GBIF and iNat.
print('Warning: multiple GBIF matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple GBIF common names for {query}')
Default to returning the query
...if we needed to look in the GBIF taxonomy
...def get_preferred_taxonomic_match()
%% Initialization
%% Test single-query lookup
%%
%%
"q = ""grevy's zebra"""
%% Read the input data
%% Run all our taxonomic lookups
i_row = 0; row = df.iloc[i_row]
query = 'lion'
...for each query
Write to the excel file that we'll use for manual review
%% Download preview images for everything we successfully mapped
uncomment this to load saved output_file
"output_df = pd.read_excel(output_file, keep_default_na=False)"
i_row = 0; row = output_df.iloc[i_row]
...for each query
%% Write HTML file with representative images to scan for obvious mis-mappings
i_row = 0; row = output_df.iloc[i_row]
...for each row
%% Look for redundancy with the master table
Note: `master_table_file` is a CSV file that is the concatenation of the
"manually-remapped files (""manual_remapped.xlsx""), which are the output of"
this script run across from different groups of datasets. The concatenation
"should be done manually. If `master_table_file` doesn't exist yet, skip this"
"code cell. Then, after going through the manual steps below, set the final"
manually-remapped version to be the `master_table_file`.
%% Manual review
Copy the spreadsheet to another file; you're about to do a ton of manual
review work and you don't want that programmatically overwrriten.
""
See manual_review_xlsx above
%% Read back the results of the manual review process
%% Look for manual mapping errors
Manually inspect df for typos in 'dataset_names' and 'taxonomy_level' columns
Identify rows where:
""
- 'taxonomy_level' does not match level of 1st element in 'taxonomy_string'
- 'scientific_name' does not match name of 1st element in 'taxonomy_string'
""
...both of which typically represent manual mapping errors.
i_row = 0; row = df.iloc[i_row]
"I'm not sure why both of these checks are necessary, best guess is that"
the Excel parser was reading blanks as na on one OS/Excel version and as ''
on another.
The taxonomy_string column is a .json-formatted string; expand it into
an object via eval()
"%% Find scientific names that were added manually, and match them to taxonomies"
i_row = 0; row = df.iloc[i_row]
...for each query
%% Write out final version
""
"Generate read-only SAS URLs for all LILA containers, to facilitate partial downloads."
""
The results of this script end up here:
""
http://lila.science/wp-content/uploads/2020/03/lila_sas_urls.txt
""
"Update: that file is manually maintained now, it can't be programmatically generated"
""
%% Imports
Read-only
%% Enumerate containers
%% Generate SAS tokens
%% Generate SAS URLs
%% Write to output file
--------some stuff needed to get AJAX to work with bottle?--------#
-------------------------------------------------------------------------------- #
PREPARE TO QUEUE IMAGES FOR LABELING
-------------------------------------------------------------------------------- #
# Connect as USER to database DB_NAME through peewee and initialize database proxy
# Load embedding model
---------------------------------------------------------------------- #
CREATE QUEUE OF IMAGES TO LABEL
---------------------------------------------------------------------- #
Use classifier to generate predictions
# Update model predicted class in PostgreSQL database
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
-------------------------------------------------------------------------------- #
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI
-------------------------------------------------------------------------------- #
"# static routes (to serve CSS, etc.)"
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps"
"return bottle.static_file(filename, root='../../../../../../../../../../../.')"
# dynamic routes
if data['display_grayscale']:
indices_to_exclude.update(set(color_indices))
elif not data['display_grayscale']:
indices_to_exclude.update(set(grayscale_indices))
data['display_images'] = {}
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices]
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices]
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices]
data['display_images']['detection_categories'] = []
for i in indices:
if str(dataset.samples[i][1]) == 'None':
data['display_images']['detection_categories'].append('None')
else:
existing_category_entries = {cat.id: cat.name for cat in Category.select()}
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()"
data['display_images']['detection_categories'].append(cat_name)
Use image ids in images_to_label to get the corresponding dataset indices
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
print(set(dataset.set_indices[4]).update(set(indices_to_label)))
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Use image id images_to_label to get the corresponding dataset index
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Train on samples that have been labeled so far
dataset.set_kind(DetectionKind.UserDetection.value)
print(y_train)
Predict on the samples that have not been labeled
print(y_pred)
Update model predicted class in PostgreSQL database
timer = time.time()
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
print('Updating the database took %0.2f seconds'%(time.time() - timer))
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
"once the classifier has been trained the first time, switch to AL sampling"
""
make_oneclass_json.py
""
"Takes a coco-camera-traps .json database and collapses species classes to binary,"
optionally removing labels from empty images (to be detector-friendly) (depending on
"""experiment_type"")."
""
"Assumes that empty images are labeled as ""empty""."
""
%% Imports and environment
%% Core conversion function
"We're removing empty images from the annotation list, but not from"
"the ""images"" list; they'll still get used in detector training."
print('Ignoring empty annotation')
%% Interactive driver
%%
Load annotations
Convert from multi-class to one-class
Write out the one-class data
%% Command-line driver
""
plot_bounding_boxes.py
""
Takes a .json database containing bounding boxes and renders those boxes on the
source images.
""
"This assumes annotations in coco-camera-traps format, with absolute bbox"
coordinates.
""
%% Imports and environment
How many images should we process?  Set to -1 to process all images.
Should we randomize the image order?
%%  Read database and build up convenience mappings
Category ID to category name
Image ID to image info
Image ID to image path
Image ID to all annotations referring to this image
"%% Iterate over images, draw bounding boxes, write to file"
For each image
image = images[0]
Build up a list of bounding boxes to draw on this image
Load the image
Create figure and axes
Display the image
ann = imageAnnotations[0]
"For each annotation associated with this image, render bounding box and label"
"In the Rectangle() function, the first argument (""location"") is the bottom-left"
of the rectangle.
""
Origin is the upper-left of the image.
Add the patch to the Axes
Add a class label
This is magic goop that removes whitespace around image plots (sort of)
Write the output image
...for each image
""
plot_imerit_annotations.py
""
Takes a .json file full of bounding box annotations and renders those boxes on the
source images.
""
"This assumes annotations in the format we receive them, specifically:"
""
1) Relative bbox coordinates
"2) A list of .json objects, not a well-formatted .json file"
""
"I.e., don't use this on a COCO-style .json file.  See plot_bounding_boxes.py"
for the same operation performed on a proper COCO-camera-traps database.
""
%% Imports and environment
"os.makedirs(outputBase, exist_ok=True)"
%%  Read all source images and build up a hash table from image name to full path
"This spans training and validation directories, so it's not the same as"
just joining the image name to a base path
"%% Iterate over annotations, draw bounding boxes, write to file"
annData has keys:
""
"annotations, categories, images"
""
Each of these are lists of dictionaries
%% Render all annotations on each image in the sequence
%% Pull out image metadata
Build up a list of bounding boxes to draw on this image
Pull out just the image name from the filename
""
File names look like:
""
seq6efffac2-5567-11e8-b3fe-dca9047ef277.frame1.img59a94e52-23d2-11e8-a6a3-ec086b02610b.jpg
"m = re.findall(r'img(.*\.jpg)$', imgFileName, re.M|re.I)"
print(m)
assert(len(m) == 1)
queryFileName = m[0]
Map this image back to the original directory
"%% Loop over annotations, find annotations that match this image"
%%
"x,y,w,h"
""
"x,y is the bottom-left of the rectangle"
""
"x,y origin is the upper-left"
...for each annotation
%% Render with PIL (scrap)
%% Render with Matplotlib
Create figure and axes
Display the image
Location is the bottom-left of the rect
""
Origin is the upper-left
Add the patch to the Axes
This is magic goop that removes whitespace around image plots (sort of)
os.startfile(outputFileName)
%% Showing figures on-screen during debugging
plt.show()
Various (mostly unsuccessful) approaches to getting the plot window to show up
"in the foreground, which is a backend-specific operation..."
""
fig.canvas.manager.window.activateWindow()
fig.canvas.manager.window.raise_()
fm = plt.get_current_fig_manager()
"fm.window.attributes('-topmost', 1)"
"fm.window.attributes('-topmost', 0)"
""
# This is the one that I found to be most robust... at like 80% robust.
plt.get_current_fig_manager().window.raise_()
%%
...for each image
...for each file
""
convert_imerit_json_to_coco_json.py
""
"Takes a .json file with bounding boxes but no class labels, and a .json file containing the"
"class labels for those images, and creates a new json file with class labels and bounding"
boxes.
""
"The bounding box .json file is in the format returned by our annotators, which is not"
actually a fully-formed .json file; rather it's a series of .json objects
""
"Leaves filenames intact.  Stores imerit ""category IDs"" (empty/human/group/animal) in a new"
"field called ""annotation_type""."
""
%% Imports and constants
%% Configure files and paths
"For Snapshot Serengeti, we stored image IDs in our annotation files as:"
""
S1_B06_R1_PICT0008
""
...but the corresponding ID in the master database is actually:
""
S1\B06\R1\S1_B06_R1_PICT0008
""
"If this is ""True"", we'll expand the former to the latter"
Handling a one-off issue in which .'s were mysteriously replaced with -'s
"in our annotations.  This will be set dynamically, but I keep it here as"
a constant to remind me to remove this code when we clean this issue up.
Used in the (rare) case where a bounding box was added to an image that was originally
annotated as empty
Used in the (rare) case where we added bounding boxes to an image with multiple species
"Should we include ambiguous bounding boxes (with the ""ambiguous"" category label)"
in the output file?  Ambiguous boxes are boxes drawn on images with multiple species.
"%%  Read metadata from the master database, bounding boxes from the annotations file"
"The bounding box .json file is in the format returned by our annotators, which is not"
actually a fully-formed .json file; rather it's a series of .json objects
"Each element of annData is a dictionary corresponding to a single sequence, with keys:"
""
"annotations, categories, images"
sequence = annData[0]
%% Build convenience mappings
Image ID to images
Category ID to categories (referring to the database categories)
"Image ID to categories (i.e., species labels)"
Utility function we'll use to create annotations for images in empty
sequences (empty images in non-empty sequences already have annotations)
"%% Reformat annotations, grabbing category IDs from the master database (prep)"
iSequence = 0; sequence = annData[0]
"%% Reformat annotations, grabbing category IDs from the master database (loop)"
Make a copy here; we're going to manipulate the sequence annotations
when we need to add synthetic annotations for empty images
im = sequenceImages[0]
Are there any annotations in this sequence?
Which images in this sequence have annotations?
For each image in this sequence...
imeritImageID = im['id']
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG
Confirm that the file exists
Hande a one-off issue with our annotations
datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG
""
...had become:
""
datasetsnapshotserengeti.seqASG000001a-frame0.imgS1_B06_R1_PICT0008.JPG
Does it look like we encountered this issue?
Convert:
""
S1_B06_R1_PICT0008
""
...to:
""
S1/B06/B06_R1/S1_B06_R1_PICT0008
Make sure we only see each image once
Create empty annotations for empty images
""
Here we use the *unmodified* file name
Create an empty annotation for this image
Annotations still use the annotation filename (not database ID) at this point;
these will get converted to database IDs below when we process the
whole sequence.
Sanity-check image size
"print('Warning: img {} was listed in DB as {}x{}, annotated as {}x{}, actual size{}x{}'.format("
"old_id,new_im['width'],new_im['height'],im['width'],im['height'],imgObj.width,imgObj.height))"
...for each image in this sequence
For each annotation in this sequence...
ann = sequenceAnnotations[0]
Prepare an annotation using the category ID from the database and
the bounding box from the annotations file
Maintain iMerit's annotation category
Generate an (arbitrary) ID for this annotation; the COCO format has a concept
"of annotation ID, but our annotation files don't"
This was a one-off quirk with our file naming
We'll do special handling of images with multiple categories later
Store the annotation type (group/human/animal/empty)
This annotation has no bounding box but the image wasn't originally
annotated as empty
This annotation has a bounding box but the image was originally
annotated as empty
unnormalize the bbox
... for each annotation in this sequence
... for each sequence
%% Post-processing
Count empty images
...for each file
%% Sanity-check empty images
""
make_ss_annotation_image_folder.py
""
Take a directory full of images with the very long filenames we give annotators:
""
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension
""
"...along with a COCO-camera-traps database referring to those files, and:"
""
1) Creates a new COCO-camera-traps database with the original filenames in them
(copying the annotations)
""
2) Optionally creates a new directory with those images named according to the
"Snapshot Serengeti naming convention, including complete relative paths."
""
See convert_imerit_json_to_coco_json to see how we get from the original annotation
.json to a COCO-camera-traps database.
""
%% Constants and imports
%% Configure files/paths
%% Read the annotations (referring to the old filenames)
"%% Update filenames, optionally copying files"
im = data['images'][0]
For each image...
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG
"Find the image name, e.g. S1_B06_R1_PICT0008"
Convert:
""
S1_B06_R1_PICT0008.JPG
""
...to:
""
S1/B06/B06_R1/S1_B06_R1_PICT0008.JPG
...for each image
%% Write the revised database
""
get_annotation_tool_link.py
""
"Takes a COCO-camera-traps-style .json file with URLs already embedded, and prepares"
a link to the visipedia annotation tool that reviews a subset of those images.
""
""
create_new_annotation_json.py
""
"Creates a subset of a larger .json database, in this case specifically to pick some images"
from Snapshot Serengeti.
""
from utils import get_db_dicts
for seq in already_annotated:
seq_to_ims.pop(seq)
remove already annotated images
add lion images
lion_seqs = cat_to_seqs[cat_to_id['lionMale']] + cat_to_seqs[cat_to_id['lionFemale']]
#print(len(lion_seqs))
lion_seqs = [seq for seq in lion_seqs if seq not in already_annotated]
#print(len(lion_seqs))
"lion_seqs_to_annotate.extend(random.sample(lion_seqs, min(len(lion_seqs),num_lions)))"
print(len(lion_seqs_to_annotate))
num_elephants = 1000
elephant_seqs = cat_to_seqs[cat_to_id['elephant']]
#print(len(lion_seqs))
elephant_seqs = [seq for seq in elephant_seqs if seq not in already_annotated]
#print(len(lion_seqs))
"elephant_seqs_to_annotate = random.sample(elephant_seqs, num_elephants)"
num_empty = 10
empty_seqs_to_annotate = []
for loc in loc_to_seqs:
empty_seqs = cats_per_location[loc][cat_to_id['empty']]
empty_seqs = [seq for seq in empty_seqs if seq not in already_annotated]
empty_seqs = [seq for seq in empty_seqs if seq_to_season[seq] in seasons_to_keep]
"empty_seqs_to_annotate.extend(random.sample(empty_seqs, min(len(empty_seqs),num_empty)))"
ims_to_annotate.extend(empty_ims_to_annotate)
""
filter_database.py
""
"Look through a COCO-ct database and find images matching some crtieria, writing"
a subset of images and annotations to a new file.
""
%% Constants and imports
%% Configuration
%%  Read database and build up convenience mappings
Category ID to category name
Image ID to image info
Image ID to image path
%% Filter
ann = annotations[0]
Is this a tiny box or a group annotation?
"x,y,w,h"
All empty annotations should be classified as either empty or ambiguous
""
"The ambiguous cases are basically minor misses on the annotators' part,"
where two different small animals were present somewhere.
%% Write output file
""
find_images_for_classes.py
""
"Given a .json database, find images that are associated with one or more"
classes.
""
%% Constants and imports
%% Configuration
%%  Read database and build up convenience mappings
Category ID to category name
Image ID to image info
Image ID to image path
%% Look for target-class annotations
ann = annotations[0]
""
create_tfrecords_format.py
""
This script creates a tfrecords file from a dataset in VOTT format.
%% Imports and environment
set up the filenames and annotations
This loop reads the bboxes and corresponding labels and assigns them
the correct image. Kind of slow at the moment...
If needed: merging all classes
bbox_labels = ['Animal' for _ in bbox_labels]
BBox coords are stored in the format
"x_min (of width axis) y_min (of height axis), x_max, y_max"
Coordinate system starts in top left corner
"In this framework, we need ('ymin', 'xmin', 'ymax', 'xmax') format"
print out some stats
To make sure we loaded the bboxes correctly:
self.validate_bboxes()
For each image in the data set...
Make sure all are greater equal 0
%% Main tfrecord generation function
Propagate optional metadata to tfrecords
endfor each annotation for the current image
endfor each image
"Calculate number of shards to get the desired number of images per record,"
ensure it is evenly divisible by the number of threads
%% Command-line driver
%% Driver
""
make_tfrecords_cis_trans.py
""
Given a .json file that contains a three-element list (train/val/test) of image IDs and a .json database that contains
"those image IDs, generates tfrecords whose filenames include ""train""/""val""/""test"""
""
"dataset = json.load(open('/ai4efs/databases/snapshotserengeti/oneclass/SnapshotSerengeti_Seasons_1_to_4_tfrecord_format_valid_ims.json','r'))"
print('Creating trans_val tfrecords')
dataset = [im_id_to_im[idx] for idx in trans_val]
""
create_classification_tfrecords_from_json.py
""
Called from make_tfrecords_cis_trans.py if you're running a classification experiment.
""
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/'
datafile = 'eccv_18_annotation_files_oneclass/CaltechCameraTrapsECCV18'
image_file_root = datafolder+'eccv_18_all_images/'
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/'
datafolder = '/data/iwildcam/'
datafile = 'combined_iwildcam_annotations_oneclass/eccv_train_and_imerit_2'
need consecutive category ids
old_cat_id_to_new_cat_id = {categories[idx]['id']:idx+1 for idx in range(len(categories))}
print(old_cat_id_to_new_cat_id)
remove multiclass images
print(images[0])
print(vis_data[0])
""
make_tfrecords_from_json.py
""
"Given a coco-camera-traps .json file, creates tfrecords"
""
Thin wrapper for create_tfrecords_from_json.
""
%% Constants and imports
%% Constants and imports (interactive)
%%
%% Main tfrecord generation function
"check whether the input file has already been converted to the tfrecords format,"
"if not, convert"
"Calculate number of shards to get the desired number of images per record,"
ensure it is evenly divisible by the number of threads
%% Interactive driver
%%
%% Command-line driver
eMammal_make_splits.py
""
"Based on a tfrecords_format json file of the database, creates 3 splits according to"
the specified fractions based on location (images from the same location should be in
one split) or based on images.
""
"If a previous split is provided (append_to_previous_split is True), the entries in"
"each split will be preserved, and new entries will be appended, so that new models"
can warm start with a model trained on the original splits.
configurations and paths
approximate fraction for the new entries
read in the previous splits of image ID or location ID if available
"find new locations and assign them to a split, without reassigning any previous locations"
"find out which images are new, shuffle and split them"
do NOT sort the IDs to keep the shuffled order
export PYTHONPATH=$PYTHONPATH:tfmodels/research
add empty category
"add all images that don't have annotations, with cat empty"
print(detection_results.keys())
group the detections and gts by image id:
print(image_id)
print(len(scores))
print(len(labels))
recall_thresh = 0.9
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall])
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])"
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall])
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])"
exp_name = 'eccv_train'
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json'
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(len(detection_results['images']))
print(len(seqs))
"print(len(seqs[0]),len(seqs["
print(detection_results.keys())
group the detections by image id:
group the ground truth annotations by image id:
print(seq)
"for image_id, dets in per_image_detections.iteritems():"
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
print(num_gts)
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
if sum(detected_class_labels)>0:
print('detected animal box')
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])"
"print(scores, tp_fp_labels)"
num_total_gts += num_gts
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
print('valid box')
"print(best_im, best_score)"
exp_name = 'eccv_train'
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json'
export PYTHONPATH=$PYTHONPATH:tfmodels/research
need to loop over confidence values
"for each value, check if any detections on the image are > conf"
"If so, that image gets class ""animal"""
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0"
"for image_id, dets in per_image_detections.iteritems():"
"calc prec, rec for this confidence thresh"
group the ground truth annotations by image id:
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(detection_results.keys())
group the detections and gts by image id:
print(image_id)
print(len(scores))
print(len(labels))
recall_thresh = 0.9
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall])
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])"
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall])
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])"
print(sorted_ap)
plt.bar(sorted_ap)
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(detection_results.keys())
group the detections by image id:
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
print(gts)
print(num_gts)
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[:2])"
"print(scores, tp_fp_labels)"
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
export PYTHONPATH=$PYTHONPATH:tfmodels/research
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
print('detected animal box')
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])"
"print(scores, tp_fp_labels)"
num_total_gts += num_gts
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
print('valid box')
best_im = np.argmax(max_im_scores)
"print(best_im, best_score)"
"temp_labels = np.zeros(len(im_detection_labels),  dtype=np.int32)"
"temp_scores = np.zeros(len(im_detection_scores), dtype=np.float32)"
"for j in range(min(im_num_gts, len(im_detection_labels))):"
temp_labels[j] = True #TODO: this currently only works for oneclass?
temp_scores[j] = best_score
im_detection_labels = temp_labels
im_detection_scores = temp_scores
num_total_gts+=im_num_gts
"print(len(detection_scores), len(detection_scores[0]), len(detection_scores[1]))"
exp_name = 'small_balanced_cct'
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(seq)
"for image_id, dets in per_image_detections.iteritems():"
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
seq_num_gts.append(num_gts)
print(num_gts)
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
num_total_gts += 1
print('valid box')
"print(best_im, best_score)"
print('no valid box')
if sum(seq_num_gts)>0:
export PYTHONPATH=$PYTHONPATH:tfmodels/research
need to loop over confidence values
"for each value, check if any detections on the image are > conf"
"If so, that image gets class ""animal"""
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0"
"calc prec, rec for this confidence thresh"
export PYTHONPATH=$PYTHONPATH:tfmodels/research
group the detections by image id:
group the ground truth annotations by image id:
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
print('detected animal box')
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])"
"print(scores, tp_fp_labels)"
num_total_gts += num_gts
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
print('valid box')
"print(best_im, best_score)"
""
evaluate_detections.py
""
Adapted from analyze_detection.py which is now archived.
""
%% Imports and constants
%% Functions
"labels input to compute_object_detection_metrics() needs to start at 0, not 1"
num_detections = len(dets['boxes'])
to prevent 'Invalid dimensions for box data.' error
this box will not match any detections
compute one-class precision/recall/average precision (if every box is just of an object class)
%% Command-line driver
""
detection_eval_utils.py
""
Utility functions used in evaluate_detections.py
""
group the ground truth annotations by image id
#####
""
detect_and_predict_image.py
""
"Functions to load a TensorFlow detection and a classification model, run inference,"
"render bounding boxes on images, and write out the resulting"
images (with bounding boxes and classes).
""
"See the ""test driver"" cell for example invocation."
""
""
#####
"%% Constants, imports, environment"
Minimum detection confidence for showing a bounding box on the output image
Stick this into filenames before the extension for the rendered result
Number of top-scoring classes to show at each bounding box
%% Core detection functions
Load images if they're not already numpy arrays
iImage = 0; image = images[iImage]
"Load the image as an nparray of size h,w,nChannels"
"There was a time when I was loading with PIL and switched to mpimg,"
"but I can't remember why, and converting to RGB is a very good reason"
"to load with PIL, since mpimg doesn't give any indication of color"
"order, which basically breaks all .png files."
""
"So if you find a bug related to using PIL, update this comment"
"to indicate what it was, but also disable .png support."
image = mpimg.imread(image)
This shouldn't be necessarily when loading with PIL and converting to RGB
Actual detection
...for each image
"Currently ""boxes"" is a list of length nImages, where each element is shaped as"
""
"1,nDetections,4"
""
"This implicitly banks on TF giving us back a fixed number of boxes, let's assert on this"
to make sure this doesn't silently break in the future.
iBox = 0; box = boxes[iBox]
"""scores"" is a length-nImages list of elements with size 1,nDetections"
"""classes"" is a length-nImages list of elements with size 1,nDetections"
""
"Still as floats, but really representing ints"
Squeeze out the empty axis
boxes is nImages x nDetections x 4
scores and classes are both nImages x nDetections
Get input and output tensors of classification model
"imsize = cur_image['width'], cur_image['height']"
Select detections with a confidence larger 0.5
Get these boxes and convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes"
"However, we need to make sure that it box coordinates are still within the image"
For convenience:
Create an array with contains the index of the corresponding crop_box for each selected box
i.e. [False False 0 False 1 2 3 False False]
For each box
If this box should be classified
Run inference
if box should not be classified
...for each box
species_scores should have shape len(images) x len(boxes) x num_species
...for each image
...with tf.Session
with classification_graph
species_scores should have shape len(images) x len(boxes) x num_species
%% Rendering functions
Display the image
plt.show()
"top, left, bottom, right"
""
"x,y origin is the upper-left"
Generate bounding box text
Choose color based on class
Location is the bottom-left of the rect
""
Origin is the upper-left
iRight = x + w
iTop = y + h
Add the patch to the Axes
Add class description
First determine best location by finding the corner that is closest to the image center
relative corner coordinates
relative coordinates of image center
Compute pair-wise squared distance and get the index of the one with minimal distance
Get the corresponding coordinates ...
... and alignment for the text box
Plot the text box with background
...for each box
This is magic goop that removes whitespace around image plots (sort of)
"plt.savefig(outputFileName, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)"
os.startfile(outputFileName)
...for each image
...def render_bounding_boxes
Load and run detector on target images
Read the name of all classes
remove empty lines
%% Interactive driver
%%
%%
%% File helper functions
%% Command-line driver
Hack to avoid running on already-detected images
""
"Script for evaluating a frozen graph given a COCO-style dataset, which was generated by"
the ../data_management/databases/classification/make_classification_dataset.py script.
This is useful for debugging as the accuracy reported by this script should match the
accuracy reported by the Tensorflow training.
""
Check that all files exists for easier debugging
Load frozen graph
Collect tensors for input and output
Read image
Run inference
Check if correct
""
Script for evaluating precision/recall on a two-class problem given
"a frozen graph and a COCO-style dataset, which was generated by"
the ../data_management/databases/classification/make_classification_dataset.py script.
We assume the positive class is at index 0 (with a zero-based indexing)
""
Check that all files exists for easier debugging
Load frozen graph
Collect tensors for input and output
Read image
"with open(image_path, 'rb') as fi:"
"image =  sess.run(tf.image.decode_jpeg(fi.read(), channels=3))"
image = image / 255.
Run inference
predicted_class = np.argmax(predictions)
Check if correct
if coco.imgToAnns[image_id][0]['category_id'] == predicted_class:
correct = correct + 1
"In matplotlib < 1.5, plt.fill_between does not have a 'step' argument"
""
Script for selecting testing images of a COCO-style dataset generated by the script
../data_management/databases/classification/make_classification_dataset.py in a consistent
manner and predicting the class for it.
""
Mandatory parameters
Optional parameters
Validate parameters
Derived parameters
"We assume that the dataset was generated with the make_classification_dataset.py script,"
hence the images should be located in the same folder as the json
Make seletion deterministic
Load frozen graph
Get dataset information
Get classes
...and the class list corresponding to the model outputs by assuming
that they are in order of their ids
Get images of each class
Shuffle the image list
Start the image sampling
"Set of avaiable class IDs, will be filled below"
If there are still images left for that class
"Get image for the sampled class, we already shuffled the class images before so"
we can simply pop()
Start prediction
Collect tensors for input and output
Read image
Run inference
Print output to log file
#####
""
api_apply_classifier_single_node.py
""
Takes the JSON file produced by the detection API and
classifies all boxes above a confidence threshold.
""
#####
"%% Constants, imports, environment"
Assumes that the root of the CameraTraps repo is on the PYTHONPATH
Minimum detection confidence for classifying an object
Number of top-scoring classes to show at each bounding box
Enlargment factor applied to boxes before passing them to the classifier
""
Provides more context and can lead to better results
List of detection categories for which we will run the classification
""
"Currently there are {""1"": ""animal"", ""2"": ""person"", ""4"": ""vehicle""}"
""
Should be a list of string-formatted ints.
Number of significant float digits in JSON output
%% Core detection functions
Read the name of all classes
remove empty lines
Create field with name *classification_categories*
Add classes using 0-based indexing
def add_classification_categories
Make sure we have the right json object
Get input and output tensors of classification model
For each image
Read image
"Scale pixel values to [0,1]"
For each box
Skip detections with low confidence
Skip if detection category is not in whitelist
Skip if already classified
"Get current box in relative coordinates and format [x_min, y_min, width_of_box, height_of_box]"
"Convert to [ymin, xmin, ymax, xmax] and store it as 1x4 numpy array so we can"
re-use the generic multi-box padding code
Convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes."
""
"However, we need to make sure that it box coordinates are still within the image."
Get the first (and only) row as our bbox to classify
Get the image data for that box
Run inference
Add an empty list to the json for our predictions
Add the *num_annotated_classes* top scoring classes
...for each box
...for each image
...with tf.Session
with classification_graph
def classify_boxes
Load classification model
Load detector json
Add classes to detector_json
"Run classifier on all images, changes will be writting directly to the json"
Write output json
def load_and_run_classifier
%% Command-line driver
""
Script for selecting testing images of a COCO-style dataset generated by the script
../data_management/databases/classification/make_classification_dataset.py in a consistent
manner and predicting the class for it.
""
Assumes the cameratraps repo root is on the path
Make seletion deterministic
Mandatory parameters
Optional parameters
Validate parameters
Tranfer parameters to post-processing format
Load frozen graph
Reading image list
Reading class list
Image sampling
Start prediction
Collect tensors for input and output
Array for collecting infos for rendering the html
Read image
"Scale pixel values to [0,1]"
Run inference
Add links to all available classes
""
predict_image.py
""
"Given a pointer to a frozen detection graph, runs inference on a single image,"
printing the top classes to the console
""
%% Imports
%% Command-line processing
Check that all files exist for easier debugging
%% Inference
Load frozen graph
Load class list
Remove empty lines
Collect tensors for input and output
Read image
Run inference
Print output
""
Mostly unmodified script for freezing a model
Added for convenience and for possible future optimizations
""
Copyright 2015 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
'input_checkpoint' may be a prefix if we're using Saver V2 format
Remove all the explicit device specifications for this node. This helps to
make the graph more portable.
List of all partition variables. Because the condition is heuristic
"based, the list could include false positives."
This tensor doesn't exist in the graph (for example it's
'global_step' or a similar housekeeping element) so skip it.
`var_list` is required to be a map of variable names to Variable
tensors. Partition variables are Identity tensors that cannot be
handled by Saver.
Models that have been frozen previously do not contain Variables.
optimized_output_graph_def = optimize_for_inference_lib.optimize_for_inference(
"output_graph_def,"
"input_node_names.replace("" "", """").split("",""),"
"output_node_names.replace("" "", """").split("",""),"
tf.float32.as_datatype_enum)
Write GraphDef to file if output path has been given.
""
"Creates a graph description, which is required to create a frozen graph."
Adapted from from ./tf-slim/export_inference_graph.py
Added preprocessing to the definition for easier handling
""
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
tf.app.flags.DEFINE_integer(
"'batch_size', None,"
"'Batch size for the exported model. Defaulted to ""None"" so batch size can '"
'be specified at model runtime.')
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
#####################
Optimization Flags #
#####################
######################
Learning Rate Flags #
######################
######################
Dataset Flags #
######################
####################
Fine-Tuning Flags #
####################
"Note: when num_clones is > 1, this will actually have each clone to go"
over each epoch FLAGS.num_epochs_per_decay times. This is different
behavior from sync replicas and is expected to produce different results.
Warn the user if a checkpoint exists in the train_dir. Then we'll be
ignoring the checkpoint anyway.
TODO(sguada) variables.filter_variables()
######################
Config model_deploy #
######################
Create global_step
#####################
Select the dataset #
#####################
#####################
Select the network #
#####################
####################################
Select the preprocessing function #
####################################
#############################################################
Create a dataset provider that loads data from the dataset #
#############################################################
###################
Define the model #
###################
############################
Specify the loss function #
############################
Gather initial summaries.
"Gather update_ops from the first clone. These contain, for example,"
the updates for the batch_norm variables created by network_fn.
Add summaries for end_points.
Add summaries for losses.
Add summaries for variables.
################################
Configure the moving averages #
################################
########################################
Configure the optimization procedure. #
########################################
"If sync_replicas is enabled, the averaging will be done in the chief"
queue runner.
Update ops executed locally by trainer.
Variables to train.
and returns a train_tensor and summary_op
Add total_loss to summary.
Create gradient updates.
Add the summaries from the first clone. These contain the summaries
created by model_fn and either optimize_clones() or _gather_clone_loss().
Merge all summaries together.
##########################
Kicks off the training. #
##########################
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
#####################
Select the dataset #
#####################
###################
Select the model #
###################
#############################################################
Create a dataset provider that loads data from the dataset #
#############################################################
####################################
Select the preprocessing function #
####################################
###################
Define the model #
###################
Define the metrics:
Print the summaries to screen.
TODO(sguada) use num_epochs=1
This ensures that we make a single pass over all of the data.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Transform the image to floats.
"Randomly crop a [height, width] section of the image."
Randomly flip the image horizontally.
"Because these operations are not commutative, consider randomizing"
the order their operation.
Subtract off the mean and divide by the variance of the pixels.
Transform the image to floats.
Resize and crop if needed.
Subtract off the mean and divide by the variance of the pixels.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Use tf.slice instead of crop_to_bounding box as it accepts tensors to
define the crop size.
Compute the rank assertions.
Create a random bounding box.
""
Use tf.random_uniform and not numpy.random.rand as doing the former would
"generate random numbers at graph eval time, unlike the latter which"
generates random numbers at graph definition time.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Pass the real x only to one of the func calls.
The random_* ops do not necessarily clamp.
"Each bounding box has shape [1, num_boxes, box coords] and"
"the coordinates are ordered [ymin, xmin, ymax, xmax]."
A large fraction of image datasets contain a human-annotated bounding
box delineating the region of the image containing the object of interest.
We choose to create a new bounding box for the object which is a randomly
distorted version of the human-annotated bounding box that obeys an
"allowed range of aspect ratios, sizes and overlap with the human-annotated"
"bounding box. If no box is supplied, then we assume the bounding box is"
the entire image.
Crop the image to the specified bounding box.
"Each bounding box has shape [1, num_boxes, box coords] and"
"the coordinates are ordered [ymin, xmin, ymax, xmax]."
Restore the shape since the dynamic slice based upon the bbox_size loses
the third dimension.
This resizing operation may distort the images because the aspect
ratio is not respected. We select a resize method in a round robin
fashion based on the thread number.
Note that ResizeMethod contains 4 enumerated resizing methods.
We select only 1 case for fast_mode bilinear.
Randomly flip the image horizontally.
Randomly distort the colors. There are 1 or 4 ways to do it.
Crop the central region of the image with an area containing 87.5% of
the original image.
Resize the image to the specified height and width.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(sguada) docstring paragraph by (a) motivating the need for the file and
(b) defining clones.
TODO(sguada) describe the high-level components of model deployment.
"E.g. ""each model deployment is composed of several parts: a DeploymentConfig,"
"which captures A, B and C, an input_fn which loads data.. etc"
Set up DeploymentConfig
Create the global step on the device storing the variables.
Define the inputs
Define the optimizer.
Define the model including the loss.
Run training.
Namedtuple used to represent a clone during deployment.
"Namedtuple used to represent a DeployedModel, returned by deploy()."
Default parameters for DeploymentConfig
Create clones.
The return value.
Individual components of the loss that will need summaries.
Compute and aggregate losses on the clone device.
Add the summaries out of the clone device block.
Only use regularization_losses for the first clone
Compute the total_loss summing all the clones_losses.
Sum the gradients across clones.
Gather initial summaries.
Create Clones.
"Gather update_ops from the first clone. These contain, for example,"
the updates for the batch_norm variables created by model_fn.
Place the global step on the device storing the variables.
Compute the gradients for the clones.
Add summaries to the gradients.
Create gradient updates.
Only use regularization_losses for the first clone
Add the summaries from the first clone. These contain the summaries
created by model_fn and either optimize_clones() or _gather_clone_loss().
Add total_loss to summary.
Merge all summaries together.
Note that each grad_and_vars looks like the following:
"((grad_var0_clone0, var0), ... (grad_varN_cloneN, varN))"
!/usr/bin/python
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Failed to find ""index"" occurrence of item."
pylint: disable=broad-except
pylint: enable=broad-except
Grab the 'index' annotation.
Some images contain bounding box annotations that
"extend outside of the supplied image. See, e.g."
n03127925/n03127925_147.xml
"Additionally, for some bounding boxes, the min > max"
or the box is entirely outside of the image.
Example: <...>/n06470073/n00141669_6790.xml
Determine if the annotation is from an ImageNet Challenge label.
Note: There is a slight bug in the bounding box annotation data.
Many of the dog labels have the human label 'Scottish_deerhound'
instead of the synset ID 'n02092002' in the bbox.label field. As a
"simple hack to overcome this issue, we only exclude bbox labels"
*which are synset ID's* that do not match original synset label for
the XML file.
Guard against improperly specified boxes.
Note bbox.filename occasionally contains '%s' in the name. This is
data set noise that is fixed by just using the basename of the XML file.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The URLs where the MNIST data can be downloaded.
The names of the classes.
"First, process the training data:"
"Next, process the testing data:"
"Finally, write the labels file:"
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The URL where the Flowers data can be downloaded.
The number of images in the validation set.
Seed for repeatability.
The number of shards per dataset split.
Initializes function that decodes RGB JPEG data.
Read the filename:
Divide into train and test:
"First, convert the training and validation sets."
"Finally, write the labels file:"
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(nsilberman): Add tfrecord file type once the script is updated.
"If set to false, will not try to set label_to_names in dataset"
by reading them from labels.txt or github.
n01440764
n01443537
n02119247    black fox
n02119359    silver fox
pylint: disable=g-line-too-long
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The URL where the CIFAR data can be downloaded.
The number of training files.
The height and width of each image.
The names of the classes.
"First, process the training data:"
"Next, process the testing data:"
"Finally, write the labels file:"
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The labels file contains a list of valid labels are held in this file.
Assumes that the file contains entries as such:
n01440764
n01443537
n01484850
where each line corresponds to a label expressed as a synset. We map
each synset contained in the file to an integer (based on the alphabetical
ordering). See below for details.
This file containing mapping from synset to human-readable label.
Assumes each line of the file looks like:
""
n02119247    black fox
n02119359    silver fox
"n02119477    red fox, Vulpes fulva"
""
where each line corresponds to a unique mapping. Note that each line is
formatted as <synset>\t<human readable label>.
This file is the output of process_bounding_box.py
Assumes each line of the file looks like:
""
"n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940"
""
where each line corresponds to one bounding box annotation associated
with an image. Each line can be parsed as:
""
"<JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>"
""
Note that there might exist mulitple bounding box annotations associated
with an image file.
pylint: disable=expression-not-assigned
pylint: enable=expression-not-assigned
Create a single Session to run all image coding calls.
Initializes function that converts PNG to JPEG data.
Initializes function that converts CMYK JPEG data to RGB JPEG data.
Initializes function that decodes RGB JPEG data.
File list from:
https://groups.google.com/forum/embed/?place=forum/torch7#!topic/torch7/fOSTXHIESSU
File list from:
https://github.com/cytsai/ilsvrc-cmyk-image-list
Read the image file.
Clean the dirty data.
1 image is a PNG.
22 JPEG images are in CMYK colorspace.
Decode the RGB JPEG.
Check that image converted to RGB
Each thread produces N shards where N = int(num_shards / num_threads).
"For instance, if num_shards = 128, and the num_threads = 2, then the first"
"thread would produce shards [0, 64)."
"Generate a sharded version of the file name, e.g. 'train-00002-of-00010'"
"Break all images into batches with a [ranges[i][0], ranges[i][1]]."
Launch a thread for each batch.
Create a mechanism for monitoring when all threads are finished.
Create a generic TensorFlow-based utility for converting all image codings.
Wait for all the threads to terminate.
Leave label index 0 empty as a background class.
Construct the list of JPEG files and labels.
Shuffle the ordering of all image files in order to guarantee
random ordering of the images with respect to label in the
saved TFRecord files. Make the randomization repeatable.
Build a map from synset to human-readable label.
Run it!
Allowing None in the signature so that dataset_factory can use the default.
!/usr/bin/python
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Read in the 50000 synsets associated with the validation data set.
Make all sub-directories in the validation data dir.
Move all of the image to the appropriate sub-directory.
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
"The dataset has classes with no images (empty and human), which have ID 0 and 1,"
so we need to specify 49 here despite having only 47 classes with images
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Final pooling and prediction
Global average pooling.
Pooling with a fixed kernel size.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The current_stride variable keeps track of the effective stride of the
activations. This allows us to invoke atrous convolution whenever applying
the next residual unit would result in the activations having stride larger
than the target output_stride.
The atrous convolution rate parameter.
Move stride from the block's last unit to the end of the block.
"If we have reached the target output_stride, then we need to employ"
atrous convolution with stride=1 and multiply the atrous rate by the
current unit's stride for use in subsequent layers.
Collect activations at the block's end before performing subsampling.
Subsampling of the block's output activations.
"The following implies padding='SAME' for pool1, which makes feature"
alignment easier for dense prediction tasks. This is also used in
https://github.com/facebook/fb.resnet.torch. However the accompanying
code of 'Deep Residual Learning for Image Recognition' uses
padding='VALID' for pool1. You can switch to that choice by setting
"slim.arg_scope([slim.max_pool2d], padding='VALID')."
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Collect outputs for conv2d, fully_connected and max_pool2d"
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"inputs has shape [batch, 224, 224, 3]"
"inputs has shape [batch, 513, 513, 3]"
We do not include batch normalization or activation functions in
conv1 because the first ResNet unit will perform these. Cf.
Appendix of [2].
This is needed because the pre-activation variant does not have batch
normalization or activation functions in the residual unit output. See
Appendix of [2].
Convert end_points_collection into a dictionary of end_points.
Global average pooling.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
=============================================================================
"These parameters come from the online port, which don't necessarily match"
those in the paper.
TODO(nsilberman): confirm these values with Philip.
##########
Encoder #
##########
"No normalizer for the first encoder layers as per 'Image-to-Image',"
Section 5.1.1
First layer doesn't use normalizer_fn
Last layer doesn't use activation_fn nor normalizer_fn
##########
Decoder #
##########
"Dropout is used at both train and test time as per 'Image-to-Image',"
Section 2.1 (last paragraph).
The Relu comes BEFORE the upsample op:
Explicitly set the normalizer_fn to None to override any default value
"that may come from an arg_scope, such as pix2pix_arg_scope."
No normalization on the input layer.
Stride 1 on the last layer.
"1-dim logits, stride 1, no activation, no normalization."
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
Alias
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
pylint: disable=unused-import
pylint: enable=unused-import
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
If we are fine tuning a checkpoint we need to start at a lower learning
rate since we are farther along on training.
We can start quantizing immediately if we are finetuning.
We need to wait for the model to train a bit before we quantize if we are
training from scratch.
Call rewriter to produce graph with fake quant ops and folded batch norms
"quant_delay delays start of quantization till quant_delay steps, allowing"
for better model accuracy.
Configure the learning rate using an exponential decay.
"When restoring from a floating point model, the min/max values for"
quantized weights and activations are not present.
We instruct slim to ignore variables that are missing during restoration
by setting ignore_missing_vars=True
"If we are restoring from a floating point model, we need to initialize"
the global step to zero for the exponential decay to result in
reasonable learning rates.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Orignaly, arg_scope = slim.arg_scope and layers = slim, now switch to more"
update-to-date tf.contrib.* API.
Decay for the moving averages.
epsilon to prevent 0s in variance.
Turns off fused batch norm.
collection containing the moving mean and moving variance.
"For inverted pyramid models, we start with gating switched off."
batch_size x 32 x 112 x 112 x 64
Separable conv is slow when used at first conv layer.
batch_size x 32 x 56 x 56 x 64
batch_size x 32 x 56 x 56 x 64
batch_size x 32 x 56 x 56 x 192
batch_size x 32 x 28 x 28 x 192
batch_size x 32 x 28 x 28 x 256
batch_size x 16 x 14 x 14 x 512
batch_size x 16 x 14 x 14 x 512
batch_size x 16 x 14 x 14 x 512
batch_size x 16 x 14 x 14 x 528
batch_size x 16 x 14 x 14 x 832
batch_size x 8 x 7 x 7 x 832
batch_size x 8 x 7 x 7 x 1024
Final pooling and prediction
Temporal average pooling.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
299 x 299 x 3
149 x 149 x 32
147 x 147 x 32
147 x 147 x 64
73 x 73 x 160
71 x 71 x 192
35 x 35 x 384
4 x Inception-A blocks
35 x 35 x 384
Reduction-A block
17 x 17 x 1024
7 x Inception-B blocks
17 x 17 x 1024
Reduction-B block
8 x 8 x 1536
3 x Inception-C blocks
Auxiliary Head logits
17 x 17 x 1024
Final pooling and prediction
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which"
can be set to False to disable pooling here (as in resnet_*()).
8 x 8 x 1536
1 x 1 x 1536
1536
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN
setups need the gradient of gradient FusedBatchNormGrad.
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN
setups need the gradient of gradient FusedBatchNormGrad.
First upscaling is different because it takes the input vector.
Last layer has different normalizer and activation.
Convert to proper channels.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Use clip_by_value to simulate bandpass activation.
Use clip_by_value to simulate bandpass activation.
Use clip_by_value to simulate bandpass activation.
149 x 149 x 32
147 x 147 x 32
147 x 147 x 64
73 x 73 x 64
73 x 73 x 80
71 x 71 x 192
35 x 35 x 192
35 x 35 x 320
TODO(alemi): Register intermediate endpoints
"17 x 17 x 1088 if output_stride == 8,"
33 x 33 x 1088 if output_stride == 16
TODO(alemi): register intermediate endpoints
TODO(gpapan): Properly support output_stride for the rest of the net.
8 x 8 x 2080
TODO(alemi): register intermediate endpoints
8 x 8 x 1536
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which"
can be set to False to disable pooling here (as in resnet_*()).
Set weight_decay for weights in conv2d and fully_connected layers.
Set activation_fn and parameters for batch_norm.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Decay for the moving averages.
epsilon to prevent 0s in variance.
collection containing update_ops.
use fused batch norm if possible.
Set weight_decay for weights in Conv and FC layers.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"inputs has shape [batch, 224, 224, 3]"
"inputs has shape [batch, 513, 513, 3]"
Use clip_by_value to simulate bandpass activation.
Convert end_points_collection into a dictionary of end_points.
Global average pooling.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"end_points will collect relevant activations for external use, for example"
summaries or losses.
299 x 299 x 3
149 x 149 x 32
147 x 147 x 32
147 x 147 x 64
73 x 73 x 64
73 x 73 x 80.
71 x 71 x 192.
35 x 35 x 192.
Inception blocks
mixed: 35 x 35 x 256.
mixed_1: 35 x 35 x 288.
mixed_2: 35 x 35 x 288.
mixed_3: 17 x 17 x 768.
mixed4: 17 x 17 x 768.
mixed_5: 17 x 17 x 768.
mixed_6: 17 x 17 x 768.
mixed_7: 17 x 17 x 768.
mixed_8: 8 x 8 x 1280.
mixed_9: 8 x 8 x 2048.
mixed_10: 8 x 8 x 2048.
Auxiliary Head logits
Shape of feature map before the final layer.
Final pooling and prediction
Global average pooling.
Pooling with a fixed kernel size.
1 x 1 x 2048
2048
1000
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Reflection pad by 1 in spatial dimensions (axes 1, 2 = h, w) to make a 3x3"
'valid' convolution produce an output with the same dimension as the
input.
This corrects 1 pixel offset for images with even width and height.
conv2d is left aligned and conv2d_transpose is right aligned for even
sized images (while doing 'SAME' padding).
Note: This doesn't reflect actual model in paper.
Neither dropout nor batch norm -> dont need is_training
##########
Encoder #
##########
7x7 input stage
##################
Residual Blocks #
##################
##########
Decoder #
##########
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"end_points will collect relevant activations for external use, for example"
summaries or losses.
Used to find thinned depths for each layer.
Note that sizes in the comments below assume an input spatial size of
"224x224, however, the inputs can be of any size greater 32x32."
224 x 224 x 3
depthwise_multiplier here is different from depth_multiplier.
depthwise_multiplier determines the output channels of the initial
"depthwise conv (see docs for tf.nn.separable_conv2d), while"
depth_multiplier controls the # channels of the subsequent 1x1
convolution. Must have
in_channels * depthwise_multipler <= out_channels
so that the separable convolution is not overparameterized.
Use a normal convolution instead of a separable convolution.
112 x 112 x 64
56 x 56 x 64
56 x 56 x 64
56 x 56 x 192
28 x 28 x 192
Inception module.
28 x 28 x 256
28 x 28 x 320
14 x 14 x 576
14 x 14 x 576
14 x 14 x 576
14 x 14 x 576
14 x 14 x 576
7 x 7 x 1024
7 x 7 x 1024
Final pooling and prediction
Global average pooling.
Pooling with a fixed kernel size.
1 x 1 x 1024
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
=============================================================================
Tensorflow mandates these.
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture
Conv defines 3x3 convolution layers
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.
stride is the stride of the convolution
depth is the number of channels or filters in a layer
MOBILENETV1_CONV_DEFS specifies the MobileNet body
Used to find thinned depths for each layer.
The current_stride variable keeps track of the output stride of the
"activations, i.e., the running product of convolution strides up to the"
current network layer. This allows us to invoke atrous convolution
whenever applying the next convolution would result in the activations
having output stride larger than the target output_stride.
The atrous convolution rate parameter.
"If we have reached the target output_stride, then we need to employ"
atrous convolution with stride=1 and multiply the atrous rate by the
current unit's stride for use in subsequent layers.
By passing filters=None
separable_conv2d produces only a depthwise convolution layer
Global average pooling.
Pooling with a fixed kernel size.
1 x 1 x 1024
Set weight_decay for weights in Conv and DepthSepConv layers.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Decay for the moving averages.
epsilon to prevent 0s in variance.
Turns off fused batch norm.
collection containing the moving mean and moving variance.
Final pooling and prediction
Temporal average pooling.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Orignaly, add_arg_scope = slim.add_arg_scope and layers = slim, now switch to"
more update-to-date tf.contrib.* API.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Make sure that round down does not go down by more than 10%.
"Note: want to round down, we adjust each split to match the total."
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts."
which provide numbered scopes.
b1 -> b2 * r -> b2
i -> (o * r) (bottleneck) -> o
"Note in contrast with expansion, we always have"
projection to produce the desired output size.
stride check enforces that we don't add residuals when spatial
dimensions are None
Depth matches
Don't do any splitting if we end up with less than 8 filters
on either side.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Make sure that round down does not go down by more than 10%.
Set conv defs defaults and overrides.
a) Set the tensorflow scope
b) set padding to default: note we might consider removing this
since it is also set by mobilenet_scope
c) set all defaults
d) set all extra overrides.
The current_stride variable keeps track of the output stride of the
"activations, i.e., the running product of convolution strides up to the"
current network layer. This allows us to invoke atrous convolution
whenever applying the next convolution would result in the activations
having output stride larger than the target output_stride.
The atrous convolution rate parameter.
Insert default parameters before the base scope which includes
any custom overrides set in mobilenet.
"If we have reached the target output_stride, then we need to employ"
atrous convolution with stride=1 and multiply the atrous rate by the
current unit's stride for use in subsequent layers.
Update params.
Only insert rate to params if rate > 1.
Set padding
Add all tensors that end with 'output' to
endpoints
1 x 1 x num_classes
Note: legacy scope name.
"Recover output shape, for unknown shape."
the network created will be trainble with dropout/batch norm
initialized appropriately.
Note: do not introduce parameters that would change the inference
"model here (for example whether to use bias), modify conv_def instead."
Set weight_decay for weights in Conv and FC layers.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
pyformat: disable
Architecture: https://arxiv.org/abs/1801.04381
Note: these parameters of batch norm affect the architecture
that's why they are here and not in training_scope.
pyformat: enable
NB: do not set depth_args unless they are provided to avoid overriding
whatever default depth_multiplier might have thanks to arg_scope.
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that
"'finegrain_classification_mode' is set to True, which means the embedding"
layer will not be shrinked when given a depth-multiplier < 1.0.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Notes for training NASNet Cifar Model
-------------------------------------
batch_size: 32
learning rate: 0.025
cosine (single period) learning rate decay
auxiliary head loss weighting: 0.4
clip global norm of all gradients by 5
600 epochs with a batch size of 32
This is used for the drop path probabilities since it needs to increase
the drop out probability over the course of training.
Notes for training large NASNet model on ImageNet
-------------------------------------
batch size (per replica): 16
learning rate: 0.015 * 100
learning rate decay factor: 0.97
num epochs per decay: 2.4
sync sgd with 100 replicas
auxiliary head loss weighting: 0.4
label smoothing: 0.1
clip global norm of all gradients by 10
Notes for training the mobile NASNet ImageNet model
-------------------------------------
batch size (per replica): 32
learning rate: 0.04 * 50
learning rate scaling factor: 0.97
num epochs per decay: 2.4
sync sgd with 50 replicas
auxiliary head weighting: 0.4
label smoothing: 0.1
clip global norm of all gradients by 10
Decay for the moving averages.
epsilon to prevent 0s in variance.
Decay for the moving averages.
epsilon to prevent 0s in variance.
Decay for the moving averages.
epsilon to prevent 0s in variance.
Shape of feature map before the final layer.
149 x 149 x 32
Run the reduction cells
Calculate the total number of cells in the network
Add 2 for the reduction cells
Calculate the total number of cells in the network
Add 2 for the reduction cells
"If ImageNet, then add an additional two for the stem cells"
Calculate the total number of cells in the network
Add 2 for the reduction cells
"If ImageNet, then add an additional two for the stem cells"
Find where to place the reduction cells or stride normal cells
Setup for building in the auxiliary head.
Run the cells
true_cell_num accounts for the stem cells
Final softmax layer
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Find where to place the reduction cells or stride normal cells
pylint: disable=protected-access
pylint: enable=protected-access
Setup for building in the auxiliary head.
Run the cells
true_cell_num accounts for the stem cells
pylint: disable=protected-access
pylint: enable=protected-access
Final softmax layer
pylint: disable=protected-access
pylint: enable=protected-access
Calculate the total number of cells in the network.
There is no distinction between reduction and normal cells in PNAS so the
total number of cells is equal to the number normal cells plus the number
of stem cells (two by default).
pylint: disable=protected-access
pylint: enable=protected-access
Calculate the total number of cells in the network.
There is no distinction between reduction and normal cells in PNAS so the
total number of cells is equal to the number normal cells plus the number
of stem cells (two by default).
Configuration for the PNASNet-5 model.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"The cap for tf.clip_by_value, it's hinted from the activation distribution"
"that the majority of activation values are in the range [-6, 6]."
Skip path 1
Skip path 2
"First pad with 0's on the right and bottom, then shift the filter to"
include those 0's that were added.
"If odd number of filters, add an additional one to the second path."
Concat and apply BN
Set the prev layer to the current layer if it is none
Check to be sure prev layer stuff is setup correctly
num_or_size_splits=1
Apply conv operations
Combine hidden states using 'add'.
Add hiddenstate to the list of hiddenstates we can choose from
Dont stride if this is not one of the original hiddenstates
"Check if a stride is needed, then use a strided 1x1 here"
Determine if a reduction should be applied to make the number of
filters match.
Return the concat of all the states
Scale keep prob by layer number
The added 2 is for the reduction cells
Decrease the keep probability over time
""
Script for generating a two-class dataset in COCO format for training an obscured image classifier
""
Requires Python >= 3.6 because of the glob ** expression
""
Collect images and labels
"Labels: clean = 0, obscured = 1"
""
top_folders_to_bottom.py
""
Given a base folder with files like:
""
A/1/2/a.jpg
B/3/4/b.jpg
""
"...moves the top-level folders to the bottom in a new output folder, i.e., creates:"
""
1/2/A/a.jpg
3/4/B/b.jpg
""
"In practice, this is used to make this:"
""
animal/camera01/image01.jpg
""
...look like:
""
camera01/animal/image01.jpg
""
%% Constants and imports
%% Support functions
%% Main functions
Find top-level folder
Find file/folder names
Move or copy
...def process_file()
Enumerate input folder
Convert absolute paths to relative paths
Standardize delimiters
Make sure each input file maps to a unique output file
relative_filename = relative_files[0]
Loop
...def top_folders_to_bottom()
%% Interactive driver
%%
%%
%% Command-line driver
"python top_folders_to_bottom.py ""g:\temp\separated_images"" ""g:\temp\separated_images_inverted"" --n_threads 100"
Convert to an options object
%% Constants and imports
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils)
Assumes the cameratraps repo root is on the path
%% Options
"Make sure there is no overlap between the two sets, because this will cause"
issues in the code
## Required inputs
## Options
Can be a folder or a SAS URL
# These apply only when we're doing ground-truth comparisons
Classes we'll treat as negative
""
"Include the token ""#NO_LABELS#"" to indicate that an image with no annotations"
should be considered empty.
Classes we'll treat as neither positive nor negative
"A list of output sets that we should count, but not render images for."
""
"Typically used to preview sets with lots of empties, where you don't want to"
"subset but also don't want to render 100,000 empty images."
""
"detections, non_detections"
"detections_animal, detections_person, detections_vehicle"
"By default, choose a confidence threshold based on the detector version"
Used for summary statistics only
"Number of images to sample, -1 for ""all images"""
"Random seed for sampling, or None"
Optionally separate detections into categories (animal/vehicle/human)
""
Currently only supported when ground truth is unavailable
Optionally replace one or more strings in filenames with other strings;
useful for taking a set of results generated for one folder structure
and applying them to a slightly different folder structure.
Allow bypassing API output loading when operating on previously-loaded
results
Should we also split out a separate report about the detections that were
just below our main confidence threshold?
""
Currently only supported when ground truth is unavailable
Control rendering parallelization
...PostProcessingOptions
#%% Helper classes and functions
Anything greater than this isn't clearly positive or negative
image has annotations suggesting both negative and positive
"image is not annotated or is annotated with 'unknown', 'unlabeled', ETC."
image has not yet been assigned a state
"In some analyses, we add an additional class that lets us look at"
detections just below our main confidence threshold
count the # of images with each type of DetectionStatus
Check whether this image has:
- unknown / unassigned-type labels
- negative-type labels
"- positive labels (i.e., labels that are neither unknown nor negative)"
"assert has_unknown_labels is False, '{} has unknown labels'.format(annotations)"
If there are no image annotations...
n_negative += 1
im['_detection_status'] = DetectionStatus.DS_NEGATIVE
"If the image has more than one type of labels, it's ambiguous"
"note: bools are automatically converted to 0/1, so we can sum"
"After the check above, we can be sure it's only one of positive,"
"negative, or unknown."
""
Important: do not merge the following 'unknown' branch with the first
"'unknown' branch above, where we tested 'if len(categories) == 0'"
""
If the image has only unknown labels
If the image has only negative labels
If the images has only positive labels
"Annotate the category, if it is unambiguous"
...for each image
...mark_detection_status()
"Leaving code in place for reading from blob storage, may support this"
in the future.
resize is to display them in this notebook or in the HTML more quickly
os.path.isfile() is slow when mounting remote directories; much faster
to just try/except on the image open.
return ''
Render images to a flat folder... we can use os.sep here because we've
already normalized paths
"Create class labels like ""gt_1"" or ""gt_27"""
"for i_box,box in enumerate(ground_truth_boxes):"
gt_classes.append('_' + str(box[-1]))
"errno.ENAMETOOLONG doesn't get thrown properly on Windows, so"
we awkwardly check against a hard-coded limit
Use slashes regardless of os
Optionally add links back to the original images
...render_bounding_boxes
Count items in each category
Optionally sort by filename before writing to html
Write the individual HTML files
...prepare_html_subpages()
Get unique categories above the threshold for this image
Render an image (with no ground truth information)
"This is a list of [class,confidence] pairs, sorted by confidence"
"If we either don't have a confidence threshold, or we've met our"
confidence threshold
...if this detection has classification info
...for each detection
...def render_image_no_gt()
This should already have been normalized to either '/' or '\'
...def render_image_with_gt()
%% Main function
#%% Expand some options for convenience
#%% Prepare output dir
#%% Load ground truth if available
Mark images in the ground truth as positive or negative
#%% Load detection (and possibly classification) results
"If the caller hasn't supplied results, load them"
Determine confidence thresholds if necessary
Remove failed rows
Convert keys and values to lowercase
"Add column 'pred_detection_label' to indicate predicted detection status,"
not separating out the classes
#%% Pull out descriptive metadata
This is rare; it only happens during debugging when the caller
is supplying already-loaded API results.
"#%% If we have ground truth, remove images we can't match to ground truth"
#%% Sample images for visualization
#%% Fork here depending on whether or not ground truth is available
"If we have ground truth, we'll compute precision/recall and sample tp/fp/tn/fn."
""
Otherwise we'll just visualize detections/non-detections.
#%% Detection evaluation: compute precision/recall
numpy array of detection probabilities
"numpy array of bools (0.0/1.0), and -1 as null value"
Don't include ambiguous/unknown ground truth in precision/recall analysis
"For completeness, include the result at a confidence threshold of 1.0"
Compute and print summary statistics
Thresholds go up throughout precisions/recalls/thresholds; find the last
value where recall is at or above target.  That's our precision @ target recall.
"np.where returns a tuple of arrays, but in this syntax where we're"
"comparing an array with a scalar, there will only be one element."
Convert back to a list
Flatten the confusion matrix
"#%% Collect classification results, if they exist"
Mapping of classnames to idx for the confusion matrix.
""
"The lambda is actually kind of a hack, because we use assume that"
the following code does not reassign classname_to_idx
Confusion matrix as defaultdict of defaultdict
""
"Rows / first index is ground truth, columns / second index is predicted category"
iDetection = 0; fn = detector_files[iDetection]; print(fn)
"If this image has classification predictions, and an unambiguous class"
"annotated, and is a positive image..."
"The unambiguous category, we make this a set for easier handling afterward"
"Compute the accuracy as intersection of union,"
i.e. (# of categories in both prediciton and GT)
divided by (# of categories in either prediction or GT
""
"In case of only one GT category, the result will be 1.0, if"
prediction is one category and this category matches GT
""
"It is 1.0/(# of predicted top-1 categories), if the GT is"
one of the predicted top-1 categories.
""
"It is 0.0, if none of the predicted categories is correct"
Distribute this accuracy across all predicted categories in the
confusion matrix
...for each file in the detection results
If we have classification results
Build confusion matrix as array from classifier_cm
Print some statistics
Prepare confusion matrix output
Get confusion matrix as string
Get fixed-size classname for each idx
Prepend class name on each line and add to the top
Print formatted confusion matrix
"Actually don't, this gets really messy in all but the widest consoles"
Plot confusion matrix
To manually add more space at bottom: plt.rcParams['figure.subplot.bottom'] = 0.1
""
"Add 0.5 to figsize for every class. For two classes, this will result in"
"fig = plt.figure(figsize=[4,4])"
...if we have classification results
#%% Render output
Write p/r table to .csv file in output directory
Write precision/recall plot to .png file in output directory
#%% Sampling
Sample true/false positives/negatives with correct/incorrect top-1
classification and render to html
Accumulate html image structs (in the format expected by write_html_image_lists)
"for each category, e.g. 'tp', 'fp', ..., 'class_bird', ..."
Add default entries by accessing them for the first time
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]"
"Each element will be a three-tuple with elements file,max_conf,detections"
"Assemble the information we need for rendering, so we can parallelize without"
dealing with Pandas
i_row = 0; row = images_to_visualize.iloc[0]
Filenames should already have been normalized to either '/' or '\'
Map all the rendering results in the list rendering_results into the
dictionary images_html
Prepare the individual html image files
Write index.html
Show links to each GT class
""
We could do this without classification results; currently we don't.
Add links to all available classes
Close body and html tags
...for each image
"#%% Otherwise, if we don't have ground truth..."
#%% Sample detections/non-detections
Accumulate html image structs (in the format expected by write_html_image_list)
for each category
Add default entries by accessing them for the first time
"Maps detection categories - e.g. ""human"" - to result set names, e.g."
"""detections_human"""
Add a set of results for each category and combination of categories
Create output directories
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]"
"list of 3-tuples with elements (file, max_conf, detections)"
"Assemble the information we need for rendering, so we can parallelize without"
dealing with Pandas
i_row = 0; row = images_to_visualize.iloc[0]
Filenames should already have been normalized to either '/' or '\'
"render_image_no_gt(file_info,detection_categories_to_results_name,"
"detection_categories,classification_categories)"
Map all the rendering results in the list rendering_results into the
dictionary images_html
Prepare the individual html image files
Write index.html
"We can't just sum these, because image_counts includes images in both their"
detection and classification classes
total_images = sum(image_counts.values())
Don't print classification classes here; we'll do that later with a slightly
different structure
Add links to all available classes
os.startfile(output_html_file)
...if we do/don't have ground truth
...process_batch_results
%% Interactive driver(s)
%%
options.unlabeled_classes = ['human']
os.start(ppresults.output_html_file)
%% Command-line driver
""
load_api_results.py
""
Loads the output of the batch processing API (json) into a pandas dataframe.
""
Also functions to group entries by seq_id.
""
Includes the deprecated functions that worked with the old CSV API output format.
""
%% Constants and imports
%% Functions for grouping by sequence_id
example
input 'file': 'SER/S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG'
output 'id': 'S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG'
%% Functions for loading the result as a Pandas DataFrame
Validate that this is really a detector output file
Fields in the API output json other than 'images'
Normalize paths to simplify comparisons later
"image['file'] = image['file'].replace('\\','/')"
Replace some path tokens to match local paths to original blob structure
"If this is a newer file that doesn't include maximum detection confidence values,"
"add them, because our unofficial internal dataframe format includes this."
Pack the json output into a Pandas DataFrame
Sanity-check that this is really a detector output file
Normalize paths to simplify comparisons later
De-serialize detections
Optionally replace some path tokens to match local paths to the original blob structure
string_to_replace = list(options.detector_output_filename_replacements.keys())[0]
"TODO: hit some silly issues with vectorized str() and escaped characters, vectorize"
this later.
""
"detection_results['image_path'].str.replace(string_to_replace,replacement_string)"
iRow = 0
%% Imports
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils)
%% Constants and support classes
We will confirm that this matches what we load from each file
Process-based parallelization isn't supported yet
%% Main function
"Warn the user if some ""detections"" might not get rendered"
#%% Validate inputs
#%% Load both result sets
assert results_a['detection_categories'] == default_detection_categories
assert results_b['detection_categories'] == default_detection_categories
#%% Make sure they represent the same set of images
#%% Find differences
"Each of these maps a filename to a two-element list (the image in set A, the image in set B)"
""
"Right now, we only handle a very simple notion of class transition, where the detection"
of maximum confidence changes class *and* both images have an above-threshold detection.
fn = filenames_a[0]
We shouldn't have gotten this far if error_on_non_matching_lists is set
det = im_a['detections'][0]
...for each filename
#%% Sample and plot differences
"Render two sets of results (i.e., a comparison) for a single"
image.
...def render_image_pair()
fn = image_filenames[0]
...def render_detection_comparisons()
"For each category, generate comparison images and the"
comparison HTML page.
""
category = 'common_detections'
Choose detection pairs we're going to render for this category
...for each category
#%% Write the top-level HTML file content
...def compare_batch_results()
%% Interactive driver
%% KRU
%% Command-line driver
# TODO
""
"Merge high-confidence detections from one results file into another file,"
when the target file does not detect anything on an image.
""
Does not currently attempt to merge every detection based on whether individual
detections are missing; only merges detections into images that would otherwise
be considered blank.
""
"If you want to literally merge two .json files, see combine_api_outputs.py."
""
%% Constants and imports
%% Structs
Don't bother merging into target images where the max detection is already
higher than this threshold
"If you want to merge only certain categories, specify one"
(but not both) of these.
%% Main function
im = output_data['images'][0]
"Determine whether we should be processing all categories, or just a subset"
of categories.
i_source_file = 0; source_file = source_files[i_source_file]
source_im = source_data['images'][0]
detection_category = list(detection_categories)[0]
"This is already a detection, no need to proceed looking for detections to"
transfer
Boxes are x/y/w/h
source_sizes = [det['bbox'][2]*det['bbox'][3] for det in source_detections_this_category_raw]
Only look at boxes below the size threshold
...for each detection category
"print('Adding {} detections to image {}'.format(len(detections_to_transfer),image_filename))"
Update the max_detection_conf field (if present)
...for each image
...for each source file
%% Test driver
%%
%% Command-line driver (TODO)
""
separate_detections_into_folders.py
""
## Overview
""
"Given a .json file with batch processing results, separate the files in that"
"set of results into folders that contain animals/people/vehicles/nothing,"
according to per-class thresholds.
""
"Image files are copied, not moved."
""
""
## Output structure
""
Preserves relative paths within each of those folders; cannot be used with .json
files that have absolute paths in them.
""
"For example, if your .json file has these images:"
""
a/b/c/1.jpg
a/b/d/2.jpg
a/b/e/3.jpg
a/b/f/4.jpg
a/x/y/5.jpg
""
And let's say:
""
"* The results say that the first three images are empty/person/vehicle, respectively"
"* The fourth image is above threshold for ""animal"" and ""person"""
* The fifth image contains an animal
""
* You specify an output base folder of c:\out
""
You will get the following files:
""
c:\out\empty\a\b\c\1.jpg
c:\out\people\a\b\d\2.jpg
c:\out\vehicles\a\b\e\3.jpg
c:\out\animal_person\a\b\f\4.jpg
c:\out\animals\a\x\y\5.jpg
""
## Rendering bounding boxes
""
"By default, images are just copied to the target output folder.  If you specify --render_boxes,"
bounding boxes will be rendered on the output images.  Because this is no longer strictly
"a copy operation, this may result in the loss of metadata.  More accurately, this *may*"
result in the loss of some EXIF metadata; this *will* result in the loss of IPTC/XMP metadata.
""
Rendering boxes also makes this script a lot slower.
""
## Classification-based separation
""
"If you have a results file with classification data, you can also specify classes to put"
"in their own folders, within the ""animals"" folder, like this:"
""
"--classification_thresholds ""deer=0.75,cow=0.75"""
""
"So, e.g., you might get:"
""
c:\out\animals\deer\a\x\y\5.jpg
""
"In this scenario, the folders within ""animals"" will be:"
""
"deer, cow, multiple, unclassified"
""
"""multiple"" in this case only means ""deer and cow""; if an image is classified as containing a"
"bird and a bear, that would end up in ""unclassified"", since the folder separation is based only"
on the categories you provide at the command line.
""
"No classification-based separation is done within the animal_person, animal_vehicle, or"
animal_person_vehicle folders.
""
%% Constants and imports
Occasionally we have near-zero confidence detections associated with COCO classes that
didn't quite get squeezed out of the model in training.  As long as they're near zero
"confidence, we just ignore them."
%% Options class
"Dictionary mapping categories (plus combinations of categories, and 'empty') to output folders"
Populated only when using classification results
"Originally specified as a string, converted to a dict mapping name:threshold"
...__init__()
...class SeparateDetectionsIntoFoldersOptions
%% Support functions
Find the maximum confidence for each category
""
det = detections[0]
"For zero-confidence detections, we occasionally have leftover goop"
from COCO classes
assert det['conf'] < invalid_category_epsilon
...for each detection on this image
Count the number of thresholds exceeded
...for each category
If this is above multiple thresholds
"TODO: handle species-based separation in, e.g., the animal_person case"
"Are we making species classification folders, and is this an animal?"
Do we need to put this into a specific species folder?
Find the animal-class detections that are above threshold
Count the number of classification categories that are above threshold for at
least one detection
d = valid_animal_detections[0]
classification = d['classifications'][0]
"Do we have a threshold for this category, and if so, is"
this classification above threshold?
...for each classification
...for each detection
...if we have to deal with classification subfolders
...if we have 0/1/more categories above threshold
...if this is/isn't a failure case
Skip this image if it's empty and we're not processing empty images
"At this point, this image is getting copied; we may or may not also need to"
draw bounding boxes.
Do a simple copy operation if we don't need to render any boxes
Open the source image
"Render bounding boxes for each category separately, beacuse"
we allow different thresholds for each category.
"When we're not using classification folders, remove classification"
information to maintain standard detection colors.
...for each category
Read EXIF metadata
"Write output with EXIF metadata if available, and quality='keep' if this is a JPEG"
"image.  Unfortunately, neither parameter likes ""None"", so we get a slightly"
icky cascade of if's here.
Also see:
""
https://discuss.dizzycoding.com/determining-jpg-quality-in-python-pil/
""
...for more ways to preserve jpeg quality if quality='keep' doesn't do the trick.
...if we don't/do need to render boxes
...def process_detections()
%% Main function
Create output folder if necessary
Load detection results
Map class names to output folders
Create all combinations of categories
category_name = category_names[0]
Do we have a custom threshold for this category?
Create folder mappings for each category
Create the actual folders
"Handle species classification thresholds, if specified"
"E.g. deer=0.75,cow=0.75"
token = tokens[0]
...for each token
...if classification thresholds are still in string format
Validate the classes in the threshold list
...if we need to deal with classification categories
i_image = 14; im = images[i_image]; im
...for each image
...def separate_detections_into_folders
%% Interactive driver
%%
%%
%%
%% Testing various command-line invocations
"With boxes, no classification"
"No boxes, no classification (default)"
"With boxes, with classification"
"No boxes, with classification"
%% Command-line driver
"python api\batch_processing\postprocessing\separate_detections_into_folders.py ""d:\temp\rspb_mini.json"" ""d:\temp\demo_images\rspb_2018_2019_mini"" ""d:\temp\separation_test"" --nthreads 2"
Convert to an options object
"print('{} {}'.format(v,name))"
List of category numbers to use in separation; uses all categories if None
"Can be ""size"", ""width"", or ""height"""
For each image...
""
im = images[0]
d = im['detections'][0]
Are there really any detections here?
Is this a category we're supposed to process?
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing#detector-outputs
...for each detection
...for each image
...def categorize_detections_by_size()
""
add_max_conf.py
""
"The MD output format included a ""max_detection_conf"" field with each image"
up to and including version 1.2; it was removed as of version 1.3 (it's
redundant with the individual detection confidence values).
""
"Just in case someone took a dependency on that field, this script allows you"
to add it back to an existing .json file.
""
%% Imports and constants
%% Main function
%% Driver
#######
""
remove_repeat_detections.py
""
"Used after running find_repeat_detections, then manually filtering the results,"
to create a final filtered output file.
""
"If you want to use this script, we recommend that you read the user's guide:"
""
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms
""
#######
%% Constants and imports
%% Main function
%% Interactive driver
%%
%% Command-line driver
#######
""
repeat_detections_core.py
""
Core utilities shared by find_repeat_detections and remove_repeat_detections.
""
#######
%% Imports and environment
"from ai4eutils; this is assumed to be on the path, as per repo convention"
"""PIL cannot read EXIF metainfo for the images"""
"""Metadata Warning, tag 256 had too many entries: 42, expected 1"""
%% Constants
%% Classes
Relevant for rendering the folder of images for filtering
""
"imageBase can also be a SAS URL, in which case some error-checking is"
disabled.
Don't consider detections with confidence lower than this as suspicious
Don't consider detections with confidence higher than this as suspicious
What's the IOU threshold for considering two boxes the same?
How many occurrences of a single location (as defined by the IOU threshold)
are required before we declare it suspicious?
"Ignore ""suspicious"" detections larger than some size; these are often animals"
taking up the whole image.  This is expressed as a fraction of the image size.
"Ignore ""suspicious"" detections smaller than some size"
Ignore folders with more than this many images in them
A list of classes we don't want to treat as suspicious. Each element is an int.
Load detections from a filter file rather than finding them from the detector output
".json file containing detections, should be called detectionIndex.json in the filtering_* folder"
produced in the first pass
(optional) List of filenames remaining after deletion of identified
repeated detections that are actually animals.  This should be a flat
"text file, one relative filename per line.  See enumerate_images()."
Turn on/off optional outputs
Determines whether bounding-box rendering errors (typically network errors) should
be treated as failures
Box rendering options
State variables
"Replace filename tokens after reading, useful when the directory structure"
has changed relative to the structure the detector saw
How many folders up from the leaf nodes should we be going to aggregate images?
An optional function that takes a string (an image file name) and returns
"a string (the corresponding  folder ID), typically used when multiple folders"
actually correspond to the same camera in a manufacturer-specific way (e.g.
a/b/c/RECONYX100 and a/b/c/RECONYX101 may really be the same camera).
Include/exclude specific folders... only one of these may be
"specified; ""including"" folders includes *only* those folders."
"Optionally show *other* detections (i.e., detections other than the"
one the user is evaluating) in a light gray
"If bRenderOtherDetections is True, what color should we use to render the"
(hopefully pretty subtle) non-target detections?
""
"In theory I'd like these ""other detection"" rectangles to be partially"
"transparent, but this is not straightforward, and the alpha is ignored"
"here.  But maybe if I leave it here and wish hard enough, someday it"
will work.
""
otherDetectionsColors = ['dimgray']
Sort detections within a directory so nearby detections are adjacent
"in the list, for faster review."
""
"Can be None, 'xsort', or 'clustersort'"
""
* None sorts detections chronologically by first occurrence
* 'xsort' sorts detections from left to right
* 'clustersort' clusters detections and sorts by cluster
Only relevant if smartSort == 'clustersort'
"The data table (Pandas DataFrame), as loaded from the input json file via"
load_api_results()
"The other fields in the input json file, loaded via load_api_results()"
The data table after modification
dict mapping folder names to whole rows from the data table
dict mapping filenames to rows in the master table
"An array of length nDirs, where each element is a list of DetectionLocation"
objects for that directory that have been flagged as suspicious
"This is a bit of a hack right now, but for future-proofing, I don't want to call this"
"to retrieve anything other than the highest-confidence detection, and I'm assuming this"
"is already sorted, so assert() that."
It's not clear whether it's better to use instances[0].bbox or self.bbox
"here... they should be very similar, unless iouThreshold is very low."
self.bbox is a better representation of the overal DetectionLocation.
%% Helper functions
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t"
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t"
%% Sort a list of candidate detections to make them visually easier to review
Just sort by the X location of each box
"Prepare a list of points to represent each box,"
that's what we'll use for clustering
Upper-left
"points.append([det.bbox[0],det.bbox[1]])"
Center
"Labels *could* be any unique labels according to the docs, but in practice"
they are unique integers from 0:nClusters
Make sure the labels are unique incrementing integers
Store the label assigned to each cluster
"Now sort the clusters by their x coordinate, and re-assign labels"
so the labels are sortable
"Compute the centroid for debugging, but we're only going to use the x"
"coordinate.  This is the centroid of points used to represent detections,"
which may be box centers or box corners.
old_cluster_label_to_new_cluster_label[old_cluster_label] =\
new_cluster_labels[old_cluster_label]
%% Look for matches (one directory)
List of DetectionLocations
candidateDetections = []
Create a tree to store candidate detections
For each image in this directory
""
iDirectoryRow = 0; row = rows.iloc[iDirectoryRow]
""
"iDirectoryRow is a pandas index, so it may not start from zero;"
"for debugging, we maintain i_iteration as a loop index."
print('Searching row {} of {} (index {}) in dir {}'.\
"format(i_iteration,len(rows),iDirectoryRow,dirName))"
Don't bother checking images with no detections above threshold
"Array of dicts, where each element is"
{
"'category': '1',  # str value, category ID"
"'conf': 0.926,  # confidence of this detections"
""
"# (x_min, y_min) is upper-left, all in relative coordinates"
"'bbox': [x_min, y_min, width_of_box, height_of_box]"
""
}
For each detection in this image
"This is no longer strictly true; I sometimes run RDE in stages, so"
some probabilities have already been made negative
""
assert confidence >= 0.0 and confidence <= 1.0
Optionally exclude some classes from consideration as suspicious
Is this detection too big to be suspicious?
print('Illegal zero-size bounding box on image {}'.format(filename))
These are relative coordinates
print('Ignoring very small detection with area {}'.format(area))
print('Ignoring very large detection with area {}'.format(area))
This will return candidates of all classes
For each detection in our candidate list
Don't match across categories
Is this a match?
"If so, add this example to the list for this detection"
We *don't* break here; we allow this instance to possibly
match multiple candidates.  There isn't an obvious right or
wrong here.
...for each detection on our candidate list
"If we found no matches, add this to the candidate list"
candidateDetections.append(candidate)
pyqtree
...for each detection
...for each row
Get all candidate detections
print('Found {} candidate detections for folder {}'.format(
"len(candidateDetections),dirName))"
"For debugging only, it's convenient to have these sorted"
as if they had never gone into a tree structure.  Typically
this is in practce a sort by filename.
...def find_matches_in_directory(dirName)
"%% Update the detection table based on suspicious results, write .csv output"
"An array of length nDirs, where each element is a list of DetectionLocation"
objects for that directory that have been flagged as suspicious
For each directory
For each suspicious detection group in this directory
For each instance of this suspicious detection
This should match the bbox for the detection event
The bbox for this instance should be almost the same as the bbox
"for this detection group, where ""almost"" is defined by the IOU"
threshold.
if iou < options.iouThreshold:
"print('IOU warning: {},{}'.format(iou,options.iouThreshold))"
Make sure the bounding box matches
"Make the probability negative, if it hasn't been switched by"
another bounding box
...for each instance
...for each detection
...for each directory
Update maximum probabilities
For each row...
No longer strictly true; sometimes I run RDE on RDE output
assert maxPOriginal >= 0
We should only be making detections *less* likely in this process
"If there was a meaningful change, count it"
"Negative probabilities should be the only reason maxP changed, so"
we should have found at least one negative value if we reached
this point.
...if there was a meaningful change to the max probability for this row
...for each row
If we're also writing output...
"...def update_detection_table(RepeatDetectionResults,options)"
%% Main function
#%% Input handling
Validate some options
Load the filtering file
Load the same options we used when finding repeat detections
...except for things that explicitly tell this function not to
find repeat detections.
...if we're loading from an existing filtering file
Check early to avoid problems with the output folder
"Load file to a pandas dataframe.  Also populates 'max_detection_conf', even if it's"
not present in the .json file.
detectionResults[detectionResults['failure'].notna()]
"Before doing any real work, make sure we can *probably* access images"
"This is just a cursory check on the first image, but it heads off most"
"problems related to incorrect mount points, etc.  Better to do this before"
spending 20 minutes finding repeat detections.
#%% Separate files into directories
This will be a map from a directory name to smaller data frames
This is a mapping back into the rows of the original table
iRow = 0; row = detectionResults.iloc[0]
Create a new DataFrame with just this row
rowsByDirectory[dirName] = pd.DataFrame(row)
...for each unique detection
Convert lists of rows to proper DataFrames
#% Look for matches (or load them from file)
"Are we actually looking for matches, or just loading from a file?"
length-nDirs list of lists of DetectionLocation objects
We're actually looking for matches...
"We get slightly nicer progress bar behavior using threads, by passing a pbar"
object and letting it get updated.  We can't serialize this object across
processes.
#%% Find suspicious locations based on match results
For each directory
""
iDir = 51
A list of DetectionLocation objects
A list of DetectionLocation objects
occurrenceList is a list of file/detection pairs
Sort the above-threshold detections for easier review
...for each directory
If we're just loading detections from a file...
"We're skipping detection-finding, but to see which images are actually legit false"
"positives, we may be looking for physical files or loading from a text file."
For each directory
iDir = 0; detections = suspiciousDetections[0]
""
"suspiciousDetections is an array of DetectionLocation objects,"
one per directory.
For each detection that was present before filtering
iDetection = 0; detection = detections[iDetection]
Are we checking the directory to see whether detections were actually false
"positives, or reading from a list?"
Is the image still there?
"If not, remove this from the list of suspicious detections"
...for each detection
...for each directory
...if we are/aren't finding detections (vs. loading from file)
Create filtering directory
iDir = 0; suspiciousDetectionsThisDir = suspiciousDetections[iDir]
suspiciousDetectionsThisDir is a list of DetectionLocation objects
iDetection = 0; detection = suspiciousDetectionsThisDir[0]
Sort instances in descending order by confidence
Choose the highest-confidence index
Should we render (typically in a very light color) detections
*other* than the one we're highlighting here?
Render other detections first (typically in a thin+light box)
Now render the example detection (on top of at least one
of the other detections)
This converts the *first* instance to an API standard detection;
"because we just sorted this list in descending order by confidence,"
this is the highest-confidence detection.
...if we are/aren't rendering other bounding boxes
...for each detection in this folder
...for each folder
Write out the detection index
...if we're writing filtering info
...find_repeat_detections()
#######
""
find_repeat_detections.py
""
"If you want to use this script, we recommend that you read the user's guide:"
""
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms
""
"Really, don't try to run this script without reading the user's guide, you'll think"
it's more magical than it is.
""
"This script looks through a sequence of detections in the API output json file, and finds"
"candidates that might be ""repeated false positives"", i.e. that random branch that the"
detector thinks is an animal/person/vehicle.
""
"Typically after running this script, you would do a manual step to remove"
"true positives, then run remove_repeat_detections to produce a final output file."
""
There's no way that statement was self-explanatory; see the user's guide.
""
#######
%% Constants and imports
%% Interactive driver
%%
%% Command-line driver
Convert to an options object
""
If a request has been sent to AML for batch scoring but the monitoring thread of the API was
"interrupted (uncaught exception or having to re-start the API container), we could manually"
"aggregate results from each shard using this script, assuming all jobs submitted to AML have finished."
""
Need to have set environment variables STORAGE_ACCOUNT_NAME and STORAGE_ACCOUNT_KEY to those of the
"storage account backing the API. Also need to adjust the INTERNAL_CONTAINER, AML_CONTAINER and"
AML_CONFIG fields in api_core/orchestrator_api/api_config.py to match the instance of the API that this
request was submitted to.
""
May need to change the import statement in api_core/orchestrator_api/orchestrator.py
"""from sas_blob_utils import SasBlob"" to"
"""from .sas_blob_utils import SasBlob"" to not confuse with the module in AI4Eutils;"
"and change ""import api_config"" to"
"""from api.batch_processing.api_core.orchestrator_api import api_config"""
Execute this script from the root of the repository. You may need to add the repository to PYTHONPATH.
"list_jobs_submitted cannot be serialized (""can't pickle _thread.RLock objects ""), but"
do not need it for aggregating results
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%% helper classes and functions
TODO log exception when we have more telemetry
TODO check that the expiry date of input_container_sas is at least a month
into the future
"if no permission specified explicitly but has an access policy, assumes okay"
TODO - check based on access policy as well
return current UTC time as a string in the ISO 8601 format (so we can query by
timestamp in the Cosmos DB job status table.
example: '2021-02-08T20:02:05.699689Z'
"image_paths will have length at least 1, otherwise would have ended before this step"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
a job moves from created to running/problem after the Batch Job has been submitted
"job_id should be unique across all instances, and is also the partition key"
TODO do not read the entry first to get the call_params when the Cosmos SDK add a
patching functionality:
https://feedback.azure.com/forums/263030-azure-cosmos-db/suggestions/6693091-be-able-to-do-partial-updates-on-document
need to retain other fields in 'status' to be able to restart monitoring thread
retain existing fields; update as needed
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
sentinel should change if new configurations are available
configs have not changed
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Gunicorn logger handler will get attached if needed in server.py
set for all tasks in the job
cannot execute the scoring script that is in the mounted directory; has to be copied to cwd
not luck giving the commandline arguments via formatted string - set as env vars instead
form shards of images and assign each shard to a Task
for persisting stdout and stderr
persist stdout and stderr (will be removed when node removed)
paths are relative to the Task working directory
can also just upload on failure
first try submitting Tasks
retry submitting Tasks
Change the Job's on_all_tasks_complete option to 'terminateJob' so the Job's status changes automatically
after all submitted tasks are done
This is so that we do not take up the quota for active Jobs in the Batch account.
return type: TaskAddCollectionResult
actually we should probably only re-submit if it's a server_error
docs: # https://docs.microsoft.com/en-us/rest/api/batchservice/odata-filters-in-batch#list-tasks
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%% Flask app
reference: https://trstringer.com/logging-flask-gunicorn-the-manageable-way/
%% Helper classes
%% Flask endpoints
required params
can be an URL to a file not hosted in an Azure blob storage container
"if use_url, then images_requested_json_sas is required"
optional params
check model_version is among the available model versions
check request_name has only allowed characters
optional params for telemetry collection - logged to status table for now as part of call_params
All API instances / node pools share a quota on total number of active Jobs;
we cannot accept new Job submissions if we are at the quota
required fields
request_status is either completed or failed
the create_batch_job thread will stop when it wakes up the next time
"Fix for Zooniverse - deleting any ""-"" characters in the job_id"
"If the status is running, it could be a Job submitted before the last restart of this"
"API instance. If that is the case, we should start to monitor its progress again."
WARNING model_version could be wrong (a newer version number gets written to the output file) around
"the time that  the model is updated, if this request was submitted before the model update"
and the API restart; this should be quite rare
conform to previous schemes
%% undocumented endpoints
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Gunicorn logger handler will get attached if needed in server.py
request_name and request_submission_timestamp are for appending to
output file names
image_paths can be a list of strings (Azure blob names or public URLs)
"or a list of length-2 lists where each is a [image_id, metadata] pair"
Case 1: listing all images in the container
- not possible to have attached metadata if listing images in a blob
list all images to process
+ 1 so if the number of images listed > MAX_NUMBER_IMAGES_ACCEPTED_PER_JOB
we will know and not proceed
Case 2: user supplied a list of images to process; can include metadata
filter down to those conforming to the provided prefix and accepted suffixes (image file types)
prefix is case-sensitive; suffix is not
"Although urlparse(p).path preserves the extension on local paths, it will not work for"
"blob file names that contains ""#"", which will be treated as indication of a query."
"If the URL is generated via Azure Blob Storage, the ""#"" char will be properly encoded"
apply the first_n and sample_n filters
OK if first_n > total number of images
sample by shuffling image paths and take the first sample_n images
"upload the image list to the container, which is also mounted on all nodes"
all sharding and scoring use the uploaded list
now request_status moves from created to running
an extra field to allow the monitoring thread to restart after an API restart: total number of tasks
also record the number of images to process for reporting
start the monitor thread with the same name
"both succeeded and failed tasks are marked ""completed"" on Batch"
"preserving format from before, but SAS URL to 'failed_images' and 'images' are no longer provided"
"failures should be contained in the output entries, indicated by an 'error' field"
"when people download this, the timestamp will have : replaced by _"
check if the result blob has already been written (could be another instance of the API / worker thread)
"and if so, skip aggregating and uploading the results, and just generate the SAS URL, which"
could be needed still if the previous request_status was `problem`.
upload the output JSON to the Job folder
%% Helper functions *copied* from ct_utils.py and visualization/visualization_utils.py
"Determine the factor, which shifts the decimal point of x"
just behind the last significant digit
"Shift decimal point by multiplicatipon with factor, flooring, and"
division by factor
PIL.Image.convert() returns a converted copy of this image
alter orientation as needed according to EXIF tag 0x112 (274) for Orientation
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28
https://www.media.mit.edu/pia/Research/deepview/exif.html
"%% TFDetector class, an unmodified *copy* of the class in detection/tf_detector.py,"
so we do not have to import the packages required by run_detector.py
Number of decimal places to round to for confidence and bbox coordinates
"MegaDetector was trained with batch size of 1, and the resizing function is a part"
of the inference graph
An enumeration of failure reasons
"change from [y1, x1, y2, x2] to [x1, y1, width, height]"
convert numpy floats to Python floats
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size
"np_images = [np.asarray(image, np.uint8) for image in images]"
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)"
performs inference
our batch size is 1; need to loop the batch dim if supporting batch size > 1
%% Scoring script
determine if there is metadata attached to each image_id
information to determine input and output locations
other parameters for the task
test that we can write to output path; also in case there is no image to process
list images to process
"items in this list can be strings or [image_id, metadata]"
model path
"Path to .pb TensorFlow detector model file, relative to the"
models/megadetector_copies folder in mounted container
score the images
""
manage_video_batch.py
""
Notebook-esque script to manage the process of running a local batch of videos
through MD.  Defers most of the heavy lifting to manage_local_batch.py .
""
%% Imports and constants
%% Split videos into frames
"%% List frame files, break into folders"
Find unique (relative) folders
fn = frame_files[0]
%% List videos
%% Check for videos that are missing entirely
list(folder_to_frame_files.keys())[0]
video_filenames[0]
fn = video_filenames[0]
%% Check for videos with very few frames
%% Print the list of videos that are problematic
%% Process images like we would for any other camera trap job
"...typically using manage_local_batch.py, but do this however you like, as long"
as you get a results file at the end.
""
"If you do RDE, remember to use the second folder from the bottom, rather than the"
bottom-most folder.
%% Convert frame results to video results
%% Confirm that the videos in the .json file are what we expect them to be
%% Scrap
%% Test a possibly-broken video
%% List videos in a folder
%% Imports
%% Constants
%% Classes
class variables
"instance variables, in order of when they are typically set"
Leaving this commented out to remind us that we don't want this check here; let
the API fail on these images.  It's a huge hassle to remove non-image
files.
""
for path_or_url in images_list:
if not is_image_file_or_url(path_or_url):
raise ValueError('{} is not an image'.format(path_or_url))
Commented out as a reminder: don't check task status (which is a rest API call)
in __repr__; require the caller to explicitly request status
"status=getattr(self, 'status', None))"
estimate # of failed images from failed shards
Download all three JSON urls to memory
Remove files that were submitted but don't appear to be images
assert all(is_image_file_or_url(s) for s in submitted_images)
Diff submitted and processed images
Confirm that the procesed images are a subset of the submitted images
https://www.geeksforgeeks.org/break-list-chunks-size-n-python/
%% Interactive driver
%%
%%
%%
%% Imports and constants
%% Constants I set per script
## Required
base_output_folder_name = os.path.expanduser('~/postprocessing/' + organization_name_short)
Shared Access Signature (SAS) tokens for the Azure Blob Storage container.
Leading question mark is optional.
""
The read-only token is used for accessing images; the write-enabled token is
used for writing file lists.
## Typically left as default
"Pre-pended to all folder names/prefixes, if they're defined below"
"This is how we break the container up into multiple taskgroups, e.g., for"
separate surveys. The typical case is to do the whole container as a single
taskgroup.
"If your ""folders"" are really logical folders corresponding to multiple folders,"
map them here
"A list of .json files to load images from, instead of enumerating.  Formatted as a"
"dictionary, like folder_prefixes."
This is only necessary if you will be performing postprocessing steps that
"don't yet support SAS URLs, specifically the ""subsetting"" step, or in some"
cases the splitting of files into multiple output directories for
empty/animal/vehicle/people.
""
"For those applications, you will need to mount the container to a local drive."
For this case I recommend using rclone whether you are on Windows or Linux;
rclone is much easier than blobfuse for transient mounting.
""
"But most of the time, you can ignore this."
"Supported model_versions: '4', '3', '4_prelim'"
""
Also available at the /supported_model_versions and /default_model_version
endpoints
""
"Unless you have any specific reason to set this to a non-default value, leave"
"it at the default, which as of 2020.04.28 is MegaDetector 4.1"
""
"additional_task_args = {""model_version"":""4_prelim""}"
""
"file_lists_by_folder will contain a list of local JSON file names,"
each JSON file contains a list of blob names corresponding to an API taskgroup
"%% Derived variables, path setup"
local folders
Turn warnings into errors if more than this many images are missing
%% Support functions
"scheme, netloc, path, query, fragment"
%% Read images from lists or enumerate blobs to files
folder_name = folder_names[0]
"Load file lists for this ""folder"""
""
file_list = input_file_lists[folder][0]
Write to file
A flat list of blob paths for each folder
folder_name = folder_names[0]
"If we don't/do have multiple prefixes to enumerate for this ""folder"""
"If this is intended to be a folder, it needs to end in '/', otherwise"
files that start with the same string will match too
...for each prefix
Write to file
...for each folder
%% Some just-to-be-safe double-checking around enumeration
Make sure each folder has at least one image matched; the opposite is usually a sign of a copy/paste issue
...for each image
...for each prefix
...for each folder
Make sure each image comes from one of our folders; the opposite is usually a sign of a bug up above
...for each prefix
...for each folder
...for each image
%% Divide images into chunks for each folder
The JSON file at folder_chunks[i][j] corresponds to task j of taskgroup i
list_file = file_lists_by_folder[0]
"%% Create taskgroups and tasks, and upload image lists to blob storage"
periods not allowed in task names
%% Generate API calls for each task
clipboard.copy(request_strings[0])
clipboard.copy('\n\n'.join(request_strings))
%% Run the tasks (don't run this cell unless you are absolutely sure!)
I really want to make sure I'm sure...
%% Estimate total time
Around 0.8s/image on 16 GPUs
%% Manually create task groups if we ran the tasks manually
%%
"%% Write task information out to disk, in case we need to resume"
%% Status check
print(task.id)
%% Resume jobs if this notebook closes
%% For multiple tasks (use this only when we're merging with another job)
%% For just the one task
%% Load into separate taskgroups
p = task_cache_paths[0]
%% Typically merge everything into one taskgroup
"%% Look for failed shards or missing images, start new tasks if necessary"
List of lists of paths
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup];
"Make a copy, because we append to taskgroup"
i_task = 0; task = tasks[i_task]
Each taskgroup corresponds to one of our folders
Check that we have (almost) all the images
Now look for failed images
Write it out as a flat list as well (without explanation of failures)
...for each task
...for each task group
%% Pull results
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup]; task = taskgroup[0]
Each taskgroup corresponds to one of our folders
...for each task
...for each task group
%% Combine results from task groups into final output files
Check that we have (almost) all the images
The only reason we should ever have a repeated request is the case where an
"image was missing and we reprocessed it, or where it failed and later succeeded"
"There may be non-image files in the request list, ignore those"
Something has gone bonkers if there are images in the results that
aren't in the request
...for each folder
%% Post-processing (no ground truth)
i_folder = 0; folder_name_raw = folder_names[i_folder]
"Omit some pages from the output, useful when animals are rare"
%% Manual processing follows
""
"Everything after this should be considered mostly manual, and no longer includes"
looping over folders.
""
%% RDE (sample directory collapsing)
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images"
"%% Repeat detection elimination, phase 1"
"Deliberately leaving these imports here, rather than at the top, because this cell is not"
typically executed
To invoke custom collapsing of folders for a particular manufacturer's naming scheme
options.customDirNameFunction = remove_overflow_folders
Exclude people and vehicles from RDE
"options.excludeClasses = [2,3]"
%% Manual RDE step
# DELETE THE VALID DETECTIONS ##
%% Re-filtering
%% Post-processing (post-RDE)
i_folder = 0; folder_name_raw = folder_names[i_folder]
"Omit some pages from the output, useful when animals are rare"
"print('No RDE file available for {}, skipping'.format(folder_name))"
continue
%% Create a new category for large boxes
%% Subsetting
i_folder = 0; folder_name = folders[i_folder]
%% String replacement
%% Folder splitting
i_folder = 0; folder_name_raw = folder_names[i_folder]
api_output_file = folder_name_to_combined_output_file[folder_name]
%% Imports and constants
from ai4eutils
To specify a non-default confidence threshold for including detections in the .json file
Turn warnings into errors if more than this many images are missing
Only relevant when we're using a single GPU
"Specify a target image size when running MD... strongly recommended to leave this at ""None"""
Only relevant when running on CPU
OS-specific script line continuation character
OS-specific script comment character
"Prefer threads on Windows, processes on Linux"
"This is for things like image rendering, not for MegaDetector"
Should we use YOLOv5's val.py instead of run_detector_batch.py?
Directory in which to run val.py.  Only relevant if use_yolo_inference_scripts is True.
Should we remove intermediate files used for running YOLOv5's val.py?
""
Only relevant if use_yolo_inference_scripts is True.
Should we apply YOLOv5's augmentation?  Only allowed when use_yolo_inference_scripts
is True.
%% Constants I set per script
Optional descriptor
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v5.0.0/md_v5b.0.0.pt')
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v4.1.0/md_v4.1.0.pb')
"Number of jobs to split data into, typically equal to the number of available GPUs"
Only used to print out a time estimate
"%% Derived variables, constant validation, path setup"
%% Enumerate files
%% Load files from prior enumeration
%% Divide images into chunks
%% Estimate total time
%% Write file lists
%% Generate commands
"A list of the scripts tied to each GPU, as absolute paths.  We'll write this out at"
the end so each GPU's list of commands can be run at once.  Generally only used when
"running lots of small batches via YOLOv5's val.py, which doesn't support checkpointing."
i_task = 0; task = task_info[i_task]
Generate the script to run MD
Check whether this output file exists
Generate the script to resume from the checkpoint (only supported with MD inference code)
...for each task
Write out a script for each GPU that runs all of the commands associated with
that GPU.  Typically only used when running lots of little scripts in lieu
of checkpointing.
...for each GPU
%% Run the tasks
%%% Run the tasks (commented out)
i_task = 0; task = task_info[i_task]
"This will write absolute paths to the file, we'll fix this later"
...for each chunk
...if False
"%% Load results, look for failed or missing images in each task"
i_task = 0; task = task_info[i_task]
im = task_results['images'][0]
...for each task
%% Merge results files and make images relative
im = combined_results['images'][0]
%% Post-processing (pre-RDE)
options.sample_seed = 0
"Omit some pages from the output, useful when animals are rare"
%% RDE (sample directory collapsing)
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images"
%%
%%
%%
relativePath = image_filenames[0]
"%% Repeat detection elimination, phase 1"
"Deliberately leaving these imports here, rather than at the top, because this"
cell is not typically executed
options.minSuspiciousDetectionSize = 0.05
This will cause a very light gray box to get drawn around all the detections
we're *not* considering as suspicious.
options.lineThickness = 5
options.boxExpansion = 8
To invoke custom collapsing of folders for a particular manufacturer's naming scheme
options.customDirNameFunction = remove_overflow_folders
Exclude people and vehicles from RDE
"options.excludeClasses = [2,3]"
options.maxImagesPerFolder = 50000
options.includeFolders = ['a/b/c']
options.excludeFolder = ['a/b/c']
"Can be None, 'xsort', or 'clustersort'"
import clipboard; clipboard.copy(os.path.dirname(suspiciousDetectionResults.filterFile))
path_utils.open_file(os.path.dirname(suspiciousDetectionResults.filterFile))
%% Manual RDE step
# DELETE THE VALID DETECTIONS ##
%% Re-filtering
%% Post-processing (post-RDE)
options.sample_seed = 0
"Omit some pages from the output, useful when animals are rare"
"%% Run MegaClassifier (actually, write out a script that runs MegaClassifier)"
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
#%% Set up environment
commands.append('cd CameraTraps/classification\n')
commands.append('conda activate cameratraps-classifier\n')
#%% Crop images
fn = input_files[0]
#%% Run classifier
fn = input_files[0]
#%% Remap classifier outputs
fn = input_files[0]
#%% Merge classification and detection outputs
fn = input_files[0]
#%% Write  out classification script
"%% Run a non-MegaClassifier classifier (i.e., a classifier with no output mapping)"
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
#%% Set up environment
#%% Crop images
fn = input_files[0]
#%% Run classifier
fn = input_files[0]
#%% Merge classification and detection outputs
fn = input_files[0]
#%% Write everything out
%% Run the classifier(s) via the .sh script(s) or batch file(s) we just wrote
...
%% Within-image classification smoothing
""
Only count detections with a classification confidence threshold above
"*classification_confidence_threshold*, which in practice means we're only"
looking at one category per detection.
""
If an image has at least *min_detections_above_threshold* such detections
"in the most common category, and no more than *max_detections_secondary_class*"
"in the second-most-common category, flip all detections to the most common"
category.
""
"Optionally treat some classes as particularly unreliable, typically used to overwrite an"
"""other"" class."
""
This cell also removes everything but the non-dominant classification for each detection.
""
How many detections do we need above the classification threshold to determine a dominant category
for an image?
"Even if we have a dominant class, if a non-dominant class has at least this many classifications"
"in an image, leave them alone."
"If the dominant class has at least this many classifications, overwrite ""other"" classifications"
What confidence threshold should we use for assessing the dominant category in an image?
Which classifications should we even bother over-writing?
Detection confidence threshold for things we count when determining a dominant class
Which detections should we even bother over-writing?
"Before we do anything else, get rid of everything but the top classification"
for each detection.
...for each detection in this image
...for each image
im = d['images'][0]
...for each classification
...if there are classifications for this detection
...for each detection
"Handle a quirky special case: if the most common category is ""other"" and"
"it's ""tied"" with the second-most-common category, swap them"
secondary_count = category_to_count[keys[1]]
The 'secondary count' is the most common non-other class
If we have at least *min_detections_to_overwrite_other* in a category that isn't
"""other"", change all ""other"" classifications to that category"
...for each classification
...if there are classifications for this detection
...for each detection
"...if we should overwrite all ""other"" classifications"
"At this point, we know we have a dominant category; change all other above-threshold"
"classifications to that category.  That category may have been ""other"", in which"
case we may have already made the relevant changes.
det = detections[0]
...for each classification
...if there are classifications for this detection
...for each detection
...for each image
...for each file we want to smooth
"%% Post-processing (post-classification, post-within-image-smoothing)"
classification_detection_file = classification_detection_files[1]
%% Read EXIF data from all images
%% Prepare COCO-camera-traps-compatible image objects for EXIF results
import dateutil
"This is a standard format for EXIF datetime, and dateutil.parser"
doesn't handle it correctly.
return dateutil.parser.parse(s)
exif_result = exif_results[0]
Currently we assume that each leaf-node folder is a location
"We collected this image this century, but not today, make sure the parsed datetime"
jives with that.
""
The latter check is to make sure we don't repeat a particular pathological approach
"to datetime parsing, where dateutil parses time correctly, but swaps in the current"
date when it's not sure where the date is.
...for each exif image result
%% Assemble into sequences
Make a list of images appearing at each location
im = image_info[0]
%% Load classification results
Map each filename to classification results for that file
%% Smooth classification results over sequences (prep)
These are the only classes to which we're going to switch other classifications
Only switch classifications to the dominant class if we see the dominant class at least
this many times
"If we see more than this many of a class that are above threshold, don't switch those"
classifications to the dominant class.
"If the ratio between a dominant class and a secondary class count is greater than this,"
"regardless of the secondary class count, switch those classificaitons (i.e., ignore"
max_secondary_class_classifications_above_threshold_for_class_smoothing).
""
"This may be different for different dominant classes, e.g. if we see lots of cows, they really"
"tend to be cows.  Less so for canids, so we set a higher ""override ratio"" for canids."
"If there are at least this many classifications for the dominant class in a sequence,"
"regardless of what that class is, convert all 'other' classifications (regardless of"
confidence) to that class.
"If there are at least this many classifications for the dominant class in a sequence,"
"regardless of what that class is, classify all previously-unclassified detections"
as that class.
Only count classifications above this confidence level when determining the dominant
"class, and when deciding whether to switch other classifications."
Confidence values to use when we change a detection's classification (the
original confidence value is irrelevant at that point)
%% Smooth classification results over sequences (supporting functions)
im = images_this_sequence[0]
det = results_this_image['detections'][0]
Only process animal detections
Only process detections with classification information
"We only care about top-1 classifications, remove everything else"
Make sure the list of classifications is already sorted by confidence
...and just keep the first one
"Confidence values should be sorted within a detection; verify this, and ignore"
...for each detection in this image
...for each image in this sequence
...top_classifications_for_sequence()
Count above-threshold classifications in this sequence
Sort the dictionary in descending order by count
"Handle a quirky special case: if the most common category is ""other"" and"
"it's ""tied"" with the second-most-common category, swap them."
...def count_above_threshold_classifications()
%% Smooth classifications at the sequence level (main loop)
Break if this token is contained in a filename (set to None for normal operation)
i_sequence = 0; seq_id = all_sequences[i_sequence]
Count top-1 classifications in this sequence (regardless of confidence)
Handy debugging code for looking at the numbers for a particular sequence
Count above-threshold classifications for each category
"If our dominant category ID isn't something we want to smooth to, don't mess around with this sequence"
"# Smooth ""other"" classifications ##"
"By not re-computing ""max_count"" here, we are making a decision that the count used"
"to decide whether a class should overwrite another class does not include any ""other"""
classifications we changed to be the dominant class.  If we wanted to include those...
""
sorted_category_to_count = count_above_threshold_classifications(classifications_this_sequence)
max_count = get_first_value_from_sorted_dictionary(sorted_category_to_count)
assert dominant_category_id == get_first_key_from_sorted_dictionary(sorted_category_to_count)
# Smooth non-dominant classes ##
Don't flip classes to the dominant class if they have a large number of classifications
"Don't smooth over this class if there are a bunch of them, and the ratio"
if primary to secondary class count isn't too large
Default ratio
Does this dominant class have a custom ratio?
# Smooth unclassified detections ##
...for each sequence
%% Write smoothed classification results
"%% Post-processing (post-classification, post-within-image-and-within-sequence-smoothing)"
%% Zip .json files
%% 99.9% of jobs end here
Everything after this is run ad hoc and/or requires some manual editing.
%% Compare results files for different model versions (or before/after RDE)
Choose all pairwise combinations of the files in [filenames]
%% Merge in high-confidence detections from another results file
%% Create a new category for large boxes
"This is a size threshold, not a confidence threshold"
size_options.categories_to_separate = [3]
%% Preview large boxes
%% .json splitting
options.query = None
options.replacement = None
Reminder: 'n_from_bottom' with a parameter of zero is the same as 'bottom'
%% Custom splitting/subsetting
i_folder = 0; folder_name = folders[i_folder]
"This doesn't do anything in this case, since we're not splitting folders"
options.make_folder_relative = True
%% String replacement
%% Splitting images into folders
%% Generate commands for a subset of tasks
i_task = 8
...for each task
%% End notebook: turn this script into a notebook (how meta!)
Exclude everything before the first cell
Remove the first [first_non_empty_lines] from the list
Add the last cell
""
xmp_integration.py
""
"Tools for loading MegaDetector batch API results into XMP metadata, specifically"
for consumption in digiKam:
""
https://cran.r-project.org/web/packages/camtrapR/vignettes/camtrapr2.html
""
%% Imports and constants
%% Class definitions
Folder where images are stored
.json file containing MegaDetector output
"String to remove from all path names, typically representing a"
prefix that was added during MegaDetector processing
Optionally *rename* (not copy) all images that have no detections
above [rename_conf] for the categories in rename_cats from x.jpg to
x.check.jpg
"Comma-deleted list of category names (or ""all"") to apply the rename_conf"
behavior to.
"Minimum detection threshold (applies to all classes, defaults to None,"
i.e. 0.0
%% Functions
Relative image path
Absolute image path
List of categories to write to XMP metadata
Categories with above-threshold detections present for
this image
Maximum confidence for each category
Have we already added this to the list of categories to
write out to this image?
If we're supposed to compare to a threshold...
Else we treat *any* detection as valid...
Keep track of the highest-confidence detection for this class
If we're doing the rename/.check behavior...
Legacy code to rename files where XMP writing failed
%% Interactive/test driver
%%
%% Command-line driver
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Cosmos DB `batch-api-jobs` table for job status
"aggregate the number of images, country and organization names info from each job"
submitted during yesterday (UTC time)
create the card
""
api_frontend.py
""
"Defines the Flask app, which takes requests (one or more images) from"
"remote callers and pushes the images onto the shared Redis queue, to be processed"
by the main service in api_backend.py .
""
%% Imports
%% Initialization
%% Support functions
Make a dict that the request_processing_function can return to the endpoint
function to notify it of an error
Verify that the content uploaded is not too big
""
request.content_length is the length of the total payload
Verify that the number of images is acceptable
...def check_posted_data(request)
%% Main loop
Check whether the request_processing_function had an error
Write images to temporary files
""
TODO: read from memory rather than using intermediate files
"Submit the image(s) for processing by api_backend.py, who is waiting on this queue"
TODO: convert to a blocking read and eliminate the sleep() statement in this loop
"image = Image.open(os.path.join(temp_direc, image_name))"
...if we do/don't have a request available on the queue
...while(True)
...def detect_sync()
%% Command-line driver
use --non-docker if you are testing without Docker
""
python api_frontend.py --non-docker
# Camera trap real-time API configuration
"Full path to the temporary folder for image storage, only meaningful"
within the Docker container
Upper limit on total content length (all images and parameters)
Minimum confidence threshold for detections
Minimum confidence threshold for showing a bounding box on the output image
Use this when testing without Docker
""
api_backend.py
""
"Defines the model execution service, which pulls requests (one or more images)"
"from the shared Redis queue, and runs them through the TF model."
""
%% Imports
%% Initialization
%% Main loop
TODO: convert to a blocking read and eliminate the sleep() statement in this loop
Filter the detections by the confidence threshold
""
"Each result is [ymin, xmin, ymax, xmax, confidence, category]"
""
"Coordinates are relative, with the origin in the upper-left"
...if serialized_entry
...while(True)
...def detect_process()
%% Command-line driver
use --non-docker if you are testing without Docker
""
python api_frontend.py --non-docker
run detections on a test image to load the model
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Importing necessary basic libraries and modules
%%
PyTorch imports for tensor operations
%%
"Importing the models, transformations, and utility functions from PytorchWildlife"
%%
Setting the device to use for computations ('cuda' indicates GPU)
%%
Initializing the model for image detection
%%
Initializing the model for image classification
%%
Defining transformations for detection and classification
%%
Initializing a box annotator for visualizing detections
Processing the video and saving the result with annotated detections and classifications
%%
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Importing basic libraries
%%
%%
"Importing the models, dataset, transformations, and utility functions from PytorchWildlife"
%%
Setting the device to use for computations ('cuda' indicates GPU)
Initializing a supervision box annotator for visualizing detections
Create a temp folder
Initializing the detection and classification models
Defining transformations for detection and classification
%% Defining functions for different detection scenarios
Only run classifier when detection class is animal
%% Building Gradio UI
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Importing necessary basic libraries and modules
%%
PyTorch imports
%%
"Importing the model, dataset, transformations and utility functions from PytorchWildlife"
%%
Setting the device to use for computations ('cuda' indicates GPU)
%%
Initializing the MegaDetectorV5 model for image detection
%% Single image detection
Specifying the path to the target image TODO: Allow argparsing
Opening and converting the image to RGB format
Initializing the Yolo-specific transform for the image
Performing the detection on the single image
Saving the detection results
%% Batch detection
Specifying the folder path containing multiple images for batch detection
Creating a dataset of images with the specified transform
Creating a DataLoader for batching and parallel processing of the images
Performing batch detection on the images
%% Output to annotated images
Saving the batch detection results as annotated images
%% Output to cropped images
Saving the detected objects as cropped images
%% Output to JSON results
Saving the detection results in JSON format
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Making the DetectionImageFolder class available for import from this module
Listing and sorting all image files in the specified directory
Get image filename and path
Load and convert image to RGB
Apply transformation if specified
Only run recognition on animal detections
Get image path and corresponding bbox xyxy for cropping
Load and crop image with supervision
Apply transformation if specified
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Making the provided classes available for import from this module
Resize and pad the image using the letterbox function
Transpose and convert image to PyTorch tensor
Normalization constants
Define the sequence of transformations
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
!!! Output paths need to be optimized !!!
!!! Output paths need to be optimized !!!
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Importing basic libraries
Placeholder class-level attributes to be defined in derived classes
"If there are size differences in the input images, use a for loop instead of matrix processing for scaling"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Image size for the Opossum classifier
Class names for prediction
"If pretrained, use the provided URL to fetch the weights"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Making the PlainResNetInference class available for import from this module
Following the ResNet structure to extract features
Initialize the network and weights
... [Missing weight URL definition for ResNet18]
... [Missing weight URL definition for ResNet50]
Print missing and unused keys for debugging purposes
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Image size for the Opossum classifier
Class names for prediction
"If pretrained, use the provided URL to fetch the weights"
Configuration file for the Sphinx documentation builder.
""
"For the full list of built-in configuration values, see the documentation:"
https://www.sphinx-doc.org/en/master/usage/configuration.html
-- Project information -----------------------------------------------------
https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information
-- General configuration ---------------------------------------------------
https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration
-- Options for HTML output -------------------------------------------------
https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output
-- Options for todo extension ----------------------------------------------
https://www.sphinx-doc.org/en/master/usage/extensions/todo.html#configuration
"Determine the factor, which shifts the decimal point of x"
just behind the last significant digit
"Shift decimal point by multiplicatipon with factor, flooring, and"
division by factor
_ = pretty_print_object(obj)
Sloppy that I'm making a module-wide change here...
Determine the coordinates of the intersection rectangle
The intersection of two axis-aligned bounding boxes is always an
axis-aligned bounding box
Compute the area of both AABBs
Compute the intersection over union by taking the intersection
area and dividing it by the sum of prediction + ground-truth
areas - the intersection area.
%% Functions for running commands as subprocesses
%%
%% Test driver for execute_and_print
%% Parallel test driver for execute_command_and_print
Should we use threads (vs. processes) for parallelization?
"Only relevant if n_workers == 1, i.e. if we're not parallelizing"
%% Constants and imports
%% Classes
Collect all names
Make names unique and sort
cast location to string as the entries in locations are strings
Convert classnames to lowercase to simplify comparisons later
Normalize paths to simplify comparisons later
"Make custom replacements in filenames, typically used to"
accommodate changes in root paths after DB construction
## Build useful mappings to facilitate working with the DB
Category ID <--> name
Image filename --> ID
Image ID --> image object
Image ID --> annotations
"Each image can potentially multiple annotations, hence using lists"
...__init__
...class IndexedJsonDb
%% Functions
Find all unique locations
i_location = 0; location = locations[i_location]
Sorting datetimes fails when there are None's in the list.  So instead of sorting datetimes
"directly, sort tuples with a boolean for none-ness, then the datetime itself."
""
https://stackoverflow.com/questions/18411560/sort-list-while-pushing-none-values-to-the-end
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_location[1]
"Start a new sequence if necessary, including the case where this datetime is invalid"
"If this was an invalid datetime, this will record the previous datetime"
"as None, which will force the next image to start a new sequence."
...for each image in this location
Fill in seq_num_frames
...for each location
...create_sequences()
""
cct_to_md.py
""
"""Converts"" a COCO Camera Traps file to a MD results file.  Currently ignores"
"non-bounding-box annotations, and gives all annotations a confidence of 1.0."
""
The only reason to do this is if you are going to add information to an existing
"CCT-formatted dataset, and want to do that in Timelapse."
""
"Currently assumes that width and height are present in the input data, does not"
read them from images.
""
%% Constants and imports
%% Functions
# Validate input
# Read input
# Prepare metadata
ann = d['annotations'][0]
# Process images
im = d['images'][0]
"MegaDetector: [x,y,width,height] (normalized, origin upper-left)"
"CCT: [x,y,width,height] (absolute, origin upper-left)"
...if there's a bounding box
...for each annotation
This field is no longer included in MD output files by default
im_out['max_detection_conf'] = max_detection_conf
...for each image
# Write output
...cct_to_md()
%% Command-line driver
TODO
%% Interactive driver
%%
%%
""
cct_json_to_filename_json.py
""
"Given a .json file in COCO Camera Traps format, outputs a .json-formatted list of"
relative file names present in the CCT file.
""
%% Constants and environment
%% Main function
"json.dump(s,open(outputFilename,'w'))"
%% Command-line driver
%% Interactive driver
%%
""
cct_to_csv.py
""
"""Converts"" a COCO Camera Traps .json file to .csv, in quotes because"
"all kinds of assumptions are made here, and if you have a particular .csv"
"format in mind, YMMV.  Most notably, does not include any bounding box information"
or any non-standard fields that may be present in the .json file.  Does not
propagate information about sequence-level vs. image-level annotations.
""
"Does not assume access to the images, therefore does not open .jpg files to find"
"datetime information if it's not in the metadata, just writes datetime as 'unknown'."
""
%% Imports
%% Main function
#%% Read input
#%% Build internal mappings
annotation = annotations[0]
#%% Write output file
im = images[0]
Write out one line per class:
...for each class name
...for each image
...with open(output_file)
...def cct_to_csv
%% Interactive driver
%%
%% Command-line driver
""
remove_exif.py
""
"Removes all EXIF/IPTC/XMP metadata from a folder of images, without making"
"backup copies, using pyexiv2."
""
%% Imports and constants
%% List files
%% Remove EXIF data (support)
PYEXIV2 IS NOT THREAD SAFE; DO NOT CALL THIS IN PARALLEL FROM A SINGLE PROCESS
data = img.read_exif(); print(data)
%% Debug
%%
%%
%% Remove EXIF data (execution)
fn = image_files[0]
"joblib.Parallel defaults to a process-based backend, but let's be sure"
"results = Parallel(n_jobs=n_exif_threads,verbose=2,prefer='processes')(delayed(remove_exif)(fn) for fn in image_files[0:10])"
""
yolo_to_coco.py
""
Converts a YOLO-formatted dataset to a COCO-formatted dataset.
""
"Currently supports only a single folder (i.e., no recursion).  Treats images without"
corresponding .txt files as empty.
""
%% Imports and constants
from ai4eutils
%% Support functions
Validate input
Class names
Blank lines should only appear at the end
Enumerate images
fn = image_files[0]
Create the image object for this image
Is there an annotation file for this image?
"This is an image with no annotations, currently don't do anything special"
here
s = lines[0]
"COCO: [x_min, y_min, width, height] in absolute coordinates"
"YOLO: [class, x_center, y_center, width, height] in normalized coordinates"
...for each annotation
...if this image has annotations
...for each image
...def yolo_to_coco()
%% Interactive driver
%% Convert YOLO folders to COCO
%% Check DB integrity
%% Preview some images
%% Command-line driver
TODO
""
read_exif.py
""
"Given a folder of images, read relevant metadata (EXIF/IPTC/XMP) fields from all images,"
and write them to  a .json or .csv file.
""
This module can use either PIL (which can only reliably read EXIF data) or exiftool (which
can read everything).  The latter approach expects that exiftool is available on the system
path.  No attempt is made to be consistent in format across the two approaches.
""
%% Imports and constants
From ai4eutils
%% Options
Number of concurrent workers
Should we use threads (vs. processes) for parallelization?
""
Not relevant if n_workers is 1.
Should we use exiftool or pil?
%% Functions
exif_tags = img.info['exif'] if ('exif' in img.info) else None
print('Warning: unrecognized EXIF tag: {}'.format(k))
"-G means ""Print group name for each tag"", e.g. print:"
""
[File]          Bits Per Sample                 : 8
""
...instead of:
""
Bits Per Sample                 : 8
A list of three-element lists (type/tag/value)
line_raw = exif_lines[0]
A typical line:
""
[ExifTool]      ExifTool Version Number         : 12.13
"Split on the first occurrence of "":"""
...for each output line
...which processing library are we using?
...read_exif_tags_for_image()
...populate_exif_data()
Enumerate *relative* paths
Find all EXIF tags that exist in any image
...for each tag in this image
...for each image
Write header
...for each key that *might* be present in this image
...for each image
...with open()
...if we're writing to .json/.csv
https://stackoverflow.com/questions/11210104/check-if-a-program-exists-from-a-python-script
%% Interactive driver
%%
output_file = os.path.expanduser('~/data/test-exif.csv')
options.processing_library = 'pil'
"file_path = os.path.join(input_folder,'KRU_S1_11_R1_IMAG0148.JPG')"
%%
%% Command-line driver
""
"Given a json-formatted list of image filenames, retrieve the width and height of every image."
""
%% Constants and imports
%% Processing functions
Is this image on disk?
"all_results = list(tqdm(pool.imap(process_image, filenames), total=len(filenames)))"
%% Interactive driver
%%
List images in a test folder
%%
"process_list_file(image_list_file,image_size_file,image_prefix=base_dir)"
%% Command-line driver
"python sanity_check_json_db.py ""e:\wildlife_data\wellington_data\wellington_camera_traps.json"" --baseDir ""e:\wildlife_data\wellington_data\images"" --bFindUnusedImages --bCheckImageSizes"
"python sanity_check_json_db.py ""D:/wildlife_data/mcgill_test/mcgill_test.json"" --baseDir ""D:/wildlife_data/mcgill_test"" --bFindUnusedImages --bCheckImageSizes"
"Here the '-u' prevents buffering, which makes tee happier"
""
python -u sanity_check_json_db.py '/datadrive1/nacti_metadata.json' --baseDir '/datadrive1/nactiUnzip/' --bFindUnusedImages --bCheckImageSizes | tee ~/nactiTest.out
""
Partially-formed stub to get from MegaDetector output files to COCO Camera Traps data.
""
"Was actually written to convert *many* MD .json files to a single CCT file, hence"
the loop over .json files.
""
"THIS CODE HAS NEVER BEEN RUN, it was added as a demonstration of how to do this.  YMMV."
""
"You may find a more polished, command-line-ready version of this code at:"
""
https://github.com/StewartWILDlab/mdtools
""
%% Constants and imports
"Images sizes are required to convert between absolute and relative coordinates,"
so we need to read the images.
Only required if you want to write a database preview
%% Create CCT dictionaries
image_ids_to_images = {}
Force the empty category to be ID 0
Load .json annotations for this data set
i_entry = 0; entry = data['images'][i_entry]
""
"PERF: Not exactly trivially parallelizable, but about 100% of the"
time here is spent reading image sizes (which we need to do to get from
"absolute to relative coordinates), so worth parallelizing."
Generate a unique ID from the path
detection = detections[0]
Have we seen this category before?
Create an annotation
"MegaDetector: [x,y,width,height] (normalized, origin upper-left)"
"CCT: [x,y,width,height] (absolute, origin upper-left)"
...for each detection
...for each image
Remove non-reviewed images and associated annotations
%% Create info struct
%% Write .json output
%% Clean start
## Everything after this should work from a clean start ###
%% Validate output
%% Preview animal labels
%% Preview empty labels
"viz_options.classes_to_exclude = ['empty','human']"
""
generate_crops_from_cct.py
""
"Given a .json file in COCO Camera Traps format, create a cropped image for"
each bounding box.
""
%% Imports and constants
%% Functions
# Read and validate input
# Find annotations for each image
"This actually maps image IDs to annotations, but only to annotations"
containing boxes
# Generate crops
TODO: parallelize this loop
im = d['images'][0]
Load the image
Generate crops
i_ann = 0; ann = annotations_this_image[i_ann]
"x/y/w/h, origin at the upper-left"
...for each box
...for each image
...generate_crops_from_cct()
%% Interactive driver
%%
%%
%%
%% Command-line driver
TODO
%% Scrap
%%
""
coco_to_yolo.py
""
Converts a COCO-formatted dataset to a YOLO-formatted dataset.
""
"If the input and output folders are the same, writes .txt files to the input folder,"
and neither moves nor modifies images.
""
"Currently ignores segmentation masks, and errors if an annotation has a"
segmentation polygon but no bbox
""
Has only been tested on a handful of COCO Camera Traps data sets; if you
"use it for more general COCO conversion, YMMV."
""
%% Imports and constants
%% Support functions
Validate input
Read input data
Parse annotations
i_ann = 0; ann = data['annotations'][0]
Make sure no annotations have *only* segmentation data
Re-map class IDs to make sure they run from 0...n-classes-1
""
"TODO: this allows unused categories in the output data set, which I *think* is OK,"
but I'm only 81% sure.
Process images (everything but I/O)
"List of dictionaries with keys 'source_image','dest_image','bboxes','dest_txt'"
i_image = 0; im = data['images'][i_image]
"assert os.path.isfile(source_image), 'Could not find image {}'.format(source_image)"
If this annotation has no bounding boxes...
"This is not entirely clear from the COCO spec, but it seems to be consensus"
"that if you want to specify an image with no objects, you don't include any"
annotations for that image.
We allow empty bbox lists in COCO camera traps; this is typically a negative
"example in a dataset that has bounding boxes, and 0 is typically the empty"
category.
...if this is an empty annotation
"COCO: [x_min, y_min, width, height] in absolute coordinates"
"YOLO: [class, x_center, y_center, width, height] in normalized coordinates"
Convert from COCO coordinates to YOLO coordinates
...for each annotation
...if this image has annotations
...for each image
Write output
Category IDs should range from 0..N-1
TODO: parallelize this loop
""
output_info = images_to_copy[0]
Only write an annotation file if there are bounding boxes.  Images with
"no .txt files are treated as hard negatives, at least by YOLOv5:"
""
https://github.com/ultralytics/yolov5/issues/3218
""
"I think this is also true for images with empty annotation files, but"
"I'm using the convention suggested on that issue, i.e. hard negatives"
are expressed as images without .txt files.
bbox = bboxes[0]
...for each image
...def coco_to_yolo()
%% Interactive driver
%% CCT data
"If preview_export is True, I'm exporting to preview these with BoundingBoxEditor:"
""
https://github.com/mfl28/BoundingBoxEditor
""
"This export will be compatible, other than the fact that you need to move"
"""object.data"" into the ""labels"" folder."
""
"Otherwise I'm exporting for training, in the YOLOv5 flat format."
%% Command-line driver
TODO
""
cct_to_wi.py
""
Converts COCO Camera Traps .json files to the Wildlife Insights
batch upload format
""
Also see:
""
https://github.com/ConservationInternational/Wildlife-Insights----Data-Migration
""
https://data.naturalsciences.org/wildlife-insights/taxonomy/search
""
%% Imports
%% Paths
A COCO camera traps file with information about this dataset
A .json dictionary mapping common names in this dataset to dictionaries with the
"WI taxonomy fields: common_name, wi_taxon_id, class, orer, family, genus, species"
%% Constants
%% Project information
%% Read templates
%% Compare dictionary to template lists
Write the header
Write values
%% Project file
%% Camera file
%% Deployment file
%% Images file
Read .json file with image information
Read taxonomy dictionary
Populate output information
df = pd.DataFrame(columns = images_fields)
annotation = annotations[0]
im = input_data['images'][0]
"We don't have counts, but we can differentiate between zero and 1"
This is the label mapping used for our incoming iMerit annotations
"Only used to parse the incoming annotations. In our database, the string name is used to avoid confusion"
MegaDetector outputs
""
add_bounding_boxes_to_megadb.py
""
"Given COCO-formatted JSONs containing manually labeled bounding box annotations, add them to"
"MegaDB sequence entries, which can then be ingested into MegaDB."
the category map that comes in the COCO JSONs for iMerit batch 12 - to check that each
JSON
"dataset name : (seq_id, frame_num) : [bbox, bbox]"
where bbox is a dict with str 'category' and list 'bbox'
iterate over image_id_to_image rather than image_id_to_annotations so we include
the confirmed empty images
The file_name field in the incoming json looks like
alka_squirrels.seq2020_05_07_25C.frame119221.jpg
there seems to be a bug in the annotations where sometimes there's a
non-empty label along with a label of category_id 5
ignore the empty label (they seem to be actually non-empty)
""
add_bounding_boxes_to_json.py
""
This script takes a image database in the COCO Camera Traps format and merges in a set of bounding
box annotations in the format that iMerit uses (a .json where actually only each row is a valid json).
""
"If you need to update an existing bbox database, please get all the original annotation files and"
re-generate from scratch
""
%% Imports
%% Configurations and paths
images database
output bboxes database
annotation files (pseudo json) obtained from our annotation vendor that contain annotations for this dataset
None or a string or tuple of strings that is the prefix to all file_name of interest / in this dataset in the annotation files
functions for mapping the image_id in the annotation files (pseudo jsons) to the image_id used in the image DB
our img_id doesn't contain frame info
"batch3 - ""file_name"":""ASG0000019_0_S1_B06_R1_PICT0007.JPG"""
"batch5 and 7 - ""file_name"":""datasetsnapshotserengeti.seqASG000002m-frame0.imgS1_B06_R1_PICT0056.JPG"""
sometimes - 'datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG'
"id in DB (old_token): 'S6/J01/J01_R1/S6_J01_R1_IMAG0001', 'S1/B05/B05_R1/S1_B05_R1_PICT0036'"
specify which one to use for your dataset here
%% Load the image database and fill in DB info for the output bbox database
load the images database
%% Find the height and width of images from the annotation files
""
...if they are not available in the images DB
each row in this pseudo-json is a COCO formatted entry for an image sequence
%% Other functions required by specific datasets
the IDFG image database does not include images from unlabeled folders that were annotated with bounding boxes
%% Create the bbox database from all annotation files pertaining to this dataset
"for the incoming annotations, look up by category name (common) and convert to the numerical id used in our databases"
"for each annotation pseudo-json, check that the image it refers to exists in the original database"
each row in this pseudo-json is a COCO formatted entry for an image sequence
check that entry is for this dataset
category map for this entry in the annotation file - usually the same across all entries but just in case
rspb_add_image_entry(img_id)
use the image length and width in the image DB
"[top left x, top left y, width, height] in relative coordinates"
"add all images that have been sent to annotation, some of which may be empty of bounding boxes"
rspb_add_image_entry(db_image_id)
%% Common queries
This query is used when preparing tfrecords for object detector training.
We do not want to get the whole seq obj where at least one image has bbox because
some images in that sequence will not be bbox labeled so will be confusing.
Include images with bbox length 0 - these are confirmed empty by bbox annotators.
"If frame_num is not available, it will not be a field in the result iterable."
"Note that the seq_id is the Cosmos DB assigned ID for that sequence, not the"
"seq_id field, which may contain ""/"" characters."
"For public datasets to be converted to the CCT format, we get the whole seq object because"
sequence level attributes need to be included too. megadb/converters/megadb_to_cct.py handles
the case of bbox-only JSONs with the flag exclude_species_class
Getting all sequences in a dataset - for updating or deleting entries which need the id field
%% Parameters
Use None if querying across all partitions
"The `sequences` table has the `dataset` as the partition key, so if only querying"
"entries from one dataset, set the dataset name here."
"e.g. {'name': '@top_n', 'value': 100} - see query_and_upsert_examples/query_for_data.ipynb"
Use False if do not want all results stored in a single JSON.
%% Script
execute the query
loop through and save the results
MODIFY HERE depending on the query
wiitigers Unicode issue - no good mapping from DB file names to file names in blob URL
build filename
if need to re-download a dataset's images in case of corruption
entries_to_download = {
"filename: entry for filename, entry in entries_to_download.items()"
if entry['dataset'] == DATASET
}
input validation
"existing files, with paths relative to <store_dir>"
parse JSON or TXT file
"create a new storage container client for this dataset,"
and cache it
schema already checks that the min possible value of frame_num is 0
"if there are more than one image item, each needs a frame_num"
checks across all sequence items
per sequence item checks
load the schema
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory
load the schema
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory
checks across all datasets items
check for expiry date of the SAS keys
update the sequences
"`id` is from the database, as well as all attributes starting with _"
"if valuable sequence information is available, add them to the image"
required fields for an image object
add seq-level class labels for this image
add other sequence-level properties to each image too
add other image-level properties
... for im in seq['images']
... for seq in mega_db
consolidate categories
some property names have changed in the new schema
a dummy sequence ID will be generated if the image entry does not have a seq_id field
seq_id only needs to be unique within this dataset; MegaDB does not rely on it as the _id field
"""annotations"" fields are opened and have its sub-field surfaced one level up"
set the `dataset` property on each sequence to the provided dataset_name
check that the location field is the same for all images in a sequence
check which fields in a CCT image entry are sequence-level
image-level properties that really should be sequence-level
need to add (misidentified) seq properties not present for each image in a sequence to img_level_properties
"(some properties act like flags - all have the same value, but not present on each img)"
add the sequence-level properties to the sequence objects
not every sequence have to have all the seq_level_properties
get the value of this sequence-level property from the first image entry
check which fields are really dataset-level and should be included in the dataset table instead.
delete sequence-level properties that should be dataset-level
make all `class` fields lower-case; cast `seq_id` to type string in case they're integers
turn all float NaN values into None so it gets converted to null when serialized
this was an issue in the Snapshot Safari datasets
%% validation
"at first a dict of image_id: image_obj with annotations embedded, then it becomes"
an array of image objects
%% integrate the image DB
takes in image entries and species and other annotations in the image DB
convert the species category to explicit string name
there may be other fields in the annotation object
these fields should already be gotten from the image object
%% integrate the bbox DB
add any images that are not in the image DB
also add any fields in the image object that are not present already
add bbox to the annotations field
for any newly added images
"'bbox_abs': bbox_anno['bbox'],"
not keeping height and width
""
jb_csv_to_json.py
""
Convert a particular .csv file to CCT format.  Images were not available at
"the time I wrote this script, so this is much shorter than other scripts"
in this folder.
""
%% Constants and environment
%% Read source data
%% Confirm filename uniqueness (this data set has one label per image)
"Build up a map from filenames to a list of rows, checking image existence as we go"
%% Create CCT dictionaries
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
imageName = imageFilenames[0]
"As per above, this is convenient and appears to be true; asserting to be safe"
"Filenames look like ""290716114012001a1116.jpg"""
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
%% Create info struct
%% Write output
%% Sanity-check
%% Imports and constants
%% Load data
"with open(image_json,'r') as f:"
data = json.load(f)
%% Sanity-check data
%% Label previews
%% Collect images to annotate
%% Sort by sequence and frame
%% Copy to a folder by GUID
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension
im = images_to_annotate[0]
...for each image
%% Write out the annotation list
%% Constants and imports
%% Enumerate files
edited_image_folder = edited_image_folders[0]
fn = edited_image_files[0]
%% Read metadata and capture location information
i_row = 0; row = df.iloc[i_row]
Sometimes '2017' was just '17' in the date column
%% Read the .json files and build output dictionaries
json_fn = json_files[0]
if 'partial' in json_fn:
continue
line = lines[0]
if image_fn == 'SD1_238_6_26_17_16_76.73.jpg':
asdfad
SD29_079_5_14_2018_17_52.85.jpg
Re-write two-digit years as four-digit years
Sometimes the year was written with two digits instead of 4
assert len(tokens[4]) == 4 and tokens[4].startswith('20')
Have we seen this location already?
"Not a typo, it's actually ""formateddata"""
An image shouldn't be annotated as both empty and non-empty
An image shouldn't be annotated as both empty and non-empty
box = formatteddata[0]
"open_file(os.path.join(base_folder,jpeg_image_folder,image_fn))"
...for each box
...if there are boxes on this image
...for each line
...with open()
...for each json file
%% Prepare the output .json
%% Check DB integrity
%% Print unique locations
SD12_202_6_23_2017_1_31.85.jpg
%% Preview some images
%% Statistics
""
"Import a Snapshot Safari project (one project, one season)"
""
Before running this script:
""
"* Mount the blob container where the images live, or copy the"
images to local storage
""
What this script does:
""
* Creates a .json file
* Creates zip archives of the season without humans.
* Copies animals and humans to separate folders
""
After running this script:
""
* Create or update LILA page
* Push zipfile and unzipped images to LILA
* Push unzipped humans to wildlifeblobssc
* Delete images from UMN uplaod storage
""
"Snapshot Serengeti is handled specially, because we're dealing with bounding"
boxes too.  See snapshot_serengeti_lila.py.
""
%% Imports
From ai4eutils
From CameraTraps
%% Constants
project_name = 'XXX'; season_name = 'S1'; project_friendly_name = 'Snapshot Unknown'
project_name = 'SER'; season_name = 'S1-11'; project_friendly_name = 'Snapshot Serengeti'
project_name = 'KRU'; season_name = 'S1'; project_friendly_name = 'Snapshot Kruger'
project_name = 'CDB'; season_name = 'S1'; project_friendly_name = 'Snapshot Camdeboo'
project_name = 'MTZ'; season_name = 'S1'; project_friendly_name = 'Snapshot Mountain Zebra'
project_name = 'ENO'; season_name = 'S1'; project_friendly_name = 'Snapshot Enonkishu'
project_name = 'KAR'; season_name = 'S1'; project_friendly_name = 'Snapshot Karoo'
project_name = 'KGA'; season_name = 'S1'; project_friendly_name = 'Snapshot Kgalagadi'
%% Folder/file creation
E.g. KRU_S1
E.g. Z:\KRU
E.g. Z:\KRU\KRU_S1
Contains annotations for each capture event (sequence)
Maps image IDs to filenames; each line looks like:
""
"KRU_S1#1#1#2,3,KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0004.JPG"
"Total number of each answer to each question, e.g. total number of times each species was identified"
""
Not used here
Create output folders
Images will be placed in a season-specific folder inside this (the source data includes
this in path names)
%% Load metadata files
%% Convert to dictionaries (prep)
%% Convert to dictionaries (loops)
"TODO: iterrows() is a terrible way to do this, but this is one of those days"
"where I want to get this done, not get better at Python."
irow = 0; row = image_table.iloc[0]
"Loaded as an int64, converting to int here"
...for each row in the image table
Make sure image IDs are what we think they are
...for each row in the annotation table
%% Take a look at categories (just sanity-checking)
print('\nCategories by species:')
pp.pprint(categories_by_species)
%% Fill in some image fields we didn't have when we created the image table
"width, height, corrupt, seq_num_frames, location, datetime"
Every annotation in this list should have the same sequence ID
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;"
there's no reason to do this redundantly for every image
Every image in this sequence should point back to the same equence
Every annotation in this list should have the same location
Every annotation in this list should have the same datetime
Is this image on disk?
iImage = 0; im = images[0]
...for each image
"images_processed = pool.map(process_image, images)"
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))"
%% Count missing/corrupted images
%% Print distribution of sequence lengths (sanity-check)
%% Replicate annotations across images
iAnn = 0; ann = annotations[iAnn]
%% See what files are on disk but not annotated
%% Sanity-check image and annotation uniqueness
%% Minor updates to fields
%% Write .json file
%% Create a list of human files
ann = annotations[0]
%% Create public archive and public/private folders
im = images[0]
E.g. KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0001.JPG
Copy to private output folder
Add to zipfile
Possibly start a new archive
Copy to public output folder
...for each image
%% Sanity-check .json file
"This will produce some validation errors, because this zipfile doesn't include humans"
%% Zip up .json and .csv files
%% When I skip to this part (using a pre-rendered .json file)
%%
%%
ann = annotations[0]
%% Summary prep for LILA
"%% Generate preview, sanity-check labels"
"viz_options.classes_to_include = ['jackalblackbacked','bustardkori']"
%% Scrap
%% Find annotations for a particular image
%% Write a list of missing images
""
save_the_elephants_survey_A.py
""
Convert the .csv file provided for the Save the Elephants Survey A data set to a
COCO-camera-traps .json file
""
%% Constants and environment
Handle all unstructured fields in the source data as extra fields in the annotations
"photo_type really should be an image property, but there are a few conflicts"
that forced me to handle it as an annotation proprerty
%% Read source data
"%% Map filenames to rows, verify image existence"
Maps relative paths to row indices in input_metadata
"Build up a map from filenames to a list of rows, checking image existence as we go"
Ignore directories
%% Make sure the multiple-annotation cases make sense
%%
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
Force the empty category to be ID 0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image]
Example filename:
""
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG'
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG'
i_row = row_indices[0]
There are a small handful of datetime mismatches across annotations
for the same image
assert im['datetime'] == timestamp
Special cases based on the 'photo type' field
Various spellings of 'community'
Have we seen this category before?
Create an annotation
fieldname = list(mapped_fields.keys())[0]
...for each row
...for each image
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
%% Scrap
%% Find unique photo types
##
""
timelapse_csv_set_to_json.py
""
Given a directory full of reasonably-consistent Timelapse-exported
".csvs, assemble a CCT .json."
""
"Assumes that you have a list of all files in the directory tree, including"
image and .csv files.
""
##
%% Constants and imports
Text file with relative paths to all files (images and .csv files)
"%% Read file list, make a list of all image files and all .csv files"
"%% Verify column consistency, create a giant array with all rows from all .csv files"
i_csv = 0; csv_filename = csv_files[0]
%% Prepare some data structures we'll need for mapping image rows in .csv files to actual image files
Enumerate all folders containing image files
"In this data set, a site folder looks like:"
""
Processed Images\\site_name
%% Map .csv files to candidate camera folders
fn = valid_csv_files[0]
"Some site folders appear as ""XXNNNN"", some appear as ""XXNNNN_complete"""
...for each .csv file
%% Map camera folders to candidate image folders
%% Main loop over labels (prep)
Force the empty category to be ID 0
Images that are marked empty and also have a species label
%% Main loop over labels (loop)
i_row = 0; row = input_metadata.iloc[i_row]
"for i_row,row in input_metadata.iterrows():"
"Usually this is just a single folder name, sometimes it's a full path,"
which we don't want
Check whether this file exists on disk
Retrieve image width and height
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each row in the big table of concatenated .csv files
%% Fix cases where an image was annotated as 'unlabeled' and as something else
This annotation is 'unlabeled'
Was there another category associated with this image?
%% Check for un-annnotated images
Enumerate all images
list(relative_path_to_image.keys())[0]
%% Create info struct
%% Write output
%% Sanity-check the database's integrity
%% Render a bunch of images to make sure the labels got carried along correctly
""
auckland_doc_to_json.py
""
Convert Auckland DOC data set to COCO camera traps format.  This was
for a training data set where class names were encoded in path names.
""
%% Constants and imports
Filenames will be stored in the output .json relative to this base dir
%% Enumerate files
%% Assemble dictionaries
Force the empty category to be ID 0
fn = image_files[0]; print(fn)
Typically y:\Maukahuka_Auckland_Island\1_Training\Winter_Trial_2019\cat\cat\eat\20190903_IDdY_34_E3_tmp_201908240051.JPG
"This data set has two top-level folders, ""1_Training"" (which has class names encoded"
"in paths) and ""2_Testing"" (which has no class information)."
...for each image
%% Write output .json
%% Write train/test .jsons
%% Validate .json files
%% Preview labels
checkpoint
""
awc_to_json.py
""
Convert a particular .csv file to CCT format.
""
%% Constants and environment
%% Read source data
%% Main loop over labels
Force the empty category to be ID 0
iRow = 0; row = input_metadata.iloc[iRow]
"ImageID,FileName,FilePath,SpeciesID,CommonName"
"This makes an assumption of one annotation per image, which happens to be"
true in this data set.
Retrieve image width and height
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
%% Check for images that aren't included in the metadata file
Enumerate all images
list(relativePathToImage.keys())[0]
%% Create info struct
%% Write output
%% Sanity-check the database's integrity
%% Render a bunch of images to make sure the labels got carried along correctly
""
idaho-camera-traps.py
""
Prepare the Idaho Camera Traps dataset for release on LILA.
""
%% Imports and constants
Multi-threading for .csv file comparison and image existence validation
"We are going to map the original filenames/locations to obfuscated strings, but once"
"we've done that, we will re-use the mappings every time we run this script."
This is the file to which mappings get saved
The maximum time (in seconds) between images within which two images are considered the
same sequence.
"This is a two-column file, where each line is [string in the original metadata],[category name we want to map it to]"
"The output file, using the original strings"
"The output file, using obfuscated strings for everything but filenamed"
"The output file, using obfuscated strings and obfuscated filenames"
"One time only, I ran MegaDetector on the whole dataset..."
...then set aside any images that *may* have contained humans that had not already been
annotated as such.  Those went in this folder...
...and the ones that *actually* had humans (identified via manual review) got
copied to this folder...
"...which was enumerated to this text file, which is a manually-curated list of"
images that were flagged as human.
Unopinionated .json conversion of the .csv metadata
%% List files (images + .csv)
Ignore .csv files in folders with multiple .csv files
...which would require some extra work to decipher.
fn = csv_files[0]
%% Parse each .csv file into sequences (function)
csv_file = csv_files[-1]
os.startfile(csv_file_absolute)
survey = csv_file.split('\\')[0]
Sample paths from which we need to derive locations:
""
St.Joe_elk\AM99\Trip 1\100RECNX\TimelapseData.csv
Beaverhead_elk\AM34\Trip 1\100RECNX\TimelapseData.csv
""
ClearCreek_mustelids\Winter2015-16\FS-001-P\FS-001-P.csv
ClearCreek_mustelids\Summer2015\FS-001\FS-001.csv
ClearCreek_mustelids\Summer2016\IDFG-016\IDFG-016.csv
""
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017b
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017a
Load .csv file
Validate the opstate column
# Create datetimes
print('Creating datetimes')
i_row = 0; row = df.iloc[i_row]
Make sure data are sorted chronologically
""
"In odd circumstances, they are not... so sort them first, but warn"
Debugging when I was trying to see what was up with the unsorted dates
# Parse into sequences
print('Creating sequences')
i_row = 0; row = df.iloc[i_row]
Start a new sequence if:
""
* This image has no timestamp
* This image has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each row
# Parse labels for each sequence
sequence_id = location_sequences[0]
Row indices in a sequence should be adjacent
sequence_df = df[df['seq_id']==sequence_id]
# Determine what's present
Be conservative; assume humans are present in all maintenance images
The presence columns are *almost* always identical for all images in a sequence
assert single_presence_value
print('Warning: presence value for {} is inconsistent for {}'.format(
"presence_column,sequence_id))"
...for each presence column
Tally up the standard (survey) species
"If no presence columns are marked, all counts should be zero"
count_column = count_columns[0]
Occasionally a count gets entered (correctly) without the presence column being marked
"assert len(values) == 1 and values[0] == 0, 'Non-zero counts with no presence"
columns marked for sequence {}'.format(sequence_id)
"Handle this by virtually checking the ""right"" box"
Make sure we found a match
Handle 'other' tags
column_name = otherpresent_columns[0]
print('Found non-survey counted species column: {}'.format(column_name))
...for each non-empty presence column
"Very rarely, the ""otherpresent"" column is checked, but no more detail is available"
...handling non-survey species
Build the sequence data
i_row = 0; row = sequence_df.iloc[i_row]
Only one folder used a single .csv file for two subfolders
...for each sequence
...def csv_to_sequences()
%% Parse each .csv file into sequences (loop)
%%
%%
i_file = -1; csv_file = csv_files[i_file]
%% Save sequence data
%% Load sequence data
%%
%% Validate file mapping (based on the existing enumeration)
sequences = sequences_by_file[0]
sequence = sequences[0]
"Actually, one folder has relative paths"
assert '\\' not in image_file_relative and '/' not in image_file_relative
os.startfile(csv_folder)
assert os.path.isfile(image_file_absolute)
found_file = os.path.isfile(image_file_absolute)
...for each image
...for each sequence
...for each .csv file
%% Load manual category mappings
The second column is blank when the first column already represents the category name
%% Convert to CCT .json (original strings)
Force the empty category to be ID 0
For each .csv file...
""
sequences = sequences_by_file[0]
For each sequence...
""
sequence = sequences[0]
Find categories for this image
"When 'unknown' is used in combination with another label, use that"
"label; the ""unknown"" here doesn't mean ""another unknown species"", it means"
there is some other unknown property about the main species.
category_name_string = species_present[0]
"This piece of text had a lot of complicated syntax in it, and it would have"
been too complicated to handle in a general way
print('Ignoring category {}'.format(category_name_string))
Don't process redundant labels
category_name = category_names[0]
If we've seen this category before...
If this is a new category...
print('Adding new category for {}'.format(category_name))
...for each category (inner)
...for each category (outer)
...if we do/don't have species in this sequence
"We should have at least one category assigned (which may be ""empty"" or ""unknown"")"
assert len(sequence_category_ids) > 0
Was any image in this sequence manually flagged as human?
print('Flagging sequence {} as human based on manual review'.format(sequence_id))
For each image in this sequence...
""
i_image = 0; im = images[i_image]
Create annotations for this image
...for each image in this sequence
...for each sequence
...for each .csv file
Verify that all images have annotations
ann = ict_data['annotations'][0]
For debugging only
%% Create output (original strings)
%% Validate .json file
%% Preview labels
%% Look for humans that were found by MegaDetector that haven't already been identified as human
This whole step only needed to get run once
%%
Load MD results
Get a list of filenames that MD tagged as human
im = md_results['images'][0]
...for each detection
...for each image
Map images to annotations in ICT
ann = ict_data['annotations'][0]
For every image
im = ict_data['images'][0]
Does this image already have a human annotation?
...for each annotation
...for each image
%% Copy images for review to a new folder
fn = missing_human_images[0]
%% Manual step...
Copy any images from that list that have humans in them to...
%% Create a list of the images we just manually flagged
fn = human_tagged_filenames[0]
E.g. '0000_Beaverhead_elk~AM174~Trip 1~100RECNX~IMG_1397.JPG'
"%% Translate location, image, sequence IDs"
Load mappings if available
Generate mappings
If we've seen this location before...
Otherwise assign a string-formatted int as the ID
If we've seen this sequence before...
Otherwise assign a string-formatted int as the ID
Assign an image ID
...for each image
Assign annotation mappings
Save mappings
"Back this file up, lest we should accidentally re-run this script"
with force_generate_mappings = True and overwrite the mappings we used.
...if we are/aren't re-generating mappings
%% Apply mappings
"%% Write new dictionaries (modified strings, original files)"
"%% Validate .json file (modified strings, original files)"
%% Preview labels (original files)
"viz_options.classes_to_exclude = ['empty','deer','elk']"
viz_options.classes_to_include = ['bobcat']
%% Copy images to final output folder (prep)
ann = d['annotations'][0]
Is this a public or private image?
Generate absolute path
Copy to output
Update the filename reference
...def process_image(im)
%% Copy images to final output folder (execution)
For each image
im = images[0]
Write output .json
%% Make sure the right number of images got there
%% Validate .json file (final filenames)
%% Preview labels (final filenames)
"viz_options.classes_to_exclude = ['empty','deer','elk']"
viz_options.classes_to_include = ['horse']
viz_options.classes_to_include = [viz_options.multiple_categories_tag]
"viz_options.classes_to_include = ['human','vehicle','domestic dog']"
%% Create zipfiles
%% List public files
%% Find the size of each file
fn = all_public_output_files[0]
%% Split into chunks of approximately-equal size
...for each file
%% Create a zipfile for each chunk
...for each filename
with ZipFile()
...def create_zipfile()
i_file_list = 0; file_list = file_lists[i_file_list]
"....if __name__ == ""__main__"""
""
wellington_to_json.py
""
Convert the .csv file provided for the Wellington data set to a
COCO-camera-traps .json file
""
%% Constants and environment
%% Read source data
"Filenames were provided as *.jpg, but images were *.JPG, converting here"
"%% Map filenames to rows, verify image existence"
"Takes ~30 seconds, since it's checking the existence of ~270k images"
"Build up a map from filenames to a list of rows, checking image existence as we go"
"I didn't expect this to be true a priori, but it appears to be true, and"
it saves us the trouble of checking consistency across multiple occurrences
of an image.
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
imageName = imageFilenames[0]
"As per above, this is convenient and appears to be true; asserting to be safe"
"Filenames look like ""290716114012001a1116.jpg"""
This gets imported as an int64
"These appear as ""image1"", ""image2"", etc."
"In the form ""001a"""
Can be in the form '111' or 's46'
"In the form ""7/29/2016 11:40"""
Check image height and width
"Use 'empty', to be consistent with other data on lila"
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
""
mcgill_to_json.py
""
Convert the .csv file provided for the McGill test data set to a
COCO-camera-traps .json file
""
%% Constants and environment
%% Read source data
"%% Map filenames to rows, verify image existence"
Create an additional column for concatenated filenames
Maps relative filenames to rows
"Build up a map from filenames to a list of rows, checking image existence as we go"
row = input_metadata.iloc[0]
"I didn't expect this to be true a priori, but it appears to be true, and"
it saves us the trouble of checking consistency across multiple occurrences
of an image.
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
row = input_metadata.iloc[0]
"Filenames look like ""290716114012001a1116.jpg"""
"In the form ""001a"""
Can be in the form '111' or 's46'
"In the form ""7/29/2016 11:40"""
Check image height and width
NaN is the only thing we should see that's not a string
NaN is the only thing we should see that's not a string
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
""
bellevue_to_json.py
""
"""Bellevue Camera Traps"" is the rather unremarkable camera trap data set"
used by one of the repo's maintainers for testing.  It's organized as:
""
approximate_date/[loose_camera_specifier/]/species
""
E.g.:
""
"""2018.03.30\coyote\DSCF0091.JPG"""
"""2018.07.18\oldcam\empty\DSCF0001.JPG"""
""
%% Constants and imports
from the ai4eutils repo
Filenames will be stored in the output .json relative to this base dir
%% Exif functions
"%% Enumerate files, create image/annotation/category info"
Force the empty category to be ID 0
Keep track of unique camera folders
Each element will be a dictionary with fields:
""
"relative_path, width, height, datetime"
fname = image_files[0]
Corrupt or not an image
Store file info
E.g. 2018.03.30/coyote/DSCF0091.JPG
...for each image file
%% Synthesize sequence information
Sort images by time within each folder
camera_path = camera_folders[0]
previous_datetime = sorted_images_this_camera[0]['datetime']
im = sorted_images_this_camera[1]
Start a new sequence if necessary
...for each image in this camera
...for each camera
Fill in seq_num_frames
%% A little cleanup
%% Write output .json
%% Sanity-check data
%% Label previews
""
snapshot_safar_importer_reprise.py
""
This is a 2023 update to snapshot_safari_importer.py.  We do a bunch of things now that
we didn't do the last time we imported Snapshot data (like updating the big taxonomy)
"file, and we skip a bunch of things now that we used to do (like generating massive"
"zipfiles).  So, new year, new importer."
""
%% Constants and imports
%% List files
"Do a one-time enumeration of the entire drive; this will take a long time,"
but will save a lot of hassle later.
%% Create derived lists
Takes about 60 seconds
CSV files are one of:
""
"_report_lila.csv (this is the one we want to use, with the species/count/etc. for each sequence)"
_report_lila_image_inventory.csv (maps captures to images)
_report_lila_overview.csv (distrubution of species)
%% List project folders
Project folders look like one of these:
""
APN
Snapshot Cameo/DEB
%% Map report and inventory files to codes
fn = csv_files[0]
%% Make sure that every report has a corresponding inventory file
%% Count species based on overview and report files
%% Print counts
%% Make sure that capture IDs in the reports/inventory files match
...and that all the images in the inventory tables are actually present on disk.
assert image_path_relative in all_files_relative_set
Make sure this isn't just a case issue
...for each report on this project
...for each project
"%% For all the files we have on disk, see which are and aren't in the inventory files"
"There aren't any capital-P .PNG files, but if I don't include that"
"in this list, I'll look at this in a year and wonder whether I forgot"
to include it.
fn = all_files_relative[0]
print('Skipping project {}'.format(project_code))
""
plot_wni_giraffes.py
""
Plot keypoints on a random sample of images from the wni-giraffes data set.
""
%% Constants and imports
%% Load and select data
%% Support functions
https://stackoverflow.com/questions/32504246/draw-ellipse-in-python-pil-with-line-thickness
Use a single channel image (mode='L') as mask.
The size of the mask can be increased relative to the imput image
to get smoother looking results.
draw outer shape in white (color) and inner shape in black (transparent)
downsample the mask using PIL.Image.LANCZOS
(a high-quality downsampling filter).
paste outline color to input image through the mask
%% Plot some images
ann = annotations_to_plot[0]
i_tool = 0; tool_name = short_tool_names[i_tool]
Don't plot tools that don't have a consensus annotation
...for each tool
...for each annotation
""
idfg_iwildcam_lila_prep.py
""
Adding class labels (from the private test .csv) to the iWildCam 2019 IDFG
"test set, in preparation for release on LILA."
""
This version works with the public iWildCam release images.
""
"%% ############ Take one, from iWildCam .json files ############"
%% Imports and constants
%% Read input files
Remove the header line
%% Parse annotations
Lines look like:
""
"b005e5b2-2c0b-11e9-bcad-06f1011196c4,1,Private"
%% Minor cleanup re: images
%% Create annotations
%% Prepare info
%% Minor adjustments to categories
Remove unused categories
Name adjustments
%% Create output
%% Write output
%% Validate .json file
%% Preview labels
viz_options.classes_to_exclude = ['test']
"%% ############ Take two, from pre-iWildCam .json files created from IDFG .csv files ############"
%% Imports and constants
%% One-time line break addition
%% Read input files
%% Prepare info
%% Minor adjustments to categories
%% Minor adjustments to annotations
%% Create output
%% Write output
%% Validate .json file
%% Preview labels
viz_options.classes_to_exclude = ['test']
""
missouri_to_json.py
""
Create .json files from the original source files for the Missouri Camera Traps
data set.  Metadata was provided here in two formats:
""
"1) In one subset of the data, folder names indicated species names.  In Set 1,"
there are no empty sequences.  Set 1 has a metadata file to indicate image-level
bounding boxes.
""
2) A subset of the data (overlapping with (1)) was annotated with bounding
"boxes, specified in a whitespace-delimited text file.  In set 2, there are"
"some sequences omitted from the metadata file, which implied emptiness."
""
"In the end, set 2 labels were not reliable enough to publish, so LILA includes only set 1."
""
%% Constants and imports
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Filenames will be stored in the output .json relative to this base dir
This will be a list of filenames that need re-annotation due to redundant boxes
"%% Enumerate files, read image sizes (both sets)"
"Takes a few minutes, since we're reading image sizes."
Each element will be a list of relative path/full path/width/height
"Only process leaf nodes corresponding to sequences, which look like:"
""
E:\wildlife_data\missouri_camera_traps\Set1\1.02-Agouti\SEQ75583
E:\wildlife_data\missouri_camera_traps\Set2\p1d101
""
assert len(files) <= 2
Read the image
Not an image...
Store file info
"...if we didn't hit the max file limit, keep going"
...for each file
%% Add sequence lengths (both sets)
%% Load the set 1 metadata file
"List of lists, length varies according to number of bounding boxes"
""
Preserves original ordering
iLine = 0; line = metadataSet1Lines[0]
"Lines should be filename, number of bounding boxes, boxes (four values per box)"
Make sure we have image info for this image
%% Print missing files from Set 1 metadata
Manual changes I made to the metadata file:
""
'IMG' --> 'IMG_'
Red_Brocket_Deer --> Red_Deer
European-Hare --> European_Hare
Wood-Mouse --> Wood_Mouse
Coiban-Agouti --> Coiban_Agouti
%% Load the set 2 metadata file
"This metadata file contains most (but not all) images, and a class label (person/animal/empty)"
"for each, plus bounding boxes."
"List of lists, length varies according to number of bounding boxes"
""
Preserves original ordering
"Create class IDs for each *sequence*, which we'll use to attach classes to"
images for which we don't have metadata
""
This only contains mappings for sequences that appear in the metadata.
iLine = 0; line = metadataSet2Lines[0]
"Lines should be filename, number of bounding boxes, labeled boxes (five values per box)"
""
Empty images look like filename\t0\t0
E.g. 'Set2\\p1d101\\p1d101s100i10.JPG'
Make sure we don't have mixed classes within an image
"Figure out what class this *sequence* is, so we know how to handle unlabeled"
images from this sequence
Can't un-do a mixed sequence
Previously-empty sequences get the image class label
"If the sequence has a non-empty class, possibly change it"
Make sure we have image info for this image
...for each line in the set 2 metadata file
%% What Set 2 images do I not have metadata for?
These are *mostly* empty images
iImage = 0; imageID = set2ImageIDs[iImage]
%% Create categories and annotations for set 1
"Though we have no empty sequences, we do have empty images in this set"
For each image
""
iImage = 0; imageID = set1ImageIDs[iImage]
E.g. Set1\\1.80-Coiban_Agouti\\SEQ83155\\SEQ83155_IMG_0010.JPG
Find the species name
This image may still be empty...
category['count'] = category['count'] + 1
"If we have bounding boxes, create image-level annotations"
"filename, number of bounding boxes, boxes (four values per box)"
"Make sure the relative filename matches, allowing for the fact that"
some of the filenames in the metadata aren't quite right
This image is non-empty
"Some redundant bounding boxes crept in, don't add them twice"
Check this bbox against previous bboxes
""
Inefficient?  Yes.  In an important way?  No.
"Bounding box values are in absolute coordinates, with the origin"
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1]."
""
"Convert to floats and to x/y/w/h, as per CCT standard"
...for each box
if we do/don't have boxes for this image
Else create a sequence-level annotation
...for each image
%% Write out the list of images with redundant boxes
%% Create categories and annotations for set 2
For each image
""
iImage = 0; imageID = set2ImageIDs[iImage]
E.g. 'Set2\\p1d100\\p1d100s10i1.JPG'
"Find the sequence ID, sanity check filename against what we stored"
"If we have bounding boxes or an explicit empty label, create image-level annotations"
"filename, number of bounding boxes, labeled boxes (five values per box)"
"Make sure the relative filename matches, allowing for the fact that"
some of the filenames in the metadata aren't quite right
"Bounding box values are in absolute coordinates, with the origin"
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1]."
""
Convert to floats and to x/y/w/h
...for each box
...if we do/don't have boxes for this image
Else create a sequence-level annotation
...for each image
"%% The 'count' field isn't really meaningful, delete it"
"It's really the count of image-level annotations, not total images assigned to a class"
%% Write output .json files
%% Sanity-check final set 1 .json file
"python sanity_check_json_db.py --bCheckImageSizes --baseDir ""E:\wildlife_data\missouri_camera_traps"" ""E:\wildlife_data\missouri_camera_traps\missouri_camera_traps_set1.json"""
%% Generate previews
""
filenames_to_json.py
""
Take a directory of images in which species labels are encoded by folder
"names, and produces a COCO-style .json file"
""
%% Constants and imports
from the ai4eutils repo
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Filenames will be stored in the output .json relative to this base dir
"rawClassListFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_classes.csv')"
"classMappingsFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_class_mapping.csv')"
"%% Enumerate files, read image sizes"
Each element will be a list of relative path/full path/width/height
Read the image
Corrupt or not an image
Store file info
Write to output file
...for each image file
...csv file output
...if the file list is/isn't available
%% Enumerate classes
Maps classes to counts
We like 'empty' to be class 0
%% Assemble dictionaries
...for each category
Each element is a list of relative path/full path/width/height/className
...for each image
%% External class mapping
%% Write raw class table
cat = categories[0]
%% Read the mapped class table
"id, source, count, target"
"%% Make classMappings contain *all* classes, not just remapped classes"
cat = categories[0]
%% Create new class list
"Start at 1, explicitly assign 0 to ""empty"""
One-off issue with character encoding
%% Re-map annotations
ann = annotations[0]
%% Write output .json
%% Utilities
%%
Find images with a particular tag
%% Randomly sample annotations
""
umn_to_json.py
""
Prepare images and metadata for the Orinoquía Camera Traps dataset.
""
%% Imports and constants
"For two deployments, we're only processing imagse in the ""detections"" subfolder"
%% Enumerate deployment folders
%% Load ground truth
i_row = 0; row = ground_truth_df.iloc[i_row]
%% Create relative paths for ground truth data
"Some deployment folders have no subfolders, e.g. this is a valid file name:"
""
M00/01010132.JPG
""
"But some deployment folders have subfolders, e.g. this is also a valid file name:"
""
N17/100EK113/07160020.JPG
""
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly"
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths."
deployment_name = list(deployment_folders)[0]
Enumerate all files in this folder
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))"
filename = files[100]
...for each file in this deployment
...for each deployment
%% Add relative paths to our ground truth table
i_row = 0; row = ground_truth_df.iloc[i_row]
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's"
"just the deployment ID and the image name, separated by a slash."
Find the relative path for this image
Make sure this image file exists
...for each row in the ground truth table
%% Take everything out of Pandas
%% Convert string timestamps to Python datetimes
im = ground_truth_dicts[0]
"Filenames look like, e.g., N36/100EK113/06040726.JPG"
%% Synthesize sequence information
Sort images by time within each location
i_location=0; location = locations[i_location]
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_camera[1]
"Timestamp for this image, may be None"
Start a new sequence if:
""
* This image has no timestamp
* This iamge has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each image in this location
...for each location
%% Create category dict and category IDs
im = images[0]
%% Imports and constants (.json generation)
%% Count frames in each sequence
%% Double check images with multiple annotations
im = images[0]
%% Assemble dictionaries
Force the empty category to be ID 0
input_im = images[0]
...for each image
%% Write output .json
%% Validate .json file
%% Map relative paths to annotation categories
ann = data['annotations'][0]
%% Copy images to output
EXCLUDE HUMAN AND MISSING
im = data['images'][0]
im = images[0]
%% Preview labels
viz_options.classes_to_exclude = ['test']
""
nacti_fieldname_adjustments.py
""
"NACTI metadata was posted with ""filename"" in images instead of ""file_name"", and"
"used string (rather than int) category IDs (in categories, but not in annotations)."
""
This script fixes those issues and rev's the version number.
""
%% Constants and environment
%% Read .json file
"%% Rev version number, update field names and types"
%% Write json file
""
cct_field_adjustments.py
""
CCT metadata was posted with int locations instead of strings.
""
This script fixes those issues and rev's the version number.
""
%% Constants and environment
%% Read .json file
"%% Rev version number, update field names and types"
%% Write json file
%% Check output data file
""
carrizo_shrubfree_2018.py
""
Convert the .csv file provided for the Carrizo Mojave data set to a
COCO-camera-traps .json file
""
%% Constants and environment
%% Read source data
Original .csv file had superfluous spaces in column names
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
58 missing files (of 17652)
%% Check for images that aren't included in the metadata file
3012 of 20606 files are not in metadata
%% Create CCT dictionaries
Map categories to integer IDs
""
The category '0' is reserved for 'empty'
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
Each filename should just match one row
Don't include images that don't exist on disk
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
""
snapshot_serengeti_lila.py
""
Create zipfiles of Snapshot Serengeti S1-S11.
""
"Create a metadata file for S1-S10, plus separate metadata files"
"for S1-S11.  At the time this code was written, S11 was under embargo."
""
Create zip archives of each season without humans.
""
Create a human zip archive.
""
%% Constants and imports
import sys; sys.path.append(r'c:\git\ai4eutils')
import sys; sys.path.append(r'c:\git\cameratraps')
assert(os.path.isdir(metadata_base))
"There are two redundant categories, and we re-map ""blank"" to ""empty"" as per CCT convention"
"%% Load metadata files, concatenate into a single table"
iSeason = 1
%% Convert to dictionaries (prep)
%% Convert to dictionaries (loops)
"TODO: iterrows() is a terrible way to do this, but this is one of those days"
"where I want to get this done, not get better at Python."
irow = 0; row = image_table.iloc[0]
"Loaded as an int64, converting to int here"
...for each row in the image table
Make sure image IDs are what we think they are
...for each row in the annotation table
%% Load previously-saved dictionaries when re-starting mid-script
%%
%% Take a look at categories (just sanity-checking)
%%
%% Fill in some image fields we didn't have when we created the image table
"width, height, corrupt, seq_num_frames, location, datetime"
Every annotation in this list should have the same sequence ID
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;"
there's no reason to do this redundantly for every image
Every image in this sequence should point back to the same equence
Every annotation in this list should have the same location
Every annotation in this list should have the same datetime
Is this image on disk?
iImage = 0; im = images[0]
...for each image
"images_processed = pool.map(process_image, images)"
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))"
%%
%% Count missing/corrupted images
%% Print distribution of sequence lengths (sanity-check)
%% Replicate annotations across images
iAnn = 0; ann = annotations[iAnn]
%% See what files are on disk but not annotated (~15 mins)
247370 files not in the database (of 7425810)
%% Load old image database
%% Look for old images not in the new DB and vice-versa
"At the time this was written, ""old"" was S1-S6"
old_im = cct_old['images'][0]
new_im = images[0]
4 old images not in new db
12 new images not in old db
%% Save our work
%% Load our work
%%
%% Examine size mismatches
i_mismatch = -1; old_im = size_mismatches[i_mismatch]
%% Sanity-check image and annotation uniqueness
"%% Split data by seasons, create master list for public seasons"
ann = annotations[0]
%% Minor updates to fields
"%% Write master .json out for S1-10, write individual season .jsons (including S11)"
"Loop over all seasons, plus one iteration for the ""all public data"" iteration, and"
"one for the ""all data"" iteration"
%% Find categories that only exist in S11
List of categories in each season
Category 55 (fire) only in S11
Category 56 (hyenabrown) only in S11
Category 57 (wilddog) only in S11
Category 58 (kudu) only in S11
Category 59 (pangolin) only in S11
Category 60 (lioncub) only in S11
%% Prepare season-specific .csv files
iSeason = 1
%% Create a list of human files
ann = annotations[0]
%% Save our work
%% Load our work
%%
"%% Create archives (human, per-season) (prep)"
im = images[0]
im = images[0]
Don't include humans
Only include files from this season
Possibly start a new archive
...for each image
i_season = 0
"for i_season in range(0,nSeasons):"
create_season_archive(i_season)
%% Create archives (loop)
pool = ThreadPool(nSeasons+1)
"n_images = pool.map(create_archive, range(-1,nSeasons))"
"seasons_to_zip = range(-1,nSeasons)"
...for each season
%% Sanity-check .json files
%logstart -o r'E:\snapshot_temp\python.txt'
%% Zip up .json and .csv files
pool = ThreadPool(len(files_to_zip))
"pool.map(zip_single_file, files_to_zip)"
%% Super-sanity-check that S11 info isn't leaking
im = data_public['images'][0]
ann = data_public['annotations'][0]
iRow = 0; row = annotation_df.iloc[iRow]
iRow = 0; row = image_df.iloc[iRow]
%% Create bounding box archive
i_image = 0; im = data['images'][0]
i_box = 0; boxann = bbox_data['annotations'][0]
%% Sanity-check a few files to make sure bounding boxes are still sensible
import sys; sys.path.append(r'C:\git\CameraTraps')
%% Check categories
%% Summary prep for LILA
""
wi_to_json
""
Prepares CCT-formatted metadata based on a Wildlife Insights data export.
""
"Mostly assumes you have the images also, for validation/QA."
""
%% Imports and constants
"Because WI filenames are GUIDs, it's not practical to page through sequences in an"
image viewer.  So we're going to (optionally) create a copy of the data set where
images are ordered.
%% Load ground truth
%% Take everything out of Pandas
%% Synthesize common names when they're not available
"Blank rows should always have ""Blank"" as the common name"
"print('Warning: missing common name for row {} ({})'.format(i_row,row['filename']))"
%% Convert string timestamps to Python datetimes
im = ground_truth_dicts[0]
"The field called ""location"" in the WI .csv file is a URL, we want to reclaim"
"the ""location"" keyword for CCT output"
"Filenames look like, e.g., N36/100EK113/06040726.JPG"
%% Synthesize sequence information
Sort images by time within each location
i_location=0; location = locations[i_location]
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_camera[1]
"Timestamp for this image, may be None"
Start a new sequence if:
""
* This image has no timestamp
* This iamge has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each image in this location
...for each location
%% Create category dict and category IDs
im = images[0]
%% Count frames in each sequence
%% Build relative paths
im = images[0]
Sample URL:
""
gs://project-asfasdfd/deployment/21444549/asdfasdfd-616a-4d10-a921-45ac456c568a.jpg'
%% Double check images with multiple annotations
im = images[0]
%% Assemble dictionaries
Force the empty category to be ID 0
input_im = images[0]
"print('Warning: image ID {} ({}) has multiple annotations'.format(im['id'],im['id'].replace('_','/')))"
...for each image
%% Write output .json
%% Validate .json file
%% Preview labels
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))"
%%
%% Create ordered dataset
"Because WI filenames are GUIDs, it's not practical to page through sequences in an"
image viewer.  So we're going to create a copy of the data set where images are
ordered.
im = images_out[0]; im
%% Create ordered .json
%% Copy files to their new locations
im = ordered_images[0]
im = data_ordered['images'][0]
%% Preview labels in the ordered dataset
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))"
%% Open an ordered filename from the unordered filename
""
rspb_to_json.py
""
Convert the .csv file provided for the RSPB data set to a
COCO-camera-traps .json file
""
%% Constants and environment
[location] is an obfuscation
%% Create info struct
%% Read source data
metadataTable.columns.values
""
"array(['Project', 'inDir', 'FileName', 'Station', 'Camera',"
"'StationCameraFileName', 'DateTimeOriginal', 'DateReadable',"
"'outDir', 'filename_new', 'fileExistsAlready', 'CopyStatus',"
"'Species'], dtype=object)"
We'll populate these later
keys should be lowercase
"%% Enumerate images, confirm filename uniqueness"
"%% Update metadata filenames to include site and camera folders, check existence"
""
Takes ~1min
iRow = 0; row = metadataTable.iloc[iRow]
There's a bug in the metadata; the 'camera' column isn't correct.
camera = row['Camera']
"These appear as, e.g., '3.22e12'"
camera = str(int(float(camera)))
Let's pull this out of the file name instead
""
Filenames look like one of the following:
""
A1__03224850850507__2015-11-28__10-45-04(1).JPG
Bayama2PH__C05__NA(NA).JPG
assert(os.path.isfile(fullPath))
metadataTable.iloc[iRow] = row
Re-assemble into an updated table
%% Check for images that aren't included in the metadata file
Enumerate all images
Write to a text file
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
iRow = 0; row = metadataTable.iloc[iRow]
A1__03224850850507__2015-11-28__10-45-04(1).JPG
'A1\\03224850850507\\A1__03224850850507__2015-11-28__10-45-04(1).JPG'
Not currently populated
"Often -1, sometimes a semi-meaningful int"
A1
03224850850507
"In variable form, but sometimes '28/11/2015 10:45'"
Check image height and width
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Write output
%% Check database integrity
%% Preview a few images to make sure labels were passed along sensibly
%% One-time processing step: copy images to a flat directory for annotation
%%
""
pc_to_json.py
""
Convert a particular collection of .csv files to CCT format.
""
%% Constants and environment
%% Read and concatenate source data
List files
"List of dataframes, one per .csv file; we'll concatenate later"
i_file = 87; fn = input_files[i_file]
Concatenate into a giant data frame
%% List files
%% Main loop over labels (prep)
Force the empty category to be ID 0
%% Main loop over labels (loop)
iRow = 0; row = input_metadata.iloc[iRow]
"ImageID,FileName,FilePath,SpeciesID,CommonName"
assert os.path.isfile(full_path)
Retrieve image width and height
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
%% See what's up with missing files
s = list(image_relative_paths)[0]
s = missing_files[0]
%% Check for images that aren't included in the metadata file
%% Sample the database
%%
Collect the images we want
%% Create info struct
%% Write output
%% Sanity-check the database's integrity
%% Render a bunch of images to make sure the labels got carried along correctly
options.classes_to_exclude = ['unlabeled']
%% Write out a list of files to annotate
""
save_the_elephants_survey_B.py
""
Convert the .csv file provided for the Save the Elephants Survey B data set to a
COCO-camera-traps .json file
""
%% Constants and environment
input_base = r'/mnt/blobfuse/wildlifeblobssc/ste_2019_08_drop'
output_base = r'/home/gramener/survey_b'
Handle all unstructured fields in the source data as extra fields in the annotations
"photo_type really should be an image property, but there are a few conflicts"
that forced me to handle it as an annotation proprerty
%% Read source data
"%% Map filenames to rows, verify image existence"
"%% Map filenames to rows, verify image existence"
Maps relative paths to row indices in input_metadata
"Build up a map from filenames to a list of rows, checking image existence as we go"
Ignore directories
%% Make sure the multiple-annotation cases make sense
%%
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
Force the empty category to be ID 0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image]
Example filename:
""
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG'
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG'
i_row = row_indices[0]
timestamp = row['Date']
There are a small handful of datetime mismatches across annotations
for the same image
assert im['datetime'] == timestamp
Special cases based on the 'photo type' field
Various spellings of 'community'
Have we seen this category before?
Create an annotation
fieldname = list(mapped_fields.keys())[0]
...for each row
...for each image
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
%% Scrap
%% Find unique photo types
""
ubc_to_json.py
""
Convert the .csv file provided for the UBC data set to a
COCO-camera-traps .json file
""
"Images were provided in eight folders, each of which contained a .csv"
file with annotations.  Those annotations came in two slightly different
"formats, the two formats corresponding to folders starting with ""SC_"" and"
otherwise.
""
%% Constants and environment
Map Excel column names - which vary a little across spreadsheets - to a common set of names
%% Enumerate images
Load from file if we've already enumerated
%% Create CCT dictionaries
Force the empty category to be ID 0
To simplify debugging of the loop below
#%% Create CCT dictionaries (loop)
#%%
Read source data for this folder
Rename columns
Folder name is the first two characters of the filename
""
Create relative path names from the filename itself
Folder name is the camera name
""
Create relative path names from camera name and filename
Which of our images are in the spreadsheet?
i_row = 0; fn = input_metadata['image_relative_path'][i_row]
#%% Check for images that aren't included in the metadata file
Find all the images in this folder
Which of these aren't in the spreadsheet?
#%% Create entries in CCT dictionaries
Only process images we have on disk
"This is redundant, but doing this for clarity, at basically no performance"
cost since we need to *read* the images below to check validity.
i_row = row_indices[0]
"These generally represent zero-byte images in this data set, don't try"
to find the very small handful that might be other kinds of failures we
might want to keep around.
print('Error opening image {}'.format(image_relative_path))
If we've seen this category before...
...make sure it used the same latin --> common mapping
""
"If the previous instance had no mapping, use the new one."
assert common_name == category['common_name']
Create an annotation
...for each annotation we found for this image
...for each image
...for each dataset
Print all of our species mappings
"%% Copy images for which we actually have annotations to a new folder, lowercase everything"
im = images[0]
%% Create info struct
"%% Convert image IDs to lowercase in annotations, tag as sequence level"
"While there isn't any sequence information, the nature of false positives"
"here leads me to believe the images were labeled at the sequence level, so"
we should trust labels more when positives are verified.  Overall false
positive rate looks to be between 1% and 5%.
%% Write output
%% Validate output
%% Preview labels
""
helena_to_cct.py
""
Convert the Helena Detections data set to a COCO-camera-traps .json file
""
%% Constants and environment
This is one time process
%% Create Filenames and timestamps mapping CSV
import pdb;pdb.set_trace()
%% To create CCT JSON for RSPB dataset
%% Read source data
Original Excel file had timestamp in different columns
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
%% Skipping this check because one image has multiple species
assert len(duplicate_rows) == 0
%% Check for images that aren't included in the metadata file
%% Create CCT dictionaries
Map categories to integer IDs
""
The category '0' is reserved for 'empty'
For each image
Don't include images that don't exist on disk
Some filenames will match to multiple rows
assert(len(rows) == 1)
iRow = rows[0]
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
ann['datetime'] = row['datetime']
ann['site'] = row['site']
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
%% Imports and constants
from github.com/microsoft/ai4eutils
from github.com/ecologize/CameraTraps
A list of files in the lilablobssc container for this data set
The raw detection files provided by NOAA
A version of the above with filename columns added
%% Read input .csv
%% Read list of files
%% Convert paths to full paths
i_row = 0; row = df.iloc[i_row]
assert ir_image_path in all_files
...for each row
%% Write results
"%% Load output file, just to be sure"
%% Render annotations on an image
i_image = 2004
%% Download the image
%% Find all the rows (detections) associated with this image
"as l,r,t,b"
%% Render the detections on the image(s)
In pixel coordinates
In pixel coordinates
%% Save images
%% Clean up
""
carrizo_trail_cam_2017.py
""
"Convert the .csv files provided for the ""Trail Cam Carrizo"" 2017 data set to"
a COCO-camera-traps .json file.
""
%% Constants and environment
%% Read source data
Removing the empty records
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
908 missing files (of 60562)
%% Check for images that aren't included in the metadata file
105329 of 164983 files are not in metadata
%% Create CCT dictionaries
Map categories to integer IDs
""
The category '0' is reserved for 'empty'
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
Each filename should just match one row
Don't include images that don't exist on disk
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
""
channel_islands_to_cct.py
""
Convert the Channel Islands data set to a COCO-camera-traps .json file
""
"Uses the command-line tool ExifTool (exiftool.org) to pull EXIF tags from images,"
"because every Python package we tried failed to pull the ""Maker Notes"" field properly."
""
"%% Imports, constants, paths"
# Imports ##
# Constants ##
# Paths ##
Confirm that exiftool is available
"assert which(exiftool_command_name) is not None, 'Could not locate the ExifTool executable'"
%% Load information from every .json file
"Ignore the sample file... actually, first make sure there is a sample file"
...and now ignore that sample file.
json_file = json_files[0]
ann = annotations[0]
...for each annotation in this file
...for each .json file
"%% Confirm URL uniqueness, handle redundant tags"
Have we already added this image?
"One .json file was basically duplicated, but as:"
""
Ellie_2016-2017 SC12.json
Ellie_2016-2017-SC12.json
"If the new image has no output, just leave the old one there"
"If the old image has no output, and the new one has output, default to the one with output"
Don't worry about the cases where someone tagged 'fox' and someone tagged 'fox_partial'
...for each image we've already added
...if this URL is/isn't in the list of URLs we've already processed
...for each image
%% Save progress
%%
%%
%% Download files (functions)
https://www.quickprogrammingtips.com/python/how-to-download-multiple-files-concurrently-in-python.html
"This is returned with a leading slash, remove it"
%% Download files (execution)
%% Read required fields from EXIF data (functions)
"-G means ""Print group name for each tag"", e.g. print:"
""
[File]          Bits Per Sample                 : 8
""
...instead of:
""
Bits Per Sample                 : 8
"If we don't get any EXIF information, this probably isn't an image"
line_raw = exif_lines[0]
"Split on the first occurrence of "":"""
Typically:
""
"'[MakerNotes]    Sequence                        ', '1 of 3']"
Not a typo; we are using serial number as a location
"If there are multiple timestamps, make sure they're *almost* the same"
"If there are multiple timestamps, make sure they're *almost* the same"
...for each line in the exiftool output
"This isn't directly related to the lack of maker notes, but it happens that files that are missing"
maker notes also happen to be missing EXIF date information
...process_exif()
"This is returned with a leading slash, remove it"
Ignore non-image files
%% Read EXIF data (execution)
ann = images[0]
%% Save progress
Use default=str to handle datetime objects
%%
%%
"Not deserializing datetimes yet, will do this if I actually need to run this"
%% Check for EXIF read errors
%% Remove junk
Ignore non-image files
%% Fill in some None values
"...so we can sort by datetime later, and let None's be sorted arbitrarily"
%% Find unique locations
%% Synthesize sequence information
Sort images by time within each location
i_location=0; location = locations[i_location]
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_camera[1]
"Timestamp for this image, may be None"
Start a new sequence if:
""
* This image has no timestamp
* This image has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each image in this location
...for each location
%% Count frames in each sequence
images_this_sequence = [im for im in images if im['seq_id'] == seq_id]
"%% Create output filenames for each image, store original filenames"
i_location = 0; location = locations[i_location]
i_image = 0; im = sorted_images_this_location[i_image]
%% Save progress
Use default=str to handle datetime objects
%%
%%
%% Copy images to their output files (functions)
%% Copy images to output files (execution)
%% Rename the main image list for consistency with other scripts
%% Create CCT dictionaries
Make sure this is really a box
Transform to CCT format
Force the empty category to be ID 0
i_image = 0; input_im = all_image_info[0]
"This issue only impacted one image that wasn't a real image, it was just a screenshot"
"showing ""no images available for this camera"""
Convert datetime if necessary
Process temperature if available
Read width and height if necessary
I don't know what this field is; confirming that it's always None
Process object and bbox
os.startfile(output_image_full_path)
"Zero is hard-coded as the empty category, but check to be safe"
"I can't figure out the 'index' field, but I'm not losing sleep about it"
assert input_annotation['index'] == 1+i_ann
"Some annotators (but not all) included ""_partial"" when animals were partially obscured"
"Annotators *mostly* used 'none', but sometimes 'empty'.  'empty' is CCT-correct."
If we've seen this category before...
If this is a new category...
...if this is an empty/non-empty annotation
Create an annotation
...for each annotation on this image
...for each image
%% Change *two* annotations on images that I discovered contains a human after running MDv4
%% Move human images
ann = annotations[0]
%% Count images by location
%% Write output
%% Validate output
%% Preview labels
viz_options.classes_to_exclude = [0]
""
ena24_to_json_2017.py
""
Convert the ENA24 data set to a COCO-camera-traps .json file
""
%% Constants and environment
Temporary folders for human and non-human images
Clean existing output folders/zipfiles
%% Support functions
%% Read source data
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
Check image height and width
"Each row is category, [box coordinates]"
"If there's just one row, loadtxt reads it as a 1d array; make it a 2d array"
with one row
Each row is a bounding box
Have we seen this category before?
Create an annotation
...for each bounding box
"This was here for debugging; nearly every instance is Human+Horse, Human+Vehicle,"
"or Human+Dog, but there is one Rabbit+Opossium, and a few Deer+Chicken!"
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Create ZIP files for human and non human
%% Validate output
%% Preview labels
%% Imports and constants
configurations and paths
%% Helper functions
"dest_path = copy(source_path, dest_folder)"
num_workers = multiprocessing.cpu_count()
pool = ThreadPool(num_workers)
"results = pool.starmap(_copy_unzip, zip(sources, itertools.repeat(dest_folder)))"
""
print('Waiting for processes to finish...')
pool.close()
pool.join()
sequential
%% Command-line driver
if the blob container is already mounted on the VM
or you can download them using the storage Python SDK
store storage account key in environment variable AZ_STORAGE_KEY
""
eMammal_helpers.py
""
Support functions for processing eMammal metadata
""
%% Constants and imports
%% Support functions
"pad to a total of 3 digits if < 1000, or 4 digits otherwise"
img_frame is a string from the xml tree
"length 4 frame order is returned as is, others are left padded to be 3 digit long"
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels
pad to a total of 4 digits
img_frame is a string from the xml tree
"length 4 frame order is returned as is, others are left padded to be 3 digit long"
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels
others column
summer day hours: 6am - 7pm
others day hours: 7am - 6pm
""
make_eMammal_json.py
""
"Produces the COCO-formatted json database for an eMammal dataset, i.e. a"
"collection of folders, each of which contains a deployment_manifest.xml file."
""
"In this process, each image needs to be loaded to size it."
""
"To add bounding box annotations to the resulting database, use"
add_annotations_to_eMammal_json.py.
""
%% Constants and imports
"Either add the eMammal directory to your path, or run from there"
os.chdir(r'd:\git\CameraTraps\database_tools\eMammal')
import warnings
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
warnings.filterwarnings('ignore')
Should we run the image size retrieval in parallel?
%% Support functions
%% Main loop (metadata processing; image sizes are retrieved later)
deployment = folders[0]
sequence = image_sequences[0]
get species info for this sequence
add each image's info to database
img = images[0]
"some manifests don't have the ImageOrder info, but the info is in the file name"
full_img_id has no frame info
""
frame number only used in requests to iMerit for ordering
...for each image
...for each sequence
...for each deployment
%% Get image sizes
"'tasks' is currently a list of 2-tuples, with each entry as [image dictionary,path]."
""
"Go through that and copy just the image dictionaries to 'db_images', adding size"
information to each entry.  Takes a couple hours.
opening each image seems too fast for this multi-threaded version to be faster than sequential code.
%% Assemble top-level dictionaries
%% Write out .json
""
make_full_SS_json.py
""
Create a COCO-camera-traps .json file for Snapshot Serengeti data from
the original .csv files provided on Dryad.
""
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no"
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated
Snapshot Safari preparation process.
""
%% Imports and constants
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays"
Count the number of images with multiple species
"%% Read image .csv file, format into a dictionary mapping images to capture events"
%% Create CCT-style .json
still need image width and height
...for each image
%% Write output files
""
make_per_season_SS_json.py
""
Create a COCO-camera-traps .json file for each Snapshot Serengeti season from
the original .csv files provided on Dryad.
""
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no"
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated
Snapshot Safari preparation process.
""
%% Imports and constants
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays"
"%% Read image .csv file, format into a dictionary mapping images to capture events"
%% Create CCT-style .json
still need image width and height
...for each image ID
%% Write output files
...for each season
""
generate_lila_per_image_labels.py
""
"Generate a .csv file with one row per annotation, containing full URLs to every"
"camera trap image on LILA, with taxonomically expanded labels."
""
"Typically there will be one row per image, though images with multiple annotations"
will have multiple rows.
""
"Some images may not physically exist, particularly images that are labeled as ""human""."
This script does not validate image URLs.
""
Does not include bounding box annotations.
""
%% Constants and imports
"We'll write images, metadata downloads, and temporary files here"
"Some datasets don't have ""sequence_level_annotation"" fields populated, but we know their"
annotation level
%% Download and parse the metadata file
To select an individual data set for debugging
%% Download and extract metadata for the datasets we're interested in
%% Load taxonomy data
"%% Build a dictionary that maps each [dataset,query] pair to the full taxonomic label set"
i_row = 0; row = taxonomy_df.iloc[i_row]
%% Process annotations for each dataset
ds_name = list(metadata_table.keys())[0]
"Go through annotations, marking each image with the categories that are present"
""
ann = annotations[0]
im = images[10]
This field name was only used for Caltech Camera Traps
raise ValueError('Suspicious date parsing result')
Special case we don't want to print a warning about
"Location, sequence, and image IDs are only guaranteed to be unique within"
"a dataset, so for the output .csv file, include both"
category_name = list(categories_this_image)[0]
Only print a warning the first time we see an unmapped label
...for each category that was applied at least once to this image
...for each image in this dataset
print('Warning: no date information available for this dataset')
print('Warning: no location information available for this dataset')
...for each dataset
...with open()
%% Read the .csv back
%% Do some post-hoc integrity checking
"-1 isn't *really* valid, but we use it sometimes for sequences of unknown length"
i_row = 0; row = df.iloc[i_row]
%% Preview constants
%% Choose images to download
ds_name = list(metadata_table.keys())[2]
Find all rows for this dataset
...for each dataset
%% Download images
i_image = 0; image = images_to_download[i_image]
%% Write preview HTML
im = images_to_download[0]
""
Common constants and functions related to LILA data management/retrieval.
""
%% Imports and constants
LILA camera trap master metadata file
"wildlife_insights_taxonomy_url = 'https://api.wildlifeinsights.org/api/v1/taxonomy?fields=class,order,family,genus,species,authority,taxonomyType,uniqueIdentifier,commonNameEnglish&page[size]={}'.format(wildlife_insights_page_size)"
from ai4eutils
%% Common functions
"We haven't implemented paging, make sure that's not an issue"
d['data'] is a list of items that look like:
Put the master metadata file in the same folder where we're putting images
Read lines from the master metadata file
Parse those lines into a table
Each line in this file is name/sas_url/json_url/[bbox_json_url]
Create a separate entry for bounding boxes if they exist
Unzip if necessary
""
get_lila_category_list.py
""
Generates a .json-formatted dictionary mapping each LILA dataset to all categories
"that exist for that dataset, with counts for the number of occurrences of each category"
"(the number of *annotations* for each category, not the number of *images*)."
""
"Also loads the taxonomy mapping file, to include scientific names for each category."
""
get_lila_category_counts counts the number of *images* for each category in each dataset.
""
%% Constants and imports
array to fill for output
"We'll write images, metadata downloads, and temporary files here"
Created by get_lila_category_list.py... contains counts for each category
%% Load category and taxonomy files
%% Map dataset names and category names to scientific names
i_row = 1; row = taxonomy_df.iloc[i_row]; row
%% Download and parse the metadata file
%% Download and extract metadata for the datasets we're interested in
%% Get category names and counts for each dataset
ds_name = 'NACTI'
Open the metadata file
Collect list of categories and mappings to category name
ann = annotations[0]
c = categories[0]
"Don't do taxonomy mapping for bbox data sets, which are sometimes just binary and are"
always redundant with the class-level data sets.
"As of right now, this is the only quirky case"
...for each dataset
%% Save dict
%% Print the results
ds_name = list(dataset_to_categories.keys())[0]
...for each dataset
""
download_lila_subset.py
""
"Example of how to download a list of files from LILA, e.g. all the files"
in a data set corresponding to a particular species.
""
%% Constants and imports
LILA camera trap master metadata file
"In this example, we're using the Missouri Camera Traps data set and the Caltech Camera Traps dataset"
All lower-case; we'll convert category names to lower-case when comparing
"We'll write images, metadata downloads, and temporary files here"
"We will demonstrate two approaches to downloading, one that loops over files"
"and downloads directly in Python, another that uses AzCopy."
""
AzCopy will generally be more performant and supports resuming if the
transfers are interrupted.  This script assumes that azcopy is on the system path.
Number of concurrent download threads (when not using AzCopy) (AzCopy does its
own magical parallelism)
%% Support functions
remove the leading '/'
%% Download and parse the metadata file
Put the master metadata file in the same folder where we're putting images
Read lines from the master metadata file
Parse those lines into a table
Each line in this file is name/base_url/json_url/[box_url]
%% Download and extract metadata for the datasets we're interested in
Unzip if necessary
...for each dataset of interest
%% List of files we're going to download (for all data sets)
"Flat list or URLS, for use with direct Python downloads"
For use with azcopy
This may or may not be a SAS URL
# Open the metadata file
# Build a list of image files (relative path names) that match the target species
Retrieve all the images that match that category
Retrieve image file names
Convert to URLs
...for each dataset
%% Download those image files
ds_name = 'Caltech Camera Traps'
ds_name = 'SWG Camera Traps'
We want to use the whole relative path for this script (relative to the base of the container)
"to build the output filename, to make sure that different data sets end up in different folders."
This may or may not be a SAS URL
For example:
""
caltech-unzipped/cct_images
swg-camera-traps
Check whether the URL includes a folder
E.g. caltech-unzipped
E.g. cct_images
E.g. swg-camera-traps
"Write out a list of files, and use the azcopy ""list-of-files"" option to download those files."
""
This azcopy feature is unofficially documented at:
""
https://github.com/Azure/azure-storage-azcopy/wiki/Listing-specific-files-to-transfer
""
import clipboard; clipboard.copy(cmd)
Loop over files
""
get_lila_category_counts.py
""
Count the number of images and bounding boxes with each label in one or more LILA datasets.
""
"This script doesn't write these counts out anywhere other than the console, it's just intended"
as a template for doing operations like this on LILA data.  get_lila_category_list.py writes
"information out to a .json file, but it counts *annotations*, not *images*, for each category."
""
%% Constants and imports
"If None, will use all datasets"
"We'll write images, metadata downloads, and temporary files here"
%% Download and parse the metadata file
%% Download and extract metadata for the datasets we're interested in
%% Count categories
ds_name = datasets_of_interest[0]
"Go through annotations, marking each image with the categories that are present"
""
ann = annotations[0]
Now go through images and count categories
im = images[0]
...for each dataset
%% Print the results
...for each dataset
""
add_url_to_database.py
""
"Adds a ""url"" field to a coco-camera-traps .json database, specifically to allow the db to"
be reviewed in the Visipedia annotation tool.
""
""
remove_corrupted_images_from_database.py
""
"Given a coco-camera-traps .json file, checks all images for TF-friendliness and generates"
a new .json file that only contains the non-corrupted images.
""
%% Imports and constants
import multiprocessing
"I leave this at an annoying low number, since by definition weird stuff will"
"be happening in the TF kernel, and it's useful to keep having content in the console."
%% Function definitions
"I sometimes pass in a list of images, sometimes a dict with a single"
element mapping a job ID to the list of images
"We're about to start a lot of TF sessions, and we don't want gobs"
of debugging information printing out for every session.
At some point we were creating a single session and looping over images
"within that session, but the only way I found to reliably not run out"
of GPU memory was to create a session per image and gc.collect() after
each session.
Map Image IDs to boolean (should I keep this image?)
"Convert to lists, append job numbers to the image lists"
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)"
Merge results
%% Interactive driver
%%
base_dir = r'D:\temp\snapshot_serengeti_tfrecord_generation'
Load annotations
Check for corruption
Write out only the uncorrupted data
%% Command-line driver
#######
""
integrity_check_json_db.py
""
"Does some integrity-checking and computes basic statistics on a db, specifically:"
""
* Verifies that required fields are present and have the right types
* Verifies that annotations refer to valid images
* Verifies that annotations refer to valid categories
"* Verifies that image, category, and annotation IDs are unique"
""
* Optionally checks file existence
""
* Finds un-annotated images
* Finds unused categories
""
* Prints a list of categories sorted by count
""
#######
%% Constants and environment
%% Functions
"If baseDir is non-empty, checks image existence"
This is used in a medium-hacky way to share modified options across threads
print('Image path {} does not exist'.format(filePath))
"#%% Read .json file if necessary, integrity-check fields"
info = data['info']
"#%% Build dictionaries, checking ID uniqueness and internal validity as we go"
Confirm that required fields are present
Confirm ID uniqueness
...for each category
image = images[0]
Confirm that required fields are present
Confirm ID uniqueness
We previously supported ints here; this should be strings now
"assert isinstance(image['location'], str) or isinstance(image['location'], int), \"
'Illegal image location type'
Are we checking for unused images?
Recursively enumerate images
print('Image {} is unused'.format(p))
Are we checking file existence and/or image size?
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)"
...for each image
Confirm that required fields are present
Confirm ID uniqueness
Confirm validity
...for each annotation
#%% Print statistics
Find un-annotated images and multi-annotation images
Find unused categories
Prints a list of categories sorted by count
""
https://stackoverflow.com/questions/72899/how-do-i-sort-a-list-of-dictionaries-by-a-value-of-the-dictionary
...def integrity_check_json_db()
%% Command-line driver
%% Interactive driver(s)
%%
Integrity-check .json files for LILA
options.iMaxNumImages = 10
""
combine_two_json_files.py
""
"Merges two coco-camera-traps .json files. In particular, categories are combined and re-numbered."
""
Combined Info
Combined Images
Combined Categories
## categories to merge
Combined Annotations
""
make_detection_db_for_viewing.py
""
"Given a .json file with ground truth bounding boxes, and a .p file containing detections for the same images,"
"creates a new .json file with separate classes for ground truth and detection, suitable for viewing in the Visipedia"
annotation tool.
""
%% Imports and constants
%% Main function
im_id_to_im = {im['id']:im for im in images}
make new categories to distinguish between ground truth and detections
"update all gt annotations to be class ""gt"""
collect all detections by image
keep any detection with score above det_thresh
"need to convert bbox from [x1,y1,x2,y2] to [x,y,w,h]"
"add ""info"" and ""licenses"" for annotation tools to function"
create new db
%% Command-line handling
""
analyze_json_database.py
""
Plots location/class/etc. distributions for classes in a coco-camera-traps .json file.
""
Currently includes some one-off code for specific species.
""
%% Constants and imports
%% Path configuration
%% Load source data
%% Build image/category dictionaries
%% Make plot of category distribution
%% make plots of location distribution
"plt.title('Number of images per location, by category')"
"plt.tight_layout(rect=[0,0,1,0.9])"
#make plot of images per season
%% Make plot of lions per location
%% Make plot of elephants per location
for loc in sorted_by_total[:25]:
"print('Location:' + loc[0] +', Lions: ' + str(len(loc_to_lion_ims[loc[0]])) + ', Elephants: ' + str(len(loc_to_elephant_ims[loc[0]])) + ', Total ims: ' + str(len(loc_to_ims[loc[0]])))"
""
subset_json_db.py
""
Select a subset of images (and associated annotations) from a .json file
in COCO Camera Traps format.
""
"To subset the .json files produced by our batch processing API, see"
subset_json_detector_output.py
""
Sample invocation:
""
"python subset_json_db.py ""E:\Statewide_wolf_container\idfg_20190409.json"" ""E:\Statewide_wolf_container\idfg_20190409_clearcreek.json"" ""clearcreek"" --ignore_case"
""
%% Constants and imports
%% Functions
Load the input file if necessary
Find images matching the query
Find annotations referring to those images
Write the output file if requested
%% Interactive driver
%%
%% Command-line driver
%% Constants and imports
%% Merge functions
i_input_dict = 0; input_dict = input_dicts[i_input_dict]
We will prepend an index to every ID to guarantee uniqueness
Map detection categories from the original data set into the merged data set
...for each category
Merge original image list into the merged data set
Create a unique ID
...for each image
Same for annotations
...for each annotation
"Merge info dicts, don't check completion time fields"
...for each dictionary
Convert merged image dictionaries to a sorted list
%% Driver
""
add_width_and_height_to_database.py
""
Grabs width and height from actual image files for a .json database that is missing w/h.
""
Originally used when we created a .json file for snapshot serengeti from .csv.
""
"This file converts the JSON output of the batch processing API to a pickle file, which can be used by the"
script ./make_classification_dataset.py
"Convert boxes from JSON   [x_min, y_min, width_of_box, height_of_box]"
"to PICKLE [ymin,  xmin,  ymax,         xmax]"
Write detections to file with pickle
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the"
script ./make_classification_dataset.py
Write detections to file with pickle
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the"
script ./make_classification_dataset.py
Assumes that the root of the CameraTrap repo is in the PYTHONPATH
Minimum threshold to put a detection into the output JSON file
Parameter check
Load detections from input
Load COCO style annotations
Build output JSON in format version 1.0
Adding the only known metadata info
"The pickle file does not contain category information, so we assume the default"
For each image with detections
for each detection
"Convert boxes from [ymin, xmin, ymax, xmax] format to"
"[x_min, y_min, width_of_box, height_of_box]"
Write output json
global flag for whether or not we encounter missing images
"- will only print ""missing image"" warning once"
TFRecords variables
1.3 for the cropping during test time and 1.3 for the context that the
CNN requires in the left-over image
Create output directories
Load COCO style annotations from the input dataset
"Get all categories, their names, and create updated ID for the json file"
Prepare the coco-style json files
Split the dataset by locations
Load detections
Write out COCO-style json files to the output directory
Write detections to file with pickle
## Preparations: get all the output tensors
For all images listed in the annotations file
Skip the image if it is annotated with more than one category
"Get ""old"" and ""new"" category ID and category name for this image."
Skip if in excluded categories.
get path to image
"If we already have detection results, we can use them"
Otherwise run detector and add detections to the collection
Only select detections with confidence larger than DETECTION_THRESHOLD
Skip image if no detection selected
whether it belongs to a training or testing location
Skip images that we do not have available right now
- this is useful for processing parts of large datasets
Load image
Run inference
"remove batch dimension, and convert from float32 to appropriate type"
convert normalized bbox coordinates to pixel coordinates
Pad the detected animal to a square box and additionally by
"PADDING_FACTOR. The result will be in crop_boxes. However, we need to make"
sure that its box coordinates are still within the image.
"for each bounding box, crop the image to the padded box and save it"
"Create the file path as it will appear in the annotation json,"
adding the box number if there are multiple boxes
"if the cropped file already exists, verify its size"
Add annotations to the appropriate json
js_keys = ['/'.join(im['file_name'].split('/')[1:])[:-4] for im in js['images']]
for tk in js_keys:
"assert np.isclose(1, np.sum(detections[tk]['detection_scores'] > 0.5))"
""
separate_detections_by_size
""
Not-super-well-maintained script to break a list of API output files up
based on bounding box size.
""
%% Imports and constants
Folder with one or more .json files in it that we want to split up
Enumerate .json files
Define size thresholds and confidence thresholds
"Not used directly in this script, but useful if we want to generate previews"
%% Split by size
For each size threshold...
For each file...
fn = input_files[0]
Just double-checking; we already filtered this out above
Don't reprocess .json files we generated with this script
Load the input file
For each image...
1.1 is the same as infinity here; no box can be bigger than a whole image
What's the smallest detection above threshold?
"[x_min, y_min, width_of_box, height_of_box]"
""
size = w * h
...for each detection
Which list do we put this image on?
...for each image in this file
Make sure the number of images adds up
Write out all files
...for each size threshold
...for each file
""
tile_images.py
""
Split a folder of images into tiles.  Preserves relative folder structure in a
"new output folder, with a/b/c/d.jpg becoming, e.g.:"
""
a/b/c/d_row_0_col_0.jpg
a/b/c/d_row_0_col_1.jpg
""
%% Imports and constants
from ai4eutils
%% Main function
TODO: parallelization
""
i_fn = 2; relative_fn = image_relative_paths[i_fn]
Can we skip this image because we've already generated all the tiles?
TODO: super-sloppy that I'm pasting this code from below
From:
""
https://github.com/whiplashoo/split-image/blob/main/src/split_image/split.py
i_col = 0; i_row = 1
left/top/right/bottom
...for each row
...for each column
...for each image
%% Interactive driver
%%
%%
""
rde_debug.py
""
Some useful cells for comparing the outputs of the repeat detection
"elimination process, specifically to make sure that after optimizations,"
results are the same up to ordering.
""
%% Compare two RDE files
i_dir = 0
break
"Regardless of ordering within a directory, we should have the same"
number of unique detections
Re-sort
Make sure that we have the same number of instances for each detection
Make sure the box values match
""
aggregate_video.py
""
Aggregate results and render output video for a video that's already been run through MD
""
%% Constants
%% Processing
im = d['images'][0]
...for each detection
This is no longer included in output files by default
# Split into frames
# Render output video
## Render detections to images
## Combine into a video
""
ocr_sandbox.py
""
sandbox for experimenting with using OCR to pull metadata from camera trap images
""
The general approach is:
""
"* Crop a fixed percentage from the top and bottom of an image, slightly larger"
than the largest examples we've seen of how much space is used for metadata.
""
"* Refine that crop by blurring a little, then looking for huge peaks in the"
"color histogram suggesting a solid background, then finding rows that are"
mostly that color.
""
"* Crop to the refined crop, then run pytesseract to extract text"
""
"* Use regular expressions to find time and date, in the future can add, e.g.,"
"temperature (which is often present *only* in the images, unlike time/date which"
are also usually in EXIF but often wrong or lost in processing)
""
"The metadata extraction (EXIF, IPTC) here is just sample code that seemed to"
belong in this file.
""
Contact: Dan Morris (cameratraps@lila.science)
""
%% Constants and imports
pip install pytesseract
""
"Also intall tesseract from: https://github.com/UB-Mannheim/tesseract/wiki, and add"
"the installation dir to your path (on Windows, typically C:\Program Files (x86)\Tesseract-OCR)"
pip install IPTCInfo3
from the ai4eutils repo: https://github.com/Microsoft/ai4eutils
""
"Only used for writing out a summary, not important for the core metadata extraction"
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Using a semi-arbitrary metric of how much it feels like we found the
"text-containing region, discard regions that appear to be extraction failures"
Pad each crop with a few pixels to make tesseract happy
Discard text from the top
"When we're looking for pixels that match the background color, allow some"
tolerance around the dominant color
We need to see a consistent color in at least this fraction of pixels in our rough
crop to believe that we actually found a candidate metadata region.
"What fraction of the [top,bottom] of the image should we use for our rough crop?"
A row is considered a probable metadata row if it contains at least this fraction
"of the background color.  This is used only to find the top and bottom of the crop area,"
"so it's not that *every* row needs to hit this criteria, only the rows that are generally"
above and below the text.
%% Support functions
"%% Load some images, pull EXIF and IPTC data for fun"
%% Rough crop
"This will be an nImages x 1 list of 2 x 1 lists (image top, image bottom)"
image = images[0]
"l,t,r,b"
""
"0,0 is upper-left"
"%% Close-crop around the text, return a revised image and success metric"
Did we find a sensible mode that looks like a background value?
"This looks very scientific, right?  Definitely a probability?"
"print('Failed min background fraction test: {} of {}'.format(pBackGroundValue,minBackgroundFraction))"
"Notes to self, things I tried that didn't really go anywhere..."
"analysisImage = cv2.blur(analysisImage, (3,3))"
"analysisImage = cv2.medianBlur(analysisImage,5)"
"analysisImage = cv2.Canny(analysisImage,100,100)"
imagePil = Image.fromarray(analysisImage); imagePil
Use row heuristics to refine the crop
""
This egregious block of code makes me miss my fluency in Matlab.
"print('Cropping to {},{},{},{}'.format(x,y,w,h))"
Crop the image
"For some reason, tesseract doesn't like characters really close to the edge"
imagePil = Image.fromarray(croppedImage); imagePil
%% Go to OCR-town
"An nImages x 2 list of strings, extracted from the top and bottom of each image"
An nImages x 2 list of cropped images
iImage = 0; iRegion = 1; regionSet = imageRegions[iImage]; region = regionSet[iRegion]
text = pytesseract.image_to_string(region)
pil --> cv2
"image = cv2.medianBlur(image, 3)"
"image = cv2.erode(image, None, iterations=2)"
"image = cv2.dilate(image, None, iterations=4)"
"image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]"
"image = cv2.blur(image, (3,3))"
"image = cv2.copyMakeBorder(image,10,10,10,10,cv2.BORDER_CONSTANT,value=[0,0,0])"
"text = pytesseract.image_to_string(imagePil, lang='eng')"
https://github.com/tesseract-ocr/tesseract/wiki/Command-Line-Usage
"psm 6: ""assume a single uniform block of text"""
""
...for each cropped region
...for each image
%% Extract dates and times
s = '1:22 pm'
s = '1:23:44 pm'
%% Write results to a handy html file
Add image name and resized image
Add results and individual region images
"textStyle = ""font-family:calibri,verdana,arial;font-weight:bold;font-size:150%;text-align:left;margin-left:50px;"""
%% Scrap
Alternative approaches to finding the text/background  region
Using findCountours()
imagePil = Image.fromarray(analysisImage); imagePil
"analysisImage = cv2.erode(analysisImage, None, iterations=3)"
"analysisImage = cv2.dilate(analysisImage, None, iterations=3)"
"analysisImage = cv2.threshold(analysisImage, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]"
Find object with the biggest bounding box
Using connectedComponents()
analysisImage = image
print('Found {} components'.format(nb_components))
We just want the *background* image
""
"Takes a .json file with MD results for an individual video, and converts to a .csv that includes"
frame times.  This is very bespoke to animal detection and does not include other classes.
""
%% Imports and constants
Only necessary if you want to extract the sample rate from the video
%% Extract the sample rate if necessary
%% Load results
%% Convert to .csv
i_image = 0; im = results['images'][i_image]
""
umn-pr-analysis.py
""
Precision/recall analysis for UMN data
""
%% Imports and constants
results_file = results_file_filtered
"For two deployments, we're only processing imagse in the ""detections"" subfolder"
String to remove from MegaDetector results
%% Enumerate deployment folders
%% Load MD results
im = md_results['images'][0]
%% Load ground truth
i_row = 0; row = ground_truth_df.iloc[i_row]
%% Create relative paths for ground truth data
"Some deployment folders have no subfolders, e.g. this is a valid file name:"
""
M00/01010132.JPG
""
"But some deployment folders have subfolders, e.g. this is also a valid file name:"
""
N17/100EK113/07160020.JPG
""
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly"
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths."
deployment_name = list(deployment_folders)[0]
Enumerate all files in this folder
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))"
filename = files[100]
...for each file in this deployment
...for each deployment
%% Map relative paths to MD results
%% Add relative paths to our ground truth table
i_row = 0; row = ground_truth_df.iloc[i_row]
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's"
"just the deployment ID and the image name, separated by a slash."
Find the relative path for this image
Make sure we have MegaDetector results for this file
Make sure this image file exists
...for each row in the ground truth table
%% Take everything out of Pandas
%% Some additional error-checking of the ground truth
An early version of the data required consistency between common_name and is_blank
%% Combine MD and ground truth results
d = ground_truth_dicts[0]
Find the maximum confidence for each category
...for each detection
...for each image
%% Precision/recall analysis
...for each image
Thresholds go up throughout precisions/recalls/thresholds; find the last
value where recall is at or above target.  That's our precision @ target recall.
Flatten the confusion matrix
Write precision/recall plot to .png file in output directory
pr_figure_relative_filename = 'prec_recall.png'
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)"
plt.show(block=False)
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')"
%% Find and manually review all images of humans
%%
"...if this image is annotated as ""human"""
...for each image
%% Find and manually review all MegaDetector animal misses
%%
im = merged_images[0]
GT says this is not an animal
GT says this is an animal
%% Convert .json to .csv
%%
""
kga-pr-analysis.py
""
Precision/recall analysis for KGA data
""
%% Imports and constants
%% Load and filter MD results
%% Load and filter ground truth
%% Map images to image-level results
%% Map sequence IDs to images and annotations to images
Verify consistency of annotation within a sequence
TODO
%% Find max confidence values for each category for each sequence
seq_id = list(sequence_id_to_images.keys())[1000]
im = images_this_sequence[0]
det = md_result['detections'][]
...for each detection
...for each image in this sequence
...for each sequence
%% Prepare for precision/recall analysis
seq_id = list(sequence_id_to_images.keys())[1000]
cat_id = list(category_ids_this_sequence)[0]
...for each category in this sequence
...for each sequence
%% Precision/recall analysis
"Confirm that thresholds are increasing, recall is decreasing"
This is not necessarily true
assert np.all(precisions[:-1] <= precisions[1:])
Thresholds go up throughout precisions/recalls/thresholds; find the max
value where recall is at or above target.  That's our precision @ target recall.
"This is very slightly optimistic in its handling of non-monotonic recall curves,"
but is an easy scheme to deal with.
Flatten the confusion matrix
Write precision/recall plot to .png file in output directory
pr_figure_relative_filename = 'prec_recall.png'
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)"
plt.show(block=False)
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')"
%% Scrap
%% Find and manually review all sequence-level MegaDetector animal misses
input_base = '/media/user/lila-01/lila/snapshot-safari/KGA/KGA_public'
i_sequence = 0; seq_id = pr_sequence_ids[i_sequence]
i_seq = 0; seq_id = false_negative_sequences[i_seq]
"sequence_folder = os.path.join(sequence_preview_dir,'seq_{}'.format(str(i_seq).zfill(3)))"
fn = image_files[0]
"print('Copying {} to {}'.format(input_path,output_path))"
...for each file in this sequence.
...for each sequence
%% Image-level postprocessing
parse arguments
check if a GPU is available
load a pretrained embedding model
setup experiment
load the embedding model
setup the target dataset
setup finetuning criterion
setup an active learning environment
create a classifier
the main active learning loop
Active Learning
finetune the embedding model and load new embedding values
gather labeled pool and train the classifier
save a snapshot
Load a checkpoint if necessary
setup the training dataset and the validation dataset
setup data loaders
check if a GPU is available
create a model
setup loss criterion
define optimizer
load a checkpoint if provided
setup a deep learning engine and start running
train the model
train for one epoch
evaluate on validation set
save a checkpoint
open the file
read it
do the substitution
matplotlib.use('Agg')
from UIComponents.DBObjects import *
Initialize Database
# database connection credentials
# try to connect as USER to database DB_NAME through peewee
Load the saved embedding model
dataset_query = Detection.select().limit(5)
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})"
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getalllabels()) , dataset.getallpaths(), {})"
Random examples to start
"random_ids = np.random.choice(dataset.current_set, 1000, replace=False).tolist()"
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)"
print(random_ids)
Move Records
"moveRecords(dataset, DetectionKind.ModelDetection.value, DetectionKind.UserDetection.value, random_ids)"
#print([len(x) for x in dataset.set_indices])
# Finetune the embedding model
#dataset.set_kind(DetectionKind.UserDetection.value)
#dataset.train()
"#train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)"
"#finetune_embedding(model, checkpoint['loss_type'], dataset, 32, 4, 100)"
#save_checkpoint({
"#        'arch': model.arch,"
"#        'state_dict': model.state_dict(),"
"#        'optimizer' : optimizer.state_dict(),"
"#        'loss_type' : loss_type,"
"#        }, False, ""%s%s_%s_%04d.tar""%('finetuned', loss_type, model.arch, len(dataset.set_indices[DetectionKind.UserDetection.value])))"
Get indices of samples to get user to label
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
"kwargs[""already_selected""].extend(indices)"
Train on samples that have been labeled so far
Test on the samples that have not been labeled
"'optimizer' : optimizer.state_dict(),"
num_classes= len(train_dataset.getClassesInfo()[0])
"print(""Num Classes= ""+str(num_classes))"
define loss function (criterion) and optimizer
"optimizer = torch.optim.SGD(params, momentum = 0.9, lr = args.lr, weight_decay = args.weight_decay)"
optimizer.load_state_dict(checkpoint['optimizer'])
train for one epoch
"adjust_lr(optimizer,epoch)"
if epoch % 1 == 0 and epoch > 0:
"a, b, c = e.predict(train_embd_loader, load_info = True, dim = args.feat_dim)"
"plot_embedding(reduce_dimensionality(a), b, c, {})"
evaluate on validation set
--------some stuff needed to get AJAX to work with bottle?--------#
-------------------------------------------------------------------------------- #
PREPARE TO QUEUE IMAGES FOR LABELING
-------------------------------------------------------------------------------- #
# Connect as USER to database DB_NAME through peewee and initialize database proxy
# Load embedding model
---------------------------------------------------------------------- #
CREATE QUEUE OF IMAGES TO LABEL
---------------------------------------------------------------------- #
Use classifier to generate predictions
# Update model predicted class in PostgreSQL database
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
-------------------------------------------------------------------------------- #
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI
-------------------------------------------------------------------------------- #
"# static routes (to serve CSS, etc.)"
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps"
"return bottle.static_file(filename, root='../../../../../../../../../../../.')"
# dynamic routes
if data['display_grayscale']:
indices_to_exclude.update(set(color_indices))
elif not data['display_grayscale']:
indices_to_exclude.update(set(grayscale_indices))
data['display_images'] = {}
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices]
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices]
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices]
data['display_images']['detection_categories'] = []
for i in indices:
if str(dataset.samples[i][1]) == 'None':
data['display_images']['detection_categories'].append('None')
else:
existing_category_entries = {cat.id: cat.name for cat in Category.select()}
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()"
data['display_images']['detection_categories'].append(cat_name)
Use image ids in images_to_label to get the corresponding dataset indices
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
print(set(dataset.set_indices[4]).update(set(indices_to_label)))
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Use image id images_to_label to get the corresponding dataset index
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Train on samples that have been labeled so far
dataset.set_kind(DetectionKind.UserDetection.value)
print(y_train)
Predict on the samples that have not been labeled
print(y_pred)
Update model predicted class in PostgreSQL database
timer = time.time()
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
print('Updating the database took %0.2f seconds'%(time.time() - timer))
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
"once the classifier has been trained the first time, switch to AL sampling"
# sourceMappingURL=bootstrap.bundle.min.js.map
"print(ap_distances.size(),an_distances.size())"
losses = -(((-ap_distances)/128)+1+1e-16).log() - (((-(128-an_distances))/128)+1+1e-16).log()
import pdb
pdb.set_trace()
losses = ap_distances - an_distances + self.margin
print(losses.size())
from UIComponents.DBObjects import *
TODO: should this also change self.kind?
get the embedding representations for all samples (i.e. set current_set to all indices)
"return PILImage.open(os.path.join(self.img_base,path)).convert('RGB')"
"print(self.labels_set, self.n_classes)"
from sklearn.manifold import TSNE
embedding= TSNE(n_components=2).fit_transform(X)
embedding= PCA(n_components=2).fit_transform(X)
return X
"print(dir(event), type(sc))"
"print(label,bgcolor)"
"patches.append(mpatches.Patch(color=indexcolors[i], label=label))"
"plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,"
"ncol=12, mode=""expand"", borderaxespad=0., handles=patches)"
plt.legend(handles=patches)
"plt.xlabel('Dim 1', fontsize=12)"
"plt.ylabel('Dim 2', fontsize=12)"
plt.grid(True)
"plt.xlabel('Dim 1', fontsize=12)"
"plt.ylabel('Dim 2', fontsize=12)"
plt.grid(True)
print(thumb)
"img.thumbnail((16, 12), PILImage.ANTIALIAS)"
plt.show()
Add all negatives for all positive pairs
print(triplets.shape[0])
compute output
measure accuracy
compute loss on this batch
"train on a batch, record loss, and measure accuracy (if calc_accuracy)"
compute output
measure accuracy and record loss
switch to evaluate mode
compute output
switch to evaluate mode
"self.fc13 = nn.Linear(128, 64)"
self.bn2 = nn.BatchNorm1d(64)
x = F.relu(self.fc12(x))
x = F.relu(self.bn1(self.fc13(x)))
x = F.relu(self.fc13(x))
"x = F.dropout(x, training=self.training)"
save features last FC layer
x = F.relu(x)
save features last FC layer
import matplotlib.pyplot as plt
plt.switch_backend('agg')
class EmbeddingNet(nn.Module):
"def __init__(self, architecture, feat_dim, use_pretrained=False):"
"super(EmbeddingNet, self).__init__()"
self.feat_dim= feat_dim
self.inner_model = models.__dict__[architecture](pretrained=use_pretrained)
if architecture.startswith('resnet'):
in_feats= self.inner_model.fc.in_features
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)"
elif architecture.startswith('inception'):
in_feats= self.inner_model.fc.in_features
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)"
if architecture.startswith('densenet'):
in_feats= self.inner_model.classifier.in_features
"self.inner_model.classifier = nn.Linear(in_feats, feat_dim)"
if architecture.startswith('vgg'):
in_feats= self.inner_model.classifier._modules['6'].in_features
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)"
if architecture.startswith('alexnet'):
in_feats= self.inner_model.classifier._modules['6'].in_features
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)"
"def forward(self, x):"
return self.inner_model.forward(x)
class NormalizedEmbeddingNet(EmbeddingNet):
"def __init__(self, architecture, feat_dim, use_pretrained=False):"
"EmbeddingNet.__init__(self, architecture, feat_dim, use_pretrained = use_pretrained)"
"def forward(self, x):"
embedding =  F.normalize(self.inner_model.forward(x))*10.0
"return embedding, embedding"
"def get_random_images(num, image_dir, test_transforms):"
"data = datasets.ImageFolder(image_dir, transform=test_transforms) # slight abuse; this expects subfolders corresponding to classes but we have no classes here"
indices = list(range(len(data)))
np.random.shuffle(indices)
idx = indices[:num]
from torch.utils.data.sampler import SubsetRandomSampler
sampler = SubsetRandomSampler(idx)
"loader = torch.utils.data.DataLoader(data,"
"sampler=sampler, batch_size=num)"
dataiter = iter(loader)
"images, labels = dataiter.next()"
"return images, labels"
"def predict_image(image, model, test_transforms):"
"device = torch.device(""cuda"" if torch.cuda.is_available()"
"else ""cpu"")"
image_tensor = test_transforms(image).float()
image_tensor = image_tensor.unsqueeze_(0)
input = Variable(image_tensor)
input = input.to(device)
output = model(input)[0]
return output.data.cpu().numpy()
Connect to database and initialize db_proxy
# database connection credentials
# load the dataset
Load the saved embedding model from the checkpoint
# update the dataset embedding
# Create a folder for saving embedding visualizations with this model checkpoint
model_emb_dirname = os.path.basename(args.base_model).split('.')[0]
"os.makedirs(model_emb_dirname, exist_ok=True)"
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')"
dataset.embedding_mode()
"assert 2==3, 'break'"
datasetindices = list(range(len(dataset)))
np.random.shuffle(datasetindices)
random_indices = datasetindices[:args.num]
print(random_indices)
"selected_sample_features = np.array([]).reshape(0, 256)"
selected_sample_labels = []
for idx in random_indices:
"selected_sample_features = np.vstack([selected_sample_features, X_train[idx]])"
selected_sample_labels.append(y_train[idx])
img_path = imagepaths[idx].split('.JPG')[0]
image = dataset.loader(img_path)
selected_sample_images.append(image)
# TRY NEAREST NEIGHBORS WALK THROUGH EMBEDDING
nbrs = NearestNeighbors(n_neighbors=args.num).fit(selected_sample_features)
"distances, indices = nbrs.kneighbors(selected_sample_features)"
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')"
"idx_w_closest_nbr = np.where(distances[:,1] == min(distances[:,1]))[0][0]"
order = [idx_w_closest_nbr]
for ii in range(len(distances)):
"distances[ii, 0] = np.inf"
while len(order)<args.num:
curr_idx = order[-1]
curr_neighbors = indices[curr_idx]
curr_dists = list(distances[curr_idx])
# print(min(curr_dists))
next_closest_pos = curr_dists.index(min(curr_dists))
next_closest = curr_neighbors[next_closest_pos]
order.append(next_closest)
# make sure you can't revisit past nodes
for vi in order:
vi_pos = list(indices[next_closest]).index(vi)
"distances[next_closest, vi_pos] = np.inf"
for ii in range(len(order)):
imgidx = order[ii]
image = selected_sample_images[imgidx]
"image.save(model_emb_dirname+""/img""+str(ii)+""_""+str(selected_sample_labels[imgidx])+"".png"")"
# Specify the transformations on the input images before inference
"# test_transforms = transforms.Compose([transforms.Resize([224, 224]), transforms.ToTensor()])"
"test_transforms = transforms.Compose([transforms.Resize([256, 256]), transforms.RandomCrop([224, 224]), transforms.RandomHorizontalFlip(), transforms.ColorJitter(), transforms.ToTensor(), transforms.Normalize([0.407328, 0.407328, 0.407328], [0.118641, 0.118641, 0.118641])])"
"images, labels = get_random_images(num, image_dir, test_transforms)"
"all_features = np.array([]).reshape(0, 256)"
for ii in range(len(images)):
image = to_pil(images[ii])
"features = predict_image(image, model, test_transforms)"
"all_features = np.vstack([all_features, features])"
# for ii in range(len(images)):
#     image = to_pil(images[ii])
"#     image.save(""img""+str(ii)+"".png"")"
# TRY CLUSTERING
kmeans1 = KMeans(n_clusters=5).fit(StandardScaler().fit_transform(all_features))
print(kmeans1.labels_)
for ii in range(len(images)):
image = to_pil(images[ii])
"filename = str(kmeans1.labels_[ii])+""/img""+str(ii)+"".png"""
if not os.path.exists(os.path.dirname(filename)):
os.makedirs(os.path.dirname(filename))
image.save(filename)
class SaveFeatures():
"def __init__(self, module):"
self.hook = module.register_forward_hook(self.hook_fn)
"def hook_fn(self, module, input, output):"
"self.features = torch.tensor(output, requires_grad=True).cuda()"
def close(self):
self.hook.remove()
Load the saved embedding model from the checkpoint
"Get a sample from the database, with eval transforms applied, etc."
Connect to database and sample a dataset
output = model.forward(sample_image.unsqueeze(0))
print(output)
with torch.no_grad():
sample_image_input = sample_image.cuda(non_blocking=True)
"_, output = model(sample_image_input) # compute output"
print(output)
sample_image = PILImage.open(sample_image_path).convert('RGB')
"sample_image = transforms.Compose([Resize([256, 256]), CenterCrop(([[224,224]])), ToTensor(), Normalize([0.369875, 0.388726, 0.347536], [0.136821, 0.143952, 0.145229])])(sample_image)"
print(list(model_inner_resnet.children()))
print(model_inner_resnet.fc)
print(model_inner_resnet.fc0)
# print(model_inner_resnet.layer4[0].conv2)
# print(type(model))
# print(len(list(model_inner_resnet.children())))
# print(list(model.children()))
# print(list(list(model.children())[0].children()))
"img = np.uint8(np.random.uniform(150, 180, (56, 56, 3)))/255"
"img_tensor = torch.unsqueeze(torch.from_numpy(img), 0)"
full_out = model_inner_resnet.forward(img_tensor)
print(full_out)
model(img_tensor)
activations = SaveFeatures(model_inner_resnet.layer4[0].conv2)
print(activations.features)
print(type(activations.features))
activations.close()
Connect to database and sample a dataset
Load the saved embedding model from the checkpoint
Update the dataset embedding
Get a random query image
# # # IMAGES IN THE SAME SEQUENCE # # # #
"assert 2==3, 'break'"
# # # CLOSEST IN (EMBEDDING) FEATURE SPACE # # # #
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Variables for the hierarchical cluster
Variables for the AL algorithm
Data variables
"connectivity = kneighbors_graph(self.transformed_X,max_features)"
Fit cluster and update cluster variables
The sklearn hierarchical clustering algo numbers leaves which correspond
to actual datapoints 0 to n_points - 1 and all internal nodes have
ids greater than n_points - 1 with the root having the highest node id
"If no labels have been observed, simply return uniform distribution"
"If no observations, return worst possible upper lower bounds"
Loop through generations from bottom to top
Update admissible labels for node
Calculate score
Determine if node should be split
Make sure label set for node so that we can flow to children
if necessary
Only split if all ancestors are admissible nodes
This is part  of definition of admissible pruning
Check that pruning covers all leave nodes
Fill in labels
Observe labels for previously recommended batches
TODO(lishal): implement multiple selection methods
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
A list of initialized samplers is allowed as an input because
"for AL_methods that search over different mixtures, may want mixtures to"
have shared AL_methods so that initialization is only performed once for
computation intensive methods like HierarchicalClusteringAL and
states are shared between mixtures.
"If initialized samplers are not provided, initialize them ourselves."
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copy these objects to make sure they are not modified while simulating
trajectories as they are used later by the main run_experiment script.
Assumes that model has already by fit using all labeled data so
the probabilities can be used immediately to hallucinate labels
All models need to have predict_proba method
Hallucinate labels for selected datapoints to be label
using class probabilities from model
"Not saving already_selected here, if saving then should sort"
only for the input to fit but preserve ordering of indices in
already_selected
Useful to know how accuracy compares for model trained on hallucinated
labels vs trained on true labels.  But can remove this train to speed
up simulations.  Won't speed up significantly since many more models
are being trained inside the loop above.
Save trajectory for reference
Delete created copies
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER
Should check model but kernel_svm does not have coef_ so need better
handling here
Make sure that model object fed in did not change during simulations
Return indices based on return type specified
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Set gamma for gaussian kernel to be equal to 1/n_features
kneighbors graph is constructed using k=10
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of"
"another point, make it vice versa"
Graph edges are weighted by applying gaussian kernel to manhattan dist.
"By default, gamma for rbf kernel is equal to 1/n_features but may"
get better results if gamma is tuned.
Define graph density for an observation to be sum of weights for all
edges to the node representing the datapoint.  Normalize sum weights
by total number of neighbors.
"If a neighbor has already been sampled, reduce the graph density"
for its direct neighbors to promote diversity.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update observed reward and arm probabilities
Sample an arm
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
y only used for determining how many clusters there should be
probably not practical to assume we know # of classes before hand
should also probably scale with dimensionality of data
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
TODO(lishal): have MarginSampler and this share margin function
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update min_distances for all examples given new cluster center.
Assumes that the transform function takes in original data and not
flattened data.
Initialize centers with a randomly selected datapoint
New examples should not be in already selected since those points
should have min_distance of zero to a cluster center.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
This is uniform given the remaining pool but biased wrt the entire pool.
sample = [i for i in range(self.X.shape[0]) if i not in already_selected]
return sample[0:N]
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Fields for hierarchical clustering AL
Setting parent and storing nodes in dict for fast access
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Connect to database DB_NAME as USER and initialize tables
Populate Info table
Populate Category table
"# For now, we have a predefined list of species we expect to see in the camera trap database (e.g. maybe from a quick look through the images)"
# TODO: allow user to update the class list through the labeling tool UI as they see different species
Populate Image and Detection tables
Image entry data
Detection entry data
Connect to database DB_NAME as USER and initialize tables
Populate Info table
Populate Category table
# Create category records from species present in the COCO camera trap classification dataset (COCO classes minus any excluded classes)
# TODO: allow user to update the class list through the labeling tool UI as they see different species
Populate Image and Detection tables
"with open(os.path.join(args.crop_dir,'crops.json'), 'r') as infile:"
crops_json = json.load(infile)
counter = 0
timer = time.time()
num_detections = len(crops_json)
for detectionid in crops_json:
counter += 1
detection_data = crops_json[detectionid]
# Image entry data
existing_image_entries = Image.select().where((Image.file_name == detection_data['file_name']))
try:
existing_image_entry = existing_image_entries.get()
except:
"image_entry = Image.create(id=detectionid, file_name=detection_data['file_name'], width=detection_data['width'], height=detection_data['height'], grayscale=detection_data['grayscale'],"
"source_file_name=detection_data['source_file_name'], relative_size=detection_data['relative_size'],"
"seq_id=detection_data['seq_id'], seq_num_frames=detection_data['seq_num_frames'], frame_num=detection_data['frame_num'])"
image_entry.save()
# Detection entry data
"detection_entry = Detection.create(id=detectionid, image=detectionid, bbox_confidence=detection_data['bbox_confidence'],"
"bbox_X1=detection_data['bbox_X1'], bbox_Y1=detection_data['bbox_Y1'], bbox_X2=detection_data['bbox_X2'], bbox_Y2=detection_data['bbox_Y2'],"
kind=DetectionKind.ModelDetection.value)
detection_entry.save()
if counter%100 == 0:
"print('Updated database with Image and Detection table entries for %d out of %d crops in %0.2f seconds'%(counter, num_detections, time.time() - timer))"
# data related to original image
Get class names from .txt list
Initialize Oracle table
Map filenames to classes (NOTE: we assume a single image does not contain more than one class)
"For each detection, use source image path to get class"
TODO update: Assumes that crops have already
been generated for the images using make_active_learning_classification_dataset.py. The created DB contains tables:
- info: information about the dataset
- image: images present in the dataset
- detections: crops of images with detections with confidence greater than a specified threshold
Initialize Database
# database connection credentials
HOST = 'localhost'
PORT = 5432
"# first, make sure the (user, password) has been created"
"# sudo -u postgres psql -c ""CREATE USER <db_user> WITH PASSWORD <db_password>;"""
"# sudo -u postgres psql -c ""CREATE DATABASE <db_name> WITH OWNER <db_user> CONNECTION LIMIT -1;"""
"# sudo -u postgres psql -c ""GRANT CONNECT ON DATABASE <db_name> TO <db_user>;"""
"# sudo -u postgres psql -d <db_name> -c ""CREATE EXTENSION IF NOT EXISTS \""uuid-ossp\"";"""
# Try to connect as USER to database DB_NAME through peewee
Populate Tables
# create Info table
# get class names for Category table
Faster anD available in Python 3.5 and above
# iterate through images in each class folder
killing this process after over 38 hours adding over 500k white-tailed deer crops from emammal
resuming for remaining classes
# get cropped image data for Image table
"if mean of each channel is about the same, image is likely grayscale"
# still have no info on these:
seq_id = CharField(null= True)                # sequence identifier for the original image
seq_num_frames = IntegerField(null = True)    # number of frames in sequence
frame_num = IntegerField(null = True)         # which frame number in sequence
location = CharField(null = True)             # location of camera trap
datetime = DateTimeField(null = True)
# store info about the detection corresponding to this image
# store info about the true labels for the detection
#  - for pretrain dataset this is the same as the detection_category if the detection categories
print(classes)
Connect to database and sample a dataset
Load the saved embedding model from the checkpoint
Update the dataset embedding
save the images
save the features
"with open(os.path.join(args.output_dir, 'lastlayer_features.mat'), 'wb') as f:"
"pickle.dump(sample_features, f)"
"with open(os.path.join(args.output_dir, 'labels.mat'), 'wb') as f:"
"pickle.dump(sample_labels, f)"
"parser.add_argument('--db_name', default='missouricameratraps', type=str, help='Name of the training (target) data Postgres DB.')"
"parser.add_argument('--db_user', default='user', type=str, help='Name of the user accessing the Postgres DB.')"
"parser.add_argument('--db_password', default='password', type=str, help='Password of the user accessing the Postgres DB.')"
"parser.add_argument('--base_model', type=str, help='Path to latest embedding model checkpoint.')"
"parser.add_argument('--output_dir', type=str, help='Output directory for subset of crops')"
Add json entry for this crop
Copy file for this crop to subset dataset crop dir
Copy file for its full-size source image to subset dataset image dir
Write crops.json to subset dataset crop dir
store info about the crops produced in a JSON file
------------------------------------------------------------------------------------------------------------#
COMMENT OUT IF NOT USING A SPECIFIC PROJECT WITHIN ROBERT LONG EMAMMAL DATASET
------------------------------------------------------------------------------------------------------------#
get some information about the source image
------------------------------------------------------------------------------------------------------------#
NOTE: EDIT THIS SECTION BASED ON DATASET SOURCE
get info about sequence the source image belongs to from path and directory
# missouricameratraps:
imgframenum = int(os.path.basename(imgfile).split('.JPG')[0].split('_')[-1])
imgseqid = int(os.path.split(os.path.dirname(imgfile))[-1])
"imgseqnumframes = len([name for name in os.listdir(os.path.dirname(imgfile)) if os.path.isfile(os.path.join(os.path.dirname(imgfile), name))])"
# emammal:
------------------------------------------------------------------------------------------------------------#
utility function
compute output
measure accuracy and record loss
switch to train mode
measure accuracy and record loss
compute output
measure accuracy and record loss
switch to evaluate mode
compute output
switch to evaluate mode
compute output
switch to evaluate mode
"print(self.labels_set, self.n_classes)"
Add all negatives for all positive pairs
print(triplets.shape[0])
constructor
update embedding values after a finetuning
select either the default or active pools
gather test set
gather train set
finetune the embedding model over the labeled pool
a utility function for saving the snapshot
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Variables for the hierarchical cluster
Variables for the AL algorithm
Data variables
"connectivity = kneighbors_graph(self.transformed_X,max_features)"
Fit cluster and update cluster variables
The sklearn hierarchical clustering algo numbers leaves which correspond
to actual datapoints 0 to n_points - 1 and all internal nodes have
ids greater than n_points - 1 with the root having the highest node id
"If no labels have been observed, simply return uniform distribution"
"If no observations, return worst possible upper lower bounds"
Loop through generations from bottom to top
Update admissible labels for node
Calculate score
Determine if node should be split
Make sure label set for node so that we can flow to children
if necessary
Only split if all ancestors are admissible nodes
This is part  of definition of admissible pruning
Check that pruning covers all leave nodes
Fill in labels
Observe labels for previously recommended batches
TODO(lishal): implement multiple selection methods
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
A list of initialized samplers is allowed as an input because
"for AL_methods that search over different mixtures, may want mixtures to"
have shared AL_methods so that initialization is only performed once for
computation intensive methods like HierarchicalClusteringAL and
states are shared between mixtures.
"If initialized samplers are not provided, initialize them ourselves."
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copy these objects to make sure they are not modified while simulating
trajectories as they are used later by the main run_experiment script.
Assumes that model has already by fit using all labeled data so
the probabilities can be used immediately to hallucinate labels
All models need to have predict_proba method
Hallucinate labels for selected datapoints to be label
using class probabilities from model
"Not saving already_selected here, if saving then should sort"
only for the input to fit but preserve ordering of indices in
already_selected
Useful to know how accuracy compares for model trained on hallucinated
labels vs trained on true labels.  But can remove this train to speed
up simulations.  Won't speed up significantly since many more models
are being trained inside the loop above.
Save trajectory for reference
Delete created copies
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER
Should check model but kernel_svm does not have coef_ so need better
handling here
Make sure that model object fed in did not change during simulations
Return indices based on return type specified
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Set gamma for gaussian kernel to be equal to 1/n_features
kneighbors graph is constructed using k=10
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of"
"another point, make it vice versa"
Graph edges are weighted by applying gaussian kernel to manhattan dist.
"By default, gamma for rbf kernel is equal to 1/n_features but may"
get better results if gamma is tuned.
Define graph density for an observation to be sum of weights for all
edges to the node representing the datapoint.  Normalize sum weights
by total number of neighbors.
"If a neighbor has already been sampled, reduce the graph density"
for its direct neighbors to promote diversity.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update observed reward and arm probabilities
Sample an arm
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
y only used for determining how many clusters there should be
probably not practical to assume we know # of classes before hand
should also probably scale with dimensionality of data
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
TODO(lishal): have MarginSampler and this share margin function
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update min_distances for all examples given new cluster center.
Assumes that the transform function takes in original data and not
flattened data.
Initialize centers with a randomly selected datapoint
New examples should not be in already selected since those points
should have min_distance of zero to a cluster center.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
This is uniform given the remaining pool but biased wrt the entire pool.
sample = [i for i in range(self.X.shape[0]) if i not in already_selected]
return sample[0:N]
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Fields for hierarchical clustering AL
Setting parent and storing nodes in dict for fast access
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Faster and available in Python 3.5 and above
transform_list.append(CenterCrop((processed_size)))
transform_list.append(Lambda(lambda X: normalize(X)))
"print(self.labels_set, self.n_classes)"
"from PyQt5 import QtCore, QtWidgets,QtGui"
from collections import deque
from peewee import *
from UIComponents.Tag import Tag
policy.setHeightForWidth(True)
"print(self.tab1.parentWidget(),self)"
self.tab4.add.clicked.connect(self.addSpecies)
self.tab4.update.clicked.connect(self.updateSpecies)
checkpoint= load_checkpoint('../merge/triplet_model_0054.tar')
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))"
"embedding_net = EmbeddingNet(checkpoint['arch'], checkpoint['feat_dim'])"
if checkpoint['loss_type'].lower()=='center':
"model = torch.nn.DataParallel(ClassificationNet(embedding_net, n_classes=14)).cuda()"
else:
model= torch.nn.DataParallel(embedding_net).cuda()
model.load_state_dict(checkpoint['state_dict'])
self.parentWidget().progressBar.setMaximum(len(run_dataset)//2048)
"e=Engine(model,None,None, verbose=True,progressBar= self.parentWidget().progressBar)"
label = [ x[1] for x in run_dataset.samples]
"print(indices,selected_set)"
print(query.sql())
src.delete().where(src.image_id<<rList))
"det= UserDetection.create(category_id=0, id=str(index+label[1][2]),image_id=final[0], bbox_X=label[1][0], bbox_Y=label[1][1], bbox_W=label[1][2], bbox_H=label[1][3])"
for x in self.tab1.grid.tags:
x.delete_instance()
db.create_tables([Detection])
This is simply to show the bar
"p = Process(target=ex.active, args=())"
p.start()
p.join()
ex.active()
ex.centralWidget().setCurrentIndex(1)
main()
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))"
print(row)
"unq_id= ""crops_""+str(uuid.uuid1())"
"print(line,imageWidth,imageHeight)"
"print(""%s,%s,%s,%0.6f,%0.6f,%0.6f,%0.6f,%0.6f""%(line[0], line[1],line[2],float(line[3]),topRel,leftRel,bottomRel,rightRel))"
if not os.path.exists(dest):
os.mkdir(dest)
raise
out.close()
"print length,(i-1)*length,i*length"
matplotlib.use('Agg')
get the inputs
zero the parameter gradients
forward + backward + optimize
print statistics
get the inputs
forward + backward + optimize
print statistics
get the inputs
zero the parameter gradients
forward + backward + optimize
get the inputs
forward + backward + optimize
print statistics
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
"conf.reset()"""""""
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
conf.reset()
for clf in classifiers:
"clf.fit(X_train, y_train)"
preds= clf.predict_proba(X)
uncertainty+= preds.max(axis=1)
print(uncertainty[ind])
print(uncertainty[ind])
true_labels = y[unlabeled_indices]
select up to 5 digit examples that the classifier is most uncertain about
remember best acc@1 and save checkpoint
"completeClassificationLoop(run_dataset, model,num_classes)"
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)"
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd)
labels = db.labels_
"mapp=(find_probablemap(label,labels, K=args.K))"
"print(""Clusters"")"
"for i,x in enumerate(labels):"
labels[i]= mapp[x]
print(np.sum(labels == label)/labels.size)
"print(""Confidence Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning(embd, label, idx)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(""Entropy Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning_entropy(embd, label, idx)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(idx,idx.shape)"
for i in idx:
print(paths[i])
"print(""Silohette active learning"")"
"idx= active_learning2(embd, 1000, args.num_clusters)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
embd= reduce_dimensionality(embd)#[0:10000])
labels= labels[0:10000]
label= label[0:10000]
paths= paths[0:10000]
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])"
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])"
plt.show()
"np.save(args.name_prefix+""_embeddings.npy"",embd)"
"np.save(args.name_prefix+""_labels.npy"",label)"
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")"
del output
define loss function (criterion) and optimizer
"optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay= args.weight_decay)"
optimizer.load_state_dict(checkpoint['optimizer'])
train for one epoch
evaluate on validation set
matplotlib.use('Agg')
selected_set.add(rand_ind[i])
"embedding_net = EmbeddingNet('resnet50', 256, True)"
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)"
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)"
print('Embedding Done')
sys.stdout.flush()
Random examples to start
"random_ids = noveltySamples(unlabeled_dataset.em, unlabeled_dataset.getIDs(), 1000)"
print(random_ids)
Move Records
Finetune the embedding model
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)"
unlabeled_dataset.updateEmbedding(model)
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})"
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()"
matplotlib.use('Agg')
"uncertainty= np.apply_along_axis(stats.entropy,1,probs) * (1 - probs.max(axis=1))"
"copy(paths[srt[i]], ""active"")"
"plot_together( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()), preds, base_ind, dataset.getpaths(), {})"
"return np.random.choice(range(0,prob_list[0].shape[0]), 100, replace=False).tolist()"
selected_set.add(rand_ind[i])
"query= Detection.update(kind = destKind.value).where(Detection.id.in_(rList), Detection.kind == srcKind.value)"
query.execute()
"embedding_net = EmbeddingNet('resnet50', 256, True)"
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)"
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)"
print('Embedding Done')
sys.stdout.flush()
"plot_embedding(dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})"
Random examples to start
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)"
print(random_ids)
Move Records
Finetune the embedding model
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)"
unlabeled_dataset.updateEmbedding(model)
"print(dataset.em[dataset.current_set].shape, np.asarray(dataset.getlabels()).shape, len(dataset.getpaths()))"
"plot_embedding( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})"
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})"
train_eval_classifier()
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()"
"names = [""Linear SVM"", ""RBF SVM"", ""Random Forest"", ""Neural Net"", ""Naive Bayes""]"
"classifiers = [SVC(kernel=""linear"", C=0.025, probability= True, class_weight='balanced'),"
"SVC(gamma=2, C=1, probability= True, class_weight='balanced'),"
"RandomForestClassifier(max_depth=None, n_estimators=100, class_weight='balanced'),"
"MLPClassifier(alpha=1),"
GaussianNB()]
estimators= []
"for name, clf in zip(names, classifiers):"
"estimators.append((name, clf))"
"eclf1 = VotingClassifier(estimators= estimators, voting='hard')"
"eclf2 = VotingClassifier(estimators= estimators, voting='soft')"
"names.append(""ensemble hard"")"
classifiers.append(eclf1)
"names.append(""ensemble soft"")"
classifiers.append(eclf2)
dataset.image_mode()
dataset.updateEmbedding(model)
y_pred= clf.predict(X_test)
"print(confusion_matrix(y_test, y_pred))"
paths= dataset.getpaths()
"for i, (yp, yt) in enumerate(zip(y_pred, y_test)):"
if yp != yt:
"copy(paths[i],""mistakes"")"
"print(yt, yp, paths[i],i)"
"clf_output= clf_e.embedding(eval_loader, dim=48)"
"self.train_transform = transforms.Compose([Resize(raw_size), CenterCrop((processed_size)), ToTensor(), Normalize(mean, std)])"
"print(self.labels_set, self.n_classes)"
"print(line,imageWidth,imageHeight)"
"print length,(i-1)*length,i*length"
#########################################################
## Configuration
#########################################################
## The actual code
Check arguments
Create output directories
Padding around the detected objects when cropping
1.3 for the cropping during test time and 1.3 for
the context that the CNN requires in the left-over
image
Load a (frozen) Tensorflow model into memory.
## Preparations: get all the output tensors
The following processing is only for single image
Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
Follow the convention by adding back the batch dimension
For all images in the image directoryig
Load image
Run inference
"all outputs are float32 numpy arrays, so convert types as appropriate"
Add detections to the collection
Get info about the image
Select detections with a confidence larger than DETECTION_CONFIDENCE
Get these boxes and convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes"
"However, we need to make sure that it box coordinates are still within the image"
"For each detected bounding box with high confidence, we will"
crop the image to the padded box and save it
generate a unique identifier for the detection
"bbox is the detected box, crop_box the padded / enlarged box"
Add numbering to the original file name if there are multiple boxes
The absolute file path where we will store the image
"if COCO_OUTPUT_DIR is set, then we will only use the shape"
of cropped_img in the following code. So instead of reading
cropped_img = np.array(Image.open(out_file))
we can speed everything up by reading only the size of the image
matplotlib.use('Agg')
conf= ConfusionMatrix(24)
get the inputs
zero the parameter gradients
forward + backward + optimize
print statistics
get the inputs
forward + backward + optimize
print statistics
get the inputs
zero the parameter gradients
forward + backward + optimize
get the inputs
forward + backward + optimize
print statistics
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
"conf.reset()"""""""
define loss function (criterion) and optimizer
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
conf.reset()
for clf in classifiers:
"clf.fit(X_train, y_train)"
preds= clf.predict_proba(X)
uncertainty+= preds.max(axis=1)
print(uncertainty[ind])
print(uncertainty[ind])
true_labels = y[unlabeled_indices]
select up to 5 digit examples that the classifier is most uncertain about
"print(indices,selected_set)"
print(query.sql())
remember best acc@1 and save checkpoint
"completeClassificationLoop(run_dataset, model,num_classes)"
"embd, label, paths = extract_embeddings(run_loader, model)"
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)"
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd)
labels = db.labels_
"mapp=(find_probablemap(label,labels, K=args.K))"
"print(""Clusters"")"
"for i,x in enumerate(labels):"
labels[i]= mapp[x]
print(np.sum(labels == label)/labels.size)
"print(""Confidence Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning(embd, label, idx)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(""Entropy Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning_entropy(embd, label, idx)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
"new_selected= selectSamples(embd,paths,3000)"
"print(idx,idx.shape)"
for i in idx:
print(paths[i])
"print(""Silohette active learning"")"
"idx= active_learning2(embd, 1000, args.num_clusters)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(""Random"")"
"idx = np.random.choice(np.arange(len(paths)), 1000, replace=False)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
embd= reduce_dimensionality(embd)#[0:10000])
labels= labels[0:10000]
label= label[0:10000]
paths= paths[0:10000]
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])"
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])"
plt.show()
"np.save(args.name_prefix+""_embeddings.npy"",embd)"
"np.save(args.name_prefix+""_labels.npy"",label)"
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")"
combo.setEnabled(not finalized)
print(self.bbox)
self.addWidget(self.child)
Moving container with arrows
Left - Bottom
Right-Bottom
Left-Top
Right-Top
Left - Bottom
Right - Bottom
Left - Top
Right - Top
check cursor horizontal position
check cursor vertical position
self.resizeEvent=self.onResize
"print(""Parent"", parent, parent.width(), parent.height())"
"self.setGeometry(0,0,410,307)"
"print(w,h,""w,h"")"
"print(""final"",tag.getFinal())"
pass
"self.tags.append(TContainer(self,Category.get(-1),[0,0,0.1,0.1],True, Qt.red))"
label= CharField()
fullname=str(self.model)
"self.name= (fullname[fullname.find("":"")+2:fullname.find("">"")].strip()+'_set').lower()"
"print(""Parent"", self.parentWidget().width(), self.parentWidget().height())"
"print(self.model,self.name,query.sql())"
self.tab4.speciesList.setModel(species)
"self.tab4.speciesList.setRowHidden(len(species.stringList())-1, True)"
self.speciesList.itemChanged.connect(self.itemChanged)
#####
""
video_utils.py
""
"Utilities for splitting, rendering, and assembling videos."
""
#####
"%% Constants, imports, environment"
from ai4eutils
%% Path utilities
%% Function for rendering frames to video and vice-versa
http://tsaith.github.io/combine-images-into-a-video-with-python-3-and-opencv-3.html
Determine the width and height from the first image
Define the codec and create VideoWriter object
"If we're not over-writing, check whether all frame images already exist"
"OpenCV seems to over-report the number of frames by 1 in some cases, or fails"
"to read the last frame; either way, I'm allowing one missing frame."
"print(""Rendering video {}, couldn't find frame {}"".format("
"input_video_file,missing_frame_number))"
...if we need to check whether to skip this video entirely
"for frame_number in tqdm(range(0,n_frames)):"
print('Skipping frame {}'.format(frame_filename))
Recursively enumerate video files
Create the target output folder
Render frames
input_video_file = input_fn_absolute; output_folder = output_folder_video
For each video
""
input_fn_relative = input_files_relative_paths[0]
"process_detection_with_options = partial(process_detection, options=options)"
zero-indexed
Load results
# Break into videos
im = images[0]
# For each video...
video_name = list(video_to_frames.keys())[0]
frame = frames[0]
At most one detection for each category for the whole video
category_id = list(detection_categories.keys())[0]
Find the nth-highest-confidence video to choose a confidence value
Prepare the output representation for this video
'max_detection_conf' is no longer included in output files by default
...for each video
Write the output file
%% Test driver
%% Constants
%% Split videos into frames
"%% List image files, break into folders"
Find unique folders
fn = frame_files[0]
%% Load detector output
%% Render detector frames
folder = list(folders)[0]
d = detection_results_this_folder[0]
...for each file in this folder
...for each folder
%% Render output videos
folder = list(folders)[0]
...for each video
""
run_inference_with_yolov5_val.py
""
Runs a folder of images through MegaDetector (or another YOLOv5 model) with YOLOv5's
"val.py, converting the output to the standard MD format.  The main goal is to leverage"
YOLO's test-time augmentation tools.
""
"YOLOv5's val.py uses each file's base name as a unique identifier, which doesn't work"
when you have typical camera trap images like:
""
a/b/c/RECONYX0001.JPG
d/e/f/RECONYX0001.JPG
""
...so this script jumps through a bunch of hoops to put a symlinks in a flat
"folder, run YOLOv5 on that folder, and map the results back to the real files."
""
"Currently requires the user to supply the path where a working YOLOv5 install lives,"
and assumes that the current conda environment is all set up for YOLOv5.
""
TODO:
""
* Figure out what happens when images are corrupted... right now this is the #1
"reason not to use this script, it may be the case that corrupted images look the"
same as empty images.
""
* Multiple GPU support
""
* Checkpointing
""
* Windows support (I have no idea what all the symlink operations will do on Windows)
""
"* Support alternative class names at the command line (currently defaults to MD classes,"
though other class names can be supplied programmatically)
""
%% Imports
%% Options class
# Required ##
# Optional ##
%% Main function
#%% Path handling
#%% Enumerate images
#%% Create symlinks to give a unique ID to each image
i_image = 0; image_fn = image_files_absolute[i_image]
...for each image
#%% Create the dataset file
Category IDs need to be continuous integers starting at 0
#%% Prepare YOLOv5 command
#%% Run YOLOv5 command
#%% Convert results to MD format
"We'll use the absolute path as a relative path, and pass '/'"
as the base path in this case.
#%% Clean up
...def run_inference_with_yolo_val()
%% Command-line driver
%% Scrap
%% Test driver (folder)
%% Test driver (file)
%% Preview results
options.sample_seed = 0
...for each prediction file
%% Compare results
Choose all pairwise combinations of the files in [filenames]
"%% Constants, imports, environment"
from multiprocessing.pool import ThreadPool as workerpool
Number of images to pre-fetch
Useful hack to force CPU inference.
""
"Need to do this before any PT/TF imports, which happen when we import"
from run_detector.
Numpy FutureWarnings from tensorflow import
%% Support functions for multiprocessing
TODO
""
The queue system is a little more elegant if we start one thread for reading and one
"for processing, and this works fine on Windows, but because we import TF at module load,"
"CUDA will only work in the main process, so currently the consumer function runs here."
""
"To enable proper multi-GPU support, we may need to move the TF import to a separate module"
that isn't loaded until very close to where inference actually happens.
%% Other support funtions
%% Image processing functions
%% Main function
Handle the case where image_file_names is not yet actually a list
"Find the images to score; images can be a directory, may need to recurse"
A json list of image paths
A single image file
Load the detector
Does not count those already processed
Will not add additional entries not in the starter checkpoint
Write a checkpoint if necessary
"Back up any previous checkpoints, to protect against crashes while we're writing"
the checkpoint file.
Write the new checkpoint
Remove the backup checkpoint if it exists
...if it's time to make a checkpoint
"When using multiprocessing, let the workers load the model"
"Results may have been modified in place, but we also return it for"
backwards-compatibility.
The typical case: we need to build the 'info' struct
"If the caller supplied the entire ""info"" struct"
"The 'max_detection_conf' field used to be included by default, and it caused all kinds"
"of headaches, so it's no longer included unless the user explicitly requests it."
%% Interactive driver
%%
%% Command-line driver
This is an experimental hack to allow the use of non-MD YOLOv5 models through
the same infrastructure; it disables the code that enforces MDv5-like class lists.
Load the checkpoint if available
""
Relative file names are only output at the end; all file paths in the checkpoint are
still full paths.
"Find the images to score; images can be a directory, may need to recurse"
A json list of image paths
A single image file
Test that we can write to the output_file's dir if checkpointing requested
"Don't overwrite existing checkpoint files, this is a sure-fire way to eventually"
erase someone's checkpoint.
"Commenting this out for now... the scenario where we are resuming from a checkpoint,"
then immediately overwrite that checkpoint with empty data is higher-risk than the
annoyance of crashing a few minutes after starting a job.
Confirm that we can write to the checkpoint path; this avoids issues where
we crash after several thousand images.
%% Imports
import pre- and post-processing functions from the YOLOv5 repo
scale_coords() became scale_boxes() in later YOLOv5 versions
%% Classes
padded resize
"Image size can be an int (which translates to a square target size) or (h,w)"
...if the caller has specified an image size
NMS
"As of v1.13.0.dev20220824, nms is not implemented for MPS."
""
Send predication back to the CPU to fix.
format detections/bounding boxes
"This is a loop over detection batches, which will always be length 1 in our case,"
since we're not doing batch inference.
Rescale boxes from img_size to im0 size
"normalized center-x, center-y, width and height"
"MegaDetector output format's categories start at 1, but the MD"
model's categories start at 0.
...for each detection in this batch
...if this is a non-empty batch
...for each detection batch
...try
for testing
"MegaDetector was trained with batch size of 1, and the resizing function is a part"
of the inference graph
"change from [y1, x1, y2, x2] to [x1, y1, width, height]"
convert numpy floats to Python floats
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size
"np_images = [np.asarray(image, np.uint8) for image in images]"
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)"
performs inference
our batch size is 1; need to loop the batch dim if supporting batch size > 1
"%% Constants, imports, environment"
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Numpy FutureWarnings from tensorflow import
Useful hack to force CPU inference
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
An enumeration of failure reasons
Number of decimal places to round to for confidence and bbox coordinates
Label mapping for MegaDetector
Should we allow classes that don't look anything like the MegaDetector classes?
"Each version of the detector is associated with some ""typical"" values"
"that are included in output files, so that downstream applications can"
use them as defaults.
%% Classes
Stick this into filenames before the extension for the rendered result
%% Utility functions
mps backend only available in torch >= 1.12.0
%% Main function
Dictionary mapping output file names to a collision-avoidance count.
""
"Since we'll be writing a bunch of files to the same folder, we rename"
as necessary to avoid collisions.
...def input_file_to_detection_file()
Image is modified in place
...for each image
...def load_and_run_detector()
%% Command-line driver
Must specify either an image file or a directory
"but for a single image, args.image_dir is also None"
%% Interactive driver
%%
#####
""
process_video.py
""
"Split a video (or folder of videos) into frames, run the frames through run_detector_batch.py,"
and optionally stitch together results into a new video with detection boxes.
""
TODO: allow video rendering when processing a whole folder
TODO: allow video rendering from existing results
""
#####
"%% Constants, imports, environment"
Only relevant if render_output_video is True
Folder to use for extracted frames
Folder to use for rendered frames (if rendering output video)
Should we render a video with detection boxes?
""
"Only supported when processing a single video, not a folder."
"If we are rendering boxes to a new video, should we keep the temporary"
rendered frames?
Should we keep the extracted frames?
%% Main functions
"TODO: keep track of whether we created this folder, delete if we're deleting the extracted"
"frames and we created the folder, and the output files aren't in the same folder.  For now,"
we're just deleting the extracted frames and leaving the empty folder around in this case.
Render detections to images
"TODO: keep track of whether we created this folder, delete if we're deleting the rendered"
"frames and we created the folder, and the output files aren't in the same folder.  For now,"
we're just deleting the rendered frames and leaving the empty folder around in this case.
Combine into a video
Delete the temporary directory we used for detection images
shutil.rmtree(rendering_output_dir)
(Optionally) delete the frames on which we ran MegaDetector
shutil.rmtree(frame_output_folder)
...process_video()
# Validate options
# Split every video into frames
# Run MegaDetector on the extracted frames
# (Optionally) delete the frames on which we ran MegaDetector
shutil.rmtree(frame_output_folder)
# Convert frame-level results to video-level results
...process_video_folder()
%% Interactive driver
%% Process a folder of videos
import clipboard; clipboard.copy(cmd)
%% Process a single video
import clipboard; clipboard.copy(cmd)
"%% Render a folder of videos, one file at a time"
import clipboard; clipboard.copy(s)
%% Command-line driver
Lint as: python3
Copyright 2020 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TPU is automatically inferred if tpu_name is None and
we are running under cloud ai-platform.
""
copy_checkpoints.py
""
Run this script with specified source_dir and target_dir while the model is training to make a copy
of every checkpoint (checkpoints are kept once an hour by default and is difficult to adjust)
""
do not copy event or evaluation results
input validation
plot the images
adjust the figure
"read in dataset CSV and create merged (dataset, location) col"
map label to label_index
load the splits
only weight the training set by detection confidence
TODO: consider weighting val and test set as well
isotonic regression calibration of MegaDetector confidence
treat each split separately
new_weight[i] = confidence[i] * (n / c) / total_confidence(i's label)
- n = # examples in split (weighted by confidence); c = # labels
- weight allocated to each label is n/c
"- within each label, weigh each example proportional to confidence"
- new weights sum to n
error checking
"maps output label name to set of (dataset, dataset_label) tuples"
find which other label (label_b) has intersection
input validation
create label index JSON
look into sklearn.preprocessing.MultiLabelBinarizer
Note: JSON always saves keys as strings!
"True for ground truth, False for MegaDetector"
always save as .jpg for consistency
get bounding boxes
convert from category ID to category name
"check if crops are already downloaded, and ignore bboxes below the"
confidence threshold
assign all images without location info to 'unknown_location'
remove images from labels that have fewer than min_locs locations
merge dataset and location into a single string '<dataset>/<location>'
"create DataFrame of counts. rows = locations, columns = labels"
label_count: label => number of examples
loc_count: label => number of locs containing that label
generate a new split
score the split
SSE for # of images per label (with 2x weight)
SSE for # of locs per label
label => list of datasets to prioritize for test and validation sets
"merge dataset and location into a tuple (dataset, location)"
sorted smallest to largest
greedily add to test set until it has >= 15% of images
sort the resulting locs
"modify loc_to_size in place, so copy its keys before iterating"
arguments relevant to both creating the dataset CSV and splits.json
arguments only relevant for creating the dataset CSV
arguments only relevant for creating the splits JSON
comment lines starting with '#' are allowed
""
prepare_classification_script.py
""
Notebook-y script used to prepare a series of shell commands to run a classifier
(other than MegaClassifier) on a MegaDetector result set.
""
Differs from prepare_classification_script_mc.py only in the final class mapping step.
""
%% Job options
%% Constants
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
%% Set up environment
commands.append('cd CameraTraps/classification\n')
commands.append('conda activate cameratraps-classifier\n')
%% Crop images
fn = input_files[0]
%% Run classifier
fn = input_files[0]
%% Merge classification and detection outputs
fn = input_files[0]
%% Write everything out
input validation
"Evaluating with accimage is much faster than Pillow or Pillow-SIMD, but accimage"
is Linux-only.
create output directory
override saved params with kwargs
"For now, we don't weight crops by detection confidence during"
evaluation. But consider changing this.
"create model, compile with TorchScript if given checkpoint is not compiled"
"verify that target names matches original ""label names"" from dataset"
"if the dataset does not already have a 'other' category, then the"
'other' category must come last in label_names to avoid conflicting
with an existing label_id
define loss function (criterion)
"this file ends up being huge, so we GZIP compress it"
double check that the accuracy metrics are computed properly
save the confusion matrices to .npz
save per-label statistics
set dropout and BN layers to eval mode
"even if batch contains sample weights, don't use them"
Do target mapping on the outputs (unnormalized logits) instead of
"the normalized (softmax) probabilities, because the loss function"
uses unnormalized logits. Summing probabilities is equivalent to
log-sum-exp of unnormalized logits.
"a confusion matrix C is such that C[i,j] is the # of observations known to"
be in group i and predicted to be in group j.
match pytorch EfficientNet model names
images dataset
"for smaller disk / memory usage, we cache the raw JPEG bytes instead"
of the decoded Tensor
convert JPEG bytes to a 3D uint8 Tensor
"keras EfficientNet already includes normalization from [0, 255] to [0, 1],"
so we don't need to do that here
labels dataset
img_files dataset
weights dataset
define the transforms
efficientnet data preprocessing:
- train:
"1) random crop: aspect_ratio_range=(0.75, 1.33), area_range=(0.08, 1.0)"
2) bicubic resize to img_size
3) random horizontal flip
- test:
1) center crop
2) bicubic resize to img_size
weights sums to:
- if weight_by_detection_conf: (# images in split - conf delta)
- otherwise: (# images in split)
"freeze the base model's weights, including BatchNorm statistics"
https://www.tensorflow.org/guide/keras/transfer_learning#fine-tuning
rebuild output
input validation
set seed
create logdir and save params
create dataloaders and log the index_to_label mapping
define loss function and optimizer
using EfficientNet training defaults
- batch norm momentum: 0.99
"- optimizer: RMSProp, decay 0.9 and momentum 0.9"
- epochs: 350
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs"
- weight decay: 1e-5
TODO: change weighted to False if oversampling minority classes
stop training after 8 epochs without improvement
log metrics
log confusion matrix
log tp/fp/fn images
"tf.summary.image requires input of shape [N, H, W, C]"
false positive for top3_pred[0]
false negative for label
"if evaluating or finetuning, set dropout & BN layers to eval mode"
"for each label, track 5 most-confident and least-confident examples"
"even if batch contains sample weights, don't use them"
we do not track L2-regularization loss in the loss metric
This dictionary will get written out at the end of this process; store
diagnostic variables here
error checking
refresh detection cache
save log of bad images
cache of Detector outputs: dataset name => {img_path => detection_dict}
img_path: <dataset-name>/<img-filename>
get SAS URL for images container
strip image paths of dataset name
save list of dataset names and task IDs for resuming
complete task name: 'detect_for_classifier_caltech_20200722_110816_task01'
HACK! Sleep for 10s between task submissions in the hopes that it
"decreases the chance of backend JSON ""database"" corruption"
task still running => continue
"task finished successfully, save response to disk"
error checking before we download and crop any images
convert from category ID to category name
we need the datasets table for getting SAS keys
"we already did all error checking above, so we don't do any here"
get ContainerClient
get bounding boxes
we must include the dataset <ds> in <crop_path_template> because
'{img_path}' actually gets populated with <img_file> in
load_and_crop()
"get the image, either from disk or from Blob Storage"
inelegant way to close the container_clients
""
prepare_classification_script_mc.py
""
Notebook-y script used to prepare a series of shell commands to run MegaClassifier
on a MegaDetector result set.
""
Differs from prepare_classification_script.py only in the final class mapping step.
""
%% Job options
%% Constants
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
%% Set up environment
commands.append('cd CameraTraps/classification\n')
commands.append('conda activate cameratraps-classifier\n')
%% Crop images
fn = input_files[0]
%% Run classifier
fn = input_files[0]
%% Remap classifier outputs
fn = input_files[0]
%% Merge classification and detection outputs
fn = input_files[0]
%% Write everything out
mean/std values from https://pytorch.org/docs/stable/torchvision/models.html
define the transforms
resizes smaller edge to img_size
weights sums to:
- if weight_by_detection_conf: (# images in split - conf delta)
- otherwise: # images in split
for normal (non-weighted) shuffling
set all parameters to not require gradients except final FC layer
replace final fully-connected layer (which has 1000 ImageNet classes)
"detect GPU, use all if available"
input validation
set seed
create logdir and save params
create dataloaders and log the index_to_label mapping
create model
define loss function and optimizer
using EfficientNet training defaults
- batch norm momentum: 0.99
"- optimizer: RMSProp, decay 0.9 and momentum 0.9"
- epochs: 350
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs"
- weight decay: 1e-5
stop training after 8 epochs without improvement
do a complete evaluation run
log metrics
log confusion matrix
log tp/fp/fn images
"for every image: undo normalization, clamp to [0, 1], CHW -> HWC"
"- cannot be in-place, because the HeapItem might be in multiple heaps"
writer.add_figure() has issues => using add_image() instead
"writer.add_figure(f'{label_name}/{tag}', fig, global_step=epoch)"
false positive for top3_pred[0]
false negative for label
"preds and labels both have shape [N, k]"
"if evaluating or finetuning, set dropout and BN layers to eval mode"
"for each label, track k_extreme most-confident and least-confident images"
"even if batch contains sample weights, don't use them"
lock before changing ImageFile.LOAD_TRUNCATED_IMAGES
input validation
use MegaDB to generate list of images
only keep images that:
"1) end in a supported file extension, and"
2) actually exist in Azure Blob Storage
3) belong to a label with at least min_locs locations
write out log of images / labels that were removed
"save label counts, pre-subsampling"
"save label counts, post-subsampling"
spec_dict['taxa']: list of dict
[
"{'level': 'family', 'name': 'cervidae', 'datasets': ['idfg']},"
"{'level': 'genus',  'name': 'meleagris'}"
]
"spec_dict['dataset_labels']: dict, dataset => list of dataset_label"
{
"""idfg"": [""deer"", ""elk"", ""prong""],"
"""idfg_swwlf_2019"": [""elk"", ""muledeer"", ""whitetaileddeer""]"
}
"maps output label name to set of (dataset, dataset_label) tuples"
"because MegaDB is organized by dataset, we do the same"
ds_to_labels = {
'dataset_name': {
"'dataset_label': [output_label1, output_label2]"
}
}
we need the datasets table for getting full image paths
The line
"[img.class[0], seq.class[0]][0] as class"
selects the image-level class label if available. Otherwise it selects the
"sequence-level class label. This line assumes the following conditions,"
expressed in the WHERE clause:
- at least one of the image or sequence class label is given
- the image and sequence class labels are arrays with length at most 1
- the image class label takes priority over the sequence class label
""
"In Azure Cosmos DB, if a field is not defined, then it is simply excluded"
"from the result. For example, on the following JSON object,"
{
"""dataset"": ""camera_traps"","
"""seq_id"": ""1234"","
"""location"": ""A1"","
"""images"": [{""file"": ""abcd.jpeg""}],"
"""class"": [""deer""],"
}
"the array [img.class[0], seq.class[0]] just gives ['deer'] because"
img.class is undefined and therefore excluded.
"if no path prefix, set it to the empty string '', because"
"os.path.join('', x, '') = '{x}/'"
result keys
"- already has: ['dataset', 'location', 'file', 'class', 'bbox']"
"- add ['label'], remove ['file']"
"if img is mislabeled, but we don't know the correct class, skip it"
"otherwise, update the img with the correct class, but skip the"
img if the correct class is not one we queried for
sort keys for determinism
we need the datasets table for getting SAS keys
strip leading '?' from SAS token
only check Azure Blob Storage
check local directory first before checking Azure Blob Storage
1st pass: populate label_to_locs
"label (tuple of str) => set of (dataset, location)"
2nd pass: eliminate bad images
prioritize is a list of prioritization levels
number of already matching images
main(
"label_spec_json_path='idfg_classes.json',"
"taxonomy_csv_path='../../camera-traps-private/camera_trap_taxonomy_mapping.csv',"
"output_dir='run_idfg',"
json_indent=4)
recursively find all files in cropped_images_dir
only find crops of images from detections JSON
resizes smaller edge to img_size
"Evaluating with accimage is much faster than Pillow or Pillow-SIMD, but accimage"
is Linux-only.
create dataset
create model
set dropout and BN layers to eval mode
load files
dataset => set of img_file
crop_path: <dataset>/<img_file>___cropXX_mdvY.Y.jpg
[----<img_path>----]
error checking
any row with 'correct_class' should be marked 'mislabeled'
filter to only the mislabeled rows
convert '\' to '/'
verify that overlapping indices are the same
"""add"" any new mislabelings"
write out results
error checking
load detections JSON
get detector version
convert from category ID to category name
copy keys to modify dict in-place
This will be removed later when we filter for animals
save log of bad images
"True for ground truth, False for MegaDetector"
always save as .jpg for consistency
"we already did all error checking above, so we don't do any here"
"get the image, either from disk or from Blob Storage"
inelegant way to close the container_client
"crop_path => normalized bbox coordinates [xmin, ymin, width, height]"
"only ground-truth bboxes do not have a ""confidence"" value"
try loading image from local directory
try to download image from Blob Storage
crop the image
"expand box width or height to be square, but limit to img size"
"Image.crop() takes box=[left, upper, right, lower]"
pad to square using 0s
Author: lukemelas (github username)
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch
With adjustments and added comments by workingcoder (github username).
Support the construction of 'efficientnet-l2' without pretrained weights
Expansion phase (Inverted Bottleneck)
"image_size = calculate_output_image_size(image_size, 1) <-- this wouldn't modify image_size"
Depthwise convolution phase
"Squeeze and Excitation layer, if desired"
Pointwise convolution phase
Expansion and Depthwise Convolution
Squeeze and Excitation
Pointwise Convolution
Skip connection and drop connect
The combination of skip connection and drop connect brings about stochastic depth.
Batch norm parameters
Get stem static or dynamic convolution depending on image size
Stem
Build blocks
Update block input and output filters based on depth multiplier.
The first block needs to take care of stride and filter size increase.
"image_size = calculate_output_image_size(image_size, block_args.stride)  # stride = 1"
Head
Final linear layer
Stem
Blocks
Head
Stem
Blocks
Head
Convolution layers
Pooling and final linear layer
Author: lukemelas (github username)
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch
With adjustments and added comments by workingcoder (github username).
###############################################################################
## Help functions for model architecture
###############################################################################
GlobalParams and BlockArgs: Two namedtuples
Swish and MemoryEfficientSwish: Two implementations of the method
round_filters and round_repeats:
Functions to calculate params for scaling model width and depth ! ! !
get_width_and_height_from_size and calculate_output_image_size
drop_connect: A structural design
get_same_padding_conv2d:
Conv2dDynamicSamePadding
Conv2dStaticSamePadding
get_same_padding_maxPool2d:
MaxPool2dDynamicSamePadding
MaxPool2dStaticSamePadding
"It's an additional function, not used in EfficientNet,"
but can be used in other model (such as EfficientDet).
"Parameters for the entire model (stem, all blocks, and head)"
Parameters for an individual model block
Set GlobalParams and BlockArgs's defaults
An ordinary implementation of Swish function
A memory-efficient implementation of Swish function
TODO: modify the params names.
"maybe the names (width_divisor,min_width)"
"are more suitable than (depth_divisor,min_depth)."
follow the formula transferred from official TensorFlow implementation
follow the formula transferred from official TensorFlow implementation
"generate binary_tensor mask according to probability (p for 0, 1-p for 1)"
Note:
The following 'SamePadding' functions make output size equal ceil(input size/stride).
"Only when stride equals 1, can the output size be the same as input size."
Don't be confused by their function names ! ! !
Tips for 'SAME' mode padding.
Given the following:
i: width or height
s: stride
k: kernel size
d: dilation
p: padding
Output after Conv2d:
o = floor((i+p-((k-1)*d+1))/s+1)
"If o equals i, i = floor((i+p-((k-1)*d+1))/s+1),"
=> p = (i-1)*s+((k-1)*d+1)-i
With the same calculation as Conv2dDynamicSamePadding
Calculate padding based on image size and save it
Calculate padding based on image size and save it
###############################################################################
## Helper functions for loading model params
###############################################################################
BlockDecoder: A Class for encoding and decoding BlockArgs
efficientnet_params: A function to query compound coefficient
get_model_params and efficientnet:
Functions to get BlockArgs and GlobalParams for efficientnet
url_map and url_map_advprop: Dicts of url_map for pretrained weights
load_pretrained_weights: A function to load pretrained weights
Check stride
"Coefficients:   width,depth,res,dropout"
Blocks args for the whole model(efficientnet-b0 by default)
It will be modified in the construction of EfficientNet Class according to model
note: all models have drop connect rate = 0.2
ValueError will be raised here if override_params has fields not included in global_params.
train with Standard methods
check more details in paper(EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks)
train with Adversarial Examples(AdvProp)
check more details in paper(Adversarial Examples Improve Image Recognition)
TODO: add the petrained weights url map of 'efficientnet-l2'
AutoAugment or Advprop (different preprocessing)
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils)
from data_management.megadb.schema import sequences_schema_check
resize is for displaying them more quickly
dataset and seq_id are required fields
sort the images in the sequence
"if no class label on the image, show class label on the sequence"
pool = ThreadPool()
"print('len of rendering_info', len(rendering_info))"
"tqdm(pool.imap_unordered(render_image_info_partial, rendering_info), total=len(rendering_info))"
options = write_html_image_list()
options['headerHtml']
print('Checking that the MegaDB entries conform to the schema...')
sequences_schema_check.sequences_schema_check(sequences)
#######
""
visualize_db.py
""
Outputs an HTML page visualizing annotations (class labels and/or bounding boxes)
on a sample of images in a database in the COCO Camera Traps format
""
#######
%% Imports
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils)
Assumes the cameratraps repo root is on the path
%% Settings
Set to None to visualize all images
Target size for rendering; set either dimension to -1 to preserve aspect ratio
"These are mutually exclusive; both are category names, not IDs"
"Special tag used to say ""show me all images with multiple categories"""
We sometimes flatten image directories by replacing a path separator with
another character.  Leave blank for the typical case where this isn't necessary.
Control rendering parallelization
Process-based parallelization in this function is currently unsupported
"due to pickling issues I didn't care to look at, but I'm going to just"
"flip this with a warning, since I intend to support it in the future."
%% Helper functions
"Translate the file name in an image entry in the json database to a path, possibly doing"
some manipulation of path separators
%% Core functions
"Optionally remove all images without bounding boxes, *before* sampling"
"Optionally include/remove images with specific labels, *before* sampling"
Put the annotations in a dataframe so we can select all annotations for a given image
Construct label map
Take a sample of images
Set of dicts representing inputs to render_db_bounding_boxes:
""
"bboxes, boxClasses, image_path"
All the class labels we've seen for this image (with out without bboxes)
Iterate over annotations for this image
iAnn = 0; anno = annos_i.iloc[iAnn]
...for each of this image's annotations
"We're adding html for an image before we render it, so it's possible this image will"
fail to render.  For applications where this script is being used to debua a database
"(the common case?), this is useful behavior, for other applications, this is annoying."
...for each image
...def render_image_info
def process_images(...)
%% Command-line driver
"Copy all fields from a Namespace (i.e., the output from parse_args) to an object."
""
Skips fields starting with _.  Does not check existence in the target object.
Convert to an options object
%% Interactive driver(s)
%%
os.startfile(htmlOutputFile)
%% Constants and imports
convert category ID from int to str
Retry on blob storage read failures
%% Functions
PIL.Image.convert() returns a converted copy of this image
Alter orientation as needed according to EXIF tag 0x112 (274) for Orientation
""
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28
https://www.media.mit.edu/pia/Research/deepview/exif.html
""
Null operation
Aspect ratio as width over height
ar = w / h
h = w / ar
w = ar * h
The following three functions are modified versions of those at:
""
https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py
Convert to pixels so we can use the PIL crop() function
PIL's crop() does surprising things if you provide values outside of
"the image, clip inputs"
...if this detection is above threshold
...for each detection
"list of lists, one list of strings for each bounding box (to accommodate multiple labels)"
for color selection
"Always render objects with a confidence of ""None"", this is typically used"
for ground truth data.
"{} is the default, which means ""show labels with no mapping"", so don't use ""if label_map"" here"
if label_map:
"To avoid duplicate colors with detection-only visualization, offset"
the classification class index by the number of detection classes
...for each classification
...if we have classification results
...if the confidence of this detection is above threshold
...for each detection
"print('Input must be of size [N, 4], but is ' + str(boxes_shape))"
Deliberately trimming to the width of the image only in the case where
"box expansion is turned on.  There's not an obvious correct behavior here,"
but the thinking is that if the caller provided an out-of-range bounding
"box, they meant to do that, but at least in the eyes of the person writing"
"this comment, if you expand a box for visualization reasons, you don't want"
to end up with part of a box.
""
A slightly more sophisticated might check whether it was in fact the expansion
"that made this box larger than the image, but this is the case 99.999% of the time"
"here, so that doesn't seem necessary."
...if we need to expand boxes
If the total height of the display strings added to the top of the bounding
"box exceeds the top of the image, stack the strings below the bounding box"
instead of above.
Each display_str has a top and bottom margin of 0.05x.
Reverse list and print from bottom to top.
Skip empty strings
"list of lists, one list of strings for each bounding box (to accommodate multiple labels)"
need to be a string here because PIL needs to iterate through chars
""
stacked bar charts are made with each segment starting from a y position
"To fit the legend in, shrink current axis by 20%"
Put a legend to the right of the current axis
####
""
visualize_incoming_annotations.py
""
Spot-check the annotations received from iMerit by visualizing annotated bounding
boxes on a sample of images and display them in HTML.
""
"Modified in 2021 March to use the new format (iMerit batch 12 onwards), which is a"
COCO formatted JSON with relative coordinates for the bbox.
""
from data_management.megadb.schema import sequences_schema_check
we used frame_num of 1 when sending out images to annotators when it is not explicitly stored (wcs esp.)
The file_name field in the incoming json looks like
alka_squirrels.seq2020_05_07_25C.frame119221.jpg
"we need to use the dataset, sequence and frame info to find the actual path in blob storage"
using the sequences
category_id 5 is No Object Visible
download the image
Write to HTML
allow forward references in typing annotations
class variables
instance variables
get path to root
taxonomy CSV rows without 'taxonomy_string' entries are excluded
"from the taxonomy graph, but can be included in a classification"
label specification JSON via the 'dataset_labels' key
use the lower parent
special cases
%% Imports
%% Taxnomy checking
taxonomy CSV rows without 'taxonomy_string' entries are excluded
"from the taxonomy graph, but can be included in a classification"
label specification JSON via the 'dataset_labels' key
...for each row in the taxnomy file
%% Command-line driver
%% Interactive driver
%%
which datasets are already processed?
"sequence-level query should be fairly fast, ~1 sec"
cases when the class field is on the image level (images in a sequence
"that had different class labels, 'caltech' dataset is like this)"
"this query may take a long time, >1hr"
"this query should be fairly fast, ~1 sec"
read species presence info from the JSON files for each dataset
has this class name appeared in a previous dataset?
columns to populate the spreadsheet
sort by descending species count
make the spreadsheet
hyperlink Bing search URLs
hyperlink example image SAS URLs
TODO hardcoded columns: change if # of examples or col_order changes
""
map_lila_taxonomy_to_wi_taxonomy.py
""
Loads the LILA category mapping (in which taxonomy information comes from an iNat taxonomy snapshot)
and tries to map each class to the Wildlife Insights taxonomy.
""
%% Constants and imports
Created by get_lila_category_list.py... contains counts for each category
This is a manually-curated file used to store mappings that had to be made manually
This is the main output file from this whole process
%% Load category and taxonomy files
%% Pull everything out of pandas
%% Cache WI taxonomy lookups
This is just a handy lookup table that we'll use to debug mismatches
taxon = wi_taxonomy[21653]; print(taxon)
Look for keywords that don't refer to specific taxa: blank/animal/unknown
Do we have a species name?
"If 'species' is populated, 'genus' should always be populated; one item currently breaks"
this rule.
...for each taxon
%% Find redundant taxa
%% Manual review of redundant taxa
%% Clean up redundant taxa
taxon_name = list(taxon_name_to_preferred_taxon_id.keys())[0]
"If we've gotten this far, we should be choosing from multiple taxa."
""
"This will become untrue if any of these are resolved later, at which point we shoudl"
remove them from taxon_name_to_preferred_id
Choose the preferred taxa
%% Read supplementary mappings
"Each line is [lila query],[WI taxon name],[notes]"
%% Map LILA categories to WI categories
Must be ordered from kingdom --> species
TODO:
"['subspecies','variety']"
i_taxon = 0; taxon = lila_taxonomy[i_taxon]; print(taxon)
"Go from kingdom --> species, choosing the lowest-level description as the query"
"E.g., 'car'"
"print('Made a supplementary mapping from {} to {}'.format(query,wi_taxon['taxon_name']))"
print('No match for {}'.format(query))
...for each LILA taxon
%% Manual mapping
%% Build a dictionary from LILA dataset names and categories to LILA taxa
i_d = 0; d = lila_taxonomy[i_d]
"%% Map LILA datasets to WI taxa, and count the number of each taxon available in each dataset"
dataset_name = list(lila_dataset_to_categories.keys())[0]
dataset_category = dataset_categories[0]
"Write out the dataset name, category name, WI GUID, WI scientific name, WI common name,"
and count
...for each category in this dataset
...for each dataset
...with open()
""
retrieve_sample_image.py
""
"Downloader that retrieves images from Google images, used for verifying taxonomy"
"lookups and looking for egregious mismappings (e.g., ""snake"" being mapped to a fish called"
"""snake"")."
""
"Simple wrapper around simple_image_download, but I've had to swap in and out the underlying"
downloader a few times.
""
%% Imports and environment
%%
%% Main entry point
%% Test driver
%%
""
"Using the taxonomy .csv file, map all LILA datasets to the standard taxonomy"
""
Does not currently produce results; this is just used to confirm that all category names
have mappings in the taxonomy file.
""
%% Constants and imports
Created by get_lila_category_list.py... contains counts for each category
%% Load category and taxonomy files
%% Map dataset names and category names to scientific names
i_row = 1; row = taxonomy_df.iloc[i_row]; row
"%% For each dataset, make sure we can map every category to the taxonomy"
dataset_name = list(lila_dataset_to_categories.keys())[0]
c = categories[0]
""
"Takes the megadb taxonomy mapping, extracts the rows that are relevant to"
"LILA, and does some cleanup."
""
%% Constants and imports
This is a partially-completed taxonomy file that was created from a different set of
"scripts, but covers *most* of LILA as of June 2022"
Created by get_lila_category_list.py
%% Read the input files
Get everything out of pandas
"%% Find all unique dataset names in the input list, compare them with data names from LILA"
d = input_taxonomy_rows[0]
The script that generates this dictionary creates a separate entry for bounding box
"metadata files, but those don't represent new dataset names"
%% Map input columns to output datasets
Make sure all of those datasets actually correspond to datasets on LILA
%% Re-write the input taxonomy file to refer to LILA datasets
Map the string datasetname:token to a taxonomic tree json
mapping = input_taxonomy_rows[0]
Make sure that all occurrences of this mapping_string give us the same output
assert taxonomy_string == taxonomy_mappings[mapping_string]
%% Re-write the input file in the target format
mapping_string = list(taxonomy_mappings.keys())[0]
""
prepare_lila_taxonomy_release.py
""
"Given the private intermediate taxonomy mapping, prepare the public (release)"
taxonomy mapping file.
""
%% Imports and constants
Created by get_lila_category_list.py... contains counts for each category
%% Find out which categories are actually used
dataset_name = datasets_to_map[0]
i_row = 0; row = df.iloc[i_row]; row
%% Generate the final output file
i_row = 0; row = df.iloc[i_row]; row
match_at_level = taxonomic_match[0]
i_row = 0; row = df.iloc[i_row]; row
"E.g.: (43117, 'genus', 'lepus', ['hares and jackrabbits']"
###############
---> CONSTANTS
###############
max_progressbar = count * (list(range(limit+1))[-1]+1)
"bar = progressbar.ProgressBar(maxval=max_progressbar,"
"widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()]).start()"
bar.update(bar.currval + 1)
bar.finish()
""
"Given a subset of LILA datasets, find all the categories, and start the taxonomy"
mapping process.
""
%% Constants and imports
Created by get_lila_category_list.py
'NACTI'
'Channel Islands Camera Traps'
%% Read the list of datasets
The script that generates this dictionary creates a separate entry for bounding box
"metadata files, but those don't represent new dataset names"
%% Find all categories
dataset_name = datasets_to_map[0]
%% Initialize taxonomic lookup
from taxonomy_mapping.species_lookup import (
"get_taxonomic_info, print_taxonomy_matche)"
%% Manual lookup
%%
q = 'white-throated monkey'
raise ValueError('')
%% Match every query against our taxonomies
mapping_string = category_mappings[1]; print(mapping_string)
...for each mapping
%% Write output rows
""
"Does some consistency-checking on the LILA taxonomy file, and generates"
an HTML preview page that we can use to determine whether the mappings
make sense.
""
%% Imports and constants
"lila_taxonomy_file = r""G:\git\agentmorrisprivate\lila-taxonomy\lila-taxonomy-mapping.csv"""
"lila_taxonomy_file = r""G:\temp\lila\lila_additions_2022.06.29.csv"""
%% Support functions
%% Read the taxonomy mapping file
%% Prepare taxonomy lookup
from taxonomy_mapping.species_lookup import (
"get_taxonomic_info, print_taxonomy_matche)"
%% Optionally remap all gbif-based mappings to inat (or vice-versa)
%%
i_row = 1; row = df.iloc[i_row]; row
This should be zero for the release .csv
%%
%% Check for mappings that disagree with the taxonomy string
Look for internal inconsistency
Look for outdated mappings
i_row = 0; row = df.iloc[i_row]
%% List null mappings
""
"These should all be things like ""unidentified"" and ""fire"""
""
i_row = 0; row = df.iloc[i_row]
%% List mappings with scientific names but no common names
%% List mappings that map to different things in different data sets
x = suppress_multiple_matches[-1]
...for each row where we saw this query
...for each row
"%% Verify that nothing ""unidentified"" maps to a species or subspecies"
"E.g., ""unidentified skunk"" should never map to a specific species of skunk"
%% Make sure there are valid source and level values for everything with a mapping
%% Find WCS mappings that aren't species or aren't the same as the input
"WCS used scientific names, so these remappings are slightly more controversial"
then the standard remappings.
row = df.iloc[-500]
"Anything marked ""species"" or ""unknown"" by definition doesn't map to a species,"
so ignore these.
print('WCS query {} ({}) remapped to {} ({})'.format(
"query,common_name,scientific_name,common_names_from_taxonomy))"
%% Download sample images for all scientific names
i_row = 0; row = df.iloc[i_row]
if s != 'mirafra':
continue
Check whether we already have enough images for this query
"print('Skipping query {}, already have {} images'.format(s,len(sizes_above_threshold)))"
Check whether we've already run this query for a previous row
...for each row in the mapping table
%% Rename .jpeg to .jpg
"print('Renaming {} to {}'.format(fn,new_fn))"
%% Choose representative images for each scientific name
s = list(scientific_name_to_paths.keys())[0]
Be suspicious of duplicate sizes
...for each scientific name
%% Delete unused images
%% Produce HTML preview
i_row = 2; row = df.iloc[i_row]
"image_paths = [os.path.relpath(p, output_base) for p in image_paths]"
...for each row
%% Open HTML preview
######
""
species_lookup.py
""
Look up species names (common or scientific) in the GBIF and iNaturalist
taxonomies.
""
Run initialize_taxonomy_lookup() before calling any other function.
""
######
%% Constants and imports
As of 2020.05.12:
""
"GBIF: ~777MB zipped, ~1.6GB taxonomy"
"iNat: ~2.2GB zipped, ~51MB taxonomy"
These are un-initialized globals that must be initialized by
the initialize_taxonomy_lookup() function below.
%% Functions
Initialization function
# Load serialized taxonomy info if we've already saved it
"# If we don't have serialized taxonomy info, create it from scratch."
Download and unzip taxonomy files
taxonomy_name = list(taxonomy_urls.items())[0][0]; zip_url = list(taxonomy_urls.items())[0][1]
Don't download the zipfile if we've already unzipped what we need
Bypasses download if the file exists already
Unzip the files we need
...for each file that we need from this zipfile
Remove the zipfile
os.remove(zipfile_path)
...for each taxonomy
"Create dataframes from each of the taxonomy files, and the GBIF common"
name file
Load iNat taxonomy
Load GBIF taxonomy
Remove questionable rows from the GBIF taxonomy
Load GBIF vernacular name mapping
Only keep English mappings
Convert everything to lowercase
"For each taxonomy table, create a mapping from taxon IDs to rows"
Create name mapping dictionaries
Build iNat dictionaries
row = inat_taxonomy.iloc[0]
Build GBIF dictionaries
"The canonical name is the Latin name; the ""scientific name"""
include the taxonomy name.
""
http://globalnames.org/docs/glossary/
This only seems to happen for really esoteric species that aren't
"likely to apply to our problems, but doing this for completeness."
Don't include taxon IDs that were removed from the master table
Save everything to file
...def initialize_taxonomy_lookup()
"list of dicts: {'source': source_name, 'taxonomy': match_details}"
i_match = 0
"list of (taxon_id, taxonRank, scientific name, [vernacular names])"
corresponding to an exact match and its parents
Walk taxonomy hierarchy
This can happen because we remove questionable rows from the
GBIF taxonomy
"print(f'Warning: no row exists for parent_taxon_id {parent_taxon_id},' + \"
"f'child taxon_id: {taxon_id}, query: {query}')"
The GBIF taxonomy contains unranked entries
...while there is taxonomy left to walk
...for each match
Remove redundant matches
i_tree_a = 0; tree_a = matching_trees[i_tree_a]
i_tree_b = 1; tree_b = matching_trees[i_tree_b]
"If tree a's primary taxon ID is inside tree b, discard tree a"
""
taxonomy_level_b = tree_b['taxonomy'][0]
...for each level in taxonomy B
...for each tree (inner)
...for each tree (outer)
...def traverse_taxonomy()
"print(""Finding taxonomy information for: {0}"".format(query))"
"In GBIF, some queries hit for both common and scientific, make sure we end"
up with unique inputs
"If the species is not found in either taxonomy, return None"
Both GBIF and iNat have a 1-to-1 mapping between taxon_id and row number
Walk both taxonomies
...def get_taxonomic_info()
m = matches[0]
"For example: [(9761484, 'species', 'anas platyrhynchos')]"
...for each taxonomy level
...for each match
...def print_taxonomy_matches()
%% Taxonomy functions that make subjective judgements
query = 'person'
Do we have an iNat match?
"print_taxonomy_matches(inat_matches, verbose=True)"
"print_taxonomy_matches(gbif_matches, verbose=True)"
print('Warning: multiple iNat matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple iNat common names for {query}')
Default to returning the query
"print(f'Matched iNat {query} to {scientific_name},{common_name}')"
...if we had iNat matches
If we either prefer GBIF or didn't have iNat matches
""
Code is deliberately redundant here; I'm expecting some subtleties in how
handle GBIF and iNat.
print('Warning: multiple GBIF matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple GBIF common names for {query}')
Default to returning the query
...if we needed to look in the GBIF taxonomy
...def _get_preferred_taxonomic_match()
%% Interactive drivers and debug
%% Initialization
%% Taxonomic lookup
query = 'lion'
print(matches)
Print the taxonomy in the taxonomy spreadsheet format
%% Directly access the taxonomy tables
%% Command-line driver
Read command line inputs (absolute path)
Read the tokens from the input text file
Loop through each token and get scientific name
""
process_species_by_dataset
""
We generated a list of all the annotations in our universe; this script is
used to (interactively) map them onto the GBIF and iNat taxonomies.  Don't
"try to run this script from top to bottom; it's used like a notebook, not like"
"a script, since manual review steps are required."
""
%% Imports
%autoreload 0
%autoreload -species_lookup
%% Constants
Input file
Output file after automatic remapping
File to which we manually copy that file and do all the manual review; this
should never be programmatically written to
The final output spreadsheet
HTML file generated to facilitate the identificaiton of egregious mismappings
%% Functions
Prefer iNat matches over GBIF matches
query = 'person'
Do we have an iNat match?
"print_taxonomy_matches(inat_matches, verbose=True)"
"print_taxonomy_matches(gbif_matches, verbose=True)"
print('Warning: multiple iNat matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple iNat common names for {query}')
Default to returning the query
"print(f'Matched iNat {query} to {scientific_name},{common_name}')"
...if we had iNat matches
If we either prefer GBIF or didn't have iNat matches
""
Code is deliberately redundant here; I'm expecting some subtleties in how
handle GBIF and iNat.
print('Warning: multiple GBIF matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple GBIF common names for {query}')
Default to returning the query
...if we needed to look in the GBIF taxonomy
...def get_preferred_taxonomic_match()
%% Initialization
%% Test single-query lookup
%%
%%
"q = ""grevy's zebra"""
%% Read the input data
%% Run all our taxonomic lookups
i_row = 0; row = df.iloc[i_row]
query = 'lion'
...for each query
Write to the excel file that we'll use for manual review
%% Download preview images for everything we successfully mapped
uncomment this to load saved output_file
"output_df = pd.read_excel(output_file, keep_default_na=False)"
i_row = 0; row = output_df.iloc[i_row]
...for each query
%% Write HTML file with representative images to scan for obvious mis-mappings
i_row = 0; row = output_df.iloc[i_row]
...for each row
%% Look for redundancy with the master table
Note: `master_table_file` is a CSV file that is the concatenation of the
"manually-remapped files (""manual_remapped.xlsx""), which are the output of"
this script run across from different groups of datasets. The concatenation
"should be done manually. If `master_table_file` doesn't exist yet, skip this"
"code cell. Then, after going through the manual steps below, set the final"
manually-remapped version to be the `master_table_file`.
%% Manual review
Copy the spreadsheet to another file; you're about to do a ton of manual
review work and you don't want that programmatically overwrriten.
""
See manual_review_xlsx above
%% Read back the results of the manual review process
%% Look for manual mapping errors
Manually inspect df for typos in 'dataset_names' and 'taxonomy_level' columns
Identify rows where:
""
- 'taxonomy_level' does not match level of 1st element in 'taxonomy_string'
- 'scientific_name' does not match name of 1st element in 'taxonomy_string'
""
...both of which typically represent manual mapping errors.
i_row = 0; row = df.iloc[i_row]
"I'm not sure why both of these checks are necessary, best guess is that"
the Excel parser was reading blanks as na on one OS/Excel version and as ''
on another.
The taxonomy_string column is a .json-formatted string; expand it into
an object via eval()
"%% Find scientific names that were added manually, and match them to taxonomies"
i_row = 0; row = df.iloc[i_row]
...for each query
%% Write out final version
""
"Generate read-only SAS URLs for all LILA containers, to facilitate partial downloads."
""
The results of this script end up here:
""
http://lila.science/wp-content/uploads/2020/03/lila_sas_urls.txt
""
"Update: that file is manually maintained now, it can't be programmatically generated"
""
%% Imports
Read-only
%% Enumerate containers
%% Generate SAS tokens
%% Generate SAS URLs
%% Write to output file
--------some stuff needed to get AJAX to work with bottle?--------#
-------------------------------------------------------------------------------- #
PREPARE TO QUEUE IMAGES FOR LABELING
-------------------------------------------------------------------------------- #
# Connect as USER to database DB_NAME through peewee and initialize database proxy
# Load embedding model
---------------------------------------------------------------------- #
CREATE QUEUE OF IMAGES TO LABEL
---------------------------------------------------------------------- #
Use classifier to generate predictions
# Update model predicted class in PostgreSQL database
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
-------------------------------------------------------------------------------- #
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI
-------------------------------------------------------------------------------- #
"# static routes (to serve CSS, etc.)"
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps"
"return bottle.static_file(filename, root='../../../../../../../../../../../.')"
# dynamic routes
if data['display_grayscale']:
indices_to_exclude.update(set(color_indices))
elif not data['display_grayscale']:
indices_to_exclude.update(set(grayscale_indices))
data['display_images'] = {}
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices]
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices]
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices]
data['display_images']['detection_categories'] = []
for i in indices:
if str(dataset.samples[i][1]) == 'None':
data['display_images']['detection_categories'].append('None')
else:
existing_category_entries = {cat.id: cat.name for cat in Category.select()}
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()"
data['display_images']['detection_categories'].append(cat_name)
Use image ids in images_to_label to get the corresponding dataset indices
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
print(set(dataset.set_indices[4]).update(set(indices_to_label)))
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Use image id images_to_label to get the corresponding dataset index
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Train on samples that have been labeled so far
dataset.set_kind(DetectionKind.UserDetection.value)
print(y_train)
Predict on the samples that have not been labeled
print(y_pred)
Update model predicted class in PostgreSQL database
timer = time.time()
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
print('Updating the database took %0.2f seconds'%(time.time() - timer))
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
"once the classifier has been trained the first time, switch to AL sampling"
""
make_oneclass_json.py
""
"Takes a coco-camera-traps .json database and collapses species classes to binary,"
optionally removing labels from empty images (to be detector-friendly) (depending on
"""experiment_type"")."
""
"Assumes that empty images are labeled as ""empty""."
""
%% Imports and environment
%% Core conversion function
"We're removing empty images from the annotation list, but not from"
"the ""images"" list; they'll still get used in detector training."
print('Ignoring empty annotation')
%% Interactive driver
%%
Load annotations
Convert from multi-class to one-class
Write out the one-class data
%% Command-line driver
""
plot_bounding_boxes.py
""
Takes a .json database containing bounding boxes and renders those boxes on the
source images.
""
"This assumes annotations in coco-camera-traps format, with absolute bbox"
coordinates.
""
%% Imports and environment
How many images should we process?  Set to -1 to process all images.
Should we randomize the image order?
%%  Read database and build up convenience mappings
Category ID to category name
Image ID to image info
Image ID to image path
Image ID to all annotations referring to this image
"%% Iterate over images, draw bounding boxes, write to file"
For each image
image = images[0]
Build up a list of bounding boxes to draw on this image
Load the image
Create figure and axes
Display the image
ann = imageAnnotations[0]
"For each annotation associated with this image, render bounding box and label"
"In the Rectangle() function, the first argument (""location"") is the bottom-left"
of the rectangle.
""
Origin is the upper-left of the image.
Add the patch to the Axes
Add a class label
This is magic goop that removes whitespace around image plots (sort of)
Write the output image
...for each image
""
plot_imerit_annotations.py
""
Takes a .json file full of bounding box annotations and renders those boxes on the
source images.
""
"This assumes annotations in the format we receive them, specifically:"
""
1) Relative bbox coordinates
"2) A list of .json objects, not a well-formatted .json file"
""
"I.e., don't use this on a COCO-style .json file.  See plot_bounding_boxes.py"
for the same operation performed on a proper COCO-camera-traps database.
""
%% Imports and environment
"os.makedirs(outputBase, exist_ok=True)"
%%  Read all source images and build up a hash table from image name to full path
"This spans training and validation directories, so it's not the same as"
just joining the image name to a base path
"%% Iterate over annotations, draw bounding boxes, write to file"
annData has keys:
""
"annotations, categories, images"
""
Each of these are lists of dictionaries
%% Render all annotations on each image in the sequence
%% Pull out image metadata
Build up a list of bounding boxes to draw on this image
Pull out just the image name from the filename
""
File names look like:
""
seq6efffac2-5567-11e8-b3fe-dca9047ef277.frame1.img59a94e52-23d2-11e8-a6a3-ec086b02610b.jpg
"m = re.findall(r'img(.*\.jpg)$', imgFileName, re.M|re.I)"
print(m)
assert(len(m) == 1)
queryFileName = m[0]
Map this image back to the original directory
"%% Loop over annotations, find annotations that match this image"
%%
"x,y,w,h"
""
"x,y is the bottom-left of the rectangle"
""
"x,y origin is the upper-left"
...for each annotation
%% Render with PIL (scrap)
%% Render with Matplotlib
Create figure and axes
Display the image
Location is the bottom-left of the rect
""
Origin is the upper-left
Add the patch to the Axes
This is magic goop that removes whitespace around image plots (sort of)
os.startfile(outputFileName)
%% Showing figures on-screen during debugging
plt.show()
Various (mostly unsuccessful) approaches to getting the plot window to show up
"in the foreground, which is a backend-specific operation..."
""
fig.canvas.manager.window.activateWindow()
fig.canvas.manager.window.raise_()
fm = plt.get_current_fig_manager()
"fm.window.attributes('-topmost', 1)"
"fm.window.attributes('-topmost', 0)"
""
# This is the one that I found to be most robust... at like 80% robust.
plt.get_current_fig_manager().window.raise_()
%%
...for each image
...for each file
""
convert_imerit_json_to_coco_json.py
""
"Takes a .json file with bounding boxes but no class labels, and a .json file containing the"
"class labels for those images, and creates a new json file with class labels and bounding"
boxes.
""
"The bounding box .json file is in the format returned by our annotators, which is not"
actually a fully-formed .json file; rather it's a series of .json objects
""
"Leaves filenames intact.  Stores imerit ""category IDs"" (empty/human/group/animal) in a new"
"field called ""annotation_type""."
""
%% Imports and constants
%% Configure files and paths
"For Snapshot Serengeti, we stored image IDs in our annotation files as:"
""
S1_B06_R1_PICT0008
""
...but the corresponding ID in the master database is actually:
""
S1\B06\R1\S1_B06_R1_PICT0008
""
"If this is ""True"", we'll expand the former to the latter"
Handling a one-off issue in which .'s were mysteriously replaced with -'s
"in our annotations.  This will be set dynamically, but I keep it here as"
a constant to remind me to remove this code when we clean this issue up.
Used in the (rare) case where a bounding box was added to an image that was originally
annotated as empty
Used in the (rare) case where we added bounding boxes to an image with multiple species
"Should we include ambiguous bounding boxes (with the ""ambiguous"" category label)"
in the output file?  Ambiguous boxes are boxes drawn on images with multiple species.
"%%  Read metadata from the master database, bounding boxes from the annotations file"
"The bounding box .json file is in the format returned by our annotators, which is not"
actually a fully-formed .json file; rather it's a series of .json objects
"Each element of annData is a dictionary corresponding to a single sequence, with keys:"
""
"annotations, categories, images"
sequence = annData[0]
%% Build convenience mappings
Image ID to images
Category ID to categories (referring to the database categories)
"Image ID to categories (i.e., species labels)"
Utility function we'll use to create annotations for images in empty
sequences (empty images in non-empty sequences already have annotations)
"%% Reformat annotations, grabbing category IDs from the master database (prep)"
iSequence = 0; sequence = annData[0]
"%% Reformat annotations, grabbing category IDs from the master database (loop)"
Make a copy here; we're going to manipulate the sequence annotations
when we need to add synthetic annotations for empty images
im = sequenceImages[0]
Are there any annotations in this sequence?
Which images in this sequence have annotations?
For each image in this sequence...
imeritImageID = im['id']
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG
Confirm that the file exists
Hande a one-off issue with our annotations
datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG
""
...had become:
""
datasetsnapshotserengeti.seqASG000001a-frame0.imgS1_B06_R1_PICT0008.JPG
Does it look like we encountered this issue?
Convert:
""
S1_B06_R1_PICT0008
""
...to:
""
S1/B06/B06_R1/S1_B06_R1_PICT0008
Make sure we only see each image once
Create empty annotations for empty images
""
Here we use the *unmodified* file name
Create an empty annotation for this image
Annotations still use the annotation filename (not database ID) at this point;
these will get converted to database IDs below when we process the
whole sequence.
Sanity-check image size
"print('Warning: img {} was listed in DB as {}x{}, annotated as {}x{}, actual size{}x{}'.format("
"old_id,new_im['width'],new_im['height'],im['width'],im['height'],imgObj.width,imgObj.height))"
...for each image in this sequence
For each annotation in this sequence...
ann = sequenceAnnotations[0]
Prepare an annotation using the category ID from the database and
the bounding box from the annotations file
Maintain iMerit's annotation category
Generate an (arbitrary) ID for this annotation; the COCO format has a concept
"of annotation ID, but our annotation files don't"
This was a one-off quirk with our file naming
We'll do special handling of images with multiple categories later
Store the annotation type (group/human/animal/empty)
This annotation has no bounding box but the image wasn't originally
annotated as empty
This annotation has a bounding box but the image was originally
annotated as empty
unnormalize the bbox
... for each annotation in this sequence
... for each sequence
%% Post-processing
Count empty images
...for each file
%% Sanity-check empty images
""
make_ss_annotation_image_folder.py
""
Take a directory full of images with the very long filenames we give annotators:
""
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension
""
"...along with a COCO-camera-traps database referring to those files, and:"
""
1) Creates a new COCO-camera-traps database with the original filenames in them
(copying the annotations)
""
2) Optionally creates a new directory with those images named according to the
"Snapshot Serengeti naming convention, including complete relative paths."
""
See convert_imerit_json_to_coco_json to see how we get from the original annotation
.json to a COCO-camera-traps database.
""
%% Constants and imports
%% Configure files/paths
%% Read the annotations (referring to the old filenames)
"%% Update filenames, optionally copying files"
im = data['images'][0]
For each image...
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG
"Find the image name, e.g. S1_B06_R1_PICT0008"
Convert:
""
S1_B06_R1_PICT0008.JPG
""
...to:
""
S1/B06/B06_R1/S1_B06_R1_PICT0008.JPG
...for each image
%% Write the revised database
""
get_annotation_tool_link.py
""
"Takes a COCO-camera-traps-style .json file with URLs already embedded, and prepares"
a link to the visipedia annotation tool that reviews a subset of those images.
""
""
create_new_annotation_json.py
""
"Creates a subset of a larger .json database, in this case specifically to pick some images"
from Snapshot Serengeti.
""
from utils import get_db_dicts
for seq in already_annotated:
seq_to_ims.pop(seq)
remove already annotated images
add lion images
lion_seqs = cat_to_seqs[cat_to_id['lionMale']] + cat_to_seqs[cat_to_id['lionFemale']]
#print(len(lion_seqs))
lion_seqs = [seq for seq in lion_seqs if seq not in already_annotated]
#print(len(lion_seqs))
"lion_seqs_to_annotate.extend(random.sample(lion_seqs, min(len(lion_seqs),num_lions)))"
print(len(lion_seqs_to_annotate))
num_elephants = 1000
elephant_seqs = cat_to_seqs[cat_to_id['elephant']]
#print(len(lion_seqs))
elephant_seqs = [seq for seq in elephant_seqs if seq not in already_annotated]
#print(len(lion_seqs))
"elephant_seqs_to_annotate = random.sample(elephant_seqs, num_elephants)"
num_empty = 10
empty_seqs_to_annotate = []
for loc in loc_to_seqs:
empty_seqs = cats_per_location[loc][cat_to_id['empty']]
empty_seqs = [seq for seq in empty_seqs if seq not in already_annotated]
empty_seqs = [seq for seq in empty_seqs if seq_to_season[seq] in seasons_to_keep]
"empty_seqs_to_annotate.extend(random.sample(empty_seqs, min(len(empty_seqs),num_empty)))"
ims_to_annotate.extend(empty_ims_to_annotate)
""
filter_database.py
""
"Look through a COCO-ct database and find images matching some crtieria, writing"
a subset of images and annotations to a new file.
""
%% Constants and imports
%% Configuration
%%  Read database and build up convenience mappings
Category ID to category name
Image ID to image info
Image ID to image path
%% Filter
ann = annotations[0]
Is this a tiny box or a group annotation?
"x,y,w,h"
All empty annotations should be classified as either empty or ambiguous
""
"The ambiguous cases are basically minor misses on the annotators' part,"
where two different small animals were present somewhere.
%% Write output file
""
find_images_for_classes.py
""
"Given a .json database, find images that are associated with one or more"
classes.
""
%% Constants and imports
%% Configuration
%%  Read database and build up convenience mappings
Category ID to category name
Image ID to image info
Image ID to image path
%% Look for target-class annotations
ann = annotations[0]
""
create_tfrecords_format.py
""
This script creates a tfrecords file from a dataset in VOTT format.
%% Imports and environment
set up the filenames and annotations
This loop reads the bboxes and corresponding labels and assigns them
the correct image. Kind of slow at the moment...
If needed: merging all classes
bbox_labels = ['Animal' for _ in bbox_labels]
BBox coords are stored in the format
"x_min (of width axis) y_min (of height axis), x_max, y_max"
Coordinate system starts in top left corner
"In this framework, we need ('ymin', 'xmin', 'ymax', 'xmax') format"
print out some stats
To make sure we loaded the bboxes correctly:
self.validate_bboxes()
For each image in the data set...
Make sure all are greater equal 0
%% Main tfrecord generation function
Propagate optional metadata to tfrecords
endfor each annotation for the current image
endfor each image
"Calculate number of shards to get the desired number of images per record,"
ensure it is evenly divisible by the number of threads
%% Command-line driver
%% Driver
""
make_tfrecords_cis_trans.py
""
Given a .json file that contains a three-element list (train/val/test) of image IDs and a .json database that contains
"those image IDs, generates tfrecords whose filenames include ""train""/""val""/""test"""
""
"dataset = json.load(open('/ai4efs/databases/snapshotserengeti/oneclass/SnapshotSerengeti_Seasons_1_to_4_tfrecord_format_valid_ims.json','r'))"
print('Creating trans_val tfrecords')
dataset = [im_id_to_im[idx] for idx in trans_val]
""
create_classification_tfrecords_from_json.py
""
Called from make_tfrecords_cis_trans.py if you're running a classification experiment.
""
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/'
datafile = 'eccv_18_annotation_files_oneclass/CaltechCameraTrapsECCV18'
image_file_root = datafolder+'eccv_18_all_images/'
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/'
datafolder = '/data/iwildcam/'
datafile = 'combined_iwildcam_annotations_oneclass/eccv_train_and_imerit_2'
need consecutive category ids
old_cat_id_to_new_cat_id = {categories[idx]['id']:idx+1 for idx in range(len(categories))}
print(old_cat_id_to_new_cat_id)
remove multiclass images
print(images[0])
print(vis_data[0])
""
make_tfrecords_from_json.py
""
"Given a coco-camera-traps .json file, creates tfrecords"
""
Thin wrapper for create_tfrecords_from_json.
""
%% Constants and imports
%% Constants and imports (interactive)
%%
%% Main tfrecord generation function
"check whether the input file has already been converted to the tfrecords format,"
"if not, convert"
"Calculate number of shards to get the desired number of images per record,"
ensure it is evenly divisible by the number of threads
%% Interactive driver
%%
%% Command-line driver
eMammal_make_splits.py
""
"Based on a tfrecords_format json file of the database, creates 3 splits according to"
the specified fractions based on location (images from the same location should be in
one split) or based on images.
""
"If a previous split is provided (append_to_previous_split is True), the entries in"
"each split will be preserved, and new entries will be appended, so that new models"
can warm start with a model trained on the original splits.
configurations and paths
approximate fraction for the new entries
read in the previous splits of image ID or location ID if available
"find new locations and assign them to a split, without reassigning any previous locations"
"find out which images are new, shuffle and split them"
do NOT sort the IDs to keep the shuffled order
export PYTHONPATH=$PYTHONPATH:tfmodels/research
add empty category
"add all images that don't have annotations, with cat empty"
print(detection_results.keys())
group the detections and gts by image id:
print(image_id)
print(len(scores))
print(len(labels))
recall_thresh = 0.9
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall])
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])"
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall])
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])"
exp_name = 'eccv_train'
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json'
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(len(detection_results['images']))
print(len(seqs))
"print(len(seqs[0]),len(seqs["
print(detection_results.keys())
group the detections by image id:
group the ground truth annotations by image id:
print(seq)
"for image_id, dets in per_image_detections.iteritems():"
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
print(num_gts)
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
if sum(detected_class_labels)>0:
print('detected animal box')
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])"
"print(scores, tp_fp_labels)"
num_total_gts += num_gts
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
print('valid box')
"print(best_im, best_score)"
exp_name = 'eccv_train'
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json'
export PYTHONPATH=$PYTHONPATH:tfmodels/research
need to loop over confidence values
"for each value, check if any detections on the image are > conf"
"If so, that image gets class ""animal"""
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0"
"for image_id, dets in per_image_detections.iteritems():"
"calc prec, rec for this confidence thresh"
group the ground truth annotations by image id:
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(detection_results.keys())
group the detections and gts by image id:
print(image_id)
print(len(scores))
print(len(labels))
recall_thresh = 0.9
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall])
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])"
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall])
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])"
print(sorted_ap)
plt.bar(sorted_ap)
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(detection_results.keys())
group the detections by image id:
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
print(gts)
print(num_gts)
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[:2])"
"print(scores, tp_fp_labels)"
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
export PYTHONPATH=$PYTHONPATH:tfmodels/research
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
print('detected animal box')
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])"
"print(scores, tp_fp_labels)"
num_total_gts += num_gts
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
print('valid box')
best_im = np.argmax(max_im_scores)
"print(best_im, best_score)"
"temp_labels = np.zeros(len(im_detection_labels),  dtype=np.int32)"
"temp_scores = np.zeros(len(im_detection_scores), dtype=np.float32)"
"for j in range(min(im_num_gts, len(im_detection_labels))):"
temp_labels[j] = True #TODO: this currently only works for oneclass?
temp_scores[j] = best_score
im_detection_labels = temp_labels
im_detection_scores = temp_scores
num_total_gts+=im_num_gts
"print(len(detection_scores), len(detection_scores[0]), len(detection_scores[1]))"
exp_name = 'small_balanced_cct'
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(seq)
"for image_id, dets in per_image_detections.iteritems():"
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
seq_num_gts.append(num_gts)
print(num_gts)
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
num_total_gts += 1
print('valid box')
"print(best_im, best_score)"
print('no valid box')
if sum(seq_num_gts)>0:
export PYTHONPATH=$PYTHONPATH:tfmodels/research
need to loop over confidence values
"for each value, check if any detections on the image are > conf"
"If so, that image gets class ""animal"""
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0"
"calc prec, rec for this confidence thresh"
export PYTHONPATH=$PYTHONPATH:tfmodels/research
group the detections by image id:
group the ground truth annotations by image id:
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
print('detected animal box')
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])"
"print(scores, tp_fp_labels)"
num_total_gts += num_gts
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
print('valid box')
"print(best_im, best_score)"
""
evaluate_detections.py
""
Adapted from analyze_detection.py which is now archived.
""
%% Imports and constants
%% Functions
"labels input to compute_object_detection_metrics() needs to start at 0, not 1"
num_detections = len(dets['boxes'])
to prevent 'Invalid dimensions for box data.' error
this box will not match any detections
compute one-class precision/recall/average precision (if every box is just of an object class)
%% Command-line driver
""
detection_eval_utils.py
""
Utility functions used in evaluate_detections.py
""
group the ground truth annotations by image id
#####
""
detect_and_predict_image.py
""
"Functions to load a TensorFlow detection and a classification model, run inference,"
"render bounding boxes on images, and write out the resulting"
images (with bounding boxes and classes).
""
"See the ""test driver"" cell for example invocation."
""
""
#####
"%% Constants, imports, environment"
Minimum detection confidence for showing a bounding box on the output image
Stick this into filenames before the extension for the rendered result
Number of top-scoring classes to show at each bounding box
%% Core detection functions
Load images if they're not already numpy arrays
iImage = 0; image = images[iImage]
"Load the image as an nparray of size h,w,nChannels"
"There was a time when I was loading with PIL and switched to mpimg,"
"but I can't remember why, and converting to RGB is a very good reason"
"to load with PIL, since mpimg doesn't give any indication of color"
"order, which basically breaks all .png files."
""
"So if you find a bug related to using PIL, update this comment"
"to indicate what it was, but also disable .png support."
image = mpimg.imread(image)
This shouldn't be necessarily when loading with PIL and converting to RGB
Actual detection
...for each image
"Currently ""boxes"" is a list of length nImages, where each element is shaped as"
""
"1,nDetections,4"
""
"This implicitly banks on TF giving us back a fixed number of boxes, let's assert on this"
to make sure this doesn't silently break in the future.
iBox = 0; box = boxes[iBox]
"""scores"" is a length-nImages list of elements with size 1,nDetections"
"""classes"" is a length-nImages list of elements with size 1,nDetections"
""
"Still as floats, but really representing ints"
Squeeze out the empty axis
boxes is nImages x nDetections x 4
scores and classes are both nImages x nDetections
Get input and output tensors of classification model
"imsize = cur_image['width'], cur_image['height']"
Select detections with a confidence larger 0.5
Get these boxes and convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes"
"However, we need to make sure that it box coordinates are still within the image"
For convenience:
Create an array with contains the index of the corresponding crop_box for each selected box
i.e. [False False 0 False 1 2 3 False False]
For each box
If this box should be classified
Run inference
if box should not be classified
...for each box
species_scores should have shape len(images) x len(boxes) x num_species
...for each image
...with tf.Session
with classification_graph
species_scores should have shape len(images) x len(boxes) x num_species
%% Rendering functions
Display the image
plt.show()
"top, left, bottom, right"
""
"x,y origin is the upper-left"
Generate bounding box text
Choose color based on class
Location is the bottom-left of the rect
""
Origin is the upper-left
iRight = x + w
iTop = y + h
Add the patch to the Axes
Add class description
First determine best location by finding the corner that is closest to the image center
relative corner coordinates
relative coordinates of image center
Compute pair-wise squared distance and get the index of the one with minimal distance
Get the corresponding coordinates ...
... and alignment for the text box
Plot the text box with background
...for each box
This is magic goop that removes whitespace around image plots (sort of)
"plt.savefig(outputFileName, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)"
os.startfile(outputFileName)
...for each image
...def render_bounding_boxes
Load and run detector on target images
Read the name of all classes
remove empty lines
%% Interactive driver
%%
%%
%% File helper functions
%% Command-line driver
Hack to avoid running on already-detected images
""
"Script for evaluating a frozen graph given a COCO-style dataset, which was generated by"
the ../data_management/databases/classification/make_classification_dataset.py script.
This is useful for debugging as the accuracy reported by this script should match the
accuracy reported by the Tensorflow training.
""
Check that all files exists for easier debugging
Load frozen graph
Collect tensors for input and output
Read image
Run inference
Check if correct
""
Script for evaluating precision/recall on a two-class problem given
"a frozen graph and a COCO-style dataset, which was generated by"
the ../data_management/databases/classification/make_classification_dataset.py script.
We assume the positive class is at index 0 (with a zero-based indexing)
""
Check that all files exists for easier debugging
Load frozen graph
Collect tensors for input and output
Read image
"with open(image_path, 'rb') as fi:"
"image =  sess.run(tf.image.decode_jpeg(fi.read(), channels=3))"
image = image / 255.
Run inference
predicted_class = np.argmax(predictions)
Check if correct
if coco.imgToAnns[image_id][0]['category_id'] == predicted_class:
correct = correct + 1
"In matplotlib < 1.5, plt.fill_between does not have a 'step' argument"
""
Script for selecting testing images of a COCO-style dataset generated by the script
../data_management/databases/classification/make_classification_dataset.py in a consistent
manner and predicting the class for it.
""
Mandatory parameters
Optional parameters
Validate parameters
Derived parameters
"We assume that the dataset was generated with the make_classification_dataset.py script,"
hence the images should be located in the same folder as the json
Make seletion deterministic
Load frozen graph
Get dataset information
Get classes
...and the class list corresponding to the model outputs by assuming
that they are in order of their ids
Get images of each class
Shuffle the image list
Start the image sampling
"Set of avaiable class IDs, will be filled below"
If there are still images left for that class
"Get image for the sampled class, we already shuffled the class images before so"
we can simply pop()
Start prediction
Collect tensors for input and output
Read image
Run inference
Print output to log file
#####
""
api_apply_classifier_single_node.py
""
Takes the JSON file produced by the detection API and
classifies all boxes above a confidence threshold.
""
#####
"%% Constants, imports, environment"
Assumes that the root of the CameraTraps repo is on the PYTHONPATH
Minimum detection confidence for classifying an object
Number of top-scoring classes to show at each bounding box
Enlargment factor applied to boxes before passing them to the classifier
""
Provides more context and can lead to better results
List of detection categories for which we will run the classification
""
"Currently there are {""1"": ""animal"", ""2"": ""person"", ""4"": ""vehicle""}"
""
Should be a list of string-formatted ints.
Number of significant float digits in JSON output
%% Core detection functions
Read the name of all classes
remove empty lines
Create field with name *classification_categories*
Add classes using 0-based indexing
def add_classification_categories
Make sure we have the right json object
Get input and output tensors of classification model
For each image
Read image
"Scale pixel values to [0,1]"
For each box
Skip detections with low confidence
Skip if detection category is not in whitelist
Skip if already classified
"Get current box in relative coordinates and format [x_min, y_min, width_of_box, height_of_box]"
"Convert to [ymin, xmin, ymax, xmax] and store it as 1x4 numpy array so we can"
re-use the generic multi-box padding code
Convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes."
""
"However, we need to make sure that it box coordinates are still within the image."
Get the first (and only) row as our bbox to classify
Get the image data for that box
Run inference
Add an empty list to the json for our predictions
Add the *num_annotated_classes* top scoring classes
...for each box
...for each image
...with tf.Session
with classification_graph
def classify_boxes
Load classification model
Load detector json
Add classes to detector_json
"Run classifier on all images, changes will be writting directly to the json"
Write output json
def load_and_run_classifier
%% Command-line driver
""
Script for selecting testing images of a COCO-style dataset generated by the script
../data_management/databases/classification/make_classification_dataset.py in a consistent
manner and predicting the class for it.
""
Assumes the cameratraps repo root is on the path
Make seletion deterministic
Mandatory parameters
Optional parameters
Validate parameters
Tranfer parameters to post-processing format
Load frozen graph
Reading image list
Reading class list
Image sampling
Start prediction
Collect tensors for input and output
Array for collecting infos for rendering the html
Read image
"Scale pixel values to [0,1]"
Run inference
Add links to all available classes
""
predict_image.py
""
"Given a pointer to a frozen detection graph, runs inference on a single image,"
printing the top classes to the console
""
%% Imports
%% Command-line processing
Check that all files exist for easier debugging
%% Inference
Load frozen graph
Load class list
Remove empty lines
Collect tensors for input and output
Read image
Run inference
Print output
""
Mostly unmodified script for freezing a model
Added for convenience and for possible future optimizations
""
Copyright 2015 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
'input_checkpoint' may be a prefix if we're using Saver V2 format
Remove all the explicit device specifications for this node. This helps to
make the graph more portable.
List of all partition variables. Because the condition is heuristic
"based, the list could include false positives."
This tensor doesn't exist in the graph (for example it's
'global_step' or a similar housekeeping element) so skip it.
`var_list` is required to be a map of variable names to Variable
tensors. Partition variables are Identity tensors that cannot be
handled by Saver.
Models that have been frozen previously do not contain Variables.
optimized_output_graph_def = optimize_for_inference_lib.optimize_for_inference(
"output_graph_def,"
"input_node_names.replace("" "", """").split("",""),"
"output_node_names.replace("" "", """").split("",""),"
tf.float32.as_datatype_enum)
Write GraphDef to file if output path has been given.
""
"Creates a graph description, which is required to create a frozen graph."
Adapted from from ./tf-slim/export_inference_graph.py
Added preprocessing to the definition for easier handling
""
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
tf.app.flags.DEFINE_integer(
"'batch_size', None,"
"'Batch size for the exported model. Defaulted to ""None"" so batch size can '"
'be specified at model runtime.')
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
#####################
Optimization Flags #
#####################
######################
Learning Rate Flags #
######################
######################
Dataset Flags #
######################
####################
Fine-Tuning Flags #
####################
"Note: when num_clones is > 1, this will actually have each clone to go"
over each epoch FLAGS.num_epochs_per_decay times. This is different
behavior from sync replicas and is expected to produce different results.
Warn the user if a checkpoint exists in the train_dir. Then we'll be
ignoring the checkpoint anyway.
TODO(sguada) variables.filter_variables()
######################
Config model_deploy #
######################
Create global_step
#####################
Select the dataset #
#####################
#####################
Select the network #
#####################
####################################
Select the preprocessing function #
####################################
#############################################################
Create a dataset provider that loads data from the dataset #
#############################################################
###################
Define the model #
###################
############################
Specify the loss function #
############################
Gather initial summaries.
"Gather update_ops from the first clone. These contain, for example,"
the updates for the batch_norm variables created by network_fn.
Add summaries for end_points.
Add summaries for losses.
Add summaries for variables.
################################
Configure the moving averages #
################################
########################################
Configure the optimization procedure. #
########################################
"If sync_replicas is enabled, the averaging will be done in the chief"
queue runner.
Update ops executed locally by trainer.
Variables to train.
and returns a train_tensor and summary_op
Add total_loss to summary.
Create gradient updates.
Add the summaries from the first clone. These contain the summaries
created by model_fn and either optimize_clones() or _gather_clone_loss().
Merge all summaries together.
##########################
Kicks off the training. #
##########################
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
#####################
Select the dataset #
#####################
###################
Select the model #
###################
#############################################################
Create a dataset provider that loads data from the dataset #
#############################################################
####################################
Select the preprocessing function #
####################################
###################
Define the model #
###################
Define the metrics:
Print the summaries to screen.
TODO(sguada) use num_epochs=1
This ensures that we make a single pass over all of the data.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Transform the image to floats.
"Randomly crop a [height, width] section of the image."
Randomly flip the image horizontally.
"Because these operations are not commutative, consider randomizing"
the order their operation.
Subtract off the mean and divide by the variance of the pixels.
Transform the image to floats.
Resize and crop if needed.
Subtract off the mean and divide by the variance of the pixels.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Use tf.slice instead of crop_to_bounding box as it accepts tensors to
define the crop size.
Compute the rank assertions.
Create a random bounding box.
""
Use tf.random_uniform and not numpy.random.rand as doing the former would
"generate random numbers at graph eval time, unlike the latter which"
generates random numbers at graph definition time.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Pass the real x only to one of the func calls.
The random_* ops do not necessarily clamp.
"Each bounding box has shape [1, num_boxes, box coords] and"
"the coordinates are ordered [ymin, xmin, ymax, xmax]."
A large fraction of image datasets contain a human-annotated bounding
box delineating the region of the image containing the object of interest.
We choose to create a new bounding box for the object which is a randomly
distorted version of the human-annotated bounding box that obeys an
"allowed range of aspect ratios, sizes and overlap with the human-annotated"
"bounding box. If no box is supplied, then we assume the bounding box is"
the entire image.
Crop the image to the specified bounding box.
"Each bounding box has shape [1, num_boxes, box coords] and"
"the coordinates are ordered [ymin, xmin, ymax, xmax]."
Restore the shape since the dynamic slice based upon the bbox_size loses
the third dimension.
This resizing operation may distort the images because the aspect
ratio is not respected. We select a resize method in a round robin
fashion based on the thread number.
Note that ResizeMethod contains 4 enumerated resizing methods.
We select only 1 case for fast_mode bilinear.
Randomly flip the image horizontally.
Randomly distort the colors. There are 1 or 4 ways to do it.
Crop the central region of the image with an area containing 87.5% of
the original image.
Resize the image to the specified height and width.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(sguada) docstring paragraph by (a) motivating the need for the file and
(b) defining clones.
TODO(sguada) describe the high-level components of model deployment.
"E.g. ""each model deployment is composed of several parts: a DeploymentConfig,"
"which captures A, B and C, an input_fn which loads data.. etc"
Set up DeploymentConfig
Create the global step on the device storing the variables.
Define the inputs
Define the optimizer.
Define the model including the loss.
Run training.
Namedtuple used to represent a clone during deployment.
"Namedtuple used to represent a DeployedModel, returned by deploy()."
Default parameters for DeploymentConfig
Create clones.
The return value.
Individual components of the loss that will need summaries.
Compute and aggregate losses on the clone device.
Add the summaries out of the clone device block.
Only use regularization_losses for the first clone
Compute the total_loss summing all the clones_losses.
Sum the gradients across clones.
Gather initial summaries.
Create Clones.
"Gather update_ops from the first clone. These contain, for example,"
the updates for the batch_norm variables created by model_fn.
Place the global step on the device storing the variables.
Compute the gradients for the clones.
Add summaries to the gradients.
Create gradient updates.
Only use regularization_losses for the first clone
Add the summaries from the first clone. These contain the summaries
created by model_fn and either optimize_clones() or _gather_clone_loss().
Add total_loss to summary.
Merge all summaries together.
Note that each grad_and_vars looks like the following:
"((grad_var0_clone0, var0), ... (grad_varN_cloneN, varN))"
!/usr/bin/python
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Failed to find ""index"" occurrence of item."
pylint: disable=broad-except
pylint: enable=broad-except
Grab the 'index' annotation.
Some images contain bounding box annotations that
"extend outside of the supplied image. See, e.g."
n03127925/n03127925_147.xml
"Additionally, for some bounding boxes, the min > max"
or the box is entirely outside of the image.
Example: <...>/n06470073/n00141669_6790.xml
Determine if the annotation is from an ImageNet Challenge label.
Note: There is a slight bug in the bounding box annotation data.
Many of the dog labels have the human label 'Scottish_deerhound'
instead of the synset ID 'n02092002' in the bbox.label field. As a
"simple hack to overcome this issue, we only exclude bbox labels"
*which are synset ID's* that do not match original synset label for
the XML file.
Guard against improperly specified boxes.
Note bbox.filename occasionally contains '%s' in the name. This is
data set noise that is fixed by just using the basename of the XML file.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The URLs where the MNIST data can be downloaded.
The names of the classes.
"First, process the training data:"
"Next, process the testing data:"
"Finally, write the labels file:"
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The URL where the Flowers data can be downloaded.
The number of images in the validation set.
Seed for repeatability.
The number of shards per dataset split.
Initializes function that decodes RGB JPEG data.
Read the filename:
Divide into train and test:
"First, convert the training and validation sets."
"Finally, write the labels file:"
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(nsilberman): Add tfrecord file type once the script is updated.
"If set to false, will not try to set label_to_names in dataset"
by reading them from labels.txt or github.
n01440764
n01443537
n02119247    black fox
n02119359    silver fox
pylint: disable=g-line-too-long
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The URL where the CIFAR data can be downloaded.
The number of training files.
The height and width of each image.
The names of the classes.
"First, process the training data:"
"Next, process the testing data:"
"Finally, write the labels file:"
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The labels file contains a list of valid labels are held in this file.
Assumes that the file contains entries as such:
n01440764
n01443537
n01484850
where each line corresponds to a label expressed as a synset. We map
each synset contained in the file to an integer (based on the alphabetical
ordering). See below for details.
This file containing mapping from synset to human-readable label.
Assumes each line of the file looks like:
""
n02119247    black fox
n02119359    silver fox
"n02119477    red fox, Vulpes fulva"
""
where each line corresponds to a unique mapping. Note that each line is
formatted as <synset>\t<human readable label>.
This file is the output of process_bounding_box.py
Assumes each line of the file looks like:
""
"n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940"
""
where each line corresponds to one bounding box annotation associated
with an image. Each line can be parsed as:
""
"<JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>"
""
Note that there might exist mulitple bounding box annotations associated
with an image file.
pylint: disable=expression-not-assigned
pylint: enable=expression-not-assigned
Create a single Session to run all image coding calls.
Initializes function that converts PNG to JPEG data.
Initializes function that converts CMYK JPEG data to RGB JPEG data.
Initializes function that decodes RGB JPEG data.
File list from:
https://groups.google.com/forum/embed/?place=forum/torch7#!topic/torch7/fOSTXHIESSU
File list from:
https://github.com/cytsai/ilsvrc-cmyk-image-list
Read the image file.
Clean the dirty data.
1 image is a PNG.
22 JPEG images are in CMYK colorspace.
Decode the RGB JPEG.
Check that image converted to RGB
Each thread produces N shards where N = int(num_shards / num_threads).
"For instance, if num_shards = 128, and the num_threads = 2, then the first"
"thread would produce shards [0, 64)."
"Generate a sharded version of the file name, e.g. 'train-00002-of-00010'"
"Break all images into batches with a [ranges[i][0], ranges[i][1]]."
Launch a thread for each batch.
Create a mechanism for monitoring when all threads are finished.
Create a generic TensorFlow-based utility for converting all image codings.
Wait for all the threads to terminate.
Leave label index 0 empty as a background class.
Construct the list of JPEG files and labels.
Shuffle the ordering of all image files in order to guarantee
random ordering of the images with respect to label in the
saved TFRecord files. Make the randomization repeatable.
Build a map from synset to human-readable label.
Run it!
Allowing None in the signature so that dataset_factory can use the default.
!/usr/bin/python
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Read in the 50000 synsets associated with the validation data set.
Make all sub-directories in the validation data dir.
Move all of the image to the appropriate sub-directory.
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
"The dataset has classes with no images (empty and human), which have ID 0 and 1,"
so we need to specify 49 here despite having only 47 classes with images
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Final pooling and prediction
Global average pooling.
Pooling with a fixed kernel size.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The current_stride variable keeps track of the effective stride of the
activations. This allows us to invoke atrous convolution whenever applying
the next residual unit would result in the activations having stride larger
than the target output_stride.
The atrous convolution rate parameter.
Move stride from the block's last unit to the end of the block.
"If we have reached the target output_stride, then we need to employ"
atrous convolution with stride=1 and multiply the atrous rate by the
current unit's stride for use in subsequent layers.
Collect activations at the block's end before performing subsampling.
Subsampling of the block's output activations.
"The following implies padding='SAME' for pool1, which makes feature"
alignment easier for dense prediction tasks. This is also used in
https://github.com/facebook/fb.resnet.torch. However the accompanying
code of 'Deep Residual Learning for Image Recognition' uses
padding='VALID' for pool1. You can switch to that choice by setting
"slim.arg_scope([slim.max_pool2d], padding='VALID')."
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Collect outputs for conv2d, fully_connected and max_pool2d"
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"inputs has shape [batch, 224, 224, 3]"
"inputs has shape [batch, 513, 513, 3]"
We do not include batch normalization or activation functions in
conv1 because the first ResNet unit will perform these. Cf.
Appendix of [2].
This is needed because the pre-activation variant does not have batch
normalization or activation functions in the residual unit output. See
Appendix of [2].
Convert end_points_collection into a dictionary of end_points.
Global average pooling.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
=============================================================================
"These parameters come from the online port, which don't necessarily match"
those in the paper.
TODO(nsilberman): confirm these values with Philip.
##########
Encoder #
##########
"No normalizer for the first encoder layers as per 'Image-to-Image',"
Section 5.1.1
First layer doesn't use normalizer_fn
Last layer doesn't use activation_fn nor normalizer_fn
##########
Decoder #
##########
"Dropout is used at both train and test time as per 'Image-to-Image',"
Section 2.1 (last paragraph).
The Relu comes BEFORE the upsample op:
Explicitly set the normalizer_fn to None to override any default value
"that may come from an arg_scope, such as pix2pix_arg_scope."
No normalization on the input layer.
Stride 1 on the last layer.
"1-dim logits, stride 1, no activation, no normalization."
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
Alias
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
pylint: disable=unused-import
pylint: enable=unused-import
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
If we are fine tuning a checkpoint we need to start at a lower learning
rate since we are farther along on training.
We can start quantizing immediately if we are finetuning.
We need to wait for the model to train a bit before we quantize if we are
training from scratch.
Call rewriter to produce graph with fake quant ops and folded batch norms
"quant_delay delays start of quantization till quant_delay steps, allowing"
for better model accuracy.
Configure the learning rate using an exponential decay.
"When restoring from a floating point model, the min/max values for"
quantized weights and activations are not present.
We instruct slim to ignore variables that are missing during restoration
by setting ignore_missing_vars=True
"If we are restoring from a floating point model, we need to initialize"
the global step to zero for the exponential decay to result in
reasonable learning rates.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Orignaly, arg_scope = slim.arg_scope and layers = slim, now switch to more"
update-to-date tf.contrib.* API.
Decay for the moving averages.
epsilon to prevent 0s in variance.
Turns off fused batch norm.
collection containing the moving mean and moving variance.
"For inverted pyramid models, we start with gating switched off."
batch_size x 32 x 112 x 112 x 64
Separable conv is slow when used at first conv layer.
batch_size x 32 x 56 x 56 x 64
batch_size x 32 x 56 x 56 x 64
batch_size x 32 x 56 x 56 x 192
batch_size x 32 x 28 x 28 x 192
batch_size x 32 x 28 x 28 x 256
batch_size x 16 x 14 x 14 x 512
batch_size x 16 x 14 x 14 x 512
batch_size x 16 x 14 x 14 x 512
batch_size x 16 x 14 x 14 x 528
batch_size x 16 x 14 x 14 x 832
batch_size x 8 x 7 x 7 x 832
batch_size x 8 x 7 x 7 x 1024
Final pooling and prediction
Temporal average pooling.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
299 x 299 x 3
149 x 149 x 32
147 x 147 x 32
147 x 147 x 64
73 x 73 x 160
71 x 71 x 192
35 x 35 x 384
4 x Inception-A blocks
35 x 35 x 384
Reduction-A block
17 x 17 x 1024
7 x Inception-B blocks
17 x 17 x 1024
Reduction-B block
8 x 8 x 1536
3 x Inception-C blocks
Auxiliary Head logits
17 x 17 x 1024
Final pooling and prediction
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which"
can be set to False to disable pooling here (as in resnet_*()).
8 x 8 x 1536
1 x 1 x 1536
1536
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN
setups need the gradient of gradient FusedBatchNormGrad.
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN
setups need the gradient of gradient FusedBatchNormGrad.
First upscaling is different because it takes the input vector.
Last layer has different normalizer and activation.
Convert to proper channels.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Use clip_by_value to simulate bandpass activation.
Use clip_by_value to simulate bandpass activation.
Use clip_by_value to simulate bandpass activation.
149 x 149 x 32
147 x 147 x 32
147 x 147 x 64
73 x 73 x 64
73 x 73 x 80
71 x 71 x 192
35 x 35 x 192
35 x 35 x 320
TODO(alemi): Register intermediate endpoints
"17 x 17 x 1088 if output_stride == 8,"
33 x 33 x 1088 if output_stride == 16
TODO(alemi): register intermediate endpoints
TODO(gpapan): Properly support output_stride for the rest of the net.
8 x 8 x 2080
TODO(alemi): register intermediate endpoints
8 x 8 x 1536
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which"
can be set to False to disable pooling here (as in resnet_*()).
Set weight_decay for weights in conv2d and fully_connected layers.
Set activation_fn and parameters for batch_norm.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Decay for the moving averages.
epsilon to prevent 0s in variance.
collection containing update_ops.
use fused batch norm if possible.
Set weight_decay for weights in Conv and FC layers.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"inputs has shape [batch, 224, 224, 3]"
"inputs has shape [batch, 513, 513, 3]"
Use clip_by_value to simulate bandpass activation.
Convert end_points_collection into a dictionary of end_points.
Global average pooling.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"end_points will collect relevant activations for external use, for example"
summaries or losses.
299 x 299 x 3
149 x 149 x 32
147 x 147 x 32
147 x 147 x 64
73 x 73 x 64
73 x 73 x 80.
71 x 71 x 192.
35 x 35 x 192.
Inception blocks
mixed: 35 x 35 x 256.
mixed_1: 35 x 35 x 288.
mixed_2: 35 x 35 x 288.
mixed_3: 17 x 17 x 768.
mixed4: 17 x 17 x 768.
mixed_5: 17 x 17 x 768.
mixed_6: 17 x 17 x 768.
mixed_7: 17 x 17 x 768.
mixed_8: 8 x 8 x 1280.
mixed_9: 8 x 8 x 2048.
mixed_10: 8 x 8 x 2048.
Auxiliary Head logits
Shape of feature map before the final layer.
Final pooling and prediction
Global average pooling.
Pooling with a fixed kernel size.
1 x 1 x 2048
2048
1000
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Reflection pad by 1 in spatial dimensions (axes 1, 2 = h, w) to make a 3x3"
'valid' convolution produce an output with the same dimension as the
input.
This corrects 1 pixel offset for images with even width and height.
conv2d is left aligned and conv2d_transpose is right aligned for even
sized images (while doing 'SAME' padding).
Note: This doesn't reflect actual model in paper.
Neither dropout nor batch norm -> dont need is_training
##########
Encoder #
##########
7x7 input stage
##################
Residual Blocks #
##################
##########
Decoder #
##########
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"end_points will collect relevant activations for external use, for example"
summaries or losses.
Used to find thinned depths for each layer.
Note that sizes in the comments below assume an input spatial size of
"224x224, however, the inputs can be of any size greater 32x32."
224 x 224 x 3
depthwise_multiplier here is different from depth_multiplier.
depthwise_multiplier determines the output channels of the initial
"depthwise conv (see docs for tf.nn.separable_conv2d), while"
depth_multiplier controls the # channels of the subsequent 1x1
convolution. Must have
in_channels * depthwise_multipler <= out_channels
so that the separable convolution is not overparameterized.
Use a normal convolution instead of a separable convolution.
112 x 112 x 64
56 x 56 x 64
56 x 56 x 64
56 x 56 x 192
28 x 28 x 192
Inception module.
28 x 28 x 256
28 x 28 x 320
14 x 14 x 576
14 x 14 x 576
14 x 14 x 576
14 x 14 x 576
14 x 14 x 576
7 x 7 x 1024
7 x 7 x 1024
Final pooling and prediction
Global average pooling.
Pooling with a fixed kernel size.
1 x 1 x 1024
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
=============================================================================
Tensorflow mandates these.
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture
Conv defines 3x3 convolution layers
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.
stride is the stride of the convolution
depth is the number of channels or filters in a layer
MOBILENETV1_CONV_DEFS specifies the MobileNet body
Used to find thinned depths for each layer.
The current_stride variable keeps track of the output stride of the
"activations, i.e., the running product of convolution strides up to the"
current network layer. This allows us to invoke atrous convolution
whenever applying the next convolution would result in the activations
having output stride larger than the target output_stride.
The atrous convolution rate parameter.
"If we have reached the target output_stride, then we need to employ"
atrous convolution with stride=1 and multiply the atrous rate by the
current unit's stride for use in subsequent layers.
By passing filters=None
separable_conv2d produces only a depthwise convolution layer
Global average pooling.
Pooling with a fixed kernel size.
1 x 1 x 1024
Set weight_decay for weights in Conv and DepthSepConv layers.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Decay for the moving averages.
epsilon to prevent 0s in variance.
Turns off fused batch norm.
collection containing the moving mean and moving variance.
Final pooling and prediction
Temporal average pooling.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Orignaly, add_arg_scope = slim.add_arg_scope and layers = slim, now switch to"
more update-to-date tf.contrib.* API.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Make sure that round down does not go down by more than 10%.
"Note: want to round down, we adjust each split to match the total."
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts."
which provide numbered scopes.
b1 -> b2 * r -> b2
i -> (o * r) (bottleneck) -> o
"Note in contrast with expansion, we always have"
projection to produce the desired output size.
stride check enforces that we don't add residuals when spatial
dimensions are None
Depth matches
Don't do any splitting if we end up with less than 8 filters
on either side.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Make sure that round down does not go down by more than 10%.
Set conv defs defaults and overrides.
a) Set the tensorflow scope
b) set padding to default: note we might consider removing this
since it is also set by mobilenet_scope
c) set all defaults
d) set all extra overrides.
The current_stride variable keeps track of the output stride of the
"activations, i.e., the running product of convolution strides up to the"
current network layer. This allows us to invoke atrous convolution
whenever applying the next convolution would result in the activations
having output stride larger than the target output_stride.
The atrous convolution rate parameter.
Insert default parameters before the base scope which includes
any custom overrides set in mobilenet.
"If we have reached the target output_stride, then we need to employ"
atrous convolution with stride=1 and multiply the atrous rate by the
current unit's stride for use in subsequent layers.
Update params.
Only insert rate to params if rate > 1.
Set padding
Add all tensors that end with 'output' to
endpoints
1 x 1 x num_classes
Note: legacy scope name.
"Recover output shape, for unknown shape."
the network created will be trainble with dropout/batch norm
initialized appropriately.
Note: do not introduce parameters that would change the inference
"model here (for example whether to use bias), modify conv_def instead."
Set weight_decay for weights in Conv and FC layers.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
pyformat: disable
Architecture: https://arxiv.org/abs/1801.04381
Note: these parameters of batch norm affect the architecture
that's why they are here and not in training_scope.
pyformat: enable
NB: do not set depth_args unless they are provided to avoid overriding
whatever default depth_multiplier might have thanks to arg_scope.
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that
"'finegrain_classification_mode' is set to True, which means the embedding"
layer will not be shrinked when given a depth-multiplier < 1.0.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Notes for training NASNet Cifar Model
-------------------------------------
batch_size: 32
learning rate: 0.025
cosine (single period) learning rate decay
auxiliary head loss weighting: 0.4
clip global norm of all gradients by 5
600 epochs with a batch size of 32
This is used for the drop path probabilities since it needs to increase
the drop out probability over the course of training.
Notes for training large NASNet model on ImageNet
-------------------------------------
batch size (per replica): 16
learning rate: 0.015 * 100
learning rate decay factor: 0.97
num epochs per decay: 2.4
sync sgd with 100 replicas
auxiliary head loss weighting: 0.4
label smoothing: 0.1
clip global norm of all gradients by 10
Notes for training the mobile NASNet ImageNet model
-------------------------------------
batch size (per replica): 32
learning rate: 0.04 * 50
learning rate scaling factor: 0.97
num epochs per decay: 2.4
sync sgd with 50 replicas
auxiliary head weighting: 0.4
label smoothing: 0.1
clip global norm of all gradients by 10
Decay for the moving averages.
epsilon to prevent 0s in variance.
Decay for the moving averages.
epsilon to prevent 0s in variance.
Decay for the moving averages.
epsilon to prevent 0s in variance.
Shape of feature map before the final layer.
149 x 149 x 32
Run the reduction cells
Calculate the total number of cells in the network
Add 2 for the reduction cells
Calculate the total number of cells in the network
Add 2 for the reduction cells
"If ImageNet, then add an additional two for the stem cells"
Calculate the total number of cells in the network
Add 2 for the reduction cells
"If ImageNet, then add an additional two for the stem cells"
Find where to place the reduction cells or stride normal cells
Setup for building in the auxiliary head.
Run the cells
true_cell_num accounts for the stem cells
Final softmax layer
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Find where to place the reduction cells or stride normal cells
pylint: disable=protected-access
pylint: enable=protected-access
Setup for building in the auxiliary head.
Run the cells
true_cell_num accounts for the stem cells
pylint: disable=protected-access
pylint: enable=protected-access
Final softmax layer
pylint: disable=protected-access
pylint: enable=protected-access
Calculate the total number of cells in the network.
There is no distinction between reduction and normal cells in PNAS so the
total number of cells is equal to the number normal cells plus the number
of stem cells (two by default).
pylint: disable=protected-access
pylint: enable=protected-access
Calculate the total number of cells in the network.
There is no distinction between reduction and normal cells in PNAS so the
total number of cells is equal to the number normal cells plus the number
of stem cells (two by default).
Configuration for the PNASNet-5 model.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"The cap for tf.clip_by_value, it's hinted from the activation distribution"
"that the majority of activation values are in the range [-6, 6]."
Skip path 1
Skip path 2
"First pad with 0's on the right and bottom, then shift the filter to"
include those 0's that were added.
"If odd number of filters, add an additional one to the second path."
Concat and apply BN
Set the prev layer to the current layer if it is none
Check to be sure prev layer stuff is setup correctly
num_or_size_splits=1
Apply conv operations
Combine hidden states using 'add'.
Add hiddenstate to the list of hiddenstates we can choose from
Dont stride if this is not one of the original hiddenstates
"Check if a stride is needed, then use a strided 1x1 here"
Determine if a reduction should be applied to make the number of
filters match.
Return the concat of all the states
Scale keep prob by layer number
The added 2 is for the reduction cells
Decrease the keep probability over time
""
Script for generating a two-class dataset in COCO format for training an obscured image classifier
""
Requires Python >= 3.6 because of the glob ** expression
""
Collect images and labels
"Labels: clean = 0, obscured = 1"
""
top_folders_to_bottom.py
""
Given a base folder with files like:
""
A/1/2/a.jpg
B/3/4/b.jpg
""
"...moves the top-level folders to the bottom in a new output folder, i.e., creates:"
""
1/2/A/a.jpg
3/4/B/b.jpg
""
"In practice, this is used to make this:"
""
animal/camera01/image01.jpg
""
...look like:
""
camera01/animal/image01.jpg
""
%% Constants and imports
%% Support functions
%% Main functions
Find top-level folder
Find file/folder names
Move or copy
...def process_file()
Enumerate input folder
Convert absolute paths to relative paths
Standardize delimiters
Make sure each input file maps to a unique output file
relative_filename = relative_files[0]
Loop
...def top_folders_to_bottom()
%% Interactive driver
%%
%%
%% Command-line driver
"python top_folders_to_bottom.py ""g:\temp\separated_images"" ""g:\temp\separated_images_inverted"" --n_threads 100"
Convert to an options object
%% Constants and imports
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils)
Assumes the cameratraps repo root is on the path
%% Options
"Make sure there is no overlap between the two sets, because this will cause"
issues in the code
## Required inputs
## Options
Can be a folder or a SAS URL
# These apply only when we're doing ground-truth comparisons
Classes we'll treat as negative
""
"Include the token ""#NO_LABELS#"" to indicate that an image with no annotations"
should be considered empty.
Classes we'll treat as neither positive nor negative
"A list of output sets that we should count, but not render images for."
""
"Typically used to preview sets with lots of empties, where you don't want to"
"subset but also don't want to render 100,000 empty images."
""
"detections, non_detections"
"detections_animal, detections_person, detections_vehicle"
"By default, choose a confidence threshold based on the detector version"
Used for summary statistics only
"Number of images to sample, -1 for ""all images"""
"Random seed for sampling, or None"
Optionally separate detections into categories (animal/vehicle/human)
""
Currently only supported when ground truth is unavailable
Optionally replace one or more strings in filenames with other strings;
useful for taking a set of results generated for one folder structure
and applying them to a slightly different folder structure.
Allow bypassing API output loading when operating on previously-loaded
results
Should we also split out a separate report about the detections that were
just below our main confidence threshold?
""
Currently only supported when ground truth is unavailable
Control rendering parallelization
...PostProcessingOptions
#%% Helper classes and functions
Anything greater than this isn't clearly positive or negative
image has annotations suggesting both negative and positive
"image is not annotated or is annotated with 'unknown', 'unlabeled', ETC."
image has not yet been assigned a state
"In some analyses, we add an additional class that lets us look at"
detections just below our main confidence threshold
count the # of images with each type of DetectionStatus
Check whether this image has:
- unknown / unassigned-type labels
- negative-type labels
"- positive labels (i.e., labels that are neither unknown nor negative)"
"assert has_unknown_labels is False, '{} has unknown labels'.format(annotations)"
If there are no image annotations...
n_negative += 1
im['_detection_status'] = DetectionStatus.DS_NEGATIVE
"If the image has more than one type of labels, it's ambiguous"
"note: bools are automatically converted to 0/1, so we can sum"
"After the check above, we can be sure it's only one of positive,"
"negative, or unknown."
""
Important: do not merge the following 'unknown' branch with the first
"'unknown' branch above, where we tested 'if len(categories) == 0'"
""
If the image has only unknown labels
If the image has only negative labels
If the images has only positive labels
"Annotate the category, if it is unambiguous"
...for each image
...mark_detection_status()
"Leaving code in place for reading from blob storage, may support this"
in the future.
resize is to display them in this notebook or in the HTML more quickly
os.path.isfile() is slow when mounting remote directories; much faster
to just try/except on the image open.
return ''
Render images to a flat folder... we can use os.sep here because we've
already normalized paths
"Create class labels like ""gt_1"" or ""gt_27"""
"for i_box,box in enumerate(ground_truth_boxes):"
gt_classes.append('_' + str(box[-1]))
"errno.ENAMETOOLONG doesn't get thrown properly on Windows, so"
we awkwardly check against a hard-coded limit
Use slashes regardless of os
Optionally add links back to the original images
...render_bounding_boxes
Count items in each category
Optionally sort by filename before writing to html
Write the individual HTML files
...prepare_html_subpages()
Get unique categories above the threshold for this image
Render an image (with no ground truth information)
"This is a list of [class,confidence] pairs, sorted by confidence"
"If we either don't have a confidence threshold, or we've met our"
confidence threshold
...if this detection has classification info
...for each detection
...def render_image_no_gt()
This should already have been normalized to either '/' or '\'
...def render_image_with_gt()
%% Main function
#%% Expand some options for convenience
#%% Prepare output dir
#%% Load ground truth if available
Mark images in the ground truth as positive or negative
#%% Load detection (and possibly classification) results
"If the caller hasn't supplied results, load them"
Determine confidence thresholds if necessary
Remove failed rows
Convert keys and values to lowercase
"Add column 'pred_detection_label' to indicate predicted detection status,"
not separating out the classes
#%% Pull out descriptive metadata
This is rare; it only happens during debugging when the caller
is supplying already-loaded API results.
"#%% If we have ground truth, remove images we can't match to ground truth"
#%% Sample images for visualization
#%% Fork here depending on whether or not ground truth is available
"If we have ground truth, we'll compute precision/recall and sample tp/fp/tn/fn."
""
Otherwise we'll just visualize detections/non-detections.
#%% Detection evaluation: compute precision/recall
numpy array of detection probabilities
"numpy array of bools (0.0/1.0), and -1 as null value"
Don't include ambiguous/unknown ground truth in precision/recall analysis
"For completeness, include the result at a confidence threshold of 1.0"
Compute and print summary statistics
Thresholds go up throughout precisions/recalls/thresholds; find the last
value where recall is at or above target.  That's our precision @ target recall.
"np.where returns a tuple of arrays, but in this syntax where we're"
"comparing an array with a scalar, there will only be one element."
Convert back to a list
Flatten the confusion matrix
"#%% Collect classification results, if they exist"
Mapping of classnames to idx for the confusion matrix.
""
"The lambda is actually kind of a hack, because we use assume that"
the following code does not reassign classname_to_idx
Confusion matrix as defaultdict of defaultdict
""
"Rows / first index is ground truth, columns / second index is predicted category"
iDetection = 0; fn = detector_files[iDetection]; print(fn)
"If this image has classification predictions, and an unambiguous class"
"annotated, and is a positive image..."
"The unambiguous category, we make this a set for easier handling afterward"
"Compute the accuracy as intersection of union,"
i.e. (# of categories in both prediciton and GT)
divided by (# of categories in either prediction or GT
""
"In case of only one GT category, the result will be 1.0, if"
prediction is one category and this category matches GT
""
"It is 1.0/(# of predicted top-1 categories), if the GT is"
one of the predicted top-1 categories.
""
"It is 0.0, if none of the predicted categories is correct"
Distribute this accuracy across all predicted categories in the
confusion matrix
...for each file in the detection results
If we have classification results
Build confusion matrix as array from classifier_cm
Print some statistics
Prepare confusion matrix output
Get confusion matrix as string
Get fixed-size classname for each idx
Prepend class name on each line and add to the top
Print formatted confusion matrix
"Actually don't, this gets really messy in all but the widest consoles"
Plot confusion matrix
To manually add more space at bottom: plt.rcParams['figure.subplot.bottom'] = 0.1
""
"Add 0.5 to figsize for every class. For two classes, this will result in"
"fig = plt.figure(figsize=[4,4])"
...if we have classification results
#%% Render output
Write p/r table to .csv file in output directory
Write precision/recall plot to .png file in output directory
#%% Sampling
Sample true/false positives/negatives with correct/incorrect top-1
classification and render to html
Accumulate html image structs (in the format expected by write_html_image_lists)
"for each category, e.g. 'tp', 'fp', ..., 'class_bird', ..."
Add default entries by accessing them for the first time
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]"
"Each element will be a three-tuple with elements file,max_conf,detections"
"Assemble the information we need for rendering, so we can parallelize without"
dealing with Pandas
i_row = 0; row = images_to_visualize.iloc[0]
Filenames should already have been normalized to either '/' or '\'
Map all the rendering results in the list rendering_results into the
dictionary images_html
Prepare the individual html image files
Write index.html
Show links to each GT class
""
We could do this without classification results; currently we don't.
Add links to all available classes
Close body and html tags
...for each image
"#%% Otherwise, if we don't have ground truth..."
#%% Sample detections/non-detections
Accumulate html image structs (in the format expected by write_html_image_list)
for each category
Add default entries by accessing them for the first time
"Maps detection categories - e.g. ""human"" - to result set names, e.g."
"""detections_human"""
Add a set of results for each category and combination of categories
Create output directories
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]"
"list of 3-tuples with elements (file, max_conf, detections)"
"Assemble the information we need for rendering, so we can parallelize without"
dealing with Pandas
i_row = 0; row = images_to_visualize.iloc[0]
Filenames should already have been normalized to either '/' or '\'
"render_image_no_gt(file_info,detection_categories_to_results_name,"
"detection_categories,classification_categories)"
Map all the rendering results in the list rendering_results into the
dictionary images_html
Prepare the individual html image files
Write index.html
"We can't just sum these, because image_counts includes images in both their"
detection and classification classes
total_images = sum(image_counts.values())
Don't print classification classes here; we'll do that later with a slightly
different structure
Add links to all available classes
os.startfile(output_html_file)
...if we do/don't have ground truth
...process_batch_results
%% Interactive driver(s)
%%
options.unlabeled_classes = ['human']
os.start(ppresults.output_html_file)
%% Command-line driver
""
load_api_results.py
""
Loads the output of the batch processing API (json) into a pandas dataframe.
""
Also functions to group entries by seq_id.
""
Includes the deprecated functions that worked with the old CSV API output format.
""
%% Constants and imports
%% Functions for grouping by sequence_id
example
input 'file': 'SER/S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG'
output 'id': 'S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG'
%% Functions for loading the result as a Pandas DataFrame
Validate that this is really a detector output file
Fields in the API output json other than 'images'
Normalize paths to simplify comparisons later
"image['file'] = image['file'].replace('\\','/')"
Replace some path tokens to match local paths to original blob structure
"If this is a newer file that doesn't include maximum detection confidence values,"
"add them, because our unofficial internal dataframe format includes this."
Pack the json output into a Pandas DataFrame
Sanity-check that this is really a detector output file
Normalize paths to simplify comparisons later
De-serialize detections
Optionally replace some path tokens to match local paths to the original blob structure
string_to_replace = list(options.detector_output_filename_replacements.keys())[0]
"TODO: hit some silly issues with vectorized str() and escaped characters, vectorize"
this later.
""
"detection_results['image_path'].str.replace(string_to_replace,replacement_string)"
iRow = 0
%% Imports
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils)
%% Constants and support classes
We will confirm that this matches what we load from each file
Process-based parallelization isn't supported yet
%% Main function
"Warn the user if some ""detections"" might not get rendered"
#%% Validate inputs
#%% Load both result sets
assert results_a['detection_categories'] == default_detection_categories
assert results_b['detection_categories'] == default_detection_categories
#%% Make sure they represent the same set of images
#%% Find differences
"Each of these maps a filename to a two-element list (the image in set A, the image in set B)"
""
"Right now, we only handle a very simple notion of class transition, where the detection"
of maximum confidence changes class *and* both images have an above-threshold detection.
fn = filenames_a[0]
We shouldn't have gotten this far if error_on_non_matching_lists is set
det = im_a['detections'][0]
...for each filename
#%% Sample and plot differences
"Render two sets of results (i.e., a comparison) for a single"
image.
...def render_image_pair()
fn = image_filenames[0]
...def render_detection_comparisons()
"For each category, generate comparison images and the"
comparison HTML page.
""
category = 'common_detections'
Choose detection pairs we're going to render for this category
...for each category
#%% Write the top-level HTML file content
...def compare_batch_results()
%% Interactive driver
%% KRU
%% Command-line driver
# TODO
""
"Merge high-confidence detections from one results file into another file,"
when the target file does not detect anything on an image.
""
Does not currently attempt to merge every detection based on whether individual
detections are missing; only merges detections into images that would otherwise
be considered blank.
""
"If you want to literally merge two .json files, see combine_api_outputs.py."
""
%% Constants and imports
%% Structs
Don't bother merging into target images where the max detection is already
higher than this threshold
"If you want to merge only certain categories, specify one"
(but not both) of these.
%% Main function
im = output_data['images'][0]
"Determine whether we should be processing all categories, or just a subset"
of categories.
i_source_file = 0; source_file = source_files[i_source_file]
source_im = source_data['images'][0]
detection_category = list(detection_categories)[0]
"This is already a detection, no need to proceed looking for detections to"
transfer
Boxes are x/y/w/h
source_sizes = [det['bbox'][2]*det['bbox'][3] for det in source_detections_this_category_raw]
Only look at boxes below the size threshold
...for each detection category
"print('Adding {} detections to image {}'.format(len(detections_to_transfer),image_filename))"
Update the max_detection_conf field (if present)
...for each image
...for each source file
%% Test driver
%%
%% Command-line driver (TODO)
""
separate_detections_into_folders.py
""
## Overview
""
"Given a .json file with batch processing results, separate the files in that"
"set of results into folders that contain animals/people/vehicles/nothing,"
according to per-class thresholds.
""
"Image files are copied, not moved."
""
""
## Output structure
""
Preserves relative paths within each of those folders; cannot be used with .json
files that have absolute paths in them.
""
"For example, if your .json file has these images:"
""
a/b/c/1.jpg
a/b/d/2.jpg
a/b/e/3.jpg
a/b/f/4.jpg
a/x/y/5.jpg
""
And let's say:
""
"* The results say that the first three images are empty/person/vehicle, respectively"
"* The fourth image is above threshold for ""animal"" and ""person"""
* The fifth image contains an animal
""
* You specify an output base folder of c:\out
""
You will get the following files:
""
c:\out\empty\a\b\c\1.jpg
c:\out\people\a\b\d\2.jpg
c:\out\vehicles\a\b\e\3.jpg
c:\out\animal_person\a\b\f\4.jpg
c:\out\animals\a\x\y\5.jpg
""
## Rendering bounding boxes
""
"By default, images are just copied to the target output folder.  If you specify --render_boxes,"
bounding boxes will be rendered on the output images.  Because this is no longer strictly
"a copy operation, this may result in the loss of metadata.  More accurately, this *may*"
result in the loss of some EXIF metadata; this *will* result in the loss of IPTC/XMP metadata.
""
Rendering boxes also makes this script a lot slower.
""
## Classification-based separation
""
"If you have a results file with classification data, you can also specify classes to put"
"in their own folders, within the ""animals"" folder, like this:"
""
"--classification_thresholds ""deer=0.75,cow=0.75"""
""
"So, e.g., you might get:"
""
c:\out\animals\deer\a\x\y\5.jpg
""
"In this scenario, the folders within ""animals"" will be:"
""
"deer, cow, multiple, unclassified"
""
"""multiple"" in this case only means ""deer and cow""; if an image is classified as containing a"
"bird and a bear, that would end up in ""unclassified"", since the folder separation is based only"
on the categories you provide at the command line.
""
"No classification-based separation is done within the animal_person, animal_vehicle, or"
animal_person_vehicle folders.
""
%% Constants and imports
Occasionally we have near-zero confidence detections associated with COCO classes that
didn't quite get squeezed out of the model in training.  As long as they're near zero
"confidence, we just ignore them."
%% Options class
"Dictionary mapping categories (plus combinations of categories, and 'empty') to output folders"
Populated only when using classification results
"Originally specified as a string, converted to a dict mapping name:threshold"
...__init__()
...class SeparateDetectionsIntoFoldersOptions
%% Support functions
Find the maximum confidence for each category
""
det = detections[0]
"For zero-confidence detections, we occasionally have leftover goop"
from COCO classes
assert det['conf'] < invalid_category_epsilon
...for each detection on this image
Count the number of thresholds exceeded
...for each category
If this is above multiple thresholds
"TODO: handle species-based separation in, e.g., the animal_person case"
"Are we making species classification folders, and is this an animal?"
Do we need to put this into a specific species folder?
Find the animal-class detections that are above threshold
Count the number of classification categories that are above threshold for at
least one detection
d = valid_animal_detections[0]
classification = d['classifications'][0]
"Do we have a threshold for this category, and if so, is"
this classification above threshold?
...for each classification
...for each detection
...if we have to deal with classification subfolders
...if we have 0/1/more categories above threshold
...if this is/isn't a failure case
Skip this image if it's empty and we're not processing empty images
"At this point, this image is getting copied; we may or may not also need to"
draw bounding boxes.
Do a simple copy operation if we don't need to render any boxes
Open the source image
"Render bounding boxes for each category separately, beacuse"
we allow different thresholds for each category.
"When we're not using classification folders, remove classification"
information to maintain standard detection colors.
...for each category
Read EXIF metadata
"Write output with EXIF metadata if available, and quality='keep' if this is a JPEG"
"image.  Unfortunately, neither parameter likes ""None"", so we get a slightly"
icky cascade of if's here.
Also see:
""
https://discuss.dizzycoding.com/determining-jpg-quality-in-python-pil/
""
...for more ways to preserve jpeg quality if quality='keep' doesn't do the trick.
...if we don't/do need to render boxes
...def process_detections()
%% Main function
Create output folder if necessary
Load detection results
Map class names to output folders
Create all combinations of categories
category_name = category_names[0]
Do we have a custom threshold for this category?
Create folder mappings for each category
Create the actual folders
"Handle species classification thresholds, if specified"
"E.g. deer=0.75,cow=0.75"
token = tokens[0]
...for each token
...if classification thresholds are still in string format
Validate the classes in the threshold list
...if we need to deal with classification categories
i_image = 14; im = images[i_image]; im
...for each image
...def separate_detections_into_folders
%% Interactive driver
%%
%%
%%
%% Testing various command-line invocations
"With boxes, no classification"
"No boxes, no classification (default)"
"With boxes, with classification"
"No boxes, with classification"
%% Command-line driver
"python api\batch_processing\postprocessing\separate_detections_into_folders.py ""d:\temp\rspb_mini.json"" ""d:\temp\demo_images\rspb_2018_2019_mini"" ""d:\temp\separation_test"" --nthreads 2"
Convert to an options object
"print('{} {}'.format(v,name))"
List of category numbers to use in separation; uses all categories if None
"Can be ""size"", ""width"", or ""height"""
For each image...
""
im = images[0]
d = im['detections'][0]
Are there really any detections here?
Is this a category we're supposed to process?
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing#detector-outputs
...for each detection
...for each image
...def categorize_detections_by_size()
""
add_max_conf.py
""
"The MD output format included a ""max_detection_conf"" field with each image"
up to and including version 1.2; it was removed as of version 1.3 (it's
redundant with the individual detection confidence values).
""
"Just in case someone took a dependency on that field, this script allows you"
to add it back to an existing .json file.
""
%% Imports and constants
%% Main function
%% Driver
#######
""
remove_repeat_detections.py
""
"Used after running find_repeat_detections, then manually filtering the results,"
to create a final filtered output file.
""
"If you want to use this script, we recommend that you read the user's guide:"
""
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms
""
#######
%% Constants and imports
%% Main function
%% Interactive driver
%%
%% Command-line driver
#######
""
repeat_detections_core.py
""
Core utilities shared by find_repeat_detections and remove_repeat_detections.
""
#######
%% Imports and environment
"from ai4eutils; this is assumed to be on the path, as per repo convention"
"""PIL cannot read EXIF metainfo for the images"""
"""Metadata Warning, tag 256 had too many entries: 42, expected 1"""
%% Constants
%% Classes
Relevant for rendering the folder of images for filtering
""
"imageBase can also be a SAS URL, in which case some error-checking is"
disabled.
Don't consider detections with confidence lower than this as suspicious
Don't consider detections with confidence higher than this as suspicious
What's the IOU threshold for considering two boxes the same?
How many occurrences of a single location (as defined by the IOU threshold)
are required before we declare it suspicious?
"Ignore ""suspicious"" detections larger than some size; these are often animals"
taking up the whole image.  This is expressed as a fraction of the image size.
"Ignore ""suspicious"" detections smaller than some size"
Ignore folders with more than this many images in them
A list of classes we don't want to treat as suspicious. Each element is an int.
Load detections from a filter file rather than finding them from the detector output
".json file containing detections, should be called detectionIndex.json in the filtering_* folder"
produced in the first pass
(optional) List of filenames remaining after deletion of identified
repeated detections that are actually animals.  This should be a flat
"text file, one relative filename per line.  See enumerate_images()."
Turn on/off optional outputs
Determines whether bounding-box rendering errors (typically network errors) should
be treated as failures
Box rendering options
State variables
"Replace filename tokens after reading, useful when the directory structure"
has changed relative to the structure the detector saw
How many folders up from the leaf nodes should we be going to aggregate images?
An optional function that takes a string (an image file name) and returns
"a string (the corresponding  folder ID), typically used when multiple folders"
actually correspond to the same camera in a manufacturer-specific way (e.g.
a/b/c/RECONYX100 and a/b/c/RECONYX101 may really be the same camera).
Include/exclude specific folders... only one of these may be
"specified; ""including"" folders includes *only* those folders."
"Optionally show *other* detections (i.e., detections other than the"
one the user is evaluating) in a light gray
"If bRenderOtherDetections is True, what color should we use to render the"
(hopefully pretty subtle) non-target detections?
""
"In theory I'd like these ""other detection"" rectangles to be partially"
"transparent, but this is not straightforward, and the alpha is ignored"
"here.  But maybe if I leave it here and wish hard enough, someday it"
will work.
""
otherDetectionsColors = ['dimgray']
Sort detections within a directory so nearby detections are adjacent
"in the list, for faster review."
""
"Can be None, 'xsort', or 'clustersort'"
""
* None sorts detections chronologically by first occurrence
* 'xsort' sorts detections from left to right
* 'clustersort' clusters detections and sorts by cluster
Only relevant if smartSort == 'clustersort'
"The data table (Pandas DataFrame), as loaded from the input json file via"
load_api_results()
"The other fields in the input json file, loaded via load_api_results()"
The data table after modification
dict mapping folder names to whole rows from the data table
dict mapping filenames to rows in the master table
"An array of length nDirs, where each element is a list of DetectionLocation"
objects for that directory that have been flagged as suspicious
"This is a bit of a hack right now, but for future-proofing, I don't want to call this"
"to retrieve anything other than the highest-confidence detection, and I'm assuming this"
"is already sorted, so assert() that."
It's not clear whether it's better to use instances[0].bbox or self.bbox
"here... they should be very similar, unless iouThreshold is very low."
self.bbox is a better representation of the overal DetectionLocation.
%% Helper functions
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t"
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t"
%% Sort a list of candidate detections to make them visually easier to review
Just sort by the X location of each box
"Prepare a list of points to represent each box,"
that's what we'll use for clustering
Upper-left
"points.append([det.bbox[0],det.bbox[1]])"
Center
"Labels *could* be any unique labels according to the docs, but in practice"
they are unique integers from 0:nClusters
Make sure the labels are unique incrementing integers
Store the label assigned to each cluster
"Now sort the clusters by their x coordinate, and re-assign labels"
so the labels are sortable
"Compute the centroid for debugging, but we're only going to use the x"
"coordinate.  This is the centroid of points used to represent detections,"
which may be box centers or box corners.
old_cluster_label_to_new_cluster_label[old_cluster_label] =\
new_cluster_labels[old_cluster_label]
%% Look for matches (one directory)
List of DetectionLocations
candidateDetections = []
Create a tree to store candidate detections
For each image in this directory
""
iDirectoryRow = 0; row = rows.iloc[iDirectoryRow]
""
"iDirectoryRow is a pandas index, so it may not start from zero;"
"for debugging, we maintain i_iteration as a loop index."
print('Searching row {} of {} (index {}) in dir {}'.\
"format(i_iteration,len(rows),iDirectoryRow,dirName))"
Don't bother checking images with no detections above threshold
"Array of dicts, where each element is"
{
"'category': '1',  # str value, category ID"
"'conf': 0.926,  # confidence of this detections"
""
"# (x_min, y_min) is upper-left, all in relative coordinates"
"'bbox': [x_min, y_min, width_of_box, height_of_box]"
""
}
For each detection in this image
"This is no longer strictly true; I sometimes run RDE in stages, so"
some probabilities have already been made negative
""
assert confidence >= 0.0 and confidence <= 1.0
Optionally exclude some classes from consideration as suspicious
Is this detection too big to be suspicious?
print('Illegal zero-size bounding box on image {}'.format(filename))
These are relative coordinates
print('Ignoring very small detection with area {}'.format(area))
print('Ignoring very large detection with area {}'.format(area))
This will return candidates of all classes
For each detection in our candidate list
Don't match across categories
Is this a match?
"If so, add this example to the list for this detection"
We *don't* break here; we allow this instance to possibly
match multiple candidates.  There isn't an obvious right or
wrong here.
...for each detection on our candidate list
"If we found no matches, add this to the candidate list"
candidateDetections.append(candidate)
pyqtree
...for each detection
...for each row
Get all candidate detections
print('Found {} candidate detections for folder {}'.format(
"len(candidateDetections),dirName))"
"For debugging only, it's convenient to have these sorted"
as if they had never gone into a tree structure.  Typically
this is in practce a sort by filename.
...def find_matches_in_directory(dirName)
"%% Update the detection table based on suspicious results, write .csv output"
"An array of length nDirs, where each element is a list of DetectionLocation"
objects for that directory that have been flagged as suspicious
For each directory
For each suspicious detection group in this directory
For each instance of this suspicious detection
This should match the bbox for the detection event
The bbox for this instance should be almost the same as the bbox
"for this detection group, where ""almost"" is defined by the IOU"
threshold.
if iou < options.iouThreshold:
"print('IOU warning: {},{}'.format(iou,options.iouThreshold))"
Make sure the bounding box matches
"Make the probability negative, if it hasn't been switched by"
another bounding box
...for each instance
...for each detection
...for each directory
Update maximum probabilities
For each row...
No longer strictly true; sometimes I run RDE on RDE output
assert maxPOriginal >= 0
We should only be making detections *less* likely in this process
"If there was a meaningful change, count it"
"Negative probabilities should be the only reason maxP changed, so"
we should have found at least one negative value if we reached
this point.
...if there was a meaningful change to the max probability for this row
...for each row
If we're also writing output...
"...def update_detection_table(RepeatDetectionResults,options)"
%% Main function
#%% Input handling
Validate some options
Load the filtering file
Load the same options we used when finding repeat detections
...except for things that explicitly tell this function not to
find repeat detections.
...if we're loading from an existing filtering file
Check early to avoid problems with the output folder
"Load file to a pandas dataframe.  Also populates 'max_detection_conf', even if it's"
not present in the .json file.
detectionResults[detectionResults['failure'].notna()]
"Before doing any real work, make sure we can *probably* access images"
"This is just a cursory check on the first image, but it heads off most"
"problems related to incorrect mount points, etc.  Better to do this before"
spending 20 minutes finding repeat detections.
#%% Separate files into directories
This will be a map from a directory name to smaller data frames
This is a mapping back into the rows of the original table
iRow = 0; row = detectionResults.iloc[0]
Create a new DataFrame with just this row
rowsByDirectory[dirName] = pd.DataFrame(row)
...for each unique detection
Convert lists of rows to proper DataFrames
#% Look for matches (or load them from file)
"Are we actually looking for matches, or just loading from a file?"
length-nDirs list of lists of DetectionLocation objects
We're actually looking for matches...
"We get slightly nicer progress bar behavior using threads, by passing a pbar"
object and letting it get updated.  We can't serialize this object across
processes.
#%% Find suspicious locations based on match results
For each directory
""
iDir = 51
A list of DetectionLocation objects
A list of DetectionLocation objects
occurrenceList is a list of file/detection pairs
Sort the above-threshold detections for easier review
...for each directory
If we're just loading detections from a file...
"We're skipping detection-finding, but to see which images are actually legit false"
"positives, we may be looking for physical files or loading from a text file."
For each directory
iDir = 0; detections = suspiciousDetections[0]
""
"suspiciousDetections is an array of DetectionLocation objects,"
one per directory.
For each detection that was present before filtering
iDetection = 0; detection = detections[iDetection]
Are we checking the directory to see whether detections were actually false
"positives, or reading from a list?"
Is the image still there?
"If not, remove this from the list of suspicious detections"
...for each detection
...for each directory
...if we are/aren't finding detections (vs. loading from file)
Create filtering directory
iDir = 0; suspiciousDetectionsThisDir = suspiciousDetections[iDir]
suspiciousDetectionsThisDir is a list of DetectionLocation objects
iDetection = 0; detection = suspiciousDetectionsThisDir[0]
Sort instances in descending order by confidence
Choose the highest-confidence index
Should we render (typically in a very light color) detections
*other* than the one we're highlighting here?
Render other detections first (typically in a thin+light box)
Now render the example detection (on top of at least one
of the other detections)
This converts the *first* instance to an API standard detection;
"because we just sorted this list in descending order by confidence,"
this is the highest-confidence detection.
...if we are/aren't rendering other bounding boxes
...for each detection in this folder
...for each folder
Write out the detection index
...if we're writing filtering info
...find_repeat_detections()
#######
""
find_repeat_detections.py
""
"If you want to use this script, we recommend that you read the user's guide:"
""
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms
""
"Really, don't try to run this script without reading the user's guide, you'll think"
it's more magical than it is.
""
"This script looks through a sequence of detections in the API output json file, and finds"
"candidates that might be ""repeated false positives"", i.e. that random branch that the"
detector thinks is an animal/person/vehicle.
""
"Typically after running this script, you would do a manual step to remove"
"true positives, then run remove_repeat_detections to produce a final output file."
""
There's no way that statement was self-explanatory; see the user's guide.
""
#######
%% Constants and imports
%% Interactive driver
%%
%% Command-line driver
Convert to an options object
""
If a request has been sent to AML for batch scoring but the monitoring thread of the API was
"interrupted (uncaught exception or having to re-start the API container), we could manually"
"aggregate results from each shard using this script, assuming all jobs submitted to AML have finished."
""
Need to have set environment variables STORAGE_ACCOUNT_NAME and STORAGE_ACCOUNT_KEY to those of the
"storage account backing the API. Also need to adjust the INTERNAL_CONTAINER, AML_CONTAINER and"
AML_CONFIG fields in api_core/orchestrator_api/api_config.py to match the instance of the API that this
request was submitted to.
""
May need to change the import statement in api_core/orchestrator_api/orchestrator.py
"""from sas_blob_utils import SasBlob"" to"
"""from .sas_blob_utils import SasBlob"" to not confuse with the module in AI4Eutils;"
"and change ""import api_config"" to"
"""from api.batch_processing.api_core.orchestrator_api import api_config"""
Execute this script from the root of the repository. You may need to add the repository to PYTHONPATH.
"list_jobs_submitted cannot be serialized (""can't pickle _thread.RLock objects ""), but"
do not need it for aggregating results
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%% helper classes and functions
TODO log exception when we have more telemetry
TODO check that the expiry date of input_container_sas is at least a month
into the future
"if no permission specified explicitly but has an access policy, assumes okay"
TODO - check based on access policy as well
return current UTC time as a string in the ISO 8601 format (so we can query by
timestamp in the Cosmos DB job status table.
example: '2021-02-08T20:02:05.699689Z'
"image_paths will have length at least 1, otherwise would have ended before this step"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
a job moves from created to running/problem after the Batch Job has been submitted
"job_id should be unique across all instances, and is also the partition key"
TODO do not read the entry first to get the call_params when the Cosmos SDK add a
patching functionality:
https://feedback.azure.com/forums/263030-azure-cosmos-db/suggestions/6693091-be-able-to-do-partial-updates-on-document
need to retain other fields in 'status' to be able to restart monitoring thread
retain existing fields; update as needed
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
sentinel should change if new configurations are available
configs have not changed
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Gunicorn logger handler will get attached if needed in server.py
set for all tasks in the job
cannot execute the scoring script that is in the mounted directory; has to be copied to cwd
not luck giving the commandline arguments via formatted string - set as env vars instead
form shards of images and assign each shard to a Task
for persisting stdout and stderr
persist stdout and stderr (will be removed when node removed)
paths are relative to the Task working directory
can also just upload on failure
first try submitting Tasks
retry submitting Tasks
Change the Job's on_all_tasks_complete option to 'terminateJob' so the Job's status changes automatically
after all submitted tasks are done
This is so that we do not take up the quota for active Jobs in the Batch account.
return type: TaskAddCollectionResult
actually we should probably only re-submit if it's a server_error
docs: # https://docs.microsoft.com/en-us/rest/api/batchservice/odata-filters-in-batch#list-tasks
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%% Flask app
reference: https://trstringer.com/logging-flask-gunicorn-the-manageable-way/
%% Helper classes
%% Flask endpoints
required params
can be an URL to a file not hosted in an Azure blob storage container
"if use_url, then images_requested_json_sas is required"
optional params
check model_version is among the available model versions
check request_name has only allowed characters
optional params for telemetry collection - logged to status table for now as part of call_params
All API instances / node pools share a quota on total number of active Jobs;
we cannot accept new Job submissions if we are at the quota
required fields
request_status is either completed or failed
the create_batch_job thread will stop when it wakes up the next time
"Fix for Zooniverse - deleting any ""-"" characters in the job_id"
"If the status is running, it could be a Job submitted before the last restart of this"
"API instance. If that is the case, we should start to monitor its progress again."
WARNING model_version could be wrong (a newer version number gets written to the output file) around
"the time that  the model is updated, if this request was submitted before the model update"
and the API restart; this should be quite rare
conform to previous schemes
%% undocumented endpoints
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Gunicorn logger handler will get attached if needed in server.py
request_name and request_submission_timestamp are for appending to
output file names
image_paths can be a list of strings (Azure blob names or public URLs)
"or a list of length-2 lists where each is a [image_id, metadata] pair"
Case 1: listing all images in the container
- not possible to have attached metadata if listing images in a blob
list all images to process
+ 1 so if the number of images listed > MAX_NUMBER_IMAGES_ACCEPTED_PER_JOB
we will know and not proceed
Case 2: user supplied a list of images to process; can include metadata
filter down to those conforming to the provided prefix and accepted suffixes (image file types)
prefix is case-sensitive; suffix is not
"Although urlparse(p).path preserves the extension on local paths, it will not work for"
"blob file names that contains ""#"", which will be treated as indication of a query."
"If the URL is generated via Azure Blob Storage, the ""#"" char will be properly encoded"
apply the first_n and sample_n filters
OK if first_n > total number of images
sample by shuffling image paths and take the first sample_n images
"upload the image list to the container, which is also mounted on all nodes"
all sharding and scoring use the uploaded list
now request_status moves from created to running
an extra field to allow the monitoring thread to restart after an API restart: total number of tasks
also record the number of images to process for reporting
start the monitor thread with the same name
"both succeeded and failed tasks are marked ""completed"" on Batch"
"preserving format from before, but SAS URL to 'failed_images' and 'images' are no longer provided"
"failures should be contained in the output entries, indicated by an 'error' field"
"when people download this, the timestamp will have : replaced by _"
check if the result blob has already been written (could be another instance of the API / worker thread)
"and if so, skip aggregating and uploading the results, and just generate the SAS URL, which"
could be needed still if the previous request_status was `problem`.
upload the output JSON to the Job folder
%% Helper functions *copied* from ct_utils.py and visualization/visualization_utils.py
"Determine the factor, which shifts the decimal point of x"
just behind the last significant digit
"Shift decimal point by multiplicatipon with factor, flooring, and"
division by factor
PIL.Image.convert() returns a converted copy of this image
alter orientation as needed according to EXIF tag 0x112 (274) for Orientation
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28
https://www.media.mit.edu/pia/Research/deepview/exif.html
"%% TFDetector class, an unmodified *copy* of the class in detection/tf_detector.py,"
so we do not have to import the packages required by run_detector.py
Number of decimal places to round to for confidence and bbox coordinates
"MegaDetector was trained with batch size of 1, and the resizing function is a part"
of the inference graph
An enumeration of failure reasons
"change from [y1, x1, y2, x2] to [x1, y1, width, height]"
convert numpy floats to Python floats
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size
"np_images = [np.asarray(image, np.uint8) for image in images]"
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)"
performs inference
our batch size is 1; need to loop the batch dim if supporting batch size > 1
%% Scoring script
determine if there is metadata attached to each image_id
information to determine input and output locations
other parameters for the task
test that we can write to output path; also in case there is no image to process
list images to process
"items in this list can be strings or [image_id, metadata]"
model path
"Path to .pb TensorFlow detector model file, relative to the"
models/megadetector_copies folder in mounted container
score the images
""
manage_video_batch.py
""
Notebook-esque script to manage the process of running a local batch of videos
through MD.  Defers most of the heavy lifting to manage_local_batch.py .
""
%% Imports and constants
%% Split videos into frames
"%% List frame files, break into folders"
Find unique (relative) folders
fn = frame_files[0]
%% List videos
%% Check for videos that are missing entirely
list(folder_to_frame_files.keys())[0]
video_filenames[0]
fn = video_filenames[0]
%% Check for videos with very few frames
%% Print the list of videos that are problematic
%% Process images like we would for any other camera trap job
"...typically using manage_local_batch.py, but do this however you like, as long"
as you get a results file at the end.
""
"If you do RDE, remember to use the second folder from the bottom, rather than the"
bottom-most folder.
%% Convert frame results to video results
%% Confirm that the videos in the .json file are what we expect them to be
%% Scrap
%% Test a possibly-broken video
%% List videos in a folder
%% Imports
%% Constants
%% Classes
class variables
"instance variables, in order of when they are typically set"
Leaving this commented out to remind us that we don't want this check here; let
the API fail on these images.  It's a huge hassle to remove non-image
files.
""
for path_or_url in images_list:
if not is_image_file_or_url(path_or_url):
raise ValueError('{} is not an image'.format(path_or_url))
Commented out as a reminder: don't check task status (which is a rest API call)
in __repr__; require the caller to explicitly request status
"status=getattr(self, 'status', None))"
estimate # of failed images from failed shards
Download all three JSON urls to memory
Remove files that were submitted but don't appear to be images
assert all(is_image_file_or_url(s) for s in submitted_images)
Diff submitted and processed images
Confirm that the procesed images are a subset of the submitted images
https://www.geeksforgeeks.org/break-list-chunks-size-n-python/
%% Interactive driver
%%
%%
%%
%% Imports and constants
%% Constants I set per script
## Required
base_output_folder_name = os.path.expanduser('~/postprocessing/' + organization_name_short)
Shared Access Signature (SAS) tokens for the Azure Blob Storage container.
Leading question mark is optional.
""
The read-only token is used for accessing images; the write-enabled token is
used for writing file lists.
## Typically left as default
"Pre-pended to all folder names/prefixes, if they're defined below"
"This is how we break the container up into multiple taskgroups, e.g., for"
separate surveys. The typical case is to do the whole container as a single
taskgroup.
"If your ""folders"" are really logical folders corresponding to multiple folders,"
map them here
"A list of .json files to load images from, instead of enumerating.  Formatted as a"
"dictionary, like folder_prefixes."
This is only necessary if you will be performing postprocessing steps that
"don't yet support SAS URLs, specifically the ""subsetting"" step, or in some"
cases the splitting of files into multiple output directories for
empty/animal/vehicle/people.
""
"For those applications, you will need to mount the container to a local drive."
For this case I recommend using rclone whether you are on Windows or Linux;
rclone is much easier than blobfuse for transient mounting.
""
"But most of the time, you can ignore this."
"Supported model_versions: '4', '3', '4_prelim'"
""
Also available at the /supported_model_versions and /default_model_version
endpoints
""
"Unless you have any specific reason to set this to a non-default value, leave"
"it at the default, which as of 2020.04.28 is MegaDetector 4.1"
""
"additional_task_args = {""model_version"":""4_prelim""}"
""
"file_lists_by_folder will contain a list of local JSON file names,"
each JSON file contains a list of blob names corresponding to an API taskgroup
"%% Derived variables, path setup"
local folders
Turn warnings into errors if more than this many images are missing
%% Support functions
"scheme, netloc, path, query, fragment"
%% Read images from lists or enumerate blobs to files
folder_name = folder_names[0]
"Load file lists for this ""folder"""
""
file_list = input_file_lists[folder][0]
Write to file
A flat list of blob paths for each folder
folder_name = folder_names[0]
"If we don't/do have multiple prefixes to enumerate for this ""folder"""
"If this is intended to be a folder, it needs to end in '/', otherwise"
files that start with the same string will match too
...for each prefix
Write to file
...for each folder
%% Some just-to-be-safe double-checking around enumeration
Make sure each folder has at least one image matched; the opposite is usually a sign of a copy/paste issue
...for each image
...for each prefix
...for each folder
Make sure each image comes from one of our folders; the opposite is usually a sign of a bug up above
...for each prefix
...for each folder
...for each image
%% Divide images into chunks for each folder
The JSON file at folder_chunks[i][j] corresponds to task j of taskgroup i
list_file = file_lists_by_folder[0]
"%% Create taskgroups and tasks, and upload image lists to blob storage"
periods not allowed in task names
%% Generate API calls for each task
clipboard.copy(request_strings[0])
clipboard.copy('\n\n'.join(request_strings))
%% Run the tasks (don't run this cell unless you are absolutely sure!)
I really want to make sure I'm sure...
%% Estimate total time
Around 0.8s/image on 16 GPUs
%% Manually create task groups if we ran the tasks manually
%%
"%% Write task information out to disk, in case we need to resume"
%% Status check
print(task.id)
%% Resume jobs if this notebook closes
%% For multiple tasks (use this only when we're merging with another job)
%% For just the one task
%% Load into separate taskgroups
p = task_cache_paths[0]
%% Typically merge everything into one taskgroup
"%% Look for failed shards or missing images, start new tasks if necessary"
List of lists of paths
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup];
"Make a copy, because we append to taskgroup"
i_task = 0; task = tasks[i_task]
Each taskgroup corresponds to one of our folders
Check that we have (almost) all the images
Now look for failed images
Write it out as a flat list as well (without explanation of failures)
...for each task
...for each task group
%% Pull results
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup]; task = taskgroup[0]
Each taskgroup corresponds to one of our folders
...for each task
...for each task group
%% Combine results from task groups into final output files
Check that we have (almost) all the images
The only reason we should ever have a repeated request is the case where an
"image was missing and we reprocessed it, or where it failed and later succeeded"
"There may be non-image files in the request list, ignore those"
Something has gone bonkers if there are images in the results that
aren't in the request
...for each folder
%% Post-processing (no ground truth)
i_folder = 0; folder_name_raw = folder_names[i_folder]
"Omit some pages from the output, useful when animals are rare"
%% Manual processing follows
""
"Everything after this should be considered mostly manual, and no longer includes"
looping over folders.
""
%% RDE (sample directory collapsing)
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images"
"%% Repeat detection elimination, phase 1"
"Deliberately leaving these imports here, rather than at the top, because this cell is not"
typically executed
To invoke custom collapsing of folders for a particular manufacturer's naming scheme
options.customDirNameFunction = remove_overflow_folders
Exclude people and vehicles from RDE
"options.excludeClasses = [2,3]"
%% Manual RDE step
# DELETE THE VALID DETECTIONS ##
%% Re-filtering
%% Post-processing (post-RDE)
i_folder = 0; folder_name_raw = folder_names[i_folder]
"Omit some pages from the output, useful when animals are rare"
"print('No RDE file available for {}, skipping'.format(folder_name))"
continue
%% Create a new category for large boxes
%% Subsetting
i_folder = 0; folder_name = folders[i_folder]
%% String replacement
%% Folder splitting
i_folder = 0; folder_name_raw = folder_names[i_folder]
api_output_file = folder_name_to_combined_output_file[folder_name]
%% Imports and constants
from ai4eutils
To specify a non-default confidence threshold for including detections in the .json file
Turn warnings into errors if more than this many images are missing
Only relevant when we're using a single GPU
"Specify a target image size when running MD... strongly recommended to leave this at ""None"""
Only relevant when running on CPU
OS-specific script line continuation character
OS-specific script comment character
"Prefer threads on Windows, processes on Linux"
"This is for things like image rendering, not for MegaDetector"
Should we use YOLOv5's val.py instead of run_detector_batch.py?
Directory in which to run val.py.  Only relevant if use_yolo_inference_scripts is True.
Should we remove intermediate files used for running YOLOv5's val.py?
""
Only relevant if use_yolo_inference_scripts is True.
Should we apply YOLOv5's augmentation?  Only allowed when use_yolo_inference_scripts
is True.
%% Constants I set per script
Optional descriptor
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v5.0.0/md_v5b.0.0.pt')
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v4.1.0/md_v4.1.0.pb')
"Number of jobs to split data into, typically equal to the number of available GPUs"
Only used to print out a time estimate
"%% Derived variables, constant validation, path setup"
%% Enumerate files
%% Load files from prior enumeration
%% Divide images into chunks
%% Estimate total time
%% Write file lists
%% Generate commands
"A list of the scripts tied to each GPU, as absolute paths.  We'll write this out at"
the end so each GPU's list of commands can be run at once.  Generally only used when
"running lots of small batches via YOLOv5's val.py, which doesn't support checkpointing."
i_task = 0; task = task_info[i_task]
Generate the script to run MD
Check whether this output file exists
Generate the script to resume from the checkpoint (only supported with MD inference code)
...for each task
Write out a script for each GPU that runs all of the commands associated with
that GPU.  Typically only used when running lots of little scripts in lieu
of checkpointing.
...for each GPU
%% Run the tasks
%%% Run the tasks (commented out)
i_task = 0; task = task_info[i_task]
"This will write absolute paths to the file, we'll fix this later"
...for each chunk
...if False
"%% Load results, look for failed or missing images in each task"
i_task = 0; task = task_info[i_task]
im = task_results['images'][0]
...for each task
%% Merge results files and make images relative
im = combined_results['images'][0]
%% Post-processing (pre-RDE)
options.sample_seed = 0
"Omit some pages from the output, useful when animals are rare"
%% RDE (sample directory collapsing)
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images"
%%
%%
%%
relativePath = image_filenames[0]
"%% Repeat detection elimination, phase 1"
"Deliberately leaving these imports here, rather than at the top, because this"
cell is not typically executed
options.minSuspiciousDetectionSize = 0.05
This will cause a very light gray box to get drawn around all the detections
we're *not* considering as suspicious.
options.lineThickness = 5
options.boxExpansion = 8
To invoke custom collapsing of folders for a particular manufacturer's naming scheme
options.customDirNameFunction = remove_overflow_folders
Exclude people and vehicles from RDE
"options.excludeClasses = [2,3]"
options.maxImagesPerFolder = 50000
options.includeFolders = ['a/b/c']
options.excludeFolder = ['a/b/c']
"Can be None, 'xsort', or 'clustersort'"
import clipboard; clipboard.copy(os.path.dirname(suspiciousDetectionResults.filterFile))
path_utils.open_file(os.path.dirname(suspiciousDetectionResults.filterFile))
%% Manual RDE step
# DELETE THE VALID DETECTIONS ##
%% Re-filtering
%% Post-processing (post-RDE)
options.sample_seed = 0
"Omit some pages from the output, useful when animals are rare"
"%% Run MegaClassifier (actually, write out a script that runs MegaClassifier)"
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
#%% Set up environment
commands.append('cd CameraTraps/classification\n')
commands.append('conda activate cameratraps-classifier\n')
#%% Crop images
fn = input_files[0]
#%% Run classifier
fn = input_files[0]
#%% Remap classifier outputs
fn = input_files[0]
#%% Merge classification and detection outputs
fn = input_files[0]
#%% Write  out classification script
"%% Run a non-MegaClassifier classifier (i.e., a classifier with no output mapping)"
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
#%% Set up environment
#%% Crop images
fn = input_files[0]
#%% Run classifier
fn = input_files[0]
#%% Merge classification and detection outputs
fn = input_files[0]
#%% Write everything out
%% Run the classifier(s) via the .sh script(s) or batch file(s) we just wrote
...
%% Within-image classification smoothing
""
Only count detections with a classification confidence threshold above
"*classification_confidence_threshold*, which in practice means we're only"
looking at one category per detection.
""
If an image has at least *min_detections_above_threshold* such detections
"in the most common category, and no more than *max_detections_secondary_class*"
"in the second-most-common category, flip all detections to the most common"
category.
""
"Optionally treat some classes as particularly unreliable, typically used to overwrite an"
"""other"" class."
""
This cell also removes everything but the non-dominant classification for each detection.
""
How many detections do we need above the classification threshold to determine a dominant category
for an image?
"Even if we have a dominant class, if a non-dominant class has at least this many classifications"
"in an image, leave them alone."
"If the dominant class has at least this many classifications, overwrite ""other"" classifications"
What confidence threshold should we use for assessing the dominant category in an image?
Which classifications should we even bother over-writing?
Detection confidence threshold for things we count when determining a dominant class
Which detections should we even bother over-writing?
"Before we do anything else, get rid of everything but the top classification"
for each detection.
...for each detection in this image
...for each image
im = d['images'][0]
...for each classification
...if there are classifications for this detection
...for each detection
"Handle a quirky special case: if the most common category is ""other"" and"
"it's ""tied"" with the second-most-common category, swap them"
secondary_count = category_to_count[keys[1]]
The 'secondary count' is the most common non-other class
If we have at least *min_detections_to_overwrite_other* in a category that isn't
"""other"", change all ""other"" classifications to that category"
...for each classification
...if there are classifications for this detection
...for each detection
"...if we should overwrite all ""other"" classifications"
"At this point, we know we have a dominant category; change all other above-threshold"
"classifications to that category.  That category may have been ""other"", in which"
case we may have already made the relevant changes.
det = detections[0]
...for each classification
...if there are classifications for this detection
...for each detection
...for each image
...for each file we want to smooth
"%% Post-processing (post-classification, post-within-image-smoothing)"
classification_detection_file = classification_detection_files[1]
%% Read EXIF data from all images
%% Prepare COCO-camera-traps-compatible image objects for EXIF results
import dateutil
"This is a standard format for EXIF datetime, and dateutil.parser"
doesn't handle it correctly.
return dateutil.parser.parse(s)
exif_result = exif_results[0]
Currently we assume that each leaf-node folder is a location
"We collected this image this century, but not today, make sure the parsed datetime"
jives with that.
""
The latter check is to make sure we don't repeat a particular pathological approach
"to datetime parsing, where dateutil parses time correctly, but swaps in the current"
date when it's not sure where the date is.
...for each exif image result
%% Assemble into sequences
Make a list of images appearing at each location
im = image_info[0]
%% Load classification results
Map each filename to classification results for that file
%% Smooth classification results over sequences (prep)
These are the only classes to which we're going to switch other classifications
Only switch classifications to the dominant class if we see the dominant class at least
this many times
"If we see more than this many of a class that are above threshold, don't switch those"
classifications to the dominant class.
"If the ratio between a dominant class and a secondary class count is greater than this,"
"regardless of the secondary class count, switch those classificaitons (i.e., ignore"
max_secondary_class_classifications_above_threshold_for_class_smoothing).
""
"This may be different for different dominant classes, e.g. if we see lots of cows, they really"
"tend to be cows.  Less so for canids, so we set a higher ""override ratio"" for canids."
"If there are at least this many classifications for the dominant class in a sequence,"
"regardless of what that class is, convert all 'other' classifications (regardless of"
confidence) to that class.
"If there are at least this many classifications for the dominant class in a sequence,"
"regardless of what that class is, classify all previously-unclassified detections"
as that class.
Only count classifications above this confidence level when determining the dominant
"class, and when deciding whether to switch other classifications."
Confidence values to use when we change a detection's classification (the
original confidence value is irrelevant at that point)
%% Smooth classification results over sequences (supporting functions)
im = images_this_sequence[0]
det = results_this_image['detections'][0]
Only process animal detections
Only process detections with classification information
"We only care about top-1 classifications, remove everything else"
Make sure the list of classifications is already sorted by confidence
...and just keep the first one
"Confidence values should be sorted within a detection; verify this, and ignore"
...for each detection in this image
...for each image in this sequence
...top_classifications_for_sequence()
Count above-threshold classifications in this sequence
Sort the dictionary in descending order by count
"Handle a quirky special case: if the most common category is ""other"" and"
"it's ""tied"" with the second-most-common category, swap them."
...def count_above_threshold_classifications()
%% Smooth classifications at the sequence level (main loop)
Break if this token is contained in a filename (set to None for normal operation)
i_sequence = 0; seq_id = all_sequences[i_sequence]
Count top-1 classifications in this sequence (regardless of confidence)
Handy debugging code for looking at the numbers for a particular sequence
Count above-threshold classifications for each category
"If our dominant category ID isn't something we want to smooth to, don't mess around with this sequence"
"# Smooth ""other"" classifications ##"
"By not re-computing ""max_count"" here, we are making a decision that the count used"
"to decide whether a class should overwrite another class does not include any ""other"""
classifications we changed to be the dominant class.  If we wanted to include those...
""
sorted_category_to_count = count_above_threshold_classifications(classifications_this_sequence)
max_count = get_first_value_from_sorted_dictionary(sorted_category_to_count)
assert dominant_category_id == get_first_key_from_sorted_dictionary(sorted_category_to_count)
# Smooth non-dominant classes ##
Don't flip classes to the dominant class if they have a large number of classifications
"Don't smooth over this class if there are a bunch of them, and the ratio"
if primary to secondary class count isn't too large
Default ratio
Does this dominant class have a custom ratio?
# Smooth unclassified detections ##
...for each sequence
%% Write smoothed classification results
"%% Post-processing (post-classification, post-within-image-and-within-sequence-smoothing)"
%% Zip .json files
%% 99.9% of jobs end here
Everything after this is run ad hoc and/or requires some manual editing.
%% Compare results files for different model versions (or before/after RDE)
Choose all pairwise combinations of the files in [filenames]
%% Merge in high-confidence detections from another results file
%% Create a new category for large boxes
"This is a size threshold, not a confidence threshold"
size_options.categories_to_separate = [3]
%% Preview large boxes
%% .json splitting
options.query = None
options.replacement = None
Reminder: 'n_from_bottom' with a parameter of zero is the same as 'bottom'
%% Custom splitting/subsetting
i_folder = 0; folder_name = folders[i_folder]
"This doesn't do anything in this case, since we're not splitting folders"
options.make_folder_relative = True
%% String replacement
%% Splitting images into folders
%% Generate commands for a subset of tasks
i_task = 8
...for each task
%% End notebook: turn this script into a notebook (how meta!)
Exclude everything before the first cell
Remove the first [first_non_empty_lines] from the list
Add the last cell
""
xmp_integration.py
""
"Tools for loading MegaDetector batch API results into XMP metadata, specifically"
for consumption in digiKam:
""
https://cran.r-project.org/web/packages/camtrapR/vignettes/camtrapr2.html
""
%% Imports and constants
%% Class definitions
Folder where images are stored
.json file containing MegaDetector output
"String to remove from all path names, typically representing a"
prefix that was added during MegaDetector processing
Optionally *rename* (not copy) all images that have no detections
above [rename_conf] for the categories in rename_cats from x.jpg to
x.check.jpg
"Comma-deleted list of category names (or ""all"") to apply the rename_conf"
behavior to.
"Minimum detection threshold (applies to all classes, defaults to None,"
i.e. 0.0
%% Functions
Relative image path
Absolute image path
List of categories to write to XMP metadata
Categories with above-threshold detections present for
this image
Maximum confidence for each category
Have we already added this to the list of categories to
write out to this image?
If we're supposed to compare to a threshold...
Else we treat *any* detection as valid...
Keep track of the highest-confidence detection for this class
If we're doing the rename/.check behavior...
Legacy code to rename files where XMP writing failed
%% Interactive/test driver
%%
%% Command-line driver
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Cosmos DB `batch-api-jobs` table for job status
"aggregate the number of images, country and organization names info from each job"
submitted during yesterday (UTC time)
create the card
""
api_frontend.py
""
"Defines the Flask app, which takes requests (one or more images) from"
"remote callers and pushes the images onto the shared Redis queue, to be processed"
by the main service in api_backend.py .
""
%% Imports
%% Initialization
%% Support functions
Make a dict that the request_processing_function can return to the endpoint
function to notify it of an error
Verify that the content uploaded is not too big
""
request.content_length is the length of the total payload
Verify that the number of images is acceptable
...def check_posted_data(request)
%% Main loop
Check whether the request_processing_function had an error
Write images to temporary files
""
TODO: read from memory rather than using intermediate files
"Submit the image(s) for processing by api_backend.py, who is waiting on this queue"
TODO: convert to a blocking read and eliminate the sleep() statement in this loop
"image = Image.open(os.path.join(temp_direc, image_name))"
...if we do/don't have a request available on the queue
...while(True)
...def detect_sync()
%% Command-line driver
use --non-docker if you are testing without Docker
""
python api_frontend.py --non-docker
# Camera trap real-time API configuration
"Full path to the temporary folder for image storage, only meaningful"
within the Docker container
Upper limit on total content length (all images and parameters)
Minimum confidence threshold for detections
Minimum confidence threshold for showing a bounding box on the output image
Use this when testing without Docker
""
api_backend.py
""
"Defines the model execution service, which pulls requests (one or more images)"
"from the shared Redis queue, and runs them through the TF model."
""
%% Imports
%% Initialization
%% Main loop
TODO: convert to a blocking read and eliminate the sleep() statement in this loop
Filter the detections by the confidence threshold
""
"Each result is [ymin, xmin, ymax, xmax, confidence, category]"
""
"Coordinates are relative, with the origin in the upper-left"
...if serialized_entry
...while(True)
...def detect_process()
%% Command-line driver
use --non-docker if you are testing without Docker
""
python api_frontend.py --non-docker
run detections on a test image to load the model
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Importing necessary basic libraries and modules
%%
PyTorch imports for tensor operations
Setting the active CUDA device. This ensures that the computations take place on the specified GPU.
%%
"Importing the models, transformations, and utility functions from PytorchWildlife"
%%
Setting constants for device and video paths TODO: add argparse
%%
Initializing the model for image detection
%%
Initializing the model for image classification
%%
Defining transformations for detection and classification
%%
Initializing a box annotator for visualizing detections
Processing the video and saving the result with annotated detections and classifications
%%
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Importing basic libraries
%%
"Importing the models, dataset, transformations, and utility functions from PytorchWildlife"
%%
Setting the device to use for computations ('cuda' indicates GPU)
Initializing a supervision box annotator for visualizing detections
Initializing the detection and classification models
Defining transformations for detection and classification
%% Defining functions for different detection scenarios
Only run classifier when detection class is animal
%% Building Gradio UI
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%%
Importing necessary basic libraries and modules
%%
PyTorch imports
Setting the active CUDA device. Change the number according to your GPU setup
%%
"Importing the model, dataset, transformations and utility functions from PytorchWildlife"
%%
"Setting the device to use for computations. Change to ""cpu"" if no GPU is available"
%%
Initializing the MegaDetectorV5 model for image detection
%% Single image detection
Specifying the path to the target image TODO: Allow argparsing
Opening and converting the image to RGB format
Initializing the Yolo-specific transform for the image
Performing the detection on the single image
Saving the detection results
%% Batch detection
Specifying the folder path containing multiple images for batch detection
Creating a dataset of images with the specified transform
Creating a DataLoader for batching and parallel processing of the images
Performing batch detection on the images
%% Output to annotated images
Saving the batch detection results as annotated images
%% Output to cropped images
Saving the detected objects as cropped images
%% Output to JSON results
Saving the detection results in JSON format
"Determine the factor, which shifts the decimal point of x"
just behind the last significant digit
"Shift decimal point by multiplicatipon with factor, flooring, and"
division by factor
_ = pretty_print_object(obj)
Sloppy that I'm making a module-wide change here...
Determine the coordinates of the intersection rectangle
The intersection of two axis-aligned bounding boxes is always an
axis-aligned bounding box
Compute the area of both AABBs
Compute the intersection over union by taking the intersection
area and dividing it by the sum of prediction + ground-truth
areas - the intersection area.
%% Constants and imports
%% Classes
Collect all names
Make names unique and sort
cast location to string as the entries in locations are strings
Convert classnames to lowercase to simplify comparisons later
Normalize paths to simplify comparisons later
"Make custom replacements in filenames, typically used to"
accommodate changes in root paths after DB construction
## Build useful mappings to facilitate working with the DB
Category ID <--> name
Image filename --> ID
Image ID --> image object
Image ID --> annotations
"Each image can potentially multiple annotations, hence using lists"
...__init__
...class IndexedJsonDb
""
cct_json_to_filename_json.py
""
"Given a .json file in COCO Camera Traps format, outputs a .json-formatted list of"
relative file names present in the CCT file.
""
%% Constants and environment
%% Main function
"json.dump(s,open(outputFilename,'w'))"
%% Command-line driver
%% Interactive driver
%%
""
cct_to_csv.py
""
"""Converts"" a COCO Camera Traps .json file to .csv, in quotes because"
"all kinds of assumptions are made here, and if you have a particular .csv"
"format in mind, YMMV.  Most notably, does not include any bounding box information"
or any non-standard fields that may be present in the .json file.  Does not
propagate information about sequence-level vs. image-level annotations.
""
"Does not assume access to the images, therefore does not open .jpg files to find"
"datetime information if it's not in the metadata, just writes datetime as 'unknown'."
""
%% Imports
%% Main function
#%% Read input
#%% Build internal mappings
annotation = annotations[0]
#%% Write output file
im = images[0]
Write out one line per class:
...for each class name
...for each image
...with open(output_file)
...def cct_to_csv
%% Interactive driver
%%
%% Command-line driver
""
remove_exif.py
""
"Removes all EXIF/IPTC/XMP metadata from a folder of images, without making backup copies, using pyexiv2."
""
%% Imports and constants
%% List files
%% Remove EXIF data (support)
PYEXIV2 IS NOT THREAD SAFE; DO NOT CALL THIS IN PARALLEL FROM A SINGLE PROCESS
data = img.read_exif(); print(data)
%% Debug
%%
%%
%% Remove EXIF data (execution)
fn = image_files[0]
"joblib.Parallel defaults to a process-based backend, but let's be sure"
"results = Parallel(n_jobs=n_exif_threads,verbose=2,prefer='processes')(delayed(remove_exif)(fn) for fn in image_files[0:10])"
""
read_exif.py
""
"Given a folder of images, read relevant EXIF fields from all images, and write them to"
"a .json file.  Depends on having exiftool available, since every pure-Python approach"
we've tried fails on at least some fields.
""
Does not currently support command-line operation.
""
%% Imports and constants
from functools import partial
From ai4eutils
exiftool_command_name = r'c:\exiftool-12.13\exiftool(-k).exe'
%% Multiprocessing init
'i' means integer
%% Functions
image_files will contain the *relative* paths to all image files in the input folder
"file_path = os.path.join(input_base,image_files[0])"
"-G means ""Print group name for each tag"", e.g. print:"
""
[File]          Bits Per Sample                 : 8
""
...instead of:
""
Bits Per Sample                 : 8
A list of three-element lists (type/tag/value)
line_raw = exif_lines[0]
A typical line:
""
[ExifTool]      ExifTool Version Number         : 12.13
"Split on the first occurrence of "":"""
...for each output line
...process_exif()
results = Parallel(n_jobs=n_threads)(delayed(add_exif_data)(im) for im in images[0:10])
%% Interactive driver
%%
%% Command-line driver
%% Create image objects
%% Write results
%% Scrap
%%
%%
""
"Given a json-formatted list of image filenames, retrieve the width and height of every image."
""
%% Constants and imports
%% Processing functions
Is this image on disk?
"all_results = list(tqdm(pool.imap(process_image, filenames), total=len(filenames)))"
%% Interactive driver
%%
List images in a test folder
%%
"process_list_file(image_list_file,image_size_file,image_prefix=base_dir)"
%% Command-line driver
"python sanity_check_json_db.py ""e:\wildlife_data\wellington_data\wellington_camera_traps.json"" --baseDir ""e:\wildlife_data\wellington_data\images"" --bFindUnusedImages --bCheckImageSizes"
"python sanity_check_json_db.py ""D:/wildlife_data/mcgill_test/mcgill_test.json"" --baseDir ""D:/wildlife_data/mcgill_test"" --bFindUnusedImages --bCheckImageSizes"
"Here the '-u' prevents buffering, which makes tee happier"
""
python -u sanity_check_json_db.py '/datadrive1/nacti_metadata.json' --baseDir '/datadrive1/nactiUnzip/' --bFindUnusedImages --bCheckImageSizes | tee ~/nactiTest.out
""
Partially-formed stub to get from MegaDetector output files to COCO Camera Traps data.
""
"Was actually written to convert *many* MD .json files to a single CCT file, hence"
the loop over .json file.
""
"THIS CODE HAS NEVER BEEN RUN, it was added as a demonstration of how to do this.  YMMV."
""
%% Constants and imports
"Images sizes are required to convert between absolute and relative coordinates,"
so we need to read the images.
Only required if you want to write a database preview
%% Create CCT dictionaries
image_ids_to_images = {}
Force the empty category to be ID 0
Load .json annotations for this data set
i_entry = 0; entry = data['images'][i_entry]
""
"PERF: Not exactly trivially parallelizable, but about 100% of the"
time here is spent reading image sizes (which we need to do to get from
"absolute to relative coordinates), so worth parallelizing."
Generate a unique ID from the path
detection = detections[0]
Have we seen this category before?
Create an annotation
"MegaDetector: [x,y,width,eight] (normalized, origin upper-left)"
"CCT: [x,y,width,height] (absolute, origin upper-left)"
...for each detection
...for each image
Remove non-reviewed images and associated annotations
%% Create info struct
%% Write .json output
%% Clean start
## Everything after this should work from a clean start ###
%% Validate output
%% Preview animal labels
%% Preview empty labels
"viz_options.classes_to_exclude = ['empty','human']"
""
cct_to_wi.py
""
Converts COCO Camera Traps .json files to the Wildlife Insights
batch upload format
""
Also see:
""
https://github.com/ConservationInternational/Wildlife-Insights----Data-Migration
""
https://data.naturalsciences.org/wildlife-insights/taxonomy/search
""
%% Imports
%% Paths
A COCO camera traps file with information about this dataset
A .json dictionary mapping common names in this dataset to dictionaries with the
"WI taxonomy fields: common_name, wi_taxon_id, class, orer, family, genus, species"
%% Constants
%% Project information
%% Read templates
%% Compare dictionary to template lists
Write the header
Write values
%% Project file
%% Camera file
%% Deployment file
%% Images file
Read .json file with image information
Read taxonomy dictionary
Populate output information
df = pd.DataFrame(columns = images_fields)
annotation = annotations[0]
im = input_data['images'][0]
"We don't have counts, but we can differentiate between zero and 1"
This is the label mapping used for our incoming iMerit annotations
"Only used to parse the incoming annotations. In our database, the string name is used to avoid confusion"
MegaDetector outputs
""
add_bounding_boxes_to_megadb.py
""
"Given COCO-formatted JSONs containing manually labeled bounding box annotations, add them to"
"MegaDB sequence entries, which can then be ingested into MegaDB."
the category map that comes in the COCO JSONs for iMerit batch 12 - to check that each
JSON
"dataset name : (seq_id, frame_num) : [bbox, bbox]"
where bbox is a dict with str 'category' and list 'bbox'
iterate over image_id_to_image rather than image_id_to_annotations so we include
the confirmed empty images
The file_name field in the incoming json looks like
alka_squirrels.seq2020_05_07_25C.frame119221.jpg
there seems to be a bug in the annotations where sometimes there's a
non-empty label along with a label of category_id 5
ignore the empty label (they seem to be actually non-empty)
""
add_bounding_boxes_to_json.py
""
This script takes a image database in the COCO Camera Traps format and merges in a set of bounding
box annotations in the format that iMerit uses (a .json where actually only each row is a valid json).
""
"If you need to update an existing bbox database, please get all the original annotation files and"
re-generate from scratch
""
%% Imports
%% Configurations and paths
images database
output bboxes database
annotation files (pseudo json) obtained from our annotation vendor that contain annotations for this dataset
None or a string or tuple of strings that is the prefix to all file_name of interest / in this dataset in the annotation files
functions for mapping the image_id in the annotation files (pseudo jsons) to the image_id used in the image DB
our img_id doesn't contain frame info
"batch3 - ""file_name"":""ASG0000019_0_S1_B06_R1_PICT0007.JPG"""
"batch5 and 7 - ""file_name"":""datasetsnapshotserengeti.seqASG000002m-frame0.imgS1_B06_R1_PICT0056.JPG"""
sometimes - 'datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG'
"id in DB (old_token): 'S6/J01/J01_R1/S6_J01_R1_IMAG0001', 'S1/B05/B05_R1/S1_B05_R1_PICT0036'"
specify which one to use for your dataset here
%% Load the image database and fill in DB info for the output bbox database
load the images database
%% Find the height and width of images from the annotation files
""
...if they are not available in the images DB
each row in this pseudo-json is a COCO formatted entry for an image sequence
%% Other functions required by specific datasets
the IDFG image database does not include images from unlabeled folders that were annotated with bounding boxes
%% Create the bbox database from all annotation files pertaining to this dataset
"for the incoming annotations, look up by category name (common) and convert to the numerical id used in our databases"
"for each annotation pseudo-json, check that the image it refers to exists in the original database"
each row in this pseudo-json is a COCO formatted entry for an image sequence
check that entry is for this dataset
category map for this entry in the annotation file - usually the same across all entries but just in case
rspb_add_image_entry(img_id)
use the image length and width in the image DB
"[top left x, top left y, width, height] in relative coordinates"
"add all images that have been sent to annotation, some of which may be empty of bounding boxes"
rspb_add_image_entry(db_image_id)
%% Common queries
This query is used when preparing tfrecords for object detector training.
We do not want to get the whole seq obj where at least one image has bbox because
some images in that sequence will not be bbox labeled so will be confusing.
Include images with bbox length 0 - these are confirmed empty by bbox annotators.
"If frame_num is not available, it will not be a field in the result iterable."
"Note that the seq_id is the Cosmos DB assigned ID for that sequence, not the"
"seq_id field, which may contain ""/"" characters."
"For public datasets to be converted to the CCT format, we get the whole seq object because"
sequence level attributes need to be included too. megadb/converters/megadb_to_cct.py handles
the case of bbox-only JSONs with the flag exclude_species_class
Getting all sequences in a dataset - for updating or deleting entries which need the id field
%% Parameters
Use None if querying across all partitions
"The `sequences` table has the `dataset` as the partition key, so if only querying"
"entries from one dataset, set the dataset name here."
"e.g. {'name': '@top_n', 'value': 100} - see query_and_upsert_examples/query_for_data.ipynb"
Use False if do not want all results stored in a single JSON.
%% Script
execute the query
loop through and save the results
MODIFY HERE depending on the query
wiitigers Unicode issue - no good mapping from DB file names to file names in blob URL
build filename
if need to re-download a dataset's images in case of corruption
entries_to_download = {
"filename: entry for filename, entry in entries_to_download.items()"
if entry['dataset'] == DATASET
}
input validation
"existing files, with paths relative to <store_dir>"
parse JSON or TXT file
"create a new storage container client for this dataset,"
and cache it
schema already checks that the min possible value of frame_num is 0
"if there are more than one image item, each needs a frame_num"
checks across all sequence items
per sequence item checks
load the schema
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory
load the schema
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory
checks across all datasets items
check for expiry date of the SAS keys
update the sequences
"`id` is from the database, as well as all attributes starting with _"
"if valuable sequence information is available, add them to the image"
required fields for an image object
add seq-level class labels for this image
add other sequence-level properties to each image too
add other image-level properties
... for im in seq['images']
... for seq in mega_db
consolidate categories
some property names have changed in the new schema
a dummy sequence ID will be generated if the image entry does not have a seq_id field
seq_id only needs to be unique within this dataset; MegaDB does not rely on it as the _id field
"""annotations"" fields are opened and have its sub-field surfaced one level up"
set the `dataset` property on each sequence to the provided dataset_name
check that the location field is the same for all images in a sequence
check which fields in a CCT image entry are sequence-level
image-level properties that really should be sequence-level
need to add (misidentified) seq properties not present for each image in a sequence to img_level_properties
"(some properties act like flags - all have the same value, but not present on each img)"
add the sequence-level properties to the sequence objects
not every sequence have to have all the seq_level_properties
get the value of this sequence-level property from the first image entry
check which fields are really dataset-level and should be included in the dataset table instead.
delete sequence-level properties that should be dataset-level
make all `class` fields lower-case; cast `seq_id` to type string in case they're integers
turn all float NaN values into None so it gets converted to null when serialized
this was an issue in the Snapshot Safari datasets
%% validation
"at first a dict of image_id: image_obj with annotations embedded, then it becomes"
an array of image objects
%% integrate the image DB
takes in image entries and species and other annotations in the image DB
convert the species category to explicit string name
there may be other fields in the annotation object
these fields should already be gotten from the image object
%% integrate the bbox DB
add any images that are not in the image DB
also add any fields in the image object that are not present already
add bbox to the annotations field
for any newly added images
"'bbox_abs': bbox_anno['bbox'],"
not keeping height and width
""
jb_csv_to_json.py
""
Convert a particular .csv file to CCT format.  Images were not available at
"the time I wrote this script, so this is much shorter than other scripts"
in this folder.
""
%% Constants and environment
%% Read source data
%% Confirm filename uniqueness (this data set has one label per image)
"Build up a map from filenames to a list of rows, checking image existence as we go"
%% Create CCT dictionaries
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
imageName = imageFilenames[0]
"As per above, this is convenient and appears to be true; asserting to be safe"
"Filenames look like ""290716114012001a1116.jpg"""
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
%% Create info struct
%% Write output
%% Sanity-check
%% Imports and constants
%% Load data
"with open(image_json,'r') as f:"
data = json.load(f)
%% Sanity-check data
%% Label previews
%% Collect images to annotate
%% Sort by sequence and frame
%% Copy to a folder by GUID
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension
im = images_to_annotate[0]
...for each image
%% Write out the annotation list
""
"Import a Snapshot Safari project (one project, one season)"
""
Before running this script:
""
"* Mount the blob container where the images live, or copy the"
images to local storage
""
What this script does:
""
* Creates a .json file
* Creates zip archives of the season without humans.
* Copies animals and humans to separate folders
""
After running this script:
""
* Create or update LILA page
* Push zipfile and unzipped images to LILA
* Push unzipped humans to wildlifeblobssc
* Delete images from UMN uplaod storage
""
"Snapshot Serengeti is handled specially, because we're dealing with bounding"
boxes too.  See snapshot_serengeti_lila.py.
""
%% Imports
From ai4eutils
From CameraTraps
%% Constants
project_name = 'XXX'; season_name = 'S1'; project_friendly_name = 'Snapshot Unknown'
project_name = 'KRU'; season_name = 'S1'; project_friendly_name = 'Snapshot Kruger'
project_name = 'CDB'; season_name = 'S1'; project_friendly_name = 'Snapshot Camdeboo'
project_name = 'MTZ'; season_name = 'S1'; project_friendly_name = 'Snapshot Mountain Zebra'
project_name = 'ENO'; season_name = 'S1'; project_friendly_name = 'Snapshot Enonkishu'
project_name = 'KAR'; season_name = 'S1'; project_friendly_name = 'Snapshot Karoo'
project_name = 'KGA'; season_name = 'S1'; project_friendly_name = 'Snapshot Kgalagadi'
%% Folder/file creation
E.g. KRU_S1
E.g. Z:\KRU
E.g. Z:\KRU\KRU_S1
Contains annotations for each capture event (sequence)
Maps image IDs to filenames; each line looks like:
""
"KRU_S1#1#1#2,3,KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0004.JPG"
"Total number of each answer to each question, e.g. total number of times each species was identified"
""
Not used here
Create output folders
Images will be placed in a season-specific folder inside this (the source data includes
this in path names)
%% Load metadata files
%% Convert to dictionaries (prep)
%% Convert to dictionaries (loops)
"TODO: iterrows() is a terrible way to do this, but this is one of those days"
"where I want to get this done, not get better at Python."
irow = 0; row = image_table.iloc[0]
"Loaded as an int64, converting to int here"
...for each row in the image table
Make sure image IDs are what we think they are
...for each row in the annotation table
%% Take a look at categories (just sanity-checking)
print('\nCategories by species:')
pp.pprint(categories_by_species)
%% Fill in some image fields we didn't have when we created the image table
"width, height, corrupt, seq_num_frames, location, datetime"
Every annotation in this list should have the same sequence ID
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;"
there's no reason to do this redundantly for every image
Every image in this sequence should point back to the same equence
Every annotation in this list should have the same location
Every annotation in this list should have the same datetime
Is this image on disk?
iImage = 0; im = images[0]
...for each image
"images_processed = pool.map(process_image, images)"
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))"
%% Count missing/corrupted images
%% Print distribution of sequence lengths (sanity-check)
%% Replicate annotations across images
iAnn = 0; ann = annotations[iAnn]
%% See what files are on disk but not annotated
%% Sanity-check image and annotation uniqueness
%% Minor updates to fields
%% Write .json file
%% Create a list of human files
ann = annotations[0]
%% Create public archive and public/private folders
im = images[0]
E.g. KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0001.JPG
Copy to private output folder
Add to zipfile
Possibly start a new archive
Copy to public output folder
...for each image
%% Sanity-check .json file
"This will produce some validation errors, because this zipfile doesn't include humans"
%% Zip up .json and .csv files
%% When I skip to this part (using a pre-rendered .json file)
%%
%%
ann = annotations[0]
%% Summary prep for LILA
"%% Generate preview, sanity-check labels"
"viz_options.classes_to_include = ['jackalblackbacked','bustardkori']"
%% Scrap
%% Find annotations for a particular image
%% Write a list of missing images
""
save_the_elephants_survey_A.py
""
Convert the .csv file provided for the Save the Elephants Survey A data set to a
COCO-camera-traps .json file
""
%% Constants and environment
Handle all unstructured fields in the source data as extra fields in the annotations
"photo_type really should be an image property, but there are a few conflicts"
that forced me to handle it as an annotation proprerty
%% Read source data
"%% Map filenames to rows, verify image existence"
Maps relative paths to row indices in input_metadata
"Build up a map from filenames to a list of rows, checking image existence as we go"
Ignore directories
%% Make sure the multiple-annotation cases make sense
%%
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
Force the empty category to be ID 0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image]
Example filename:
""
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG'
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG'
i_row = row_indices[0]
There are a small handful of datetime mismatches across annotations
for the same image
assert im['datetime'] == timestamp
Special cases based on the 'photo type' field
Various spellings of 'community'
Have we seen this category before?
Create an annotation
fieldname = list(mapped_fields.keys())[0]
...for each row
...for each image
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
%% Scrap
%% Find unique photo types
##
""
timelapse_csv_set_to_json.py
""
Given a directory full of reasonably-consistent Timelapse-exported
".csvs, assemble a CCT .json."
""
"Assumes that you have a list of all files in the directory tree, including"
image and .csv files.
""
##
%% Constants and imports
Text file with relative paths to all files (images and .csv files)
"%% Read file list, make a list of all image files and all .csv files"
"%% Verify column consistency, create a giant array with all rows from all .csv files"
i_csv = 0; csv_filename = csv_files[0]
%% Prepare some data structures we'll need for mapping image rows in .csv files to actual image files
Enumerate all folders containing image files
"In this data set, a site folder looks like:"
""
Processed Images\\site_name
%% Map .csv files to candidate camera folders
fn = valid_csv_files[0]
"Some site folders appear as ""XXNNNN"", some appear as ""XXNNNN_complete"""
...for each .csv file
%% Map camera folders to candidate image folders
%% Main loop over labels (prep)
Force the empty category to be ID 0
Images that are marked empty and also have a species label
%% Main loop over labels (loop)
i_row = 0; row = input_metadata.iloc[i_row]
"for i_row,row in input_metadata.iterrows():"
"Usually this is just a single folder name, sometimes it's a full path,"
which we don't want
Check whether this file exists on disk
Retrieve image width and height
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each row in the big table of concatenated .csv files
%% Fix cases where an image was annotated as 'unlabeled' and as something else
This annotation is 'unlabeled'
Was there another category associated with this image?
%% Check for un-annnotated images
Enumerate all images
list(relative_path_to_image.keys())[0]
%% Create info struct
%% Write output
%% Sanity-check the database's integrity
%% Render a bunch of images to make sure the labels got carried along correctly
""
auckland_doc_to_json.py
""
Convert Auckland DOC data set to COCO camera traps format.  This was
for a training data set where class names were encoded in path names.
""
%% Constants and imports
Filenames will be stored in the output .json relative to this base dir
%% Enumerate files
%% Assemble dictionaries
Force the empty category to be ID 0
fn = image_files[0]; print(fn)
Typically y:\Maukahuka_Auckland_Island\1_Training\Winter_Trial_2019\cat\cat\eat\20190903_IDdY_34_E3_tmp_201908240051.JPG
"This data set has two top-level folders, ""1_Training"" (which has class names encoded"
"in paths) and ""2_Testing"" (which has no class information)."
...for each image
%% Write output .json
%% Write train/test .jsons
%% Validate .json files
%% Preview labels
checkpoint
""
awc_to_json.py
""
Convert a particular .csv file to CCT format.
""
%% Constants and environment
%% Read source data
%% Main loop over labels
Force the empty category to be ID 0
iRow = 0; row = input_metadata.iloc[iRow]
"ImageID,FileName,FilePath,SpeciesID,CommonName"
"This makes an assumption of one annotation per image, which happens to be"
true in this data set.
Retrieve image width and height
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
%% Check for images that aren't included in the metadata file
Enumerate all images
list(relativePathToImage.keys())[0]
%% Create info struct
%% Write output
%% Sanity-check the database's integrity
%% Render a bunch of images to make sure the labels got carried along correctly
""
idaho-camera-traps.py
""
Prepare the Idaho Camera Traps dataset for release on LILA.
""
%% Imports and constants
Multi-threading for .csv file comparison and image existence validation
"We are going to map the original filenames/locations to obfuscated strings, but once"
"we've done that, we will re-use the mappings every time we run this script."
This is the file to which mappings get saved
The maximum time (in seconds) between images within which two images are considered the
same sequence.
"This is a two-column file, where each line is [string in the original metadata],[category name we want to map it to]"
"The output file, using the original strings"
"The output file, using obfuscated strings for everything but filenamed"
"The output file, using obfuscated strings and obfuscated filenames"
"One time only, I ran MegaDetector on the whole dataset..."
...then set aside any images that *may* have contained humans that had not already been
annotated as such.  Those went in this folder...
...and the ones that *actually* had humans (identified via manual review) got
copied to this folder...
"...which was enumerated to this text file, which is a manually-curated list of"
images that were flagged as human.
Unopinionated .json conversion of the .csv metadata
%% List files (images + .csv)
Ignore .csv files in folders with multiple .csv files
...which would require some extra work to decipher.
fn = csv_files[0]
%% Parse each .csv file into sequences (function)
csv_file = csv_files[-1]
os.startfile(csv_file_absolute)
survey = csv_file.split('\\')[0]
Sample paths from which we need to derive locations:
""
St.Joe_elk\AM99\Trip 1\100RECNX\TimelapseData.csv
Beaverhead_elk\AM34\Trip 1\100RECNX\TimelapseData.csv
""
ClearCreek_mustelids\Winter2015-16\FS-001-P\FS-001-P.csv
ClearCreek_mustelids\Summer2015\FS-001\FS-001.csv
ClearCreek_mustelids\Summer2016\IDFG-016\IDFG-016.csv
""
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017b
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017a
Load .csv file
Validate the opstate column
# Create datetimes
print('Creating datetimes')
i_row = 0; row = df.iloc[i_row]
Make sure data are sorted chronologically
""
"In odd circumstances, they are not... so sort them first, but warn"
Debugging when I was trying to see what was up with the unsorted dates
# Parse into sequences
print('Creating sequences')
i_row = 0; row = df.iloc[i_row]
Start a new sequence if:
""
* This image has no timestamp
* This image has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each row
# Parse labels for each sequence
sequence_id = location_sequences[0]
Row indices in a sequence should be adjacent
sequence_df = df[df['seq_id']==sequence_id]
# Determine what's present
Be conservative; assume humans are present in all maintenance images
The presence columns are *almost* always identical for all images in a sequence
assert single_presence_value
"print('Warning: presence value for {} is inconsistent for {}'.format(presence_column,sequence_id))"
...for each presence column
Tally up the standard (survey) species
"If no presence columns are marked, all counts should be zero"
count_column = count_columns[0]
Occasionally a count gets entered (correctly) without the presence column being marked
"assert len(values) == 1 and values[0] == 0, 'Non-zero counts with no presence columns marked for sequence {}'.format(sequence_id)"
"Handle this by virtually checking the ""right"" box"
Make sure we found a match
Handle 'other' tags
column_name = otherpresent_columns[0]
print('Found non-survey counted species column: {}'.format(column_name))
...for each non-empty presence column
"Very rarely, the ""otherpresent"" column is checked, but no more detail is available"
...handling non-survey species
Build the sequence data
i_row = 0; row = sequence_df.iloc[i_row]
Only one folder used a single .csv file for two subfolders
...for each sequence
...def csv_to_sequences()
%% Parse each .csv file into sequences (loop)
%%
%%
i_file = -1; csv_file = csv_files[i_file]
%% Save sequence data
%% Load sequence data
%%
%% Validate file mapping (based on the existing enumeration)
sequences = sequences_by_file[0]
sequence = sequences[0]
"Actually, one folder has relative paths"
assert '\\' not in image_file_relative and '/' not in image_file_relative
os.startfile(csv_folder)
assert os.path.isfile(image_file_absolute)
found_file = os.path.isfile(image_file_absolute)
...for each image
...for each sequence
...for each .csv file
%% Load manual category mappings
The second column is blank when the first column already represents the category name
%% Convert to CCT .json (original strings)
Force the empty category to be ID 0
For each .csv file...
""
sequences = sequences_by_file[0]
For each sequence...
""
sequence = sequences[0]
Find categories for this image
"When 'unknown' is used in combination with another label, use that"
"label; the ""unknown"" here doesn't mean ""another unknown species"", it means"
there is some other unknown property about the main species.
category_name_string = species_present[0]
"This piece of text had a lot of complicated syntax in it, and it would have"
been too complicated to handle in a general way
print('Ignoring category {}'.format(category_name_string))
Don't process redundant labels
category_name = category_names[0]
If we've seen this category before...
If this is a new category...
print('Adding new category for {}'.format(category_name))
...for each category (inner)
...for each category (outer)
...if we do/don't have species in this sequence
"We should have at least one category assigned (which may be ""empty"" or ""unknown"")"
assert len(sequence_category_ids) > 0
Was any image in this sequence manually flagged as human?
print('Flagging sequence {} as human based on manual review'.format(sequence_id))
For each image in this sequence...
""
i_image = 0; im = images[i_image]
Create annotations for this image
...for each image in this sequence
...for each sequence
...for each .csv file
Verify that all images have annotations
ann = ict_data['annotations'][0]
For debugging only
%% Create output (original strings)
%% Validate .json file
%% Preview labels
%% Look for humans that were found by MegaDetector that haven't already been identified as human
This whole step only needed to get run once
%%
Load MD results
Get a list of filenames that MD tagged as human
im = md_results['images'][0]
...for each detection
...for each image
Map images to annotations in ICT
ann = ict_data['annotations'][0]
For every image
im = ict_data['images'][0]
Does this image already have a human annotation?
...for each annotation
...for each image
%% Copy images for review to a new folder
fn = missing_human_images[0]
%% Manual step...
Copy any images from that list that have humans in them to...
%% Create a list of the images we just manually flagged
fn = human_tagged_filenames[0]
E.g. '0000_Beaverhead_elk~AM174~Trip 1~100RECNX~IMG_1397.JPG'
"%% Translate location, image, sequence IDs"
Load mappings if available
Generate mappings
If we've seen this location before...
Otherwise assign a string-formatted int as the ID
If we've seen this sequence before...
Otherwise assign a string-formatted int as the ID
Assign an image ID
...for each image
Assign annotation mappings
Save mappings
"Back this file up, lest we should accidentally re-run this script with force_generate_mappings = True"
and overwrite the mappings we used.
...if we are/aren't re-generating mappings
%% Apply mappings
"%% Write new dictionaries (modified strings, original files)"
"%% Validate .json file (modified strings, original files)"
%% Preview labels (original files)
"viz_options.classes_to_exclude = ['empty','deer','elk']"
viz_options.classes_to_include = ['bobcat']
%% Copy images to final output folder (prep)
ann = d['annotations'][0]
Is this a public or private image?
Generate absolute path
Copy to output
Update the filename reference
...def process_image(im)
%% Copy images to final output folder (execution)
For each image
im = images[0]
Write output .json
%% Make sure the right number of images got there
%% Validate .json file (final filenames)
%% Preview labels (final filenames)
"viz_options.classes_to_exclude = ['empty','deer','elk']"
viz_options.classes_to_include = ['horse']
viz_options.classes_to_include = [viz_options.multiple_categories_tag]
"viz_options.classes_to_include = ['human','vehicle','domestic dog']"
%% Create zipfiles
%% List public files
%% Find the size of each file
fn = all_public_output_files[0]
%% Split into chunks of approximately-equal size
...for each file
%% Create a zipfile for each chunk
...for each filename
with ZipFile()
...def create_zipfile()
i_file_list = 0; file_list = file_lists[i_file_list]
""
wellington_to_json.py
""
Convert the .csv file provided for the Wellington data set to a
COCO-camera-traps .json file
""
%% Constants and environment
%% Read source data
"Filenames were provided as *.jpg, but images were *.JPG, converting here"
"%% Map filenames to rows, verify image existence"
"Takes ~30 seconds, since it's checking the existence of ~270k images"
"Build up a map from filenames to a list of rows, checking image existence as we go"
"I didn't expect this to be true a priori, but it appears to be true, and"
it saves us the trouble of checking consistency across multiple occurrences
of an image.
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
imageName = imageFilenames[0]
"As per above, this is convenient and appears to be true; asserting to be safe"
"Filenames look like ""290716114012001a1116.jpg"""
This gets imported as an int64
"These appear as ""image1"", ""image2"", etc."
"In the form ""001a"""
Can be in the form '111' or 's46'
"In the form ""7/29/2016 11:40"""
Check image height and width
"Use 'empty', to be consistent with other data on lila"
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
""
mcgill_to_json.py
""
Convert the .csv file provided for the McGill test data set to a
COCO-camera-traps .json file
""
%% Constants and environment
%% Read source data
"%% Map filenames to rows, verify image existence"
Create an additional column for concatenated filenames
Maps relative filenames to rows
"Build up a map from filenames to a list of rows, checking image existence as we go"
row = input_metadata.iloc[0]
"I didn't expect this to be true a priori, but it appears to be true, and"
it saves us the trouble of checking consistency across multiple occurrences
of an image.
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
row = input_metadata.iloc[0]
"Filenames look like ""290716114012001a1116.jpg"""
"In the form ""001a"""
Can be in the form '111' or 's46'
"In the form ""7/29/2016 11:40"""
Check image height and width
NaN is the only thing we should see that's not a string
NaN is the only thing we should see that's not a string
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
""
bellevue_to_json.py
""
"""Bellevue Camera Traps"" is the rather unremarkable camera trap data set"
used by one of the repo's maintainers for testing.  It's organized as:
""
approximate_date/[loose_camera_specifier/]/species
""
E.g.:
""
"""2018.03.30\coyote\DSCF0091.JPG"""
"""2018.07.18\oldcam\empty\DSCF0001.JPG"""
""
%% Constants and imports
from the ai4eutils repo
Filenames will be stored in the output .json relative to this base dir
%% Exif functions
"%% Enumerate files, create image/annotation/category info"
Force the empty category to be ID 0
Keep track of unique camera folders
Each element will be a dictionary with fields:
""
"relative_path, width, height, datetime"
fname = image_files[0]
Corrupt or not an image
Store file info
E.g. 2018.03.30/coyote/DSCF0091.JPG
...for each image file
%% Synthesize sequence information
Sort images by time within each folder
camera_path = camera_folders[0]
previous_datetime = sorted_images_this_camera[0]['datetime']
im = sorted_images_this_camera[1]
Start a new sequence if necessary
...for each image in this camera
...for each camera
Fill in seq_num_frames
%% A little cleanup
%% Write output .json
%% Sanity-check data
%% Label previews
""
auckland_doc_test_to_json.py
""
Convert Auckland DOC data set to COCO camera traps format.  This was
for a testing data set where a .csv file was provided with class
information.
""
%% Constants and imports
Filenames will be stored in the output .json relative to this base dir
%% Enumerate files
%% Create unique identifier for each image
The ground truth doesn't have full paths in it; create unique identifiers for each image
based on the camera name and filename.
""
We store file identifiers as cameraname_filename.
relative_path = relative_image_paths[0]
Example relative paths
""
Summer_Trial_2019/A1_1_42_SD114_20190210/AucklandIsland_A1_1_42_SD114_20190210_01300001.jpg
Winter_Trial_2019/Installation/10_F4/10_F4_tmp_201908210001.JPG
Find the camera name
"E..g. ""A1_1_42_SD114_20190210"" in the above example"
For camera tokens like C1_5_D_190207
%% Load input data
"The spreadsheet has a space after ""Camera"""
%% Assemble dictionaries
Force the empty category to be ID 0
"array([nan, 'Blackbird', 'Bellbird', 'Tomtit', 'Song thrush', 'Pippit',"
"'Pippet', '?', 'Dunnock', 'Song thursh', 'Kakariki', 'Tui', ' ',"
"'Silvereye', 'NZ Pipit', 'Blackbird and Dunnock', 'Unknown',"
"'Pipit', 'Songthrush'], dtype=object)"
i_row = 0; row = input_metadata.iloc[i_row]
E.g.: AucklandIsland_A1_1_42_SD114_20190210_01300009.jpg
"We have multiple files matching this identifier, can we uniquely resolve this"
to one of those files based on the camera ID?
"create_annotation(image_id,category_name,count)"
"'SD_Change', 'Cat', 'Mouse', 'Bird', 'Bird_ID', 'False_trig', 'Unknown',"
"'Human', 'Collared_cat', 'Cat_ID', 'Pig', 'Sea_lion', 'Open_adjusted',"
"'Penguin', 'Dog', 'Comments', 'Unnamed: 22']"
Each of these categories is handled a little differently...
These are straightforward
...for each image
%% Summarize errors
%% Write output .json
%% Validate .json file
%% Preview labels
%% Precision-recall analysis
""
plot_wni_giraffes.py
""
Plot keypoints on a random sample of images from the wni-giraffes data set.
""
%% Constants and imports
%% Load and select data
%% Support functions
https://stackoverflow.com/questions/32504246/draw-ellipse-in-python-pil-with-line-thickness
Use a single channel image (mode='L') as mask.
The size of the mask can be increased relative to the imput image
to get smoother looking results.
draw outer shape in white (color) and inner shape in black (transparent)
downsample the mask using PIL.Image.LANCZOS
(a high-quality downsampling filter).
paste outline color to input image through the mask
%% Plot some images
ann = annotations_to_plot[0]
i_tool = 0; tool_name = short_tool_names[i_tool]
Don't plot tools that don't have a consensus annotation
...for each tool
...for each annotation
""
idfg_iwildcam_lila_prep.py
""
Adding class labels (from the private test .csv) to the iWildCam 2019 IDFG
"test set, in preparation for release on LILA."
""
This version works with the public iWildCam release images.
""
"%% ############ Take one, from iWildCam .json files ############"
%% Imports and constants
%% Read input files
Remove the header line
%% Parse annotations
Lines look like:
""
"b005e5b2-2c0b-11e9-bcad-06f1011196c4,1,Private"
%% Minor cleanup re: images
%% Create annotations
%% Prepare info
%% Minor adjustments to categories
Remove unused categories
Name adjustments
%% Create output
%% Write output
%% Validate .json file
%% Preview labels
viz_options.classes_to_exclude = ['test']
"%% ############ Take two, from pre-iWildCam .json files created from IDFG .csv files ############"
%% Imports and constants
%% One-time line break addition
%% Read input files
%% Prepare info
%% Minor adjustments to categories
%% Minor adjustments to annotations
%% Create output
%% Write output
%% Validate .json file
%% Preview labels
viz_options.classes_to_exclude = ['test']
""
missouri_to_json.py
""
Create .json files from the original source files for the Missouri Camera Traps
data set.  Metadata was provided here in two formats:
""
"1) In one subset of the data, folder names indicated species names.  In Set 1,"
there are no empty sequences.  Set 1 has a metadata file to indicate image-level
bounding boxes.
""
2) A subset of the data (overlapping with (1)) was annotated with bounding
"boxes, specified in a whitespace-delimited text file.  In set 2, there are"
"some sequences omitted from the metadata file, which implied emptiness."
""
"In the end, set 2 labels were not reliable enough to publish, so LILA includes only set 1."
""
%% Constants and imports
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Filenames will be stored in the output .json relative to this base dir
This will be a list of filenames that need re-annotation due to redundant boxes
"%% Enumerate files, read image sizes (both sets)"
"Takes a few minutes, since we're reading image sizes."
Each element will be a list of relative path/full path/width/height
"Only process leaf nodes corresponding to sequences, which look like:"
""
E:\wildlife_data\missouri_camera_traps\Set1\1.02-Agouti\SEQ75583
E:\wildlife_data\missouri_camera_traps\Set2\p1d101
""
assert len(files) <= 2
Read the image
Not an image...
Store file info
"...if we didn't hit the max file limit, keep going"
...for each file
%% Add sequence lengths (both sets)
%% Load the set 1 metadata file
"List of lists, length varies according to number of bounding boxes"
""
Preserves original ordering
iLine = 0; line = metadataSet1Lines[0]
"Lines should be filename, number of bounding boxes, boxes (four values per box)"
Make sure we have image info for this image
%% Print missing files from Set 1 metadata
Manual changes I made to the metadata file:
""
'IMG' --> 'IMG_'
Red_Brocket_Deer --> Red_Deer
European-Hare --> European_Hare
Wood-Mouse --> Wood_Mouse
Coiban-Agouti --> Coiban_Agouti
%% Load the set 2 metadata file
"This metadata file contains most (but not all) images, and a class label (person/animal/empty)"
"for each, plus bounding boxes."
"List of lists, length varies according to number of bounding boxes"
""
Preserves original ordering
"Create class IDs for each *sequence*, which we'll use to attach classes to"
images for which we don't have metadata
""
This only contains mappings for sequences that appear in the metadata.
iLine = 0; line = metadataSet2Lines[0]
"Lines should be filename, number of bounding boxes, labeled boxes (five values per box)"
""
Empty images look like filename\t0\t0
E.g. 'Set2\\p1d101\\p1d101s100i10.JPG'
Make sure we don't have mixed classes within an image
"Figure out what class this *sequence* is, so we know how to handle unlabeled"
images from this sequence
Can't un-do a mixed sequence
Previously-empty sequences get the image class label
"If the sequence has a non-empty class, possibly change it"
Make sure we have image info for this image
...for each line in the set 2 metadata file
%% What Set 2 images do I not have metadata for?
These are *mostly* empty images
iImage = 0; imageID = set2ImageIDs[iImage]
%% Create categories and annotations for set 1
"Though we have no empty sequences, we do have empty images in this set"
For each image
""
iImage = 0; imageID = set1ImageIDs[iImage]
E.g. Set1\\1.80-Coiban_Agouti\\SEQ83155\\SEQ83155_IMG_0010.JPG
Find the species name
This image may still be empty...
category['count'] = category['count'] + 1
"If we have bounding boxes, create image-level annotations"
"filename, number of bounding boxes, boxes (four values per box)"
"Make sure the relative filename matches, allowing for the fact that"
some of the filenames in the metadata aren't quite right
This image is non-empty
"Some redundant bounding boxes crept in, don't add them twice"
Check this bbox against previous bboxes
""
Inefficient?  Yes.  In an important way?  No.
"Bounding box values are in absolute coordinates, with the origin"
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1]."
""
"Convert to floats and to x/y/w/h, as per CCT standard"
...for each box
if we do/don't have boxes for this image
Else create a sequence-level annotation
...for each image
%% Write out the list of images with redundant boxes
%% Create categories and annotations for set 2
For each image
""
iImage = 0; imageID = set2ImageIDs[iImage]
E.g. 'Set2\\p1d100\\p1d100s10i1.JPG'
"Find the sequence ID, sanity check filename against what we stored"
"If we have bounding boxes or an explicit empty label, create image-level annotations"
"filename, number of bounding boxes, labeled boxes (five values per box)"
"Make sure the relative filename matches, allowing for the fact that"
some of the filenames in the metadata aren't quite right
"Bounding box values are in absolute coordinates, with the origin"
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1]."
""
Convert to floats and to x/y/w/h
...for each box
...if we do/don't have boxes for this image
Else create a sequence-level annotation
...for each image
"%% The 'count' field isn't really meaningful, delete it"
"It's really the count of image-level annotations, not total images assigned to a class"
%% Write output .json files
%% Sanity-check final set 1 .json file
"python sanity_check_json_db.py --bCheckImageSizes --baseDir ""E:\wildlife_data\missouri_camera_traps"" ""E:\wildlife_data\missouri_camera_traps\missouri_camera_traps_set1.json"""
%% Generate previews
""
filenames_to_json.py
""
Take a directory of images in which species labels are encoded by folder
"names, and produces a COCO-style .json file"
""
%% Constants and imports
from the ai4eutils repo
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Filenames will be stored in the output .json relative to this base dir
"rawClassListFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_classes.csv')"
"classMappingsFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_class_mapping.csv')"
"%% Enumerate files, read image sizes"
Each element will be a list of relative path/full path/width/height
Read the image
Corrupt or not an image
Store file info
Write to output file
...for each image file
...csv file output
...if the file list is/isn't available
%% Enumerate classes
Maps classes to counts
We like 'empty' to be class 0
%% Assemble dictionaries
...for each category
Each element is a list of relative path/full path/width/height/className
...for each image
%% External class mapping
%% Write raw class table
cat = categories[0]
%% Read the mapped class table
"id, source, count, target"
"%% Make classMappings contain *all* classes, not just remapped classes"
cat = categories[0]
%% Create new class list
"Start at 1, explicitly assign 0 to ""empty"""
One-off issue with character encoding
%% Re-map annotations
ann = annotations[0]
%% Write output .json
%% Utilities
%%
Find images with a particular tag
%% Randomly sample annotations
""
umn_to_json.py
""
Prepare images and metadata for the Orinoquía Camera Traps dataset.
""
%% Imports and constants
"For two deployments, we're only processing imagse in the ""detections"" subfolder"
%% Enumerate deployment folders
%% Load ground truth
i_row = 0; row = ground_truth_df.iloc[i_row]
%% Create relative paths for ground truth data
"Some deployment folders have no subfolders, e.g. this is a valid file name:"
""
M00/01010132.JPG
""
"But some deployment folders have subfolders, e.g. this is also a valid file name:"
""
N17/100EK113/07160020.JPG
""
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly"
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths."
deployment_name = list(deployment_folders)[0]
Enumerate all files in this folder
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))"
filename = files[100]
...for each file in this deployment
...for each deployment
%% Add relative paths to our ground truth table
i_row = 0; row = ground_truth_df.iloc[i_row]
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's"
"just the deployment ID and the image name, separated by a slash."
Find the relative path for this image
Make sure this image file exists
...for each row in the ground truth table
%% Take everything out of Pandas
%% Convert string timestamps to Python datetimes
im = ground_truth_dicts[0]
"Filenames look like, e.g., N36/100EK113/06040726.JPG"
%% Synthesize sequence information
Sort images by time within each location
i_location=0; location = locations[i_location]
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_camera[1]
"Timestamp for this image, may be None"
Start a new sequence if:
""
* This image has no timestamp
* This iamge has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each image in this location
...for each location
%% Create category dict and category IDs
im = images[0]
%% Imports and constants (.json generation)
%% Count frames in each sequence
%% Double check images with multiple annotations
im = images[0]
%% Assemble dictionaries
Force the empty category to be ID 0
input_im = images[0]
...for each image
%% Write output .json
%% Validate .json file
%% Map relative paths to annotation categories
ann = data['annotations'][0]
%% Copy images to output
EXCLUDE HUMAN AND MISSING
im = data['images'][0]
im = images[0]
%% Preview labels
viz_options.classes_to_exclude = ['test']
""
nacti_fieldname_adjustments.py
""
"NACTI metadata was posted with ""filename"" in images instead of ""file_name"", and"
"used string (rather than int) category IDs (in categories, but not in annotations)."
""
This script fixes those issues and rev's the version number.
""
%% Constants and environment
%% Read .json file
"%% Rev version number, update field names and types"
%% Write json file
""
cct_field_adjustments.py
""
CCT metadata was posted with int locations instead of strings.
""
This script fixes those issues and rev's the version number.
""
%% Constants and environment
%% Read .json file
"%% Rev version number, update field names and types"
%% Write json file
%% Check output data file
""
carrizo_shrubfree_2018.py
""
Convert the .csv file provided for the Carrizo Mojave data set to a
COCO-camera-traps .json file
""
%% Constants and environment
%% Read source data
Original .csv file had superfluous spaces in column names
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
58 missing files (of 17652)
%% Check for images that aren't included in the metadata file
3012 of 20606 files are not in metadata
%% Create CCT dictionaries
Map categories to integer IDs
""
The category '0' is reserved for 'empty'
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
Each filename should just match one row
Don't include images that don't exist on disk
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
""
snapshot_serengeti_lila.py
""
Create zipfiles of Snapshot Serengeti S1-S11.
""
"Create a metadata file for S1-S10, plus separate metadata files"
"for S1-S11.  At the time this code was written, S11 was under embargo."
""
Create zip archives of each season without humans.
""
Create a human zip archive.
""
%% Constants and imports
import sys; sys.path.append(r'c:\git\ai4eutils')
import sys; sys.path.append(r'c:\git\cameratraps')
assert(os.path.isdir(metadata_base))
"There are two redundant categories, and we re-map ""blank"" to ""empty"" as per CCT convention"
"%% Load metadata files, concatenate into a single table"
iSeason = 1
%% Convert to dictionaries (prep)
%% Convert to dictionaries (loops)
"TODO: iterrows() is a terrible way to do this, but this is one of those days"
"where I want to get this done, not get better at Python."
irow = 0; row = image_table.iloc[0]
"Loaded as an int64, converting to int here"
...for each row in the image table
Make sure image IDs are what we think they are
...for each row in the annotation table
%% Load previously-saved dictionaries when re-starting mid-script
%%
%% Take a look at categories (just sanity-checking)
%%
%% Fill in some image fields we didn't have when we created the image table
"width, height, corrupt, seq_num_frames, location, datetime"
Every annotation in this list should have the same sequence ID
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;"
there's no reason to do this redundantly for every image
Every image in this sequence should point back to the same equence
Every annotation in this list should have the same location
Every annotation in this list should have the same datetime
Is this image on disk?
iImage = 0; im = images[0]
...for each image
"images_processed = pool.map(process_image, images)"
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))"
%%
%% Count missing/corrupted images
%% Print distribution of sequence lengths (sanity-check)
%% Replicate annotations across images
iAnn = 0; ann = annotations[iAnn]
%% See what files are on disk but not annotated (~15 mins)
247370 files not in the database (of 7425810)
%% Load old image database
%% Look for old images not in the new DB and vice-versa
"At the time this was written, ""old"" was S1-S6"
old_im = cct_old['images'][0]
new_im = images[0]
4 old images not in new db
12 new images not in old db
%% Save our work
%% Load our work
%%
%% Examine size mismatches
i_mismatch = -1; old_im = size_mismatches[i_mismatch]
%% Sanity-check image and annotation uniqueness
"%% Split data by seasons, create master list for public seasons"
ann = annotations[0]
%% Minor updates to fields
"%% Write master .json out for S1-10, write individual season .jsons (including S11)"
"Loop over all seasons, plus one iteration for the ""all public data"" iteration, and"
"one for the ""all data"" iteration"
%% Find categories that only exist in S11
List of categories in each season
Category 55 (fire) only in S11
Category 56 (hyenabrown) only in S11
Category 57 (wilddog) only in S11
Category 58 (kudu) only in S11
Category 59 (pangolin) only in S11
Category 60 (lioncub) only in S11
%% Prepare season-specific .csv files
iSeason = 1
%% Create a list of human files
ann = annotations[0]
%% Save our work
%% Load our work
%%
"%% Create archives (human, per-season) (prep)"
im = images[0]
im = images[0]
Don't include humans
Only include files from this season
Possibly start a new archive
...for each image
i_season = 0
"for i_season in range(0,nSeasons):"
create_season_archive(i_season)
%% Create archives (loop)
pool = ThreadPool(nSeasons+1)
"n_images = pool.map(create_archive, range(-1,nSeasons))"
"seasons_to_zip = range(-1,nSeasons)"
...for each season
%% Sanity-check .json files
%logstart -o r'E:\snapshot_temp\python.txt'
%% Zip up .json and .csv files
pool = ThreadPool(len(files_to_zip))
"pool.map(zip_single_file, files_to_zip)"
%% Super-sanity-check that S11 info isn't leaking
im = data_public['images'][0]
ann = data_public['annotations'][0]
iRow = 0; row = annotation_df.iloc[iRow]
iRow = 0; row = image_df.iloc[iRow]
%% Create bounding box archive
i_image = 0; im = data['images'][0]
i_box = 0; boxann = bbox_data['annotations'][0]
%% Sanity-check a few files to make sure bounding boxes are still sensible
import sys; sys.path.append(r'C:\git\CameraTraps')
%% Check categories
%% Summary prep for LILA
""
wi_to_json
""
Prepares CCT-formatted metadata based on a Wildlife Insights data export.
""
"Mostly assumes you have the images also, for validation/QA."
""
%% Imports and constants
"Because WI filenames are GUIDs, it's not practical to page through sequences in an"
image viewer.  So we're going to (optionally) create a copy of the data set where
images are ordered.
%% Load ground truth
%% Take everything out of Pandas
%% Synthesize common names when they're not available
"Blank rows should always have ""Blank"" as the common name"
"print('Warning: missing common name for row {} ({})'.format(i_row,row['filename']))"
%% Convert string timestamps to Python datetimes
im = ground_truth_dicts[0]
"The field called ""location"" in the WI .csv file is a URL, we want to reclaim"
"the ""location"" keyword for CCT output"
"Filenames look like, e.g., N36/100EK113/06040726.JPG"
%% Synthesize sequence information
Sort images by time within each location
i_location=0; location = locations[i_location]
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_camera[1]
"Timestamp for this image, may be None"
Start a new sequence if:
""
* This image has no timestamp
* This iamge has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each image in this location
...for each location
%% Create category dict and category IDs
im = images[0]
%% Count frames in each sequence
%% Build relative paths
im = images[0]
Sample URL:
""
gs://project-asfasdfd/deployment/21444549/asdfasdfd-616a-4d10-a921-45ac456c568a.jpg'
%% Double check images with multiple annotations
im = images[0]
%% Assemble dictionaries
Force the empty category to be ID 0
input_im = images[0]
"print('Warning: image ID {} ({}) has multiple annotations'.format(im['id'],im['id'].replace('_','/')))"
...for each image
%% Write output .json
%% Validate .json file
%% Preview labels
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))"
%%
%% Create ordered dataset
"Because WI filenames are GUIDs, it's not practical to page through sequences in an"
image viewer.  So we're going to create a copy of the data set where images are
ordered.
im = images_out[0]; im
%% Create ordered .json
%% Copy files to their new locations
im = ordered_images[0]
im = data_ordered['images'][0]
%% Preview labels in the ordered dataset
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))"
%% Open an ordered filename from the unordered filename
""
rspb_to_json.py
""
Convert the .csv file provided for the RSPB data set to a
COCO-camera-traps .json file
""
%% Constants and environment
[location] is an obfuscation
%% Create info struct
%% Read source data
metadataTable.columns.values
""
"array(['Project', 'inDir', 'FileName', 'Station', 'Camera',"
"'StationCameraFileName', 'DateTimeOriginal', 'DateReadable',"
"'outDir', 'filename_new', 'fileExistsAlready', 'CopyStatus',"
"'Species'], dtype=object)"
We'll populate these later
keys should be lowercase
"%% Enumerate images, confirm filename uniqueness"
"%% Update metadata filenames to include site and camera folders, check existence"
""
Takes ~1min
iRow = 0; row = metadataTable.iloc[iRow]
There's a bug in the metadata; the 'camera' column isn't correct.
camera = row['Camera']
"These appear as, e.g., '3.22e12'"
camera = str(int(float(camera)))
Let's pull this out of the file name instead
""
Filenames look like one of the following:
""
A1__03224850850507__2015-11-28__10-45-04(1).JPG
Bayama2PH__C05__NA(NA).JPG
assert(os.path.isfile(fullPath))
metadataTable.iloc[iRow] = row
Re-assemble into an updated table
%% Check for images that aren't included in the metadata file
Enumerate all images
Write to a text file
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
iRow = 0; row = metadataTable.iloc[iRow]
A1__03224850850507__2015-11-28__10-45-04(1).JPG
'A1\\03224850850507\\A1__03224850850507__2015-11-28__10-45-04(1).JPG'
Not currently populated
"Often -1, sometimes a semi-meaningful int"
A1
03224850850507
"In variable form, but sometimes '28/11/2015 10:45'"
Check image height and width
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Write output
%% Check database integrity
%% Preview a few images to make sure labels were passed along sensibly
%% One-time processing step: copy images to a flat directory for annotation
%%
""
pc_to_json.py
""
Convert a particular collection of .csv files to CCT format.
""
%% Constants and environment
%% Read and concatenate source data
List files
"List of dataframes, one per .csv file; we'll concatenate later"
i_file = 87; fn = input_files[i_file]
Concatenate into a giant data frame
%% List files
%% Main loop over labels (prep)
Force the empty category to be ID 0
%% Main loop over labels (loop)
iRow = 0; row = input_metadata.iloc[iRow]
"ImageID,FileName,FilePath,SpeciesID,CommonName"
assert os.path.isfile(full_path)
Retrieve image width and height
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
%% See what's up with missing files
s = list(image_relative_paths)[0]
s = missing_files[0]
%% Check for images that aren't included in the metadata file
%% Sample the database
%%
Collect the images we want
%% Create info struct
%% Write output
%% Sanity-check the database's integrity
%% Render a bunch of images to make sure the labels got carried along correctly
options.classes_to_exclude = ['unlabeled']
%% Write out a list of files to annotate
""
save_the_elephants_survey_B.py
""
Convert the .csv file provided for the Save the Elephants Survey B data set to a
COCO-camera-traps .json file
""
%% Constants and environment
input_base = r'/mnt/blobfuse/wildlifeblobssc/ste_2019_08_drop'
output_base = r'/home/gramener/survey_b'
Handle all unstructured fields in the source data as extra fields in the annotations
"photo_type really should be an image property, but there are a few conflicts"
that forced me to handle it as an annotation proprerty
%% Read source data
"%% Map filenames to rows, verify image existence"
"%% Map filenames to rows, verify image existence"
Maps relative paths to row indices in input_metadata
"Build up a map from filenames to a list of rows, checking image existence as we go"
Ignore directories
%% Make sure the multiple-annotation cases make sense
%%
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
Force the empty category to be ID 0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image]
Example filename:
""
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG'
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG'
i_row = row_indices[0]
timestamp = row['Date']
There are a small handful of datetime mismatches across annotations
for the same image
assert im['datetime'] == timestamp
Special cases based on the 'photo type' field
Various spellings of 'community'
Have we seen this category before?
Create an annotation
fieldname = list(mapped_fields.keys())[0]
...for each row
...for each image
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
%% Scrap
%% Find unique photo types
""
ubc_to_json.py
""
Convert the .csv file provided for the UBC data set to a
COCO-camera-traps .json file
""
"Images were provided in eight folders, each of which contained a .csv"
file with annotations.  Those annotations came in two slightly different
"formats, the two formats corresponding to folders starting with ""SC_"" and"
otherwise.
""
%% Constants and environment
Map Excel column names - which vary a little across spreadsheets - to a common set of names
%% Enumerate images
Load from file if we've already enumerated
%% Create CCT dictionaries
Force the empty category to be ID 0
To simplify debugging of the loop below
#%% Create CCT dictionaries (loop)
#%%
Read source data for this folder
Rename columns
Folder name is the first two characters of the filename
""
Create relative path names from the filename itself
Folder name is the camera name
""
Create relative path names from camera name and filename
Which of our images are in the spreadsheet?
i_row = 0; fn = input_metadata['image_relative_path'][i_row]
#%% Check for images that aren't included in the metadata file
Find all the images in this folder
Which of these aren't in the spreadsheet?
#%% Create entries in CCT dictionaries
Only process images we have on disk
"This is redundant, but doing this for clarity, at basically no performance"
cost since we need to *read* the images below to check validity.
i_row = row_indices[0]
"These generally represent zero-byte images in this data set, don't try"
to find the very small handful that might be other kinds of failures we
might want to keep around.
print('Error opening image {}'.format(image_relative_path))
If we've seen this category before...
...make sure it used the same latin --> common mapping
""
"If the previous instance had no mapping, use the new one."
assert common_name == category['common_name']
Create an annotation
...for each annotation we found for this image
...for each image
...for each dataset
Print all of our species mappings
"%% Copy images for which we actually have annotations to a new folder, lowercase everything"
im = images[0]
%% Create info struct
"%% Convert image IDs to lowercase in annotations, tag as sequence level"
"While there isn't any sequence information, the nature of false positives"
"here leads me to believe the images were labeled at the sequence level, so"
we should trust labels more when positives are verified.  Overall false
positive rate looks to be between 1% and 5%.
%% Write output
%% Validate output
%% Preview labels
""
helena_to_cct.py
""
Convert the Helena Detections data set to a COCO-camera-traps .json file
""
%% Constants and environment
This is one time process
%% Create Filenames and timestamps mapping CSV
import pdb;pdb.set_trace()
%% To create CCT JSON for RSPB dataset
%% Read source data
Original Excel file had timestamp in different columns
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
%% Skipping this check because one image has multiple species
assert len(duplicate_rows) == 0
%% Check for images that aren't included in the metadata file
%% Create CCT dictionaries
Map categories to integer IDs
""
The category '0' is reserved for 'empty'
For each image
Don't include images that don't exist on disk
Some filenames will match to multiple rows
assert(len(rows) == 1)
iRow = rows[0]
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
ann['datetime'] = row['datetime']
ann['site'] = row['site']
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
%% Imports and constants
from github.com/microsoft/ai4eutils
from github.com/microsoft/cameratraps
A list of files in the lilablobssc container for this data set
The raw detection files provided by NOAA
A version of the above with filename columns added
%% Read input .csv
%% Read list of files
%% Convert paths to full paths
i_row = 0; row = df.iloc[i_row]
assert ir_image_path in all_files
...for each row
%% Write results
"%% Load output file, just to be sure"
%% Render annotations on an image
i_image = 2004
%% Download the image
%% Find all the rows (detections) associated with this image
"as l,r,t,b"
%% Render the detections on the image(s)
In pixel coordinates
In pixel coordinates
%% Save images
%% Clean up
""
carrizo_trail_cam_2017.py
""
"Convert the .csv files provided for the ""Trail Cam Carrizo"" 2017 data set to"
a COCO-camera-traps .json file.
""
%% Constants and environment
%% Read source data
Removing the empty records
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
908 missing files (of 60562)
%% Check for images that aren't included in the metadata file
105329 of 164983 files are not in metadata
%% Create CCT dictionaries
Map categories to integer IDs
""
The category '0' is reserved for 'empty'
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
Each filename should just match one row
Don't include images that don't exist on disk
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
""
channel_islands_to_cct.py
""
Convert the Channel Islands data set to a COCO-camera-traps .json file
""
"Uses the command-line tool ExifTool (exiftool.org) to pull EXIF tags from images,"
"because every Python package we tried failed to pull the ""Maker Notes"" field properly."
""
"%% Imports, constants, paths"
# Imports ##
# Constants ##
# Paths ##
Confirm that exiftool is available
"assert which(exiftool_command_name) is not None, 'Could not locate the ExifTool executable'"
%% Load information from every .json file
"Ignore the sample file... actually, first make sure there is a sample file"
...and now ignore that sample file.
json_file = json_files[0]
ann = annotations[0]
...for each annotation in this file
...for each .json file
"%% Confirm URL uniqueness, handle redundant tags"
Have we already added this image?
"One .json file was basically duplicated, but as:"
""
Ellie_2016-2017 SC12.json
Ellie_2016-2017-SC12.json
"If the new image has no output, just leave the old one there"
"If the old image has no output, and the new one has output, default to the one with output"
Don't worry about the cases where someone tagged 'fox' and someone tagged 'fox_partial'
...for each image we've already added
...if this URL is/isn't in the list of URLs we've already processed
...for each image
%% Save progress
%%
%%
%% Download files (functions)
https://www.quickprogrammingtips.com/python/how-to-download-multiple-files-concurrently-in-python.html
"This is returned with a leading slash, remove it"
%% Download files (execution)
%% Read required fields from EXIF data (functions)
"-G means ""Print group name for each tag"", e.g. print:"
""
[File]          Bits Per Sample                 : 8
""
...instead of:
""
Bits Per Sample                 : 8
"If we don't get any EXIF information, this probably isn't an image"
line_raw = exif_lines[0]
"Split on the first occurrence of "":"""
Typically:
""
"'[MakerNotes]    Sequence                        ', '1 of 3']"
Not a typo; we are using serial number as a location
"If there are multiple timestamps, make sure they're *almost* the same"
"If there are multiple timestamps, make sure they're *almost* the same"
...for each line in the exiftool output
"This isn't directly related to the lack of maker notes, but it happens that files that are missing"
maker notes also happen to be missing EXIF date information
...process_exif()
"This is returned with a leading slash, remove it"
Ignore non-image files
%% Read EXIF data (execution)
ann = images[0]
%% Save progress
Use default=str to handle datetime objects
%%
%%
"Not deserializing datetimes yet, will do this if I actually need to run this"
%% Check for EXIF read errors
%% Remove junk
Ignore non-image files
%% Fill in some None values
"...so we can sort by datetime later, and let None's be sorted arbitrarily"
%% Find unique locations
%% Synthesize sequence information
Sort images by time within each location
i_location=0; location = locations[i_location]
previous_datetime = sorted_images_this_location[0]['datetime']
im = sorted_images_this_camera[1]
"Timestamp for this image, may be None"
Start a new sequence if:
""
* This image has no timestamp
* This image has a frame number of zero
* We have no previous image timestamp
""
Start a new sequence if necessary
...for each image in this location
...for each location
%% Count frames in each sequence
images_this_sequence = [im for im in images if im['seq_id'] == seq_id]
"%% Create output filenames for each image, store original filenames"
i_location = 0; location = locations[i_location]
i_image = 0; im = sorted_images_this_location[i_image]
%% Save progress
Use default=str to handle datetime objects
%%
%%
%% Copy images to their output files (functions)
%% Copy images to output files (execution)
%% Rename the main image list for consistency with other scripts
%% Create CCT dictionaries
Make sure this is really a box
Transform to CCT format
Force the empty category to be ID 0
i_image = 0; input_im = all_image_info[0]
"This issue only impacted one image that wasn't a real image, it was just a screenshot"
"showing ""no images available for this camera"""
Convert datetime if necessary
Process temperature if available
Read width and height if necessary
I don't know what this field is; confirming that it's always None
Process object and bbox
os.startfile(output_image_full_path)
"Zero is hard-coded as the empty category, but check to be safe"
"I can't figure out the 'index' field, but I'm not losing sleep about it"
assert input_annotation['index'] == 1+i_ann
"Some annotators (but not all) included ""_partial"" when animals were partially obscured"
"Annotators *mostly* used 'none', but sometimes 'empty'.  'empty' is CCT-correct."
If we've seen this category before...
If this is a new category...
...if this is an empty/non-empty annotation
Create an annotation
...for each annotation on this image
...for each image
%% Change *two* annotations on images that I discovered contains a human after running MDv4
%% Move human images
ann = annotations[0]
%% Count images by location
%% Write output
%% Validate output
%% Preview labels
viz_options.classes_to_exclude = [0]
""
ena24_to_json_2017.py
""
Convert the ENA24 data set to a COCO-camera-traps .json file
""
%% Constants and environment
Temporary folders for human and non-human images
Clean existing output folders/zipfiles
%% Support functions
%% Read source data
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
Check image height and width
"Each row is category, [box coordinates]"
"If there's just one row, loadtxt reads it as a 1d array; make it a 2d array"
with one row
Each row is a bounding box
Have we seen this category before?
Create an annotation
...for each bounding box
"This was here for debugging; nearly every instance is Human+Horse, Human+Vehicle,"
"or Human+Dog, but there is one Rabbit+Opossium, and a few Deer+Chicken!"
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Create ZIP files for human and non human
%% Validate output
%% Preview labels
%% Imports and constants
configurations and paths
%% Helper functions
"dest_path = copy(source_path, dest_folder)"
num_workers = multiprocessing.cpu_count()
pool = ThreadPool(num_workers)
"results = pool.starmap(_copy_unzip, zip(sources, itertools.repeat(dest_folder)))"
""
print('Waiting for processes to finish...')
pool.close()
pool.join()
sequential
%% Command-line driver
if the blob container is already mounted on the VM
or you can download them using the storage Python SDK
store storage account key in environment variable AZ_STORAGE_KEY
""
eMammal_helpers.py
""
Support functions for processing eMammal metadata
""
%% Constants and imports
%% Support functions
"pad to a total of 3 digits if < 1000, or 4 digits otherwise"
img_frame is a string from the xml tree
"length 4 frame order is returned as is, others are left padded to be 3 digit long"
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels
pad to a total of 4 digits
img_frame is a string from the xml tree
"length 4 frame order is returned as is, others are left padded to be 3 digit long"
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels
others column
summer day hours: 6am - 7pm
others day hours: 7am - 6pm
""
make_eMammal_json.py
""
"Produces the COCO-formatted json database for an eMammal dataset, i.e. a"
"collection of folders, each of which contains a deployment_manifest.xml file."
""
"In this process, each image needs to be loaded to size it."
""
"To add bounding box annotations to the resulting database, use"
add_annotations_to_eMammal_json.py.
""
%% Constants and imports
"Either add the eMammal directory to your path, or run from there"
os.chdir(r'd:\git\CameraTraps\database_tools\eMammal')
import warnings
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
warnings.filterwarnings('ignore')
Should we run the image size retrieval in parallel?
%% Support functions
%% Main loop (metadata processing; image sizes are retrieved later)
deployment = folders[0]
sequence = image_sequences[0]
get species info for this sequence
add each image's info to database
img = images[0]
"some manifests don't have the ImageOrder info, but the info is in the file name"
full_img_id has no frame info
""
frame number only used in requests to iMerit for ordering
...for each image
...for each sequence
...for each deployment
%% Get image sizes
"'tasks' is currently a list of 2-tuples, with each entry as [image dictionary,path]."
""
"Go through that and copy just the image dictionaries to 'db_images', adding size"
information to each entry.  Takes a couple hours.
opening each image seems too fast for this multi-threaded version to be faster than sequential code.
%% Assemble top-level dictionaries
%% Write out .json
""
make_full_SS_json.py
""
Create a COCO-camera-traps .json file for Snapshot Serengeti data from
the original .csv files provided on Dryad.
""
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no"
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated
Snapshot Safari preparation process.
""
%% Imports and constants
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays"
Count the number of images with multiple species
"%% Read image .csv file, format into a dictionary mapping images to capture events"
%% Create CCT-style .json
still need image width and height
...for each image
%% Write output files
""
make_per_season_SS_json.py
""
Create a COCO-camera-traps .json file for each Snapshot Serengeti season from
the original .csv files provided on Dryad.
""
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no"
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated
Snapshot Safari preparation process.
""
%% Imports and constants
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays"
"%% Read image .csv file, format into a dictionary mapping images to capture events"
%% Create CCT-style .json
still need image width and height
...for each image ID
%% Write output files
...for each season
%% Configurations
"if the following two files are provided, only the tfrecord generation section will be executed"
proceed to generate tfrecords if True
approximate fraction to split the new entries by
categories in the database to include
"in addition, any images with a 'group' label will not be included to avoid confusion"
see 'image_contains_group' in create_tfrecords_format.py
%% Input validation
%% Convert the COCO Camera Trap format data to another json
that aligns with the fields in the resulting tfrecords
%% Make train/val/test splits
%% Write the tfrecords
want the file names of all tfrecords to be of the same format and length in each part
%% Parameters
%% Load the annotations queried from megadb
"%% Make the ""dataset"" required by create_tfrecords.py"
%% Create tfrecords
Construct a Reader to read examples from the .tfrecords file
Basic info
Print out the per class image counts
Can we detect if there any missing classes?
"We expect class id for each value in the range [0, max_class_id]"
So lets see if we are missing any of these values
Construct a Reader to read examples from the .tfrecords file
Reversed coordinates?
Too small of an area?
Basic info
"print(""Images with areas < 10:"")"
for img_id in images_with_small_bboxes:
print(img_id)
"print(""Images with reversed coordinates:"")"
for img_id in images_with_reversed_coords:
print(img_id)
for img_id in images_with_bbox_count_mismatch:
print(img_id)
""
read_from_tf_records.py
""
"Reads detection results from a tfrecords file of the style generated by the TFODAPI inference script,"
"and converts it to a .p file that's friendly to other tools in this repo, e.g. detection/detector_eval."
""
"Detection and ground truth bounding box coordinates are in the format of [ymin, xmin, ymax, xmax]."
""
coding: utf-8
In[1]:
In[2]:
In[ ]:
Defaults are not specified since both keys are required.
"image, label, height, width"
"print(sess.run([output['image/filename'], output['image/class/text'], output['image/class/label'], output['image/height'], output['image/width']]))"
tasks = list(glob.glob('/data/lila/nacti/cropped_tfrecords/t*'))
"for _ in tqdm.tqdm(p.imap(analyze_record, tasks, chunksize=10), total=len(tasks)):"
pass
Parallel(n_jobs=8)(delayed(analyze_record)(path) for path in list(glob.glob('/data/lila/nacti/cropped_tfrecords/t*')))
""
iterate_tf_records.py
""
Inherited from Visipedia tfrecords repo.
""
print(feature_key)
return a dictionary of the features
Construct a Reader to read examples from the .tfrecords file
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Required
Class label for the whole image
Objects
Bounding Boxes
Parts
Areas
Ids
Any extra data (e.g. stringified json)
Additional fields for the format needed by the Object Detection repository
"For explanation of the fields, see https://github.com/visipedia/tfrecords"
Additional fields for format needed by Object Detection repository
Create a single Session to run all image coding calls.
Initializes function that converts PNG to JPEG data.
Initializes function that decodes RGB JPEG data.
Convert the image data from png to jpg
Decode the image data as a jpeg image
Read the image file.
Clean the dirty data.
Decode the RGB JPEG.
Check that image converted to RGB
Each thread produces N shards where N = int(num_shards / num_threads).
"For instance, if num_shards = 128, and the num_threads = 2, then the first"
"thread would produce shards [0, 64)."
"Generate a sharded version of the file name, e.g. 'train-00002-of-00010'"
raise
Images in the tfrecords set must be shuffled properly
"Break all images into batches with a [ranges[i][0], ranges[i][1]]."
Launch a thread for each batch.
Create a mechanism for monitoring when all threads are finished.
Create a generic TensorFlow-based utility for converting all image codings.
A Queue to hold the image examples that fail to process.
Wait for all the threads to terminate.
Collect the errors
""
create_splits.py
""
"Based on a tfrecords_format json file of the database, creates 3 splits according to"
the specified fractions based on location (images from the same location should be in
one split) or based on images.
""
"If a previous split is provided (append_to_previous_split is True), the entries in"
"each split will be preserved, and new entries will be appended, so that new models"
can warm start with a model trained on the original splits.
""
%% Imports
%% Main function
"code below references 'locatioin' as the attribute to split on, but it works for any split_by attribute"
present in the image entryes
"find new locations and assign them to a split, without reassigning any previous locations"
do NOT sort the IDs to keep the shuffled order
""
create_tfrecords_format.py
""
This script converts a COCO formatted json database to another json file that would be the
input to create_tfrecords.py or similar scripts to create tf_records
""
%% Imports and environment
%% Main tfrecord generation function
Remap category IDs; TF needs consecutive category ids
Sanity-check number of empty annotations and annotation-less images
Images without annotations don't have bounding boxes
Images with annotations *may* have bounding boxes
"prepend the dataset name to image_id because after inference on val set, records from"
different datasets are stored in one tfrecord
Propagate optional metadata to tfrecords
checking to ignore any images that contain 'group' needs to happen before ignoring non-valid categories!
Only include valid categories
...for each annotation for the current image
...for each image
""
create_tfrecords_from_coco.py
""
This script creates a tfrecords file from a classification dataset in COCO format.
%% Imports and environment
%% Main tfrecord generation function
We remap all category IDs such that they are consecutive starting from zero
"If this is already the case for the input dataset, then the remapping will not"
"have any effect, i.e. the order of the classes will remain unchanged"
Propagate optional metadata to tfrecords
endfor each annotation for the current image
endfor each image
"Calculate number of shards to get the desired number of images per record,"
ensure it is evenly divisible by the number of threads
%% Command-line driver
%% Driver
""
create_lila_test_set.py
""
"Create a test set of camera trap images, containing N empty and N non-empty"
images from each LILA data set.
""
%% Constants and imports
LILA camera trap master metadata file
"We'll write images, metadata downloads, and temporary files here"
%% Support functions
remove the leading '/'
%% Download and parse the metadata file
Put the master metadata file in the same folder where we're putting images
Read lines from the master metadata file
Parse those lines into a table
Each line in this file is name/base_url/json_url/[box_url]
%% Download and extract metadata for every dataset
ds_name = (list(metadata_table.keys()))[0]
Unzip if necessary
...for each dataset of interest
%% Choose images from each dataset
ds_name = (list(metadata_table.keys()))[0]
...for each dataset
%% Convert to URLs
ds_name = (list(metadata_table.keys()))[0]
Retrieve image file names
Convert to URLs
...for each dataset
%% Download those image files
ds_name = (list(metadata_table.keys()))[0]
"This URL may not be a SAS URL, we will remove a SAS token if it's present"
url = urls_to_download[0]
...for each url
...for each dataset
""
"Generate read-only SAS URLs for all LILA containers, to facilitate partial downloads."
""
The results of this script end up here:
""
http://lila.science/wp-content/uploads/2020/03/lila_sas_urls.txt
""
"Update: that file is manually maintained now, it can't be programmatically generated"
""
%% Imports
Read-only
%% Enumerate containers
%% Generate SAS tokens
%% Generate SAS URLs
%% Write to output file
""
get_lila_category_list.py
""
Example of making a text file listing all category names in specific LILA datasets
""
%% Constants and imports
LILA camera trap master metadata file
array to fill for output
# datasets and categories to look at
"if False, will only collect data for categories in datasets_of_interest"
only need if restrict_category is false
"We'll write images, metadata downloads, and temporary files here"
%% Support functions
remove the leading '/'
%% Download and parse the metadata file
Put the master metadata file in the same folder where we're putting images
Read lines from the master metadata file
Parse those lines into a table
Each line in this file is name/sas_url/json_url/[bbox_json_url]
Create a separate entry for bounding boxes if they exist
%% Download and extract metadata for the datasets we're interested in
Unzip if necessary
...for each dataset of interest
%% Get category names
Open the metadata file
Collect list of categories and mappings to category name
Append category to categories_list
%% Save category names to file
""
download_lila_subset.py
""
"Example of how to download a list of files from LILA, e.g. all the files"
in a data set corresponding to a particular species.
""
%% Constants and imports
LILA camera trap master metadata file
"In this example, we're using the Missouri Camera Traps data set and the Caltech Camera Traps dataset"
All lower-case; we'll convert category names to lower-case when comparing
"We'll write images, metadata downloads, and temporary files here"
"We will demonstrate two approaches to downloading, one that loops over files"
"and downloads directly in Python, another that uses AzCopy."
""
AzCopy will generally be more performant and supports resuming if the
transfers are interrupted.  This script assumes that azcopy is on the system path.
Number of concurrent download threads (when not using AzCopy) (AzCopy does its
own magical parallelism)
%% Support functions
remove the leading '/'
%% Download and parse the metadata file
Put the master metadata file in the same folder where we're putting images
Read lines from the master metadata file
Parse those lines into a table
Each line in this file is name/base_url/json_url/[box_url]
%% Download and extract metadata for the datasets we're interested in
Unzip if necessary
...for each dataset of interest
%% List of files we're going to download (for all data sets)
"Flat list or URLS, for use with direct Python downloads"
For use with azcopy
This may or may not be a SAS URL
# Open the metadata file
# Build a list of image files (relative path names) that match the target species
Retrieve all the images that match that category
Retrieve image file names
Convert to URLs
...for each dataset
%% Download those image files
ds_name = 'Caltech Camera Traps'
ds_name = 'SWG Camera Traps'
We want to use the whole relative path for this script (relative to the base of the container)
"to build the output filename, to make sure that different data sets end up in different folders."
This may or may not be a SAS URL
For example:
""
caltech-unzipped/cct_images
swg-camera-traps
Check whether the URL includes a folder
E.g. caltech-unzipped
E.g. cct_images
E.g. swg-camera-traps
"Write out a list of files, and use the azcopy ""list-of-files"" option to download those files."
""
This azcopy feature is unofficially documented at:
""
https://github.com/Azure/azure-storage-azcopy/wiki/Listing-specific-files-to-transfer
""
import clipboard; clipboard.copy(cmd)
Loop over files
""
get_lila_category_counts.py
""
Example of how to create a csv of category counts per dataset (at '<output_dir> +
category_counts.xlsx') and a coco_file (at'<output_dir> + compiled_coco.json') for
all identified images of the given category over the given datasets.
""
You can constrain the category and datasets looked at in code below.
""
Columns in the csv are:
number of images labeled with given category(im_cnt)
number of images with a bbox labeled with given category (bbox_im_cnt)
number of bbox's labeled with given category (bbox_cnt)
""
This script also outputs two pickle files which contain dictionaries.
""
The 'total_category_counts_by_set.pkl' file contains a dictionary of dictionaries of the form:
""
{<dataset-with-image-level-annotations>:
"{<category> : { 'image_urls': [<image-urls-of-images-with-category>],"
"'im_cnt :<number>},"
"<category2>: ...},"
<dataset-with-bbox-level-annotations>:
"{<category> : { 'image_urls': [<image-urls-of-images-with-category>],"
"'bbox_im_cnt :<number>},"
"'bbox_cnt':<number>},"
"<category2>: ...},"
<dataset3>: ...}
""
The 'total_category_counts.pkl' file contains a dictionary of dictionaries of the form:
""
"{<category> : { 'image_urls': [<image-urls-of-images-with-category>],"
"'im_cnt :<number,"
"'bbox_im_cnt :<number>},"
"'bbox_cnt':<number>},"
<category2>: ...}
""
%% Constants and imports
LILA camera trap master metadata file
# dictionaries to fill for output
category count over all datasets
category count seperated by datasets
# datasets and categories to look at
"if False, will only collect data from datasets_of_interest"
only meaningful if use_all_datasets is false
"if False, will only collect data for categories in categories_of_interest"
only meaningly if use_all_categories is False
"How the categories should be labeled in the csv. key is label in lila dataset,"
value is label to use in csv
"We'll write images, metadata downloads, and temporary files here"
%% Support functions
%% Download and parse the metadata file
Put the master metadata file in the same folder where we're putting images
Read lines from the master metadata file
Parse those lines into a table
Each line in this file is name/sas_url/json_url/[bbox_json_url]
Create a separate entry for bounding boxes if they exist
%% Download and extract metadata for the datasets we're interested in
Unzip if necessary
...for each dataset of interest
%% Count categories
# Open the metadata file
"Double check the annotations url provided corresponds to that implied by ds_name, or else fix"
Only need to look at first entry b/c json files with image-level annotations
are seperated from those with box-level annotations.
Build a list of image files (relative path names) that match the target categories
Add categories entry to total_category_counts if not already present
Retrieve all the images that match that category
Convert to URLs and add to category_counts dicts
Record total category count in dataset
Add relevant annotations to custom coco file
%% Save output COCO files
%% Save category counts to csv
%% Save dictionary of category counts and image urls to file
""
add_url_to_database.py
""
"Adds a ""url"" field to a coco-camera-traps .json database, specifically to allow the db to"
be reviewed in the Visipedia annotation tool.
""
""
remove_corrupted_images_from_database.py
""
"Given a coco-camera-traps .json file, checks all images for TF-friendliness and generates"
a new .json file that only contains the non-corrupted images.
""
%% Imports and constants
import multiprocessing
"I leave this at an annoying low number, since by definition weird stuff will"
"be happening in the TF kernel, and it's useful to keep having content in the console."
%% Function definitions
"I sometimes pass in a list of images, sometimes a dict with a single"
element mapping a job ID to the list of images
"We're about to start a lot of TF sessions, and we don't want gobs"
of debugging information printing out for every session.
At some point we were creating a single session and looping over images
"within that session, but the only way I found to reliably not run out"
of GPU memory was to create a session per image and gc.collect() after
each session.
Map Image IDs to boolean (should I keep this image?)
"Convert to lists, append job numbers to the image lists"
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)"
Merge results
%% Interactive driver
%%
base_dir = r'D:\temp\snapshot_serengeti_tfrecord_generation'
Load annotations
Check for corruption
Write out only the uncorrupted data
%% Command-line driver
#######
""
integrity_check_json_db.py
""
"Does some integrity-checking and computes basic statistics on a db, specifically:"
""
* Verifies that required fields are present and have the right types
* Verifies that annotations refer to valid images
* Verifies that annotations refer to valid categories
"* Verifies that image, category, and annotation IDs are unique"
""
* Optionally checks file existence
""
* Finds un-annotated images
* Finds unused categories
""
* Prints a list of categories sorted by count
""
#######
%% Constants and environment
%% Functions
"If baseDir is non-empty, checks image existence"
This is used in a medium-hacky way to share modified options across threads
print('Image path {} does not exist'.format(filePath))
"#%% Read .json file if necessary, integrity-check fields"
info = data['info']
"#%% Build dictionaries, checking ID uniqueness and internal validity as we go"
Confirm that required fields are present
Confirm ID uniqueness
...for each category
image = images[0]
Confirm that required fields are present
Confirm ID uniqueness
We previously supported ints here; this should be strings now
"assert isinstance(image['location'], str) or isinstance(image['location'], int), 'Illegal image location type'"
Are we checking for unused images?
Recursively enumerate images
print('Image {} is unused'.format(p))
Are we checking file existence and/or image size?
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)"
"print('Image validation error for image {} ({})'.format(iImage,images[iImage]['file_name']))"
...for each image
Confirm that required fields are present
Confirm ID uniqueness
Confirm validity
...for each annotation
#%% Print statistics
Find un-annotated images and multi-annotation images
Find unused categories
Prints a list of categories sorted by count
https://stackoverflow.com/questions/72899/how-do-i-sort-a-list-of-dictionaries-by-a-value-of-the-dictionary
...def integrity_check_json_db()
%% Command-line driver
"python integrity_check_json_db.py ""e:\wildlife_data\wellington_data\wellington_camera_traps.json"" --baseDir ""e:\wildlife_data\wellington_data\images"" --bFindUnusedImages --bCheckImageSizes"
"python integrity_check_json_db.py ""D:/wildlife_data/mcgill_test/mcgill_test.json"" --baseDir ""D:/wildlife_data/mcgill_test"" --bFindUnusedImages --bCheckImageSizes"
"Here the '-u' prevents buffering, which makes tee happier"
""
python -u integrity_check_json_db.py '/datadrive1/nacti_metadata.json' --baseDir '/datadrive1/nactiUnzip/' --bFindUnusedImages --bCheckImageSizes | tee ~/nactiTest.out
%% Interactive driver(s)
%%
Integrity-check .json files for LILA
options.iMaxNumImages = 10
""
combine_two_json_files.py
""
"Merges two coco-camera-traps .json files. In particular, categories are combined and re-numbered."
""
Combined Info
Combined Images
Combined Categories
## categories to merge
Combined Annotations
""
make_detection_db_for_viewing.py
""
"Given a .json file with ground truth bounding boxes, and a .p file containing detections for the same images,"
"creates a new .json file with separate classes for ground truth and detection, suitable for viewing in the Visipedia"
annotation tool.
""
%% Imports and constants
%% Main function
im_id_to_im = {im['id']:im for im in images}
make new categories to distinguish between ground truth and detections
"update all gt annotations to be class ""gt"""
collect all detections by image
keep any detection with score above det_thresh
"need to convert bbox from [x1,y1,x2,y2] to [x,y,w,h]"
"add ""info"" and ""licenses"" for annotation tools to function"
create new db
%% Command-line handling
""
analyze_json_database.py
""
Plots location/class/etc. distributions for classes in a coco-camera-traps .json file.
""
Currently includes some one-off code for specific species.
""
%% Constants and imports
%% Path configuration
%% Load source data
%% Build image/category dictionaries
%% Make plot of category distribution
%% make plots of location distribution
"plt.title('Number of images per location, by category')"
"plt.tight_layout(rect=[0,0,1,0.9])"
#make plot of images per season
%% Make plot of lions per location
%% Make plot of elephants per location
for loc in sorted_by_total[:25]:
"print('Location:' + loc[0] +', Lions: ' + str(len(loc_to_lion_ims[loc[0]])) + ', Elephants: ' + str(len(loc_to_elephant_ims[loc[0]])) + ', Total ims: ' + str(len(loc_to_ims[loc[0]])))"
""
subset_json_db.py
""
Select a subset of images (and associated annotations) from a .json file
in COCO Camera Traps format.
""
"To subset the .json files produced by our batch processing API, see"
subset_json_detector_output.py
""
Sample invocation:
""
"python subset_json_db.py ""E:\Statewide_wolf_container\idfg_20190409.json"" ""E:\Statewide_wolf_container\idfg_20190409_clearcreek.json"" ""clearcreek"" --ignore_case"
""
%% Constants and imports
%% Functions
Load the input file if necessary
Find images matching the query
Find annotations referring to those images
Write the output file if requested
%% Interactive driver
%%
%% Command-line driver
%% Constants and imports
%% Merge functions
i_input_dict = 0; input_dict = input_dicts[i_input_dict]
We will prepend an index to every ID to guarantee uniqueness
Map detection categories from the original data set into the merged data set
...for each category
Merge original image list into the merged data set
Create a unique ID
...for each image
Same for annotations
...for each annotation
"Merge info dicts, don't check completion time fields"
...for each dictionary
Convert merged image dictionaries to a sorted list
%% Driver
""
add_width_and_height_to_database.py
""
Grabs width and height from actual image files for a .json database that is missing w/h.
""
Originally used when we created a .json file for snapshot serengeti from .csv.
""
"This file converts the JSON output of the batch processing API to a pickle file, which can be used by the"
script ./make_classification_dataset.py
"Convert boxes from JSON   [x_min, y_min, width_of_box, height_of_box]"
"to PICKLE [ymin,  xmin,  ymax,         xmax]"
Write detections to file with pickle
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the"
script ./make_classification_dataset.py
Write detections to file with pickle
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the"
script ./make_classification_dataset.py
Assumes that the root of the CameraTrap repo is in the PYTHONPATH
Minimum threshold to put a detection into the output JSON file
Parameter check
Load detections from input
Load COCO style annotations
Build output JSON in format version 1.0
Adding the only known metadata info
"The pickle file does not contain category information, so we assume the default"
For each image with detections
for each detection
"Convert boxes from [ymin, xmin, ymax, xmax] format to"
"[x_min, y_min, width_of_box, height_of_box]"
Write output json
global flag for whether or not we encounter missing images
"- will only print ""missing image"" warning once"
TFRecords variables
1.3 for the cropping during test time and 1.3 for the context that the
CNN requires in the left-over image
Create output directories
Load COCO style annotations from the input dataset
"Get all categories, their names, and create updated ID for the json file"
Prepare the coco-style json files
Split the dataset by locations
Load detections
Write out COCO-style json files to the output directory
Write detections to file with pickle
## Preparations: get all the output tensors
For all images listed in the annotations file
Skip the image if it is annotated with more than one category
"Get ""old"" and ""new"" category ID and category name for this image."
Skip if in excluded categories.
get path to image
"If we already have detection results, we can use them"
Otherwise run detector and add detections to the collection
Only select detections with confidence larger than DETECTION_THRESHOLD
Skip image if no detection selected
whether it belongs to a training or testing location
Skip images that we do not have available right now
- this is useful for processing parts of large datasets
Load image
Run inference
"remove batch dimension, and convert from float32 to appropriate type"
convert normalized bbox coordinates to pixel coordinates
Pad the detected animal to a square box and additionally by
"PADDING_FACTOR. The result will be in crop_boxes. However, we need to make"
sure that its box coordinates are still within the image.
"for each bounding box, crop the image to the padded box and save it"
"Create the file path as it will appear in the annotation json,"
adding the box number if there are multiple boxes
"if the cropped file already exists, verify its size"
Add annotations to the appropriate json
js_keys = ['/'.join(im['file_name'].split('/')[1:])[:-4] for im in js['images']]
for tk in js_keys:
"assert np.isclose(1, np.sum(detections[tk]['detection_scores'] > 0.5))"
""
separate_detections_by_size
""
Not-super-well-maintained script to break a list of API output files up
based on bounding box size.
""
%% Imports and constants
Folder with one or more .json files in it that we want to split up
Enumerate .json files
Define size thresholds and confidence thresholds
"Not used directly in this script, but useful if we want to generate previews"
%% Split by size
For each size threshold...
For each file...
fn = input_files[0]
Just double-checking; we already filtered this out above
Don't reprocess .json files we generated with this script
Load the input file
For each image...
1.1 is the same as infinity here; no box can be bigger than a whole image
What's the smallest detection above threshold?
"[x_min, y_min, width_of_box, height_of_box]"
""
size = w * h
...for each detection
Which list do we put this image on?
...for each image in this file
Make sure the number of images adds up
Write out all files
...for each size threshold
...for each file
""
rde_debug.py
""
Some useful cells for comparing the outputs of the repeat detection
"elimination process, specifically to make sure that after optimizations,"
results are the same up to ordering.
""
%% Compare two RDE files
i_dir = 0
break
"Regardless of ordering within a directory, we should have the same"
number of unique detections
Re-sort
Make sure that we have the same number of instances for each detection
Make sure the box values match
""
ocr_sandbox.py
""
sandbox for experimenting with using OCR to pull metadata from camera trap images
""
The general approach is:
""
"* Crop a fixed percentage from the top and bottom of an image, slightly larger"
than the largest examples we've seen of how much space is used for metadata.
""
"* Refine that crop by blurring a little, then looking for huge peaks in the"
"color histogram suggesting a solid background, then finding rows that are"
mostly that color.
""
"* Crop to the refined crop, then run pytesseract to extract text"
""
"* Use regular expressions to find time and date, in the future can add, e.g.,"
"temperature (which is often present *only* in the images, unlike time/date which"
are also usually in EXIF but often wrong or lost in processing)
""
"The metadata extraction (EXIF, IPTC) here is just sample code that seemed to"
belong in this file.
""
Contact: Dan Morris (dan@microsoft.com)
""
%% Constants and imports
pip install pytesseract
""
"Also intall tesseract from: https://github.com/UB-Mannheim/tesseract/wiki, and add"
"the installation dir to your path (on Windows, typically C:\Program Files (x86)\Tesseract-OCR)"
pip install IPTCInfo3
from the ai4eutils repo: https://github.com/Microsoft/ai4eutils
""
"Only used for writing out a summary, not important for the core metadata extraction"
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Using a semi-arbitrary metric of how much it feels like we found the
"text-containing region, discard regions that appear to be extraction failures"
Pad each crop with a few pixels to make tesseract happy
Discard text from the top
"When we're looking for pixels that match the background color, allow some"
tolerance around the dominant color
We need to see a consistent color in at least this fraction of pixels in our rough
crop to believe that we actually found a candidate metadata region.
"What fraction of the [top,bottom] of the image should we use for our rough crop?"
A row is considered a probable metadata row if it contains at least this fraction
"of the background color.  This is used only to find the top and bottom of the crop area,"
"so it's not that *every* row needs to hit this criteria, only the rows that are generally"
above and below the text.
%% Support functions
"%% Load some images, pull EXIF and IPTC data for fun"
%% Rough crop
"This will be an nImages x 1 list of 2 x 1 lists (image top, image bottom)"
image = images[0]
"l,t,r,b"
""
"0,0 is upper-left"
"%% Close-crop around the text, return a revised image and success metric"
Did we find a sensible mode that looks like a background value?
"This looks very scientific, right?  Definitely a probability?"
"print('Failed min background fraction test: {} of {}'.format(pBackGroundValue,minBackgroundFraction))"
"Notes to self, things I tried that didn't really go anywhere..."
"analysisImage = cv2.blur(analysisImage, (3,3))"
"analysisImage = cv2.medianBlur(analysisImage,5)"
"analysisImage = cv2.Canny(analysisImage,100,100)"
imagePil = Image.fromarray(analysisImage); imagePil
Use row heuristics to refine the crop
""
This egregious block of code makes me miss my fluency in Matlab.
"print('Cropping to {},{},{},{}'.format(x,y,w,h))"
Crop the image
"For some reason, tesseract doesn't like characters really close to the edge"
imagePil = Image.fromarray(croppedImage); imagePil
%% Go to OCR-town
"An nImages x 2 list of strings, extracted from the top and bottom of each image"
An nImages x 2 list of cropped images
iImage = 0; iRegion = 1; regionSet = imageRegions[iImage]; region = regionSet[iRegion]
text = pytesseract.image_to_string(region)
pil --> cv2
"image = cv2.medianBlur(image, 3)"
"image = cv2.erode(image, None, iterations=2)"
"image = cv2.dilate(image, None, iterations=4)"
"image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]"
"image = cv2.blur(image, (3,3))"
"image = cv2.copyMakeBorder(image,10,10,10,10,cv2.BORDER_CONSTANT,value=[0,0,0])"
"text = pytesseract.image_to_string(imagePil, lang='eng')"
https://github.com/tesseract-ocr/tesseract/wiki/Command-Line-Usage
"psm 6: ""assume a single uniform block of text"""
""
...for each cropped region
...for each image
%% Extract dates and times
s = '1:22 pm'
s = '1:23:44 pm'
%% Write results to a handy html file
Add image name and resized image
Add results and individual region images
"textStyle = ""font-family:calibri,verdana,arial;font-weight:bold;font-size:150%;text-align:left;margin-left:50px;"""
%% Scrap
Alternative approaches to finding the text/background  region
Using findCountours()
imagePil = Image.fromarray(analysisImage); imagePil
"analysisImage = cv2.erode(analysisImage, None, iterations=3)"
"analysisImage = cv2.dilate(analysisImage, None, iterations=3)"
"analysisImage = cv2.threshold(analysisImage, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]"
Find object with the biggest bounding box
Using connectedComponents()
analysisImage = image
print('Found {} components'.format(nb_components))
We just want the *background* image
""
"Takes a .json file with MD results for an individual video, and converts to a .csv that includes"
frame times.  This is very bespoke to animal detection and does not include other classes.
""
%% Imports and constants
Only necessary if you want to extract the sample rate from the video
%% Extract the sample rate if necessary
%% Load results
%% Convert to .csv
i_image = 0; im = results['images'][i_image]
""
umn-pr-analysis.py
""
Precision/recall analysis for UMN data
""
%% Imports and constants
results_file = results_file_filtered
"For two deployments, we're only processing imagse in the ""detections"" subfolder"
String to remove from MegaDetector results
%% Enumerate deployment folders
%% Load MD results
im = md_results['images'][0]
%% Load ground truth
i_row = 0; row = ground_truth_df.iloc[i_row]
%% Create relative paths for ground truth data
"Some deployment folders have no subfolders, e.g. this is a valid file name:"
""
M00/01010132.JPG
""
"But some deployment folders have subfolders, e.g. this is also a valid file name:"
""
N17/100EK113/07160020.JPG
""
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly"
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths."
deployment_name = list(deployment_folders)[0]
Enumerate all files in this folder
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))"
filename = files[100]
...for each file in this deployment
...for each deployment
%% Map relative paths to MD results
%% Add relative paths to our ground truth table
i_row = 0; row = ground_truth_df.iloc[i_row]
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's"
"just the deployment ID and the image name, separated by a slash."
Find the relative path for this image
Make sure we have MegaDetector results for this file
Make sure this image file exists
...for each row in the ground truth table
%% Take everything out of Pandas
%% Some additional error-checking of the ground truth
An early version of the data required consistency between common_name and is_blank
%% Combine MD and ground truth results
d = ground_truth_dicts[0]
Find the maximum confidence for each category
...for each detection
...for each image
%% Precision/recall analysis
...for each image
Thresholds go up throughout precisions/recalls/thresholds; find the last
value where recall is at or above target.  That's our precision @ target recall.
Flatten the confusion matrix
Write precision/recall plot to .png file in output directory
pr_figure_relative_filename = 'prec_recall.png'
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)"
plt.show(block=False)
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')"
%% Find and manually review all images of humans
%%
"...if this image is annotated as ""human"""
...for each image
%% Find and manually review all MegaDetector animal misses
%%
im = merged_images[0]
GT says this is not an animal
GT says this is an animal
%% Convert .json to .csv
%%
""
kga-pr-analysis.py
""
Precision/recall analysis for KGA data
""
%% Imports and constants
%% Load and filter MD results
%% Load and filter ground truth
%% Map images to image-level results
%% Map sequence IDs to images and annotations to images
Verify consistency of annotation within a sequence
TODO
%% Find max confidence values for each category for each sequence
seq_id = list(sequence_id_to_images.keys())[1000]
im = images_this_sequence[0]
det = md_result['detections'][]
...for each detection
...for each image in this sequence
...for each sequence
%% Prepare for precision/recall analysis
seq_id = list(sequence_id_to_images.keys())[1000]
cat_id = list(category_ids_this_sequence)[0]
...for each category in this sequence
...for each sequence
%% Precision/recall analysis
"Confirm that thresholds are increasing, recall is decreasing"
This is not necessarily true
assert np.all(precisions[:-1] <= precisions[1:])
Thresholds go up throughout precisions/recalls/thresholds; find the max
value where recall is at or above target.  That's our precision @ target recall.
"This is very slightly optimistic in its handling of non-monotonic recall curves,"
but is an easy scheme to deal with.
Flatten the confusion matrix
Write precision/recall plot to .png file in output directory
pr_figure_relative_filename = 'prec_recall.png'
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)"
plt.show(block=False)
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')"
%% Scrap
%% Find and manually review all sequence-level MegaDetector animal misses
input_base = '/media/user/lila-01/lila/snapshot-safari/KGA/KGA_public'
i_sequence = 0; seq_id = pr_sequence_ids[i_sequence]
i_seq = 0; seq_id = false_negative_sequences[i_seq]
"sequence_folder = os.path.join(sequence_preview_dir,'seq_{}'.format(str(i_seq).zfill(3)))"
fn = image_files[0]
"print('Copying {} to {}'.format(input_path,output_path))"
...for each file in this sequence.
...for each sequence
%% Image-level postprocessing
parse arguments
check if a GPU is available
load a pretrained embedding model
setup experiment
load the embedding model
setup the target dataset
setup finetuning criterion
setup an active learning environment
create a classifier
the main active learning loop
Active Learning
finetune the embedding model and load new embedding values
gather labeled pool and train the classifier
save a snapshot
Load a checkpoint if necessary
setup the training dataset and the validation dataset
setup data loaders
check if a GPU is available
create a model
setup loss criterion
define optimizer
load a checkpoint if provided
setup a deep learning engine and start running
train the model
train for one epoch
evaluate on validation set
save a checkpoint
open the file
read it
do the substitution
matplotlib.use('Agg')
from UIComponents.DBObjects import *
Initialize Database
# database connection credentials
# try to connect as USER to database DB_NAME through peewee
Load the saved embedding model
dataset_query = Detection.select().limit(5)
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})"
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getalllabels()) , dataset.getallpaths(), {})"
Random examples to start
"random_ids = np.random.choice(dataset.current_set, 1000, replace=False).tolist()"
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)"
print(random_ids)
Move Records
"moveRecords(dataset, DetectionKind.ModelDetection.value, DetectionKind.UserDetection.value, random_ids)"
#print([len(x) for x in dataset.set_indices])
# Finetune the embedding model
#dataset.set_kind(DetectionKind.UserDetection.value)
#dataset.train()
"#train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)"
"#finetune_embedding(model, checkpoint['loss_type'], dataset, 32, 4, 100)"
#save_checkpoint({
"#        'arch': model.arch,"
"#        'state_dict': model.state_dict(),"
"#        'optimizer' : optimizer.state_dict(),"
"#        'loss_type' : loss_type,"
"#        }, False, ""%s%s_%s_%04d.tar""%('finetuned', loss_type, model.arch, len(dataset.set_indices[DetectionKind.UserDetection.value])))"
Get indices of samples to get user to label
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
"kwargs[""already_selected""].extend(indices)"
Train on samples that have been labeled so far
Test on the samples that have not been labeled
"'optimizer' : optimizer.state_dict(),"
num_classes= len(train_dataset.getClassesInfo()[0])
"print(""Num Classes= ""+str(num_classes))"
define loss function (criterion) and optimizer
"optimizer = torch.optim.SGD(params, momentum = 0.9, lr = args.lr, weight_decay = args.weight_decay)"
optimizer.load_state_dict(checkpoint['optimizer'])
train for one epoch
"adjust_lr(optimizer,epoch)"
if epoch % 1 == 0 and epoch > 0:
"a, b, c = e.predict(train_embd_loader, load_info = True, dim = args.feat_dim)"
"plot_embedding(reduce_dimensionality(a), b, c, {})"
evaluate on validation set
"embedding_net = EmbeddingNet('resnet50', 256, True)"
compute output
val_loader = train_dataset.getSingleLoader(batch_size = 8)
"for a, b , c in val_loader:"
print(b[0])
"plt.imshow(np.rollaxis(np.rollaxis(a[0].numpy(), 1, 0), 2, 1))"
plt.show()
"print(np.rollaxis(a[0].numpy() , 1, 0).shape)"
--------some stuff needed to get AJAX to work with bottle?--------#
-------------------------------------------------------------------------------- #
PREPARE TO QUEUE IMAGES FOR LABELING
-------------------------------------------------------------------------------- #
# Connect as USER to database DB_NAME through peewee and initialize database proxy
# Load embedding model
---------------------------------------------------------------------- #
CREATE QUEUE OF IMAGES TO LABEL
---------------------------------------------------------------------- #
Use classifier to generate predictions
# Update model predicted class in PostgreSQL database
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
-------------------------------------------------------------------------------- #
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI
-------------------------------------------------------------------------------- #
"# static routes (to serve CSS, etc.)"
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps"
"return bottle.static_file(filename, root='../../../../../../../../../../../.')"
# dynamic routes
if data['display_grayscale']:
indices_to_exclude.update(set(color_indices))
elif not data['display_grayscale']:
indices_to_exclude.update(set(grayscale_indices))
data['display_images'] = {}
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices]
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices]
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices]
data['display_images']['detection_categories'] = []
for i in indices:
if str(dataset.samples[i][1]) == 'None':
data['display_images']['detection_categories'].append('None')
else:
existing_category_entries = {cat.id: cat.name for cat in Category.select()}
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()"
data['display_images']['detection_categories'].append(cat_name)
Use image ids in images_to_label to get the corresponding dataset indices
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
print(set(dataset.set_indices[4]).update(set(indices_to_label)))
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Use image id images_to_label to get the corresponding dataset index
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Train on samples that have been labeled so far
dataset.set_kind(DetectionKind.UserDetection.value)
print(y_train)
Predict on the samples that have not been labeled
print(y_pred)
Update model predicted class in PostgreSQL database
timer = time.time()
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
print('Updating the database took %0.2f seconds'%(time.time() - timer))
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
"once the classifier has been trained the first time, switch to AL sampling"
# sourceMappingURL=bootstrap.bundle.min.js.map
"print(ap_distances.size(),an_distances.size())"
losses = -(((-ap_distances)/128)+1+1e-16).log() - (((-(128-an_distances))/128)+1+1e-16).log()
import pdb
pdb.set_trace()
losses = ap_distances - an_distances + self.margin
print(losses.size())
from UIComponents.DBObjects import *
TODO: should this also change self.kind?
get the embedding representations for all samples (i.e. set current_set to all indices)
"return PILImage.open(os.path.join(self.img_base,path)).convert('RGB')"
"print(self.labels_set, self.n_classes)"
from sklearn.manifold import TSNE
embedding= TSNE(n_components=2).fit_transform(X)
embedding= PCA(n_components=2).fit_transform(X)
return X
"print(dir(event), type(sc))"
"print(label,bgcolor)"
"patches.append(mpatches.Patch(color=indexcolors[i], label=label))"
"plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,"
"ncol=12, mode=""expand"", borderaxespad=0., handles=patches)"
plt.legend(handles=patches)
"plt.xlabel('Dim 1', fontsize=12)"
"plt.ylabel('Dim 2', fontsize=12)"
plt.grid(True)
"plt.xlabel('Dim 1', fontsize=12)"
"plt.ylabel('Dim 2', fontsize=12)"
plt.grid(True)
print(thumb)
"img.thumbnail((16, 12), PILImage.ANTIALIAS)"
plt.show()
Add all negatives for all positive pairs
print(triplets.shape[0])
compute output
measure accuracy
compute loss on this batch
"train on a batch, record loss, and measure accuracy (if calc_accuracy)"
compute output
measure accuracy and record loss
switch to evaluate mode
compute output
switch to evaluate mode
"self.fc13 = nn.Linear(128, 64)"
self.bn2 = nn.BatchNorm1d(64)
x = F.relu(self.fc12(x))
x = F.relu(self.bn1(self.fc13(x)))
x = F.relu(self.fc13(x))
"x = F.dropout(x, training=self.training)"
save features last FC layer
x = F.relu(x)
save features last FC layer
import matplotlib.pyplot as plt
plt.switch_backend('agg')
class EmbeddingNet(nn.Module):
"def __init__(self, architecture, feat_dim, use_pretrained=False):"
"super(EmbeddingNet, self).__init__()"
self.feat_dim= feat_dim
self.inner_model = models.__dict__[architecture](pretrained=use_pretrained)
if architecture.startswith('resnet'):
in_feats= self.inner_model.fc.in_features
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)"
elif architecture.startswith('inception'):
in_feats= self.inner_model.fc.in_features
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)"
if architecture.startswith('densenet'):
in_feats= self.inner_model.classifier.in_features
"self.inner_model.classifier = nn.Linear(in_feats, feat_dim)"
if architecture.startswith('vgg'):
in_feats= self.inner_model.classifier._modules['6'].in_features
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)"
if architecture.startswith('alexnet'):
in_feats= self.inner_model.classifier._modules['6'].in_features
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)"
"def forward(self, x):"
return self.inner_model.forward(x)
class NormalizedEmbeddingNet(EmbeddingNet):
"def __init__(self, architecture, feat_dim, use_pretrained=False):"
"EmbeddingNet.__init__(self, architecture, feat_dim, use_pretrained = use_pretrained)"
"def forward(self, x):"
embedding =  F.normalize(self.inner_model.forward(x))*10.0
"return embedding, embedding"
"def get_random_images(num, image_dir, test_transforms):"
"data = datasets.ImageFolder(image_dir, transform=test_transforms) # slight abuse; this expects subfolders corresponding to classes but we have no classes here"
indices = list(range(len(data)))
np.random.shuffle(indices)
idx = indices[:num]
from torch.utils.data.sampler import SubsetRandomSampler
sampler = SubsetRandomSampler(idx)
"loader = torch.utils.data.DataLoader(data,"
"sampler=sampler, batch_size=num)"
dataiter = iter(loader)
"images, labels = dataiter.next()"
"return images, labels"
"def predict_image(image, model, test_transforms):"
"device = torch.device(""cuda"" if torch.cuda.is_available()"
"else ""cpu"")"
image_tensor = test_transforms(image).float()
image_tensor = image_tensor.unsqueeze_(0)
input = Variable(image_tensor)
input = input.to(device)
output = model(input)[0]
return output.data.cpu().numpy()
Connect to database and initialize db_proxy
# database connection credentials
# load the dataset
Load the saved embedding model from the checkpoint
# update the dataset embedding
# Create a folder for saving embedding visualizations with this model checkpoint
model_emb_dirname = os.path.basename(args.base_model).split('.')[0]
"os.makedirs(model_emb_dirname, exist_ok=True)"
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')"
dataset.embedding_mode()
"assert 2==3, 'break'"
datasetindices = list(range(len(dataset)))
np.random.shuffle(datasetindices)
random_indices = datasetindices[:args.num]
print(random_indices)
"selected_sample_features = np.array([]).reshape(0, 256)"
selected_sample_labels = []
for idx in random_indices:
"selected_sample_features = np.vstack([selected_sample_features, X_train[idx]])"
selected_sample_labels.append(y_train[idx])
img_path = imagepaths[idx].split('.JPG')[0]
image = dataset.loader(img_path)
selected_sample_images.append(image)
# TRY NEAREST NEIGHBORS WALK THROUGH EMBEDDING
nbrs = NearestNeighbors(n_neighbors=args.num).fit(selected_sample_features)
"distances, indices = nbrs.kneighbors(selected_sample_features)"
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')"
"idx_w_closest_nbr = np.where(distances[:,1] == min(distances[:,1]))[0][0]"
order = [idx_w_closest_nbr]
for ii in range(len(distances)):
"distances[ii, 0] = np.inf"
while len(order)<args.num:
curr_idx = order[-1]
curr_neighbors = indices[curr_idx]
curr_dists = list(distances[curr_idx])
# print(min(curr_dists))
next_closest_pos = curr_dists.index(min(curr_dists))
next_closest = curr_neighbors[next_closest_pos]
order.append(next_closest)
# make sure you can't revisit past nodes
for vi in order:
vi_pos = list(indices[next_closest]).index(vi)
"distances[next_closest, vi_pos] = np.inf"
for ii in range(len(order)):
imgidx = order[ii]
image = selected_sample_images[imgidx]
"image.save(model_emb_dirname+""/img""+str(ii)+""_""+str(selected_sample_labels[imgidx])+"".png"")"
# Specify the transformations on the input images before inference
"# test_transforms = transforms.Compose([transforms.Resize([224, 224]), transforms.ToTensor()])"
"test_transforms = transforms.Compose([transforms.Resize([256, 256]), transforms.RandomCrop([224, 224]), transforms.RandomHorizontalFlip(), transforms.ColorJitter(), transforms.ToTensor(), transforms.Normalize([0.407328, 0.407328, 0.407328], [0.118641, 0.118641, 0.118641])])"
"images, labels = get_random_images(num, image_dir, test_transforms)"
"all_features = np.array([]).reshape(0, 256)"
for ii in range(len(images)):
image = to_pil(images[ii])
"features = predict_image(image, model, test_transforms)"
"all_features = np.vstack([all_features, features])"
# for ii in range(len(images)):
#     image = to_pil(images[ii])
"#     image.save(""img""+str(ii)+"".png"")"
# TRY CLUSTERING
kmeans1 = KMeans(n_clusters=5).fit(StandardScaler().fit_transform(all_features))
print(kmeans1.labels_)
for ii in range(len(images)):
image = to_pil(images[ii])
"filename = str(kmeans1.labels_[ii])+""/img""+str(ii)+"".png"""
if not os.path.exists(os.path.dirname(filename)):
os.makedirs(os.path.dirname(filename))
image.save(filename)
class SaveFeatures():
"def __init__(self, module):"
self.hook = module.register_forward_hook(self.hook_fn)
"def hook_fn(self, module, input, output):"
"self.features = torch.tensor(output, requires_grad=True).cuda()"
def close(self):
self.hook.remove()
Load the saved embedding model from the checkpoint
"Get a sample from the database, with eval transforms applied, etc."
Connect to database and sample a dataset
output = model.forward(sample_image.unsqueeze(0))
print(output)
with torch.no_grad():
sample_image_input = sample_image.cuda(non_blocking=True)
"_, output = model(sample_image_input) # compute output"
print(output)
sample_image = PILImage.open(sample_image_path).convert('RGB')
"sample_image = transforms.Compose([Resize([256, 256]), CenterCrop(([[224,224]])), ToTensor(), Normalize([0.369875, 0.388726, 0.347536], [0.136821, 0.143952, 0.145229])])(sample_image)"
print(list(model_inner_resnet.children()))
print(model_inner_resnet.fc)
print(model_inner_resnet.fc0)
# print(model_inner_resnet.layer4[0].conv2)
# print(type(model))
# print(len(list(model_inner_resnet.children())))
# print(list(model.children()))
# print(list(list(model.children())[0].children()))
"img = np.uint8(np.random.uniform(150, 180, (56, 56, 3)))/255"
"img_tensor = torch.unsqueeze(torch.from_numpy(img), 0)"
full_out = model_inner_resnet.forward(img_tensor)
print(full_out)
model(img_tensor)
activations = SaveFeatures(model_inner_resnet.layer4[0].conv2)
print(activations.features)
print(type(activations.features))
activations.close()
Connect to database and sample a dataset
Load the saved embedding model from the checkpoint
Update the dataset embedding
Get a random query image
# # # IMAGES IN THE SAME SEQUENCE # # # #
"assert 2==3, 'break'"
# # # CLOSEST IN (EMBEDDING) FEATURE SPACE # # # #
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Variables for the hierarchical cluster
Variables for the AL algorithm
Data variables
"connectivity = kneighbors_graph(self.transformed_X,max_features)"
Fit cluster and update cluster variables
The sklearn hierarchical clustering algo numbers leaves which correspond
to actual datapoints 0 to n_points - 1 and all internal nodes have
ids greater than n_points - 1 with the root having the highest node id
"If no labels have been observed, simply return uniform distribution"
"If no observations, return worst possible upper lower bounds"
Loop through generations from bottom to top
Update admissible labels for node
Calculate score
Determine if node should be split
Make sure label set for node so that we can flow to children
if necessary
Only split if all ancestors are admissible nodes
This is part  of definition of admissible pruning
Check that pruning covers all leave nodes
Fill in labels
Observe labels for previously recommended batches
TODO(lishal): implement multiple selection methods
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
A list of initialized samplers is allowed as an input because
"for AL_methods that search over different mixtures, may want mixtures to"
have shared AL_methods so that initialization is only performed once for
computation intensive methods like HierarchicalClusteringAL and
states are shared between mixtures.
"If initialized samplers are not provided, initialize them ourselves."
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copy these objects to make sure they are not modified while simulating
trajectories as they are used later by the main run_experiment script.
Assumes that model has already by fit using all labeled data so
the probabilities can be used immediately to hallucinate labels
All models need to have predict_proba method
Hallucinate labels for selected datapoints to be label
using class probabilities from model
"Not saving already_selected here, if saving then should sort"
only for the input to fit but preserve ordering of indices in
already_selected
Useful to know how accuracy compares for model trained on hallucinated
labels vs trained on true labels.  But can remove this train to speed
up simulations.  Won't speed up significantly since many more models
are being trained inside the loop above.
Save trajectory for reference
Delete created copies
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER
Should check model but kernel_svm does not have coef_ so need better
handling here
Make sure that model object fed in did not change during simulations
Return indices based on return type specified
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Set gamma for gaussian kernel to be equal to 1/n_features
kneighbors graph is constructed using k=10
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of"
"another point, make it vice versa"
Graph edges are weighted by applying gaussian kernel to manhattan dist.
"By default, gamma for rbf kernel is equal to 1/n_features but may"
get better results if gamma is tuned.
Define graph density for an observation to be sum of weights for all
edges to the node representing the datapoint.  Normalize sum weights
by total number of neighbors.
"If a neighbor has already been sampled, reduce the graph density"
for its direct neighbors to promote diversity.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update observed reward and arm probabilities
Sample an arm
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
y only used for determining how many clusters there should be
probably not practical to assume we know # of classes before hand
should also probably scale with dimensionality of data
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
TODO(lishal): have MarginSampler and this share margin function
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update min_distances for all examples given new cluster center.
Assumes that the transform function takes in original data and not
flattened data.
Initialize centers with a randomly selected datapoint
New examples should not be in already selected since those points
should have min_distance of zero to a cluster center.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
This is uniform given the remaining pool but biased wrt the entire pool.
sample = [i for i in range(self.X.shape[0]) if i not in already_selected]
return sample[0:N]
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Fields for hierarchical clustering AL
Setting parent and storing nodes in dict for fast access
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Connect to database DB_NAME as USER and initialize tables
Populate Info table
Populate Category table
"# For now, we have a predefined list of species we expect to see in the camera trap database (e.g. maybe from a quick look through the images)"
# TODO: allow user to update the class list through the labeling tool UI as they see different species
Populate Image and Detection tables
Image entry data
Detection entry data
Connect to database DB_NAME as USER and initialize tables
Populate Info table
Populate Category table
# Create category records from species present in the COCO camera trap classification dataset (COCO classes minus any excluded classes)
# TODO: allow user to update the class list through the labeling tool UI as they see different species
Populate Image and Detection tables
"with open(os.path.join(args.crop_dir,'crops.json'), 'r') as infile:"
crops_json = json.load(infile)
counter = 0
timer = time.time()
num_detections = len(crops_json)
for detectionid in crops_json:
counter += 1
detection_data = crops_json[detectionid]
# Image entry data
existing_image_entries = Image.select().where((Image.file_name == detection_data['file_name']))
try:
existing_image_entry = existing_image_entries.get()
except:
"image_entry = Image.create(id=detectionid, file_name=detection_data['file_name'], width=detection_data['width'], height=detection_data['height'], grayscale=detection_data['grayscale'],"
"source_file_name=detection_data['source_file_name'], relative_size=detection_data['relative_size'],"
"seq_id=detection_data['seq_id'], seq_num_frames=detection_data['seq_num_frames'], frame_num=detection_data['frame_num'])"
image_entry.save()
# Detection entry data
"detection_entry = Detection.create(id=detectionid, image=detectionid, bbox_confidence=detection_data['bbox_confidence'],"
"bbox_X1=detection_data['bbox_X1'], bbox_Y1=detection_data['bbox_Y1'], bbox_X2=detection_data['bbox_X2'], bbox_Y2=detection_data['bbox_Y2'],"
kind=DetectionKind.ModelDetection.value)
detection_entry.save()
if counter%100 == 0:
"print('Updated database with Image and Detection table entries for %d out of %d crops in %0.2f seconds'%(counter, num_detections, time.time() - timer))"
# data related to original image
Get class names from .txt list
Initialize Oracle table
Map filenames to classes (NOTE: we assume a single image does not contain more than one class)
"For each detection, use source image path to get class"
TODO update: Assumes that crops have already
been generated for the images using make_active_learning_classification_dataset.py. The created DB contains tables:
- info: information about the dataset
- image: images present in the dataset
- detections: crops of images with detections with confidence greater than a specified threshold
Initialize Database
# database connection credentials
HOST = 'localhost'
PORT = 5432
"# first, make sure the (user, password) has been created"
"# sudo -u postgres psql -c ""CREATE USER <db_user> WITH PASSWORD <db_password>;"""
"# sudo -u postgres psql -c ""CREATE DATABASE <db_name> WITH OWNER <db_user> CONNECTION LIMIT -1;"""
"# sudo -u postgres psql -c ""GRANT CONNECT ON DATABASE <db_name> TO <db_user>;"""
"# sudo -u postgres psql -d <db_name> -c ""CREATE EXTENSION IF NOT EXISTS \""uuid-ossp\"";"""
# Try to connect as USER to database DB_NAME through peewee
Populate Tables
# create Info table
# get class names for Category table
Faster anD available in Python 3.5 and above
# iterate through images in each class folder
killing this process after over 38 hours adding over 500k white-tailed deer crops from emammal
resuming for remaining classes
# get cropped image data for Image table
"if mean of each channel is about the same, image is likely grayscale"
# still have no info on these:
seq_id = CharField(null= True)                # sequence identifier for the original image
seq_num_frames = IntegerField(null = True)    # number of frames in sequence
frame_num = IntegerField(null = True)         # which frame number in sequence
location = CharField(null = True)             # location of camera trap
datetime = DateTimeField(null = True)
# store info about the detection corresponding to this image
# store info about the true labels for the detection
#  - for pretrain dataset this is the same as the detection_category if the detection categories
print(classes)
Connect to database and sample a dataset
Load the saved embedding model from the checkpoint
Update the dataset embedding
save the images
save the features
"with open(os.path.join(args.output_dir, 'lastlayer_features.mat'), 'wb') as f:"
"pickle.dump(sample_features, f)"
"with open(os.path.join(args.output_dir, 'labels.mat'), 'wb') as f:"
"pickle.dump(sample_labels, f)"
"parser.add_argument('--db_name', default='missouricameratraps', type=str, help='Name of the training (target) data Postgres DB.')"
"parser.add_argument('--db_user', default='user', type=str, help='Name of the user accessing the Postgres DB.')"
"parser.add_argument('--db_password', default='password', type=str, help='Password of the user accessing the Postgres DB.')"
"parser.add_argument('--base_model', type=str, help='Path to latest embedding model checkpoint.')"
"parser.add_argument('--output_dir', type=str, help='Output directory for subset of crops')"
Add json entry for this crop
Copy file for this crop to subset dataset crop dir
Copy file for its full-size source image to subset dataset image dir
Write crops.json to subset dataset crop dir
store info about the crops produced in a JSON file
------------------------------------------------------------------------------------------------------------#
COMMENT OUT IF NOT USING A SPECIFIC PROJECT WITHIN ROBERT LONG EMAMMAL DATASET
------------------------------------------------------------------------------------------------------------#
get some information about the source image
------------------------------------------------------------------------------------------------------------#
NOTE: EDIT THIS SECTION BASED ON DATASET SOURCE
get info about sequence the source image belongs to from path and directory
# missouricameratraps:
imgframenum = int(os.path.basename(imgfile).split('.JPG')[0].split('_')[-1])
imgseqid = int(os.path.split(os.path.dirname(imgfile))[-1])
"imgseqnumframes = len([name for name in os.listdir(os.path.dirname(imgfile)) if os.path.isfile(os.path.join(os.path.dirname(imgfile), name))])"
# emammal:
------------------------------------------------------------------------------------------------------------#
utility function
compute output
measure accuracy and record loss
switch to train mode
measure accuracy and record loss
compute output
measure accuracy and record loss
switch to evaluate mode
compute output
switch to evaluate mode
compute output
switch to evaluate mode
"print(self.labels_set, self.n_classes)"
Add all negatives for all positive pairs
print(triplets.shape[0])
constructor
update embedding values after a finetuning
select either the default or active pools
gather test set
gather train set
finetune the embedding model over the labeled pool
a utility function for saving the snapshot
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Variables for the hierarchical cluster
Variables for the AL algorithm
Data variables
"connectivity = kneighbors_graph(self.transformed_X,max_features)"
Fit cluster and update cluster variables
The sklearn hierarchical clustering algo numbers leaves which correspond
to actual datapoints 0 to n_points - 1 and all internal nodes have
ids greater than n_points - 1 with the root having the highest node id
"If no labels have been observed, simply return uniform distribution"
"If no observations, return worst possible upper lower bounds"
Loop through generations from bottom to top
Update admissible labels for node
Calculate score
Determine if node should be split
Make sure label set for node so that we can flow to children
if necessary
Only split if all ancestors are admissible nodes
This is part  of definition of admissible pruning
Check that pruning covers all leave nodes
Fill in labels
Observe labels for previously recommended batches
TODO(lishal): implement multiple selection methods
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
A list of initialized samplers is allowed as an input because
"for AL_methods that search over different mixtures, may want mixtures to"
have shared AL_methods so that initialization is only performed once for
computation intensive methods like HierarchicalClusteringAL and
states are shared between mixtures.
"If initialized samplers are not provided, initialize them ourselves."
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copy these objects to make sure they are not modified while simulating
trajectories as they are used later by the main run_experiment script.
Assumes that model has already by fit using all labeled data so
the probabilities can be used immediately to hallucinate labels
All models need to have predict_proba method
Hallucinate labels for selected datapoints to be label
using class probabilities from model
"Not saving already_selected here, if saving then should sort"
only for the input to fit but preserve ordering of indices in
already_selected
Useful to know how accuracy compares for model trained on hallucinated
labels vs trained on true labels.  But can remove this train to speed
up simulations.  Won't speed up significantly since many more models
are being trained inside the loop above.
Save trajectory for reference
Delete created copies
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER
Should check model but kernel_svm does not have coef_ so need better
handling here
Make sure that model object fed in did not change during simulations
Return indices based on return type specified
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Set gamma for gaussian kernel to be equal to 1/n_features
kneighbors graph is constructed using k=10
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of"
"another point, make it vice versa"
Graph edges are weighted by applying gaussian kernel to manhattan dist.
"By default, gamma for rbf kernel is equal to 1/n_features but may"
get better results if gamma is tuned.
Define graph density for an observation to be sum of weights for all
edges to the node representing the datapoint.  Normalize sum weights
by total number of neighbors.
"If a neighbor has already been sampled, reduce the graph density"
for its direct neighbors to promote diversity.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update observed reward and arm probabilities
Sample an arm
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
y only used for determining how many clusters there should be
probably not practical to assume we know # of classes before hand
should also probably scale with dimensionality of data
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
TODO(lishal): have MarginSampler and this share margin function
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update min_distances for all examples given new cluster center.
Assumes that the transform function takes in original data and not
flattened data.
Initialize centers with a randomly selected datapoint
New examples should not be in already selected since those points
should have min_distance of zero to a cluster center.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
This is uniform given the remaining pool but biased wrt the entire pool.
sample = [i for i in range(self.X.shape[0]) if i not in already_selected]
return sample[0:N]
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Fields for hierarchical clustering AL
Setting parent and storing nodes in dict for fast access
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Faster and available in Python 3.5 and above
transform_list.append(CenterCrop((processed_size)))
transform_list.append(Lambda(lambda X: normalize(X)))
"print(self.labels_set, self.n_classes)"
"from PyQt5 import QtCore, QtWidgets,QtGui"
from collections import deque
from peewee import *
from UIComponents.Tag import Tag
policy.setHeightForWidth(True)
"print(self.tab1.parentWidget(),self)"
self.tab4.add.clicked.connect(self.addSpecies)
self.tab4.update.clicked.connect(self.updateSpecies)
checkpoint= load_checkpoint('../merge/triplet_model_0054.tar')
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))"
"embedding_net = EmbeddingNet(checkpoint['arch'], checkpoint['feat_dim'])"
if checkpoint['loss_type'].lower()=='center':
"model = torch.nn.DataParallel(ClassificationNet(embedding_net, n_classes=14)).cuda()"
else:
model= torch.nn.DataParallel(embedding_net).cuda()
model.load_state_dict(checkpoint['state_dict'])
self.parentWidget().progressBar.setMaximum(len(run_dataset)//2048)
"e=Engine(model,None,None, verbose=True,progressBar= self.parentWidget().progressBar)"
label = [ x[1] for x in run_dataset.samples]
"print(indices,selected_set)"
print(query.sql())
src.delete().where(src.image_id<<rList))
"det= UserDetection.create(category_id=0, id=str(index+label[1][2]),image_id=final[0], bbox_X=label[1][0], bbox_Y=label[1][1], bbox_W=label[1][2], bbox_H=label[1][3])"
for x in self.tab1.grid.tags:
x.delete_instance()
db.create_tables([Detection])
This is simply to show the bar
"p = Process(target=ex.active, args=())"
p.start()
p.join()
ex.active()
ex.centralWidget().setCurrentIndex(1)
main()
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))"
print(row)
"unq_id= ""crops_""+str(uuid.uuid1())"
"print(line,imageWidth,imageHeight)"
"print(""%s,%s,%s,%0.6f,%0.6f,%0.6f,%0.6f,%0.6f""%(line[0], line[1],line[2],float(line[3]),topRel,leftRel,bottomRel,rightRel))"
if not os.path.exists(dest):
os.mkdir(dest)
raise
out.close()
"print length,(i-1)*length,i*length"
matplotlib.use('Agg')
get the inputs
zero the parameter gradients
forward + backward + optimize
print statistics
get the inputs
forward + backward + optimize
print statistics
get the inputs
zero the parameter gradients
forward + backward + optimize
get the inputs
forward + backward + optimize
print statistics
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
"conf.reset()"""""""
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
conf.reset()
for clf in classifiers:
"clf.fit(X_train, y_train)"
preds= clf.predict_proba(X)
uncertainty+= preds.max(axis=1)
print(uncertainty[ind])
print(uncertainty[ind])
true_labels = y[unlabeled_indices]
select up to 5 digit examples that the classifier is most uncertain about
remember best acc@1 and save checkpoint
"completeClassificationLoop(run_dataset, model,num_classes)"
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)"
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd)
labels = db.labels_
"mapp=(find_probablemap(label,labels, K=args.K))"
"print(""Clusters"")"
"for i,x in enumerate(labels):"
labels[i]= mapp[x]
print(np.sum(labels == label)/labels.size)
"print(""Confidence Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning(embd, label, idx)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(""Entropy Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning_entropy(embd, label, idx)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(idx,idx.shape)"
for i in idx:
print(paths[i])
"print(""Silohette active learning"")"
"idx= active_learning2(embd, 1000, args.num_clusters)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
embd= reduce_dimensionality(embd)#[0:10000])
labels= labels[0:10000]
label= label[0:10000]
paths= paths[0:10000]
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])"
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])"
plt.show()
"np.save(args.name_prefix+""_embeddings.npy"",embd)"
"np.save(args.name_prefix+""_labels.npy"",label)"
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")"
del output
define loss function (criterion) and optimizer
"optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay= args.weight_decay)"
optimizer.load_state_dict(checkpoint['optimizer'])
train for one epoch
evaluate on validation set
matplotlib.use('Agg')
selected_set.add(rand_ind[i])
"embedding_net = EmbeddingNet('resnet50', 256, True)"
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)"
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)"
print('Embedding Done')
sys.stdout.flush()
Random examples to start
"random_ids = noveltySamples(unlabeled_dataset.em, unlabeled_dataset.getIDs(), 1000)"
print(random_ids)
Move Records
Finetune the embedding model
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)"
unlabeled_dataset.updateEmbedding(model)
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})"
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()"
matplotlib.use('Agg')
"uncertainty= np.apply_along_axis(stats.entropy,1,probs) * (1 - probs.max(axis=1))"
"copy(paths[srt[i]], ""active"")"
"plot_together( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()), preds, base_ind, dataset.getpaths(), {})"
"return np.random.choice(range(0,prob_list[0].shape[0]), 100, replace=False).tolist()"
selected_set.add(rand_ind[i])
"query= Detection.update(kind = destKind.value).where(Detection.id.in_(rList), Detection.kind == srcKind.value)"
query.execute()
"embedding_net = EmbeddingNet('resnet50', 256, True)"
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)"
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)"
print('Embedding Done')
sys.stdout.flush()
"plot_embedding(dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})"
Random examples to start
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)"
print(random_ids)
Move Records
Finetune the embedding model
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)"
unlabeled_dataset.updateEmbedding(model)
"print(dataset.em[dataset.current_set].shape, np.asarray(dataset.getlabels()).shape, len(dataset.getpaths()))"
"plot_embedding( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})"
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})"
train_eval_classifier()
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()"
"names = [""Linear SVM"", ""RBF SVM"", ""Random Forest"", ""Neural Net"", ""Naive Bayes""]"
"classifiers = [SVC(kernel=""linear"", C=0.025, probability= True, class_weight='balanced'),"
"SVC(gamma=2, C=1, probability= True, class_weight='balanced'),"
"RandomForestClassifier(max_depth=None, n_estimators=100, class_weight='balanced'),"
"MLPClassifier(alpha=1),"
GaussianNB()]
estimators= []
"for name, clf in zip(names, classifiers):"
"estimators.append((name, clf))"
"eclf1 = VotingClassifier(estimators= estimators, voting='hard')"
"eclf2 = VotingClassifier(estimators= estimators, voting='soft')"
"names.append(""ensemble hard"")"
classifiers.append(eclf1)
"names.append(""ensemble soft"")"
classifiers.append(eclf2)
dataset.image_mode()
dataset.updateEmbedding(model)
y_pred= clf.predict(X_test)
"print(confusion_matrix(y_test, y_pred))"
paths= dataset.getpaths()
"for i, (yp, yt) in enumerate(zip(y_pred, y_test)):"
if yp != yt:
"copy(paths[i],""mistakes"")"
"print(yt, yp, paths[i],i)"
"clf_output= clf_e.embedding(eval_loader, dim=48)"
"self.train_transform = transforms.Compose([Resize(raw_size), CenterCrop((processed_size)), ToTensor(), Normalize(mean, std)])"
"print(self.labels_set, self.n_classes)"
"print(line,imageWidth,imageHeight)"
"print length,(i-1)*length,i*length"
#########################################################
## Configuration
#########################################################
## The actual code
Check arguments
Create output directories
Padding around the detected objects when cropping
1.3 for the cropping during test time and 1.3 for
the context that the CNN requires in the left-over
image
Load a (frozen) Tensorflow model into memory.
## Preparations: get all the output tensors
The following processing is only for single image
Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
Follow the convention by adding back the batch dimension
For all images in the image directoryig
Load image
Run inference
"all outputs are float32 numpy arrays, so convert types as appropriate"
Add detections to the collection
Get info about the image
Select detections with a confidence larger than DETECTION_CONFIDENCE
Get these boxes and convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes"
"However, we need to make sure that it box coordinates are still within the image"
"For each detected bounding box with high confidence, we will"
crop the image to the padded box and save it
generate a unique identifier for the detection
"bbox is the detected box, crop_box the padded / enlarged box"
Add numbering to the original file name if there are multiple boxes
The absolute file path where we will store the image
"if COCO_OUTPUT_DIR is set, then we will only use the shape"
of cropped_img in the following code. So instead of reading
cropped_img = np.array(Image.open(out_file))
we can speed everything up by reading only the size of the image
matplotlib.use('Agg')
conf= ConfusionMatrix(24)
get the inputs
zero the parameter gradients
forward + backward + optimize
print statistics
get the inputs
forward + backward + optimize
print statistics
get the inputs
zero the parameter gradients
forward + backward + optimize
get the inputs
forward + backward + optimize
print statistics
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
"conf.reset()"""""""
define loss function (criterion) and optimizer
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
conf.reset()
for clf in classifiers:
"clf.fit(X_train, y_train)"
preds= clf.predict_proba(X)
uncertainty+= preds.max(axis=1)
print(uncertainty[ind])
print(uncertainty[ind])
true_labels = y[unlabeled_indices]
select up to 5 digit examples that the classifier is most uncertain about
"print(indices,selected_set)"
print(query.sql())
remember best acc@1 and save checkpoint
"completeClassificationLoop(run_dataset, model,num_classes)"
"embd, label, paths = extract_embeddings(run_loader, model)"
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)"
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd)
labels = db.labels_
"mapp=(find_probablemap(label,labels, K=args.K))"
"print(""Clusters"")"
"for i,x in enumerate(labels):"
labels[i]= mapp[x]
print(np.sum(labels == label)/labels.size)
"print(""Confidence Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning(embd, label, idx)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(""Entropy Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning_entropy(embd, label, idx)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
"new_selected= selectSamples(embd,paths,3000)"
"print(idx,idx.shape)"
for i in idx:
print(paths[i])
"print(""Silohette active learning"")"
"idx= active_learning2(embd, 1000, args.num_clusters)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(""Random"")"
"idx = np.random.choice(np.arange(len(paths)), 1000, replace=False)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
embd= reduce_dimensionality(embd)#[0:10000])
labels= labels[0:10000]
label= label[0:10000]
paths= paths[0:10000]
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])"
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])"
plt.show()
"np.save(args.name_prefix+""_embeddings.npy"",embd)"
"np.save(args.name_prefix+""_labels.npy"",label)"
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")"
combo.setEnabled(not finalized)
print(self.bbox)
self.addWidget(self.child)
Moving container with arrows
Left - Bottom
Right-Bottom
Left-Top
Right-Top
Left - Bottom
Right - Bottom
Left - Top
Right - Top
check cursor horizontal position
check cursor vertical position
self.resizeEvent=self.onResize
"print(""Parent"", parent, parent.width(), parent.height())"
"self.setGeometry(0,0,410,307)"
"print(w,h,""w,h"")"
"print(""final"",tag.getFinal())"
pass
"self.tags.append(TContainer(self,Category.get(-1),[0,0,0.1,0.1],True, Qt.red))"
label= CharField()
fullname=str(self.model)
"self.name= (fullname[fullname.find("":"")+2:fullname.find("">"")].strip()+'_set').lower()"
"print(""Parent"", self.parentWidget().width(), self.parentWidget().height())"
"print(self.model,self.name,query.sql())"
self.tab4.speciesList.setModel(species)
"self.tab4.speciesList.setRowHidden(len(species.stringList())-1, True)"
self.speciesList.itemChanged.connect(self.itemChanged)
###############
""
iwildcam_dataset.py
""
Loader for the iWildCam detection data set.
""
###############
loads the taxonomy data and converts to ints
set up dummy data
create a dictionary of lists containing taxonomic labels
set up the filenames and annotations
This loop reads the bboxes and corresponding labels and assigns them
the correct image. Kind of slow at the moment...
"To speed up the loop, creating mapping for image_id to list index"
"check that the image contains an animal, if not, don't append a box or label to the"
image list
"Bboxes should have ('ymin', 'xmin', 'ymax', 'xmax') format"
"Currently we take the label from the annotation file, non-consecutive-"
label-support would be great
"self.bboxes[idx].append([-1.,-1.,0.,0.])"
self.labels[idx].append(30)
load classes
print out some stats
To make sure we loaded the bboxes correctly:
self.validate_bboxes()
Make sure all are greater equal 0
We have to add 1 as the framework assumes that labels start from 0
print(bboxes)
###############
""
util.py
""
Image utilities used in the FasterRCNN framework.
""
###############
"reshape (H, W) -> (1, H, W)"
"transpose (H, W, C) -> (C, H, W)"
pass in list of files
print(im_id)
print(images[im_id]['seq_id'])
get unique colors in segmentation
get box for each color
this is the background class
get a box around this color
"x1,y1 is top left corner, x2,y2 is bottom right corner"
create coco-style json
In settings.json first activate computer vision mode:
https://github.com/Microsoft/AirSim/blob/master/docs/image_apis.md#computer-vision-mode
import setup_path
load animal class name lookup
set segmentation values for everything to 0
set segmentation for each animal to a different value
"client.simSetCameraOrientation(""0"", airsim.to_quaternion(-0.161799, 0, 0)); #radians"
pose = client.simGetVehiclePose()
pp.pprint(pose)
print('Pose ' + str(cam_num))
print(pose)
"print(""Type %d, size %d, pos %s"" % (response.image_type, len(response.image_data_float), pprint.pformat(response.camera_position)))"
"print(""Type %d, size %d, pos %s"" % (response.image_type, len(response.image_data_uint8), pprint.pformat(response.camera_position)))"
pose = client.simGetVehiclePose()
pp.pprint(pose)
currently reset() doesn't work in CV mode. Below is the workaround
"client.simSetPose(airsim.Pose(airsim.Vector3r(0, 0, 0), airsim.to_quaternion(0, 0, 0)), True)"
environment_lookup = {}
print(len(list(env_list)))
save environment dict every time so you don't lose the info if airsim crashes
#####
""
video_utils.py
""
"Utilities for splitting, rendering, and assembling videos."
""
#####
"%% Constants, imports, environment"
from ai4eutils
%% Path utilities
%% Function for rendering frames to video and vice-versa
http://tsaith.github.io/combine-images-into-a-video-with-python-3-and-opencv-3.html
Determine the width and height from the first image
Define the codec and create VideoWriter object
"If we're not over-writing, check whether all frame images already exist"
"OpenCV seems to over-report the number of frames by 1 in some cases, or fails"
"to read the last frame; either way, I'm allowing one missing frame."
"print(""Rendering video {}, couldn't find frame {}"".format("
"input_video_file,missing_frame_number))"
...if we need to check whether to skip this video entirely
"for frame_number in tqdm(range(0,n_frames)):"
print('Skipping frame {}'.format(frame_filename))
Recursively enumerate video files
Create the target output folder
Render frames
input_video_file = input_fn_absolute; output_folder = output_folder_video
For each video
""
input_fn_relative = input_files_relative_paths[0]
"process_detection_with_options = partial(process_detection, options=options)"
zero-indexed
Load results
# Break into videos
im = images[0]
# For each video...
video_name = list(video_to_frames.keys())[0]
frame = frames[0]
At most one detection for each category for the whole video
category_id = list(detection_categories.keys())[0]
Find the nth-highest-confidence video to choose a confidence value
Prepare the output representation for this video
...for each video
Write the output file
%%
%% Test driver
%% Constants
%% Split videos into frames
"%% List image files, break into folders"
Find unique folders
fn = frame_files[0]
%% Load detector output
%% Render detector frames
folder = list(folders)[0]
d = detection_results_this_folder[0]
...for each file in this folder
...for each folder
%% Render output videos
folder = list(folders)[0]
...for each video
All on the 1212-image test subset
"%% Constants, imports, environment"
from multiprocessing.pool import ThreadPool as workerpool
Number of images to pre-fetch
Useful hack to force CPU inference.
""
"Need to do this before any PT/TF imports, which happen when we import"
from run_detector.
Numpy FutureWarnings from tensorflow import
%% Support functions for multiprocessing
TODO
""
The queue system is a little more elegant if we start one thread for reading and one
"for processing, and this works fine on Windows, but because we import TF at module load,"
"CUDA will only work in the main process, so currently the consumer function runs here."
""
"To enable proper multi-GPU support, we may need to move the TF import to a separate module"
that isn't loaded until very close to where inference actually happens.
%% Other support funtions
%% Image processing functions
%% Main function
Load the detector
Does not count those already processed
Will not add additional entries not in the starter checkpoint
Write a checkpoint if necessary
Back up any previous checkpoints
Write the new checkpoint
Remove the backup checkpoint if it exists
...if it's time to make a checkpoint
"When using multiprocessing, let the workers load the model"
"Results may have been modified in place, but we also return it for"
backwards-compatibility.
%% Interactive driver
%%
image_file_names = image_file_names[0:2]
"python run_detector_batch.py ""g:\temp\models\md_v4.1.0.pb"" ""g:\temp\demo_images\ssmini"" ""g:\temp\ssmini.json"" --recursive --output_relative_filenames --use_image_queue"
%% Command-line driver
Load the checkpoint if available
""
Relative file names are only output at the end; all file paths in the checkpoint are
still full paths.
"Find the images to score; images can be a directory, may need to recurse"
A json list of image paths
A single image file
Test that we can write to the output_file's dir if checkpointing requested
"Confirm that we can write to the checkpoint path, rather than failing after 10000 images"
%% Imports
import pre- and post-processing functions from the YOLOv5 repo https://github.com/ultralytics/yolov5
%% Classes
padded resize
NMS
format detections/bounding boxes
Rescale boxes from img_size to im0 size
"normalized center-x, center-y, width and height"
"MegaDetector output format's categories start at 1, but this model's start at 0"
for testing
"MegaDetector was trained with batch size of 1, and the resizing function is a part"
of the inference graph
"change from [y1, x1, y2, x2] to [x1, y1, width, height]"
convert numpy floats to Python floats
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size
"np_images = [np.asarray(image, np.uint8) for image in images]"
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)"
performs inference
our batch size is 1; need to loop the batch dim if supporting batch size > 1
"%% Constants, imports, environment"
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Numpy FutureWarnings from tensorflow import
Useful hack to force CPU inference
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
An enumeration of failure reasons
Number of decimal places to round to for confidence and bbox coordinates
Label mapping for MegaDetector
"Each version of the detector is associated with some ""typical"" values"
"that are included in output files, so that downstream applications can"
use them as defaults.
%% Classes
Stick this into filenames before the extension for the rendered result
%% Utility functions
%% Main function
Dictionary mapping output file names to a collision-avoidance count.
""
"Since we'll be writing a bunch of files to the same folder, we rename"
as necessary to avoid collisions.
image is modified in place
...for each image
%% Command-line driver
"but for a single image, args.image_dir is also None"
%% Interactive driver
%%
#####
""
process_video.py
""
"Split a video into frames, run the frames through run_tf_detector_batch.py, and"
optionally stitch together results into a new video with detection boxes.
""
#####
"%% Constants, imports, environment"
%% Main function
Render detections to images
Combine into a video
Delete the temporary directory we used for detection images
(Optionally) delete the frames on which we ran MegaDetector
# Validate options
# Split every video into frames
# Run MegaDetector
# Convert frame-level results to video-level results
%% Interactive driver
%% Process a folder of videos
process_video_folder(options)
import clipboard; clipboard.copy(cmd)
%% Process a single video
"python process_video.py ""c:\temp\models\md_v4.0.0.pb"" ""c:\temp\LIFT0003.MP4"" --debug_max_frames=10 --render_output_video=True"
%% For a video that's already been run through MD
im = d['images'][0]
...for each detection
Split into frames
Render output video
# Render detections to images
# Combine into a video
%% Command-line driver
Lint as: python3
Copyright 2020 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TPU is automatically inferred if tpu_name is None and
we are running under cloud ai-platform.
""
copy_checkpoints.py
""
Run this script with specified source_dir and target_dir while the model is training to make a copy
of every checkpoint (checkpoints are kept once an hour by default and is difficult to adjust)
""
do not copy event or evaluation results
keys are categories (int)
"we force *_boxes to have shape [N, 4], even in case that N = 0"
"labels input to compute_object_detection_metrics() needs to start at 0, not 1"
num_detections = len(dets['boxes'])
place holders - we don't have these
true positives < gt of that category
compute one-class precision/recall/average precision (if every box is just
of an object class)
"iterate through each image in the gt file, not the detection file"
ground truth
convert gt box coordinates to TFODAPI format
detections
only include a detection entry if that image had detections
minus the 'one_class' set of metrics
""
api_frontend.py
""
"Defines the Flask app, which takes requests (one or more images) from"
"remote callers and pushes the images onto the shared Redis queue, to be processed"
by the main service in api_backend.py .
""
%% Imports
%% Initialization
%% Support functions
Make a dict that the request_processing_function can return to the endpoint
function to notify it of an error
Verify that the content uploaded is not too big
""
request.content_length is the length of the total payload
Verify that the number of images is acceptable
...def check_posted_data(request)
%% Main loop
Check whether the request_processing_function had an error
Write images to temporary files
""
TODO: read from memory rather than using intermediate files
"Submit the image(s) for processing by api_backend.py, who is waiting on this queue"
TODO: convert to a blocking read and eliminate the sleep() statement in this loop
"image = Image.open(os.path.join(temp_direc, image_name))"
...if we do/don't have a request available on the queue
...while(True)
...def detect_sync()
%% Command-line driver
use --non-docker if you are testing without Docker
""
python api_frontend.py --non-docker
# Camera trap real-time API configuration
"Full path to the temporary folder for image storage, only meaningful"
within the Docker container
Upper limit on total content length (all images and parameters)
Minimum confidence threshold for detections
Minimum confidence threshold for showing a bounding box on the output image
Use this when testing without Docker
""
api_backend.py
""
"Defines the model execution service, which pulls requests (one or more images)"
"from the shared Redis queue, and runs them through the TF model."
""
%% Imports
%% Initialization
%% Main loop
TODO: convert to a blocking read and eliminate the sleep() statement in this loop
Filter the detections by the confidence threshold
""
"Each result is [ymin, xmin, ymax, xmax, confidence, category]"
""
"Coordinates are relative, with the origin in the upper-left"
...if serialized_entry
...while(True)
...def detect_process()
%% Command-line driver
use --non-docker if you are testing without Docker
""
python api_frontend.py --non-docker
run detections on a test image to load the model
"part is a BodyPart object with b'Content-Type', and b'Content-Disposition', the later includes 'name' and 'filename' info"
read the images anew for each request
images are read and in each request by the time we call the API in map()
input validation
plot the images
adjust the figure
"read in dataset CSV and create merged (dataset, location) col"
map label to label_index
load the splits
only weight the training set by detection confidence
TODO: consider weighting val and test set as well
isotonic regression calibration of MegaDetector confidence
treat each split separately
new_weight[i] = confidence[i] * (n / c) / total_confidence(i's label)
- n = # examples in split (weighted by confidence); c = # labels
- weight allocated to each label is n/c
"- within each label, weigh each example proportional to confidence"
- new weights sum to n
error checking
"maps output label name to set of (dataset, dataset_label) tuples"
find which other label (label_b) has intersection
input validation
create label index JSON
look into sklearn.preprocessing.MultiLabelBinarizer
Note: JSON always saves keys as strings!
"True for ground truth, False for MegaDetector"
always save as .jpg for consistency
get bounding boxes
convert from category ID to category name
"check if crops are already downloaded, and ignore bboxes below the"
confidence threshold
assign all images without location info to 'unknown_location'
remove images from labels that have fewer than min_locs locations
merge dataset and location into a single string '<dataset>/<location>'
"create DataFrame of counts. rows = locations, columns = labels"
label_count: label => number of examples
loc_count: label => number of locs containing that label
generate a new split
score the split
SSE for # of images per label (with 2x weight)
SSE for # of locs per label
label => list of datasets to prioritize for test and validation sets
"merge dataset and location into a tuple (dataset, location)"
sorted smallest to largest
greedily add to test set until it has >= 15% of images
sort the resulting locs
"modify loc_to_size in place, so copy its keys before iterating"
arguments relevant to both creating the dataset CSV and splits.json
arguments only relevant for creating the dataset CSV
arguments only relevant for creating the splits JSON
comment lines starting with '#' are allowed
""
prepare_classification_script.py
""
Notebook-y script used to prepare a series of shell commands to run a classifier
(other than MegaClassifier) on a MegaDetector result set.
""
Differs from prepare_classification_script_mc.py only in the final class mapping step.
""
%% Job options
%% Constants
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
%% Set up environment
commands.append('cd CameraTraps/classification\n')
commands.append('conda activate cameratraps-classifier\n')
%% Crop images
fn = input_files[0]
%% Run classifier
fn = input_files[0]
%% Merge classification and detection outputs
fn = input_files[0]
%% Write everything out
input validation
evaluating with accimage is much faster than Pillow or Pillow-SIMD
create output directory
override saved params with kwargs
"For now, we don't weight crops by detection confidence during"
evaluation. But consider changing this.
"create model, compile with TorchScript if given checkpoint is not compiled"
"verify that target names matches original ""label names"" from dataset"
"if the dataset does not already have a 'other' category, then the"
'other' category must come last in label_names to avoid conflicting
with an existing label_id
define loss function (criterion)
"this file ends up being huge, so we GZIP compress it"
double check that the accuracy metrics are computed properly
save the confusion matrices to .npz
save per-label statistics
set dropout and BN layers to eval mode
"even if batch contains sample weights, don't use them"
Do target mapping on the outputs (unnormalized logits) instead of
"the normalized (softmax) probabilities, because the loss function"
uses unnormalized logits. Summing probabilities is equivalent to
log-sum-exp of unnormalized logits.
"a confusion matrix C is such that C[i,j] is the # of observations known to"
be in group i and predicted to be in group j.
match pytorch EfficientNet model names
images dataset
"for smaller disk / memory usage, we cache the raw JPEG bytes instead"
of the decoded Tensor
convert JPEG bytes to a 3D uint8 Tensor
"keras EfficientNet already includes normalization from [0, 255] to [0, 1],"
so we don't need to do that here
labels dataset
img_files dataset
weights dataset
define the transforms
efficientnet data preprocessing:
- train:
"1) random crop: aspect_ratio_range=(0.75, 1.33), area_range=(0.08, 1.0)"
2) bicubic resize to img_size
3) random horizontal flip
- test:
1) center crop
2) bicubic resize to img_size
weights sums to:
- if weight_by_detection_conf: (# images in split - conf delta)
- otherwise: (# images in split)
"freeze the base model's weights, including BatchNorm statistics"
https://www.tensorflow.org/guide/keras/transfer_learning#fine-tuning
rebuild output
input validation
set seed
create logdir and save params
create dataloaders and log the index_to_label mapping
define loss function and optimizer
using EfficientNet training defaults
- batch norm momentum: 0.99
"- optimizer: RMSProp, decay 0.9 and momentum 0.9"
- epochs: 350
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs"
- weight decay: 1e-5
TODO: change weighted to False if oversampling minority classes
stop training after 8 epochs without improvement
log metrics
log confusion matrix
log tp/fp/fn images
"tf.summary.image requires input of shape [N, H, W, C]"
false positive for top3_pred[0]
false negative for label
"if evaluating or finetuning, set dropout & BN layers to eval mode"
"for each label, track 5 most-confident and least-confident examples"
"even if batch contains sample weights, don't use them"
we do not track L2-regularization loss in the loss metric
This dictionary will get written out at the end of this process; store
diagnostic variables here
error checking
refresh detection cache
save log of bad images
cache of Detector outputs: dataset name => {img_path => detection_dict}
img_path: <dataset-name>/<img-filename>
get SAS URL for images container
strip image paths of dataset name
save list of dataset names and task IDs for resuming
complete task name: 'detect_for_classifier_caltech_20200722_110816_task01'
HACK! Sleep for 10s between task submissions in the hopes that it
"decreases the chance of backend JSON ""database"" corruption"
task still running => continue
"task finished successfully, save response to disk"
error checking before we download and crop any images
convert from category ID to category name
we need the datasets table for getting SAS keys
"we already did all error checking above, so we don't do any here"
get ContainerClient
get bounding boxes
we must include the dataset <ds> in <crop_path_template> because
'{img_path}' actually gets populated with <img_file> in
load_and_crop()
"get the image, either from disk or from Blob Storage"
inelegant way to close the container_clients
""
prepare_classification_script_mc.py
""
Notebook-y script used to prepare a series of shell commands to run MegaClassifier
on a MegaDetector result set.
""
Differs from prepare_classification_script.py only in the final class mapping step.
""
%% Job options
%% Constants
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
%% Set up environment
commands.append('cd CameraTraps/classification\n')
commands.append('conda activate cameratraps-classifier\n')
%% Crop images
fn = input_files[0]
%% Run classifier
fn = input_files[0]
%% Remap classifier outputs
fn = input_files[0]
%% Merge classification and detection outputs
fn = input_files[0]
%% Write everything out
"accimage backend is faster than Pillow/Pillow-SIMD, but occasionally crashes"
tv.set_image_backend('accimage')
mean/std values from https://pytorch.org/docs/stable/torchvision/models.html
define the transforms
resizes smaller edge to img_size
weights sums to:
- if weight_by_detection_conf: (# images in split - conf delta)
- otherwise: # images in split
for normal (non-weighted) shuffling
set all parameters to not require gradients except final FC layer
replace final fully-connected layer (which has 1000 ImageNet classes)
"detect GPU, use all if available"
input validation
set seed
create logdir and save params
create dataloaders and log the index_to_label mapping
create model
define loss function and optimizer
using EfficientNet training defaults
- batch norm momentum: 0.99
"- optimizer: RMSProp, decay 0.9 and momentum 0.9"
- epochs: 350
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs"
- weight decay: 1e-5
stop training after 8 epochs without improvement
do a complete evaluation run
log metrics
log confusion matrix
log tp/fp/fn images
"for every image: undo normalization, clamp to [0, 1], CHW -> HWC"
"- cannot be in-place, because the HeapItem might be in multiple heaps"
writer.add_figure() has issues => using add_image() instead
"writer.add_figure(f'{label_name}/{tag}', fig, global_step=epoch)"
false positive for top3_pred[0]
false negative for label
"preds and labels both have shape [N, k]"
"if evaluating or finetuning, set dropout and BN layers to eval mode"
"for each label, track k_extreme most-confident and least-confident images"
"even if batch contains sample weights, don't use them"
lock before changing ImageFile.LOAD_TRUNCATED_IMAGES
"filter out confidences below the threshold, and set precision to 4"
sort from highest to lowest probability
input validation
load classification CSV
load label names
input validation
extract dataset name from crop path so we can process 1 dataset at a time
randomly sample images for each class
"load queried images JSON, needed for ground-truth bbox info"
compare info dicts
compare detection categories
crop_path: <dataset>/<img_file>___cropXX_mdvY.Y.jpg
[----<img_path>----]       [-<suffix>--]
file has detection entry
bounding box is from ground truth
crop_path: <dataset>/<img_file>___cropXX_mdvY.Y.jpg
[----<img_path>----]       [-<suffix>--]
input validation
use MegaDB to generate list of images
only keep images that:
"1) end in a supported file extension, and"
2) actually exist in Azure Blob Storage
3) belong to a label with at least min_locs locations
write out log of images / labels that were removed
"save label counts, pre-subsampling"
"save label counts, post-subsampling"
spec_dict['taxa']: list of dict
[
"{'level': 'family', 'name': 'cervidae', 'datasets': ['idfg']},"
"{'level': 'genus',  'name': 'meleagris'}"
]
"spec_dict['dataset_labels']: dict, dataset => list of dataset_label"
{
"""idfg"": [""deer"", ""elk"", ""prong""],"
"""idfg_swwlf_2019"": [""elk"", ""muledeer"", ""whitetaileddeer""]"
}
"maps output label name to set of (dataset, dataset_label) tuples"
"because MegaDB is organized by dataset, we do the same"
ds_to_labels = {
'dataset_name': {
"'dataset_label': [output_label1, output_label2]"
}
}
we need the datasets table for getting full image paths
The line
"[img.class[0], seq.class[0]][0] as class"
selects the image-level class label if available. Otherwise it selects the
"sequence-level class label. This line assumes the following conditions,"
expressed in the WHERE clause:
- at least one of the image or sequence class label is given
- the image and sequence class labels are arrays with length at most 1
- the image class label takes priority over the sequence class label
""
"In Azure Cosmos DB, if a field is not defined, then it is simply excluded"
"from the result. For example, on the following JSON object,"
{
"""dataset"": ""camera_traps"","
"""seq_id"": ""1234"","
"""location"": ""A1"","
"""images"": [{""file"": ""abcd.jpeg""}],"
"""class"": [""deer""],"
}
"the array [img.class[0], seq.class[0]] just gives ['deer'] because"
img.class is undefined and therefore excluded.
"if no path prefix, set it to the empty string '', because"
"os.path.join('', x, '') = '{x}/'"
result keys
"- already has: ['dataset', 'location', 'file', 'class', 'bbox']"
"- add ['label'], remove ['file']"
"if img is mislabeled, but we don't know the correct class, skip it"
"otherwise, update the img with the correct class, but skip the"
img if the correct class is not one we queried for
sort keys for determinism
we need the datasets table for getting SAS keys
strip leading '?' from SAS token
only check Azure Blob Storage
check local directory first before checking Azure Blob Storage
1st pass: populate label_to_locs
"label (tuple of str) => set of (dataset, location)"
2nd pass: eliminate bad images
prioritize is a list of prioritization levels
number of already matching images
main(
"label_spec_json_path='idfg_classes.json',"
"taxonomy_csv_path='../../camera-traps-private/camera_trap_taxonomy_mapping.csv',"
"output_dir='run_idfg',"
json_indent=4)
recursively find all files in cropped_images_dir
only find crops of images from detections JSON
resizes smaller edge to img_size
evaluating with accimage is much faster than Pillow or Pillow-SIMD
create dataset
create model
set dropout and BN layers to eval mode
load files
dataset => set of img_file
crop_path: <dataset>/<img_file>___cropXX_mdvY.Y.jpg
[----<img_path>----]
task finished successfully
parse the task ID
print info about missing and failed images
get the detections
add detections to the detections cache
combine detections with cache
write combined detections back out to cache
error checking
any row with 'correct_class' should be marked 'mislabeled'
filter to only the mislabeled rows
convert '\' to '/'
verify that overlapping indices are the same
"""add"" any new mislabelings"
write out results
error checking
load detections JSON
get detector version
convert from category ID to category name
copy keys to modify dict in-place
This will be removed later when we filter for animals
save log of bad images
"True for ground truth, False for MegaDetector"
always save as .jpg for consistency
"we already did all error checking above, so we don't do any here"
"get the image, either from disk or from Blob Storage"
inelegant way to close the container_client
"crop_path => normalized bbox coordinates [xmin, ymin, width, height]"
"only ground-truth bboxes do not have a ""confidence"" value"
try loading image from local directory
try to download image from Blob Storage
crop the image
"expand box width or height to be square, but limit to img size"
"Image.crop() takes box=[left, upper, right, lower]"
pad to square using 0s
Author: lukemelas (github username)
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch
With adjustments and added comments by workingcoder (github username).
Support the construction of 'efficientnet-l2' without pretrained weights
Expansion phase (Inverted Bottleneck)
"image_size = calculate_output_image_size(image_size, 1) <-- this wouldn't modify image_size"
Depthwise convolution phase
"Squeeze and Excitation layer, if desired"
Pointwise convolution phase
Expansion and Depthwise Convolution
Squeeze and Excitation
Pointwise Convolution
Skip connection and drop connect
The combination of skip connection and drop connect brings about stochastic depth.
Batch norm parameters
Get stem static or dynamic convolution depending on image size
Stem
Build blocks
Update block input and output filters based on depth multiplier.
The first block needs to take care of stride and filter size increase.
"image_size = calculate_output_image_size(image_size, block_args.stride)  # stride = 1"
Head
Final linear layer
Stem
Blocks
Head
Stem
Blocks
Head
Convolution layers
Pooling and final linear layer
Author: lukemelas (github username)
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch
With adjustments and added comments by workingcoder (github username).
###############################################################################
## Help functions for model architecture
###############################################################################
GlobalParams and BlockArgs: Two namedtuples
Swish and MemoryEfficientSwish: Two implementations of the method
round_filters and round_repeats:
Functions to calculate params for scaling model width and depth ! ! !
get_width_and_height_from_size and calculate_output_image_size
drop_connect: A structural design
get_same_padding_conv2d:
Conv2dDynamicSamePadding
Conv2dStaticSamePadding
get_same_padding_maxPool2d:
MaxPool2dDynamicSamePadding
MaxPool2dStaticSamePadding
"It's an additional function, not used in EfficientNet,"
but can be used in other model (such as EfficientDet).
"Parameters for the entire model (stem, all blocks, and head)"
Parameters for an individual model block
Set GlobalParams and BlockArgs's defaults
An ordinary implementation of Swish function
A memory-efficient implementation of Swish function
TODO: modify the params names.
"maybe the names (width_divisor,min_width)"
"are more suitable than (depth_divisor,min_depth)."
follow the formula transferred from official TensorFlow implementation
follow the formula transferred from official TensorFlow implementation
"generate binary_tensor mask according to probability (p for 0, 1-p for 1)"
Note:
The following 'SamePadding' functions make output size equal ceil(input size/stride).
"Only when stride equals 1, can the output size be the same as input size."
Don't be confused by their function names ! ! !
Tips for 'SAME' mode padding.
Given the following:
i: width or height
s: stride
k: kernel size
d: dilation
p: padding
Output after Conv2d:
o = floor((i+p-((k-1)*d+1))/s+1)
"If o equals i, i = floor((i+p-((k-1)*d+1))/s+1),"
=> p = (i-1)*s+((k-1)*d+1)-i
With the same calculation as Conv2dDynamicSamePadding
Calculate padding based on image size and save it
Calculate padding based on image size and save it
###############################################################################
## Helper functions for loading model params
###############################################################################
BlockDecoder: A Class for encoding and decoding BlockArgs
efficientnet_params: A function to query compound coefficient
get_model_params and efficientnet:
Functions to get BlockArgs and GlobalParams for efficientnet
url_map and url_map_advprop: Dicts of url_map for pretrained weights
load_pretrained_weights: A function to load pretrained weights
Check stride
"Coefficients:   width,depth,res,dropout"
Blocks args for the whole model(efficientnet-b0 by default)
It will be modified in the construction of EfficientNet Class according to model
note: all models have drop connect rate = 0.2
ValueError will be raised here if override_params has fields not included in global_params.
train with Standard methods
check more details in paper(EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks)
train with Adversarial Examples(AdvProp)
check more details in paper(Adversarial Examples Improve Image Recognition)
TODO: add the petrained weights url map of 'efficientnet-l2'
AutoAugment or Advprop (different preprocessing)
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils)
from data_management.megadb.schema import sequences_schema_check
resize is for displaying them more quickly
dataset and seq_id are required fields
sort the images in the sequence
"if no class label on the image, show class label on the sequence"
pool = ThreadPool()
"print('len of rendering_info', len(rendering_info))"
"tqdm(pool.imap_unordered(render_image_info_partial, rendering_info), total=len(rendering_info))"
options = write_html_image_list()
options['headerHtml']
print('Checking that the MegaDB entries conform to the schema...')
sequences_schema_check.sequences_schema_check(sequences)
#######
""
visualize_db.py
""
Outputs an HTML page visualizing annotations (class labels and/or bounding boxes)
on a sample of images in a database in the COCO Camera Traps format
""
#######
%% Imports
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils)
Assumes the cameratraps repo root is on the path
%% Settings
Set to None to visualize all images
Target size for rendering; set either dimension to -1 to preserve aspect ratio
"These are mutually exclusive; both are category names, not IDs"
"Special tag used to say ""show me all images with multiple categories"""
We sometimes flatten image directories by replacing a path separator with
another character.  Leave blank for the typical case where this isn't necessary.
Control rendering parallelization
%% Helper functions
"Translate the file name in an image entry in the json database to a path, possibly doing"
some manipulation of path separators
%% Core functions
"Optionally remove all images without bounding boxes, *before* sampling"
"Optionally include/remove images with specific labels, *before* sampling"
Put the annotations in a dataframe so we can select all annotations for a given image
Construct label map
Take a sample of images
Set of dicts representing inputs to render_db_bounding_boxes:
""
"bboxes, boxClasses, image_path"
iImage = 0
All the class labels we've seen for this image (with out without bboxes)
Iterate over annotations for this image
iAnn = 0; anno = annos_i.iloc[iAnn]
"We're adding html for an image before we render it, so it's possible this image will"
fail to render.  For applications where this script is being used to debua a database
"(the common case?), this is useful behavior, for other applications, this is annoying."
""
TODO: optionally write html only for images where rendering succeeded
...for each image
...def render_image_info
def process_images(...)
%% Command-line driver
"Copy all fields from a Namespace (i.e., the output from parse_args) to an object."
""
Skips fields starting with _.  Does not check existence in the target object.
Convert to an options object
%% Interactive driver(s)
%%
os.startfile(htmlOutputFile)
%% Constants and imports
convert category ID from int to str
Retry on blob storage read failures
%% Functions
PIL.Image.convert() returns a converted copy of this image
Alter orientation as needed according to EXIF tag 0x112 (274) for Orientation
""
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28
https://www.media.mit.edu/pia/Research/deepview/exif.html
""
Null operation
Aspect ratio as width over height
ar = w / h
h = w / ar
w = ar * h
The following three functions are modified versions of those at:
https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py
Convert to pixels so we can use the PIL crop() function
PIL's crop() does surprising things if you provide values outside of
"the image, clip inputs"
...if this detection is above threshold
...for each detection
"To avoid duplicate colors with detection-only visualization, offset"
the classification class index by the number of detection classes
...if we have detection results
...if the confidence of this detection is above threshold
...for each detection
"print('Input must be of size [N, 4], but is ' + str(boxes_shape))"
Deliberately trimming to the width of the image only in the case where
"box expansion is turned on.  There's not an obvious correct behavior here,"
but the thinking is that if the caller provided an out-of-range bounding
"box, they meant to do that, but at least in the eyes of the person writing"
"this comment, if you expand a box for visualization reasons, you don't want"
to end up with part of a box.
""
A slightly more sophisticated might check whether it was in fact the expansion
"that made this box larger than the image, but this is the case 99.999% of the time"
"here, so that doesn't seem necessary."
If the total height of the display strings added to the top of the bounding
"box exceeds the top of the image, stack the strings below the bounding box"
instead of above.
Each display_str has a top and bottom margin of 0.05x.
Reverse list and print from bottom to top.
%% Imports
%% Constants
convert category ID from int to str
%% Main function
arguments error checking
we don't import sas_blob_utils at the top of this file in order to
accommodate the MegaDetector Colab notebook which does not have
the azure-storage-blob package installed
%% Load detector output
"%% Load images, annotate them and save"
max_conf = entry['max_detection_conf']
resize is for displaying them more quickly
%% Command-line driver
""
stacked bar charts are made with each segment starting from a y position
"To fit the legend in, shrink current axis by 20%"
Put a legend to the right of the current axis
####
""
visualize_incoming_annotations.py
""
Spot-check the annotations received from iMerit by visualizing annotated bounding
boxes on a sample of images and display them in HTML.
""
"Modified in 2021 March to use the new format (iMerit batch 12 onwards), which is a"
COCO formatted JSON with relative coordinates for the bbox.
""
from data_management.megadb.schema import sequences_schema_check
we used frame_num of 1 when sending out images to annotators when it is not explicitly stored (wcs esp.)
The file_name field in the incoming json looks like
alka_squirrels.seq2020_05_07_25C.frame119221.jpg
"we need to use the dataset, sequence and frame info to find the actual path in blob storage"
using the sequences
category_id 5 is No Object Visible
download the image
Write to HTML
allow forward references in typing annotations
class variables
instance variables
get path to root
taxonomy CSV rows without 'taxonomy_string' entries are excluded
"from the taxonomy graph, but can be included in a classification"
label specification JSON via the 'dataset_labels' key
use the lower parent
special cases
%% Imports
%% Taxnomy checking
taxonomy CSV rows without 'taxonomy_string' entries are excluded
"from the taxonomy graph, but can be included in a classification"
label specification JSON via the 'dataset_labels' key
...for each row in the taxnomy file
%% Command-line driver
%% Interactive driver
%%
which datasets are already processed?
"sequence-level query should be fairly fast, ~1 sec"
cases when the class field is on the image level (images in a sequence
"that had different class labels, 'caltech' dataset is like this)"
"this query may take a long time, >1hr"
"this query should be fairly fast, ~1 sec"
read species presence info from the JSON files for each dataset
has this class name appeared in a previous dataset?
columns to populate the spreadsheet
sort by descending species count
make the spreadsheet
hyperlink Bing search URLs
hyperlink example image SAS URLs
TODO hardcoded columns: change if # of examples or col_order changes
""
retrieve_sample_image.py
""
"Downloader that retrieves images from Google images, used for verifying taxonomy"
"lookups and looking for egregious mismappings (e.g., ""snake"" being mapped to a fish called"
"""snake"")."
""
"Simple wrapper around simple_image_download, but I've had to swap in and out the underlying"
downloader a few times.
""
%% Imports and environment
%% Test driver
%% Main entry point
""
simple_image_download.py
""
Cloned from:
""
https://github.com/RiddlerQ/simple_image_download
""
Slighty modified to take an output directory.
""
or '.ico' in object_raw or '.gif' in object_raw:
Todo: we have no evidence these are jpegs
######
""
species_lookup.py
""
Look up species names (common or scientific) in the GBIF and iNaturalist
taxonomies.
""
Run initialize_taxonomy_lookup() before calling any other function.
""
######
%% Constants and imports
As of 2020.05.12:
""
"GBIF: ~777MB zipped, ~1.6GB taxonomy"
"iNat: ~2.2GB zipped, ~51MB taxonomy"
These are un-initialized globals that must be initialized by
the initialize_taxonomy_lookup() function below.
%% Functions
Initialization function
# Load serialized taxonomy info if we've already saved it
"# If we don't have serialized taxonomy info, create it from scratch."
Download and unzip taxonomy files
Don't download the zipfile if we've already unzipped what we need
Bypasses download if the file exists already
Unzip the files we need
...for each file that we need from this zipfile
Remove the zipfile
os.remove(zipfile_path)
...for each taxonomy
"Create dataframes from each of the taxonomy files, and the GBIF common"
name file
Load iNat taxonomy
Load GBIF taxonomy
Remove questionable rows from the GBIF taxonomy
Load GBIF vernacular name mapping
Only keep English mappings
Convert everything to lowercase
"For each taxonomy table, create a mapping from taxon IDs to rows"
Create name mapping dictionaries
Build iNat dictionaries
row = inat_taxonomy.iloc[0]
Build GBIF dictionaries
"The canonical name is the Latin name; the ""scientific name"""
include the taxonomy name.
""
http://globalnames.org/docs/glossary/
This only seems to happen for really esoteric species that aren't
"likely to apply to our problems, but doing this for completeness."
Don't include taxon IDs that were removed from the master table
Save everything to file
...def initialize_taxonomy_lookup()
"list of dicts: {'source': source_name, 'taxonomy': match_details}"
i_match = 0
"list of (taxon_id, taxonRank, scientific name, [vernacular names])"
corresponding to an exact match and its parents
Walk taxonomy hierarchy
This can happen because we remove questionable rows from the
GBIF taxonomy
The GBIF taxonomy contains unranked entries
...while there is taxonomy left to walk
...for each match
Remove redundant matches
i_tree_a = 0; tree_a = matching_trees[i_tree_a]
i_tree_b = 1; tree_b = matching_trees[i_tree_b]
"If tree a's primary taxon ID is inside tree b, discard tree a"
""
taxonomy_level_b = tree_b['taxonomy'][0]
...for each level in taxonomy B
...for each tree (inner)
...for each tree (outer)
...def traverse_taxonomy()
"print(""Finding taxonomy information for: {0}"".format(query))"
"In GBIF, some queries hit for both common and scientific, make sure we end"
up with unique inputs
"If the species is not found in either taxonomy, return None"
Both GBIF and iNat have a 1-to-1 mapping between taxon_id and row number
Walk both taxonomies
...def get_taxonomic_info()
m = matches[0]
"For example: [(9761484, 'species', 'anas platyrhynchos')]"
...for each taxonomy level
...for each match
...def print_taxonomy_matches()
%% Interactive drivers and debug
%% Initialization
%% Taxonomic lookup
query = 'lion'
print(matches)
Print the taxonomy in the taxonomy spreadsheet format
%% Directly access the taxonomy tables
%% Command-line driver
Read command line inputs (absolute path)
Read the tokens from the input text file
Loop through each token and get scientific name
""
process_species_by_dataset
""
We generated a list of all the annotations in our universe; this script is
used to (interactively) map them onto the GBIF and iNat taxonomies.  Don't
"try to run this script from top to bottom; it's used like a notebook, not like"
"a script, since manual review steps are required."
""
%% Imports
%autoreload 0
%autoreload -species_lookup
%% Constants
Input file
Output file after automatic remapping
File to which we manually copy that file and do all the manual review; this
should never be programmatically written to
The final output spreadsheet
HTML file generated to facilitate the identificaiton of egregious mismappings
%% Functions
Prefer iNat matches over GBIF matches
query = 'person'
Do we have an iNat match?
"print_taxonomy_matches(inat_matches, verbose=True)"
"print_taxonomy_matches(gbif_matches, verbose=True)"
print('Warning: multiple iNat matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple iNat common names for {query}')
Default to returning the query
"print(f'Matched iNat {query} to {scientific_name},{common_name}')"
...if we had iNat matches
If we either prefer GBIF or didn't have iNat matches
""
Code is deliberately redundant here; I'm expecting some subtleties in how
handle GBIF and iNat.
print('Warning: multiple GBIF matches for {}'.format(query))
Prefer chordates... most of the names that aren't what we want
"are esoteric insects, like a moth called ""cheetah"""
""
"If we can't find a chordate, just take the first match."
""
i_test_match = 0
"This is (taxonID, taxonLevel, scientific, [list of common])"
print(f'Warning: multiple GBIF common names for {query}')
Default to returning the query
...if we needed to look in the GBIF taxonomy
...def get_preferred_taxonomic_match()
%% Initialization
%% Test single-query lookup
%%
%%
"q = ""grevy's zebra"""
%% Read the input data
%% Run all our taxonomic lookups
i_row = 0; row = df.iloc[i_row]
query = 'lion'
...for each query
Write to the excel file that we'll use for manual review
%% Download preview images for everything we successfully mapped
uncomment this to load saved output_file
"output_df = pd.read_excel(output_file, keep_default_na=False)"
i_row = 0; row = output_df.iloc[i_row]
...for each query
%% Write HTML file with representative images to scan for obvious mis-mappings
i_row = 0; row = output_df.iloc[i_row]
...for each row
%% Look for redundancy with the master table
Note: `master_table_file` is a CSV file that is the concatenation of the
"manually-remapped files (""manual_remapped.xlsx""), which are the output of"
this script run across from different groups of datasets. The concatenation
"should be done manually. If `master_table_file` doesn't exist yet, skip this"
"code cell. Then, after going through the manual steps below, set the final"
manually-remapped version to be the `master_table_file`.
%% Manual review
Copy the spreadsheet to another file; you're about to do a ton of manual
review work and you don't want that programmatically overwrriten.
""
See manual_review_xlsx above
%% Read back the results of the manual review process
%% Look for manual mapping errors
Manually inspect df for typos in 'dataset_names' and 'taxonomy_level' columns
Identify rows where:
""
- 'taxonomy_level' does not match level of 1st element in 'taxonomy_string'
- 'scientific_name' does not match name of 1st element in 'taxonomy_string'
""
...both of which typically represent manual mapping errors.
i_row = 0; row = df.iloc[i_row]
"I'm not sure why both of these checks are necessary, best guess is that"
the Excel parser was reading blanks as na on one OS/Excel version and as ''
on another.
The taxonomy_string column is a .json-formatted string; expand it into
an object via eval()
"%% Find scientific names that were added manually, and match them to taxonomies"
i_row = 0; row = df.iloc[i_row]
...for each query
%% Write out final version
""
prepare_api_output_for_timelapse.py
""
Takes output from the batch API and does some conversions to prepare
it for use in Timelapse.
""
Specifically:
""
* Removes the class field from each bounding box
* Optionally does query-based subsetting of rows
* Optionally does a search and replace on filenames
* Replaces backslashes with forward slashes
"* Renames ""detections"" to ""predicted_boxes"""
""
"Note that ""relative"" paths as interpreted by Timelapse aren't strictly relative as"
of 6/5/2019.  If your project is in:
""
c:\myproject
""
...and your .tdb file is:
""
c:\myproject\blah.tdb
""
...and you have an image at:
""
c:\myproject\imagefolder1\img.jpg
""
The .csv that Timelapse sees should refer to this as:
""
myproject/imagefolder1/img.jpg
""
...*not* as:
""
imagefolder1/img.jpg
""
Hence all the search/replace functionality in this script.  It's very straightforward
"once you get this and doesn't take time, but it's easy to forget to do this.  This will"
be fixed in an upcoming release.
""
%% Constants and imports
Python standard
pip-installable
"AI4E repos, expected to be available on the path"
%% Helper classes
Only process rows matching this query (if not None); this is processed
after applying os.normpath to filenames.
"If not none, replace the query token with this"
"If not none, prepend matching filenames with this"
%% Helper functions
"If there's no query, we're just pre-pending"
%% Main function
Create a temporary column we'll use to mark the rows we want to keep
This is the main loop over rows
Trim to matching rows
Timelapse legacy issue; we used to call this column 'predicted_boxes'
Write output
"write_api_results(detectionResults,outputFilename)"
%% Interactive driver
%%
%% Command-line driver (** outdated **)
"Copy all fields from a Namespace (i.e., the output from parse_args) to an object."
""
Skips fields starting with _.  Does not check existence in the target object.
"print('Setting {} to {}'.format(n,v))"
Convert to an options object
--------some stuff needed to get AJAX to work with bottle?--------#
-------------------------------------------------------------------------------- #
PREPARE TO QUEUE IMAGES FOR LABELING
-------------------------------------------------------------------------------- #
# Connect as USER to database DB_NAME through peewee and initialize database proxy
# Load embedding model
---------------------------------------------------------------------- #
CREATE QUEUE OF IMAGES TO LABEL
---------------------------------------------------------------------- #
Use classifier to generate predictions
# Update model predicted class in PostgreSQL database
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
-------------------------------------------------------------------------------- #
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI
-------------------------------------------------------------------------------- #
"# static routes (to serve CSS, etc.)"
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps"
"return bottle.static_file(filename, root='../../../../../../../../../../../.')"
# dynamic routes
if data['display_grayscale']:
indices_to_exclude.update(set(color_indices))
elif not data['display_grayscale']:
indices_to_exclude.update(set(grayscale_indices))
data['display_images'] = {}
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices]
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices]
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices]
data['display_images']['detection_categories'] = []
for i in indices:
if str(dataset.samples[i][1]) == 'None':
data['display_images']['detection_categories'].append('None')
else:
existing_category_entries = {cat.id: cat.name for cat in Category.select()}
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()"
data['display_images']['detection_categories'].append(cat_name)
Use image ids in images_to_label to get the corresponding dataset indices
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
print(set(dataset.set_indices[4]).update(set(indices_to_label)))
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Use image id images_to_label to get the corresponding dataset index
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Train on samples that have been labeled so far
dataset.set_kind(DetectionKind.UserDetection.value)
print(y_train)
Predict on the samples that have not been labeled
print(y_pred)
Update model predicted class in PostgreSQL database
timer = time.time()
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
print('Updating the database took %0.2f seconds'%(time.time() - timer))
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
"once the classifier has been trained the first time, switch to AL sampling"
""
make_oneclass_json.py
""
"Takes a coco-camera-traps .json database and collapses species classes to binary,"
optionally removing labels from empty images (to be detector-friendly) (depending on
"""experiment_type"")."
""
"Assumes that empty images are labeled as ""empty""."
""
%% Imports and environment
%% Core conversion function
"We're removing empty images from the annotation list, but not from"
"the ""images"" list; they'll still get used in detector training."
print('Ignoring empty annotation')
%% Interactive driver
%%
Load annotations
Convert from multi-class to one-class
Write out the one-class data
%% Command-line driver
""
plot_bounding_boxes.py
""
Takes a .json database containing bounding boxes and renders those boxes on the
source images.
""
"This assumes annotations in coco-camera-traps format, with absolute bbox"
coordinates.
""
%% Imports and environment
How many images should we process?  Set to -1 to process all images.
Should we randomize the image order?
%%  Read database and build up convenience mappings
Category ID to category name
Image ID to image info
Image ID to image path
Image ID to all annotations referring to this image
"%% Iterate over images, draw bounding boxes, write to file"
For each image
image = images[0]
Build up a list of bounding boxes to draw on this image
Load the image
Create figure and axes
Display the image
ann = imageAnnotations[0]
"For each annotation associated with this image, render bounding box and label"
"In the Rectangle() function, the first argument (""location"") is the bottom-left"
of the rectangle.
""
Origin is the upper-left of the image.
Add the patch to the Axes
Add a class label
This is magic goop that removes whitespace around image plots (sort of)
Write the output image
...for each image
""
plot_imerit_annotations.py
""
Takes a .json file full of bounding box annotations and renders those boxes on the
source images.
""
"This assumes annotations in the format we receive them, specifically:"
""
1) Relative bbox coordinates
"2) A list of .json objects, not a well-formatted .json file"
""
"I.e., don't use this on a COCO-style .json file.  See plot_bounding_boxes.py"
for the same operation performed on a proper COCO-camera-traps database.
""
%% Imports and environment
"os.makedirs(outputBase, exist_ok=True)"
%%  Read all source images and build up a hash table from image name to full path
"This spans training and validation directories, so it's not the same as"
just joining the image name to a base path
"%% Iterate over annotations, draw bounding boxes, write to file"
annData has keys:
""
"annotations, categories, images"
""
Each of these are lists of dictionaries
%% Render all annotations on each image in the sequence
%% Pull out image metadata
Build up a list of bounding boxes to draw on this image
Pull out just the image name from the filename
""
File names look like:
""
seq6efffac2-5567-11e8-b3fe-dca9047ef277.frame1.img59a94e52-23d2-11e8-a6a3-ec086b02610b.jpg
"m = re.findall(r'img(.*\.jpg)$', imgFileName, re.M|re.I)"
print(m)
assert(len(m) == 1)
queryFileName = m[0]
Map this image back to the original directory
"%% Loop over annotations, find annotations that match this image"
%%
"x,y,w,h"
""
"x,y is the bottom-left of the rectangle"
""
"x,y origin is the upper-left"
...for each annotation
%% Render with PIL (scrap)
%% Render with Matplotlib
Create figure and axes
Display the image
Location is the bottom-left of the rect
""
Origin is the upper-left
Add the patch to the Axes
This is magic goop that removes whitespace around image plots (sort of)
os.startfile(outputFileName)
%% Showing figures on-screen during debugging
plt.show()
Various (mostly unsuccessful) approaches to getting the plot window to show up
"in the foreground, which is a backend-specific operation..."
""
fig.canvas.manager.window.activateWindow()
fig.canvas.manager.window.raise_()
fm = plt.get_current_fig_manager()
"fm.window.attributes('-topmost', 1)"
"fm.window.attributes('-topmost', 0)"
""
# This is the one that I found to be most robust... at like 80% robust.
plt.get_current_fig_manager().window.raise_()
%%
...for each image
...for each file
""
convert_imerit_json_to_coco_json.py
""
"Takes a .json file with bounding boxes but no class labels, and a .json file containing the"
"class labels for those images, and creates a new json file with class labels and bounding"
boxes.
""
"The bounding box .json file is in the format returned by our annotators, which is not"
actually a fully-formed .json file; rather it's a series of .json objects
""
"Leaves filenames intact.  Stores imerit ""category IDs"" (empty/human/group/animal) in a new"
"field called ""annotation_type""."
""
%% Imports and constants
%% Configure files and paths
"For Snapshot Serengeti, we stored image IDs in our annotation files as:"
""
S1_B06_R1_PICT0008
""
...but the corresponding ID in the master database is actually:
""
S1\B06\R1\S1_B06_R1_PICT0008
""
"If this is ""True"", we'll expand the former to the latter"
Handling a one-off issue in which .'s were mysteriously replaced with -'s
"in our annotations.  This will be set dynamically, but I keep it here as"
a constant to remind me to remove this code when we clean this issue up.
Used in the (rare) case where a bounding box was added to an image that was originally
annotated as empty
Used in the (rare) case where we added bounding boxes to an image with multiple species
"Should we include ambiguous bounding boxes (with the ""ambiguous"" category label)"
in the output file?  Ambiguous boxes are boxes drawn on images with multiple species.
"%%  Read metadata from the master database, bounding boxes from the annotations file"
"The bounding box .json file is in the format returned by our annotators, which is not"
actually a fully-formed .json file; rather it's a series of .json objects
"Each element of annData is a dictionary corresponding to a single sequence, with keys:"
""
"annotations, categories, images"
sequence = annData[0]
%% Build convenience mappings
Image ID to images
Category ID to categories (referring to the database categories)
"Image ID to categories (i.e., species labels)"
Utility function we'll use to create annotations for images in empty
sequences (empty images in non-empty sequences already have annotations)
"%% Reformat annotations, grabbing category IDs from the master database (prep)"
iSequence = 0; sequence = annData[0]
"%% Reformat annotations, grabbing category IDs from the master database (loop)"
Make a copy here; we're going to manipulate the sequence annotations
when we need to add synthetic annotations for empty images
im = sequenceImages[0]
Are there any annotations in this sequence?
Which images in this sequence have annotations?
For each image in this sequence...
imeritImageID = im['id']
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG
Confirm that the file exists
Hande a one-off issue with our annotations
datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG
""
...had become:
""
datasetsnapshotserengeti.seqASG000001a-frame0.imgS1_B06_R1_PICT0008.JPG
Does it look like we encountered this issue?
Convert:
""
S1_B06_R1_PICT0008
""
...to:
""
S1/B06/B06_R1/S1_B06_R1_PICT0008
Make sure we only see each image once
Create empty annotations for empty images
""
Here we use the *unmodified* file name
Create an empty annotation for this image
Annotations still use the annotation filename (not database ID) at this point;
these will get converted to database IDs below when we process the
whole sequence.
Sanity-check image size
"print('Warning: img {} was listed in DB as {}x{}, annotated as {}x{}, actual size{}x{}'.format("
"old_id,new_im['width'],new_im['height'],im['width'],im['height'],imgObj.width,imgObj.height))"
...for each image in this sequence
For each annotation in this sequence...
ann = sequenceAnnotations[0]
Prepare an annotation using the category ID from the database and
the bounding box from the annotations file
Maintain iMerit's annotation category
Generate an (arbitrary) ID for this annotation; the COCO format has a concept
"of annotation ID, but our annotation files don't"
This was a one-off quirk with our file naming
We'll do special handling of images with multiple categories later
Store the annotation type (group/human/animal/empty)
This annotation has no bounding box but the image wasn't originally
annotated as empty
This annotation has a bounding box but the image was originally
annotated as empty
unnormalize the bbox
... for each annotation in this sequence
... for each sequence
%% Post-processing
Count empty images
...for each file
%% Sanity-check empty images
""
make_ss_annotation_image_folder.py
""
Take a directory full of images with the very long filenames we give annotators:
""
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension
""
"...along with a COCO-camera-traps database referring to those files, and:"
""
1) Creates a new COCO-camera-traps database with the original filenames in them
(copying the annotations)
""
2) Optionally creates a new directory with those images named according to the
"Snapshot Serengeti naming convention, including complete relative paths."
""
See convert_imerit_json_to_coco_json to see how we get from the original annotation
.json to a COCO-camera-traps database.
""
%% Constants and imports
%% Configure files/paths
%% Read the annotations (referring to the old filenames)
"%% Update filenames, optionally copying files"
im = data['images'][0]
For each image...
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG
"Find the image name, e.g. S1_B06_R1_PICT0008"
Convert:
""
S1_B06_R1_PICT0008.JPG
""
...to:
""
S1/B06/B06_R1/S1_B06_R1_PICT0008.JPG
...for each image
%% Write the revised database
""
get_annotation_tool_link.py
""
"Takes a COCO-camera-traps-style .json file with URLs already embedded, and prepares"
a link to the visipedia annotation tool that reviews a subset of those images.
""
""
create_new_annotation_json.py
""
"Creates a subset of a larger .json database, in this case specifically to pick some images"
from Snapshot Serengeti.
""
from utils import get_db_dicts
for seq in already_annotated:
seq_to_ims.pop(seq)
remove already annotated images
add lion images
lion_seqs = cat_to_seqs[cat_to_id['lionMale']] + cat_to_seqs[cat_to_id['lionFemale']]
#print(len(lion_seqs))
lion_seqs = [seq for seq in lion_seqs if seq not in already_annotated]
#print(len(lion_seqs))
"lion_seqs_to_annotate.extend(random.sample(lion_seqs, min(len(lion_seqs),num_lions)))"
print(len(lion_seqs_to_annotate))
num_elephants = 1000
elephant_seqs = cat_to_seqs[cat_to_id['elephant']]
#print(len(lion_seqs))
elephant_seqs = [seq for seq in elephant_seqs if seq not in already_annotated]
#print(len(lion_seqs))
"elephant_seqs_to_annotate = random.sample(elephant_seqs, num_elephants)"
num_empty = 10
empty_seqs_to_annotate = []
for loc in loc_to_seqs:
empty_seqs = cats_per_location[loc][cat_to_id['empty']]
empty_seqs = [seq for seq in empty_seqs if seq not in already_annotated]
empty_seqs = [seq for seq in empty_seqs if seq_to_season[seq] in seasons_to_keep]
"empty_seqs_to_annotate.extend(random.sample(empty_seqs, min(len(empty_seqs),num_empty)))"
ims_to_annotate.extend(empty_ims_to_annotate)
""
filter_database.py
""
"Look through a COCO-ct database and find images matching some crtieria, writing"
a subset of images and annotations to a new file.
""
%% Constants and imports
%% Configuration
%%  Read database and build up convenience mappings
Category ID to category name
Image ID to image info
Image ID to image path
%% Filter
ann = annotations[0]
Is this a tiny box or a group annotation?
"x,y,w,h"
All empty annotations should be classified as either empty or ambiguous
""
"The ambiguous cases are basically minor misses on the annotators' part,"
where two different small animals were present somewhere.
%% Write output file
""
find_images_for_classes.py
""
"Given a .json database, find images that are associated with one or more"
classes.
""
%% Constants and imports
%% Configuration
%%  Read database and build up convenience mappings
Category ID to category name
Image ID to image info
Image ID to image path
%% Look for target-class annotations
ann = annotations[0]
""
create_tfrecords_format.py
""
This script creates a tfrecords file from a dataset in VOTT format.
%% Imports and environment
set up the filenames and annotations
This loop reads the bboxes and corresponding labels and assigns them
the correct image. Kind of slow at the moment...
If needed: merging all classes
bbox_labels = ['Animal' for _ in bbox_labels]
BBox coords are stored in the format
"x_min (of width axis) y_min (of height axis), x_max, y_max"
Coordinate system starts in top left corner
"In this framework, we need ('ymin', 'xmin', 'ymax', 'xmax') format"
print out some stats
To make sure we loaded the bboxes correctly:
self.validate_bboxes()
For each image in the data set...
Make sure all are greater equal 0
%% Main tfrecord generation function
Propagate optional metadata to tfrecords
endfor each annotation for the current image
endfor each image
"Calculate number of shards to get the desired number of images per record,"
ensure it is evenly divisible by the number of threads
%% Command-line driver
%% Driver
""
make_tfrecords_cis_trans.py
""
Given a .json file that contains a three-element list (train/val/test) of image IDs and a .json database that contains
"those image IDs, generates tfrecords whose filenames include ""train""/""val""/""test"""
""
"dataset = json.load(open('/ai4efs/databases/snapshotserengeti/oneclass/SnapshotSerengeti_Seasons_1_to_4_tfrecord_format_valid_ims.json','r'))"
print('Creating trans_val tfrecords')
dataset = [im_id_to_im[idx] for idx in trans_val]
""
create_classification_tfrecords_from_json.py
""
Called from make_tfrecords_cis_trans.py if you're running a classification experiment.
""
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/'
datafile = 'eccv_18_annotation_files_oneclass/CaltechCameraTrapsECCV18'
image_file_root = datafolder+'eccv_18_all_images/'
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/'
datafolder = '/data/iwildcam/'
datafile = 'combined_iwildcam_annotations_oneclass/eccv_train_and_imerit_2'
need consecutive category ids
old_cat_id_to_new_cat_id = {categories[idx]['id']:idx+1 for idx in range(len(categories))}
print(old_cat_id_to_new_cat_id)
remove multiclass images
print(images[0])
print(vis_data[0])
""
make_tfrecords_from_json.py
""
"Given a coco-camera-traps .json file, creates tfrecords"
""
Thin wrapper for create_tfrecords_from_json.
""
%% Constants and imports
%% Constants and imports (interactive)
%%
%% Main tfrecord generation function
"check whether the input file has already been converted to the tfrecords format,"
"if not, convert"
"Calculate number of shards to get the desired number of images per record,"
ensure it is evenly divisible by the number of threads
%% Interactive driver
%%
%% Command-line driver
eMammal_make_tfrecords_train_val_test.py
""
"From the tfrecords_format json version of the database, creates three splits"
of tf_records according to a previously decided split of full image IDs.
configurations and paths
a tfrecord_format json
these are number of images
do not include empty images in the train set; note that some images from non-empty sequences
"end up being empty (no bbox can be labeled), so these will be included in train set anyways"
eMammal_make_splits.py
""
"Based on a tfrecords_format json file of the database, creates 3 splits according to"
the specified fractions based on location (images from the same location should be in
one split) or based on images.
""
"If a previous split is provided (append_to_previous_split is True), the entries in"
"each split will be preserved, and new entries will be appended, so that new models"
can warm start with a model trained on the original splits.
configurations and paths
approximate fraction for the new entries
read in the previous splits of image ID or location ID if available
"find new locations and assign them to a split, without reassigning any previous locations"
"find out which images are new, shuffle and split them"
do NOT sort the IDs to keep the shuffled order
export PYTHONPATH=$PYTHONPATH:tfmodels/research
add empty category
"add all images that don't have annotations, with cat empty"
print(detection_results.keys())
group the detections and gts by image id:
print(image_id)
print(len(scores))
print(len(labels))
recall_thresh = 0.9
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall])
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])"
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall])
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])"
exp_name = 'eccv_train'
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json'
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(len(detection_results['images']))
print(len(seqs))
"print(len(seqs[0]),len(seqs["
print(detection_results.keys())
group the detections by image id:
group the ground truth annotations by image id:
print(seq)
"for image_id, dets in per_image_detections.iteritems():"
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
print(num_gts)
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
if sum(detected_class_labels)>0:
print('detected animal box')
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])"
"print(scores, tp_fp_labels)"
num_total_gts += num_gts
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
print('valid box')
"print(best_im, best_score)"
exp_name = 'eccv_train'
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json'
export PYTHONPATH=$PYTHONPATH:tfmodels/research
need to loop over confidence values
"for each value, check if any detections on the image are > conf"
"If so, that image gets class ""animal"""
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0"
"for image_id, dets in per_image_detections.iteritems():"
"calc prec, rec for this confidence thresh"
group the ground truth annotations by image id:
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(detection_results.keys())
group the detections and gts by image id:
print(image_id)
print(len(scores))
print(len(labels))
recall_thresh = 0.9
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall])
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])"
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall])
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])"
print(sorted_ap)
plt.bar(sorted_ap)
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(detection_results.keys())
group the detections by image id:
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
print(gts)
print(num_gts)
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[:2])"
"print(scores, tp_fp_labels)"
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
export PYTHONPATH=$PYTHONPATH:tfmodels/research
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
print('detected animal box')
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])"
"print(scores, tp_fp_labels)"
num_total_gts += num_gts
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
print('valid box')
best_im = np.argmax(max_im_scores)
"print(best_im, best_score)"
"temp_labels = np.zeros(len(im_detection_labels),  dtype=np.int32)"
"temp_scores = np.zeros(len(im_detection_scores), dtype=np.float32)"
"for j in range(min(im_num_gts, len(im_detection_labels))):"
temp_labels[j] = True #TODO: this currently only works for oneclass?
temp_scores[j] = best_score
im_detection_labels = temp_labels
im_detection_scores = temp_scores
num_total_gts+=im_num_gts
"print(len(detection_scores), len(detection_scores[0]), len(detection_scores[1]))"
exp_name = 'small_balanced_cct'
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(seq)
"for image_id, dets in per_image_detections.iteritems():"
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
seq_num_gts.append(num_gts)
print(num_gts)
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
num_total_gts += 1
print('valid box')
"print(best_im, best_score)"
print('no valid box')
if sum(seq_num_gts)>0:
export PYTHONPATH=$PYTHONPATH:tfmodels/research
need to loop over confidence values
"for each value, check if any detections on the image are > conf"
"If so, that image gets class ""animal"""
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0"
"calc prec, rec for this confidence thresh"
export PYTHONPATH=$PYTHONPATH:tfmodels/research
group the detections by image id:
group the ground truth annotations by image id:
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
print('detected animal box')
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])"
"print(scores, tp_fp_labels)"
num_total_gts += num_gts
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
print('valid box')
"print(best_im, best_score)"
""
evaluate_detections.py
""
Adapted from analyze_detection.py which is now archived.
""
%% Imports and constants
%% Functions
"labels input to compute_object_detection_metrics() needs to start at 0, not 1"
num_detections = len(dets['boxes'])
to prevent 'Invalid dimensions for box data.' error
this box will not match any detections
compute one-class precision/recall/average precision (if every box is just of an object class)
%% Command-line driver
""
detection_eval_utils.py
""
Utility functions used in evaluate_detections.py
""
group the ground truth annotations by image id
#####
""
detect_and_predict_image.py
""
"Functions to load a TensorFlow detection and a classification model, run inference,"
"render bounding boxes on images, and write out the resulting"
images (with bounding boxes and classes).
""
"See the ""test driver"" cell for example invocation."
""
""
#####
"%% Constants, imports, environment"
Minimum detection confidence for showing a bounding box on the output image
Stick this into filenames before the extension for the rendered result
Number of top-scoring classes to show at each bounding box
%% Core detection functions
Load images if they're not already numpy arrays
iImage = 0; image = images[iImage]
"Load the image as an nparray of size h,w,nChannels"
"There was a time when I was loading with PIL and switched to mpimg,"
"but I can't remember why, and converting to RGB is a very good reason"
"to load with PIL, since mpimg doesn't give any indication of color"
"order, which basically breaks all .png files."
""
"So if you find a bug related to using PIL, update this comment"
"to indicate what it was, but also disable .png support."
image = mpimg.imread(image)
This shouldn't be necessarily when loading with PIL and converting to RGB
Actual detection
...for each image
"Currently ""boxes"" is a list of length nImages, where each element is shaped as"
""
"1,nDetections,4"
""
"This implicitly banks on TF giving us back a fixed number of boxes, let's assert on this"
to make sure this doesn't silently break in the future.
iBox = 0; box = boxes[iBox]
"""scores"" is a length-nImages list of elements with size 1,nDetections"
"""classes"" is a length-nImages list of elements with size 1,nDetections"
""
"Still as floats, but really representing ints"
Squeeze out the empty axis
boxes is nImages x nDetections x 4
scores and classes are both nImages x nDetections
Get input and output tensors of classification model
"imsize = cur_image['width'], cur_image['height']"
Select detections with a confidence larger 0.5
Get these boxes and convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes"
"However, we need to make sure that it box coordinates are still within the image"
For convenience:
Create an array with contains the index of the corresponding crop_box for each selected box
i.e. [False False 0 False 1 2 3 False False]
For each box
If this box should be classified
Run inference
if box should not be classified
...for each box
species_scores should have shape len(images) x len(boxes) x num_species
...for each image
...with tf.Session
with classification_graph
species_scores should have shape len(images) x len(boxes) x num_species
%% Rendering functions
Display the image
plt.show()
"top, left, bottom, right"
""
"x,y origin is the upper-left"
Generate bounding box text
Choose color based on class
Location is the bottom-left of the rect
""
Origin is the upper-left
iRight = x + w
iTop = y + h
Add the patch to the Axes
Add class description
First determine best location by finding the corner that is closest to the image center
relative corner coordinates
relative coordinates of image center
Compute pair-wise squared distance and get the index of the one with minimal distance
Get the corresponding coordinates ...
... and alignment for the text box
Plot the text box with background
...for each box
This is magic goop that removes whitespace around image plots (sort of)
"plt.savefig(outputFileName, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)"
os.startfile(outputFileName)
...for each image
...def render_bounding_boxes
Load and run detector on target images
Read the name of all classes
remove empty lines
%% Interactive driver
%%
%%
%% File helper functions
%% Command-line driver
Hack to avoid running on already-detected images
""
"Script for evaluating a frozen graph given a COCO-style dataset, which was generated by"
the ../data_management/databases/classification/make_classification_dataset.py script.
This is useful for debugging as the accuracy reported by this script should match the
accuracy reported by the Tensorflow training.
""
Check that all files exists for easier debugging
Load frozen graph
Collect tensors for input and output
Read image
Run inference
Check if correct
""
Script for evaluating precision/recall on a two-class problem given
"a frozen graph and a COCO-style dataset, which was generated by"
the ../data_management/databases/classification/make_classification_dataset.py script.
We assume the positive class is at index 0 (with a zero-based indexing)
""
Check that all files exists for easier debugging
Load frozen graph
Collect tensors for input and output
Read image
"with open(image_path, 'rb') as fi:"
"image =  sess.run(tf.image.decode_jpeg(fi.read(), channels=3))"
image = image / 255.
Run inference
predicted_class = np.argmax(predictions)
Check if correct
if coco.imgToAnns[image_id][0]['category_id'] == predicted_class:
correct = correct + 1
"In matplotlib < 1.5, plt.fill_between does not have a 'step' argument"
""
Script for selecting testing images of a COCO-style dataset generated by the script
../data_management/databases/classification/make_classification_dataset.py in a consistent
manner and predicting the class for it.
""
Mandatory parameters
Optional parameters
Validate parameters
Derived parameters
"We assume that the dataset was generated with the make_classification_dataset.py script,"
hence the images should be located in the same folder as the json
Make seletion deterministic
Load frozen graph
Get dataset information
Get classes
...and the class list corresponding to the model outputs by assuming
that they are in order of their ids
Get images of each class
Shuffle the image list
Start the image sampling
"Set of avaiable class IDs, will be filled below"
If there are still images left for that class
"Get image for the sampled class, we already shuffled the class images before so"
we can simply pop()
Start prediction
Collect tensors for input and output
Read image
Run inference
Print output to log file
#####
""
api_apply_classifier_single_node.py
""
Takes the JSON file produced by the detection API and
classifies all boxes above a confidence threshold.
""
#####
"%% Constants, imports, environment"
Assumes that the root of the CameraTraps repo is on the PYTHONPATH
Minimum detection confidence for classifying an object
Number of top-scoring classes to show at each bounding box
Enlargment factor applied to boxes before passing them to the classifier
""
Provides more context and can lead to better results
List of detection categories for which we will run the classification
""
"Currently there are {""1"": ""animal"", ""2"": ""person"", ""4"": ""vehicle""}"
""
Should be a list of string-formatted ints.
Number of significant float digits in JSON output
%% Core detection functions
Read the name of all classes
remove empty lines
Create field with name *classification_categories*
Add classes using 0-based indexing
def add_classification_categories
Make sure we have the right json object
Get input and output tensors of classification model
For each image
Read image
"Scale pixel values to [0,1]"
For each box
Skip detections with low confidence
Skip if detection category is not in whitelist
Skip if already classified
"Get current box in relative coordinates and format [x_min, y_min, width_of_box, height_of_box]"
"Convert to [ymin, xmin, ymax, xmax] and store it as 1x4 numpy array so we can"
re-use the generic multi-box padding code
Convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes."
""
"However, we need to make sure that it box coordinates are still within the image."
Get the first (and only) row as our bbox to classify
Get the image data for that box
Run inference
Add an empty list to the json for our predictions
Add the *num_annotated_classes* top scoring classes
...for each box
...for each image
...with tf.Session
with classification_graph
def classify_boxes
Load classification model
Load detector json
Add classes to detector_json
"Run classifier on all images, changes will be writting directly to the json"
Write output json
def load_and_run_classifier
%% Command-line driver
""
Script for selecting testing images of a COCO-style dataset generated by the script
../data_management/databases/classification/make_classification_dataset.py in a consistent
manner and predicting the class for it.
""
Assumes the cameratraps repo root is on the path
Make seletion deterministic
Mandatory parameters
Optional parameters
Validate parameters
Tranfer parameters to post-processing format
Load frozen graph
Reading image list
Reading class list
Image sampling
Start prediction
Collect tensors for input and output
Array for collecting infos for rendering the html
Read image
"Scale pixel values to [0,1]"
Run inference
Add links to all available classes
""
predict_image.py
""
"Given a pointer to a frozen detection graph, runs inference on a single image,"
printing the top classes to the console
""
%% Imports
%% Command-line processing
Check that all files exist for easier debugging
%% Inference
Load frozen graph
Load class list
Remove empty lines
Collect tensors for input and output
Read image
Run inference
Print output
""
Mostly unmodified script for freezing a model
Added for convenience and for possible future optimizations
""
Copyright 2015 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
'input_checkpoint' may be a prefix if we're using Saver V2 format
Remove all the explicit device specifications for this node. This helps to
make the graph more portable.
List of all partition variables. Because the condition is heuristic
"based, the list could include false positives."
This tensor doesn't exist in the graph (for example it's
'global_step' or a similar housekeeping element) so skip it.
`var_list` is required to be a map of variable names to Variable
tensors. Partition variables are Identity tensors that cannot be
handled by Saver.
Models that have been frozen previously do not contain Variables.
optimized_output_graph_def = optimize_for_inference_lib.optimize_for_inference(
"output_graph_def,"
"input_node_names.replace("" "", """").split("",""),"
"output_node_names.replace("" "", """").split("",""),"
tf.float32.as_datatype_enum)
Write GraphDef to file if output path has been given.
""
"Creates a graph description, which is required to create a frozen graph."
Adapted from from ./tf-slim/export_inference_graph.py
Added preprocessing to the definition for easier handling
""
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
tf.app.flags.DEFINE_integer(
"'batch_size', None,"
"'Batch size for the exported model. Defaulted to ""None"" so batch size can '"
'be specified at model runtime.')
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
#####################
Optimization Flags #
#####################
######################
Learning Rate Flags #
######################
######################
Dataset Flags #
######################
####################
Fine-Tuning Flags #
####################
"Note: when num_clones is > 1, this will actually have each clone to go"
over each epoch FLAGS.num_epochs_per_decay times. This is different
behavior from sync replicas and is expected to produce different results.
Warn the user if a checkpoint exists in the train_dir. Then we'll be
ignoring the checkpoint anyway.
TODO(sguada) variables.filter_variables()
######################
Config model_deploy #
######################
Create global_step
#####################
Select the dataset #
#####################
#####################
Select the network #
#####################
####################################
Select the preprocessing function #
####################################
#############################################################
Create a dataset provider that loads data from the dataset #
#############################################################
###################
Define the model #
###################
############################
Specify the loss function #
############################
Gather initial summaries.
"Gather update_ops from the first clone. These contain, for example,"
the updates for the batch_norm variables created by network_fn.
Add summaries for end_points.
Add summaries for losses.
Add summaries for variables.
################################
Configure the moving averages #
################################
########################################
Configure the optimization procedure. #
########################################
"If sync_replicas is enabled, the averaging will be done in the chief"
queue runner.
Update ops executed locally by trainer.
Variables to train.
and returns a train_tensor and summary_op
Add total_loss to summary.
Create gradient updates.
Add the summaries from the first clone. These contain the summaries
created by model_fn and either optimize_clones() or _gather_clone_loss().
Merge all summaries together.
##########################
Kicks off the training. #
##########################
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
#####################
Select the dataset #
#####################
###################
Select the model #
###################
#############################################################
Create a dataset provider that loads data from the dataset #
#############################################################
####################################
Select the preprocessing function #
####################################
###################
Define the model #
###################
Define the metrics:
Print the summaries to screen.
TODO(sguada) use num_epochs=1
This ensures that we make a single pass over all of the data.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Transform the image to floats.
"Randomly crop a [height, width] section of the image."
Randomly flip the image horizontally.
"Because these operations are not commutative, consider randomizing"
the order their operation.
Subtract off the mean and divide by the variance of the pixels.
Transform the image to floats.
Resize and crop if needed.
Subtract off the mean and divide by the variance of the pixels.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Use tf.slice instead of crop_to_bounding box as it accepts tensors to
define the crop size.
Compute the rank assertions.
Create a random bounding box.
""
Use tf.random_uniform and not numpy.random.rand as doing the former would
"generate random numbers at graph eval time, unlike the latter which"
generates random numbers at graph definition time.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Pass the real x only to one of the func calls.
The random_* ops do not necessarily clamp.
"Each bounding box has shape [1, num_boxes, box coords] and"
"the coordinates are ordered [ymin, xmin, ymax, xmax]."
A large fraction of image datasets contain a human-annotated bounding
box delineating the region of the image containing the object of interest.
We choose to create a new bounding box for the object which is a randomly
distorted version of the human-annotated bounding box that obeys an
"allowed range of aspect ratios, sizes and overlap with the human-annotated"
"bounding box. If no box is supplied, then we assume the bounding box is"
the entire image.
Crop the image to the specified bounding box.
"Each bounding box has shape [1, num_boxes, box coords] and"
"the coordinates are ordered [ymin, xmin, ymax, xmax]."
Restore the shape since the dynamic slice based upon the bbox_size loses
the third dimension.
This resizing operation may distort the images because the aspect
ratio is not respected. We select a resize method in a round robin
fashion based on the thread number.
Note that ResizeMethod contains 4 enumerated resizing methods.
We select only 1 case for fast_mode bilinear.
Randomly flip the image horizontally.
Randomly distort the colors. There are 1 or 4 ways to do it.
Crop the central region of the image with an area containing 87.5% of
the original image.
Resize the image to the specified height and width.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(sguada) docstring paragraph by (a) motivating the need for the file and
(b) defining clones.
TODO(sguada) describe the high-level components of model deployment.
"E.g. ""each model deployment is composed of several parts: a DeploymentConfig,"
"which captures A, B and C, an input_fn which loads data.. etc"
Set up DeploymentConfig
Create the global step on the device storing the variables.
Define the inputs
Define the optimizer.
Define the model including the loss.
Run training.
Namedtuple used to represent a clone during deployment.
"Namedtuple used to represent a DeployedModel, returned by deploy()."
Default parameters for DeploymentConfig
Create clones.
The return value.
Individual components of the loss that will need summaries.
Compute and aggregate losses on the clone device.
Add the summaries out of the clone device block.
Only use regularization_losses for the first clone
Compute the total_loss summing all the clones_losses.
Sum the gradients across clones.
Gather initial summaries.
Create Clones.
"Gather update_ops from the first clone. These contain, for example,"
the updates for the batch_norm variables created by model_fn.
Place the global step on the device storing the variables.
Compute the gradients for the clones.
Add summaries to the gradients.
Create gradient updates.
Only use regularization_losses for the first clone
Add the summaries from the first clone. These contain the summaries
created by model_fn and either optimize_clones() or _gather_clone_loss().
Add total_loss to summary.
Merge all summaries together.
Note that each grad_and_vars looks like the following:
"((grad_var0_clone0, var0), ... (grad_varN_cloneN, varN))"
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Create an easy training set:
Create an easy training set:
Create an easy training set:
clone function creates a fully_connected layer with a regularizer loss.
The model summary op should have a few summary inputs and all of them
should be on the CPU.
clone function creates a fully_connected layer with a regularizer loss.
"No optimizer here, it's an eval."
The model summary op should have a few summary inputs and all of them
should be on the CPU.
!/usr/bin/python
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Failed to find ""index"" occurrence of item."
pylint: disable=broad-except
pylint: enable=broad-except
Grab the 'index' annotation.
Some images contain bounding box annotations that
"extend outside of the supplied image. See, e.g."
n03127925/n03127925_147.xml
"Additionally, for some bounding boxes, the min > max"
or the box is entirely outside of the image.
Example: <...>/n06470073/n00141669_6790.xml
Determine if the annotation is from an ImageNet Challenge label.
Note: There is a slight bug in the bounding box annotation data.
Many of the dog labels have the human label 'Scottish_deerhound'
instead of the synset ID 'n02092002' in the bbox.label field. As a
"simple hack to overcome this issue, we only exclude bbox labels"
*which are synset ID's* that do not match original synset label for
the XML file.
Guard against improperly specified boxes.
Note bbox.filename occasionally contains '%s' in the name. This is
data set noise that is fixed by just using the basename of the XML file.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The URLs where the MNIST data can be downloaded.
The names of the classes.
"First, process the training data:"
"Next, process the testing data:"
"Finally, write the labels file:"
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The URL where the Flowers data can be downloaded.
The number of images in the validation set.
Seed for repeatability.
The number of shards per dataset split.
Initializes function that decodes RGB JPEG data.
Read the filename:
Divide into train and test:
"First, convert the training and validation sets."
"Finally, write the labels file:"
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(nsilberman): Add tfrecord file type once the script is updated.
"If set to false, will not try to set label_to_names in dataset"
by reading them from labels.txt or github.
n01440764
n01443537
n02119247    black fox
n02119359    silver fox
pylint: disable=g-line-too-long
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The URL where the CIFAR data can be downloaded.
The number of training files.
The height and width of each image.
The names of the classes.
"First, process the training data:"
"Next, process the testing data:"
"Finally, write the labels file:"
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The labels file contains a list of valid labels are held in this file.
Assumes that the file contains entries as such:
n01440764
n01443537
n01484850
where each line corresponds to a label expressed as a synset. We map
each synset contained in the file to an integer (based on the alphabetical
ordering). See below for details.
This file containing mapping from synset to human-readable label.
Assumes each line of the file looks like:
""
n02119247    black fox
n02119359    silver fox
"n02119477    red fox, Vulpes fulva"
""
where each line corresponds to a unique mapping. Note that each line is
formatted as <synset>\t<human readable label>.
This file is the output of process_bounding_box.py
Assumes each line of the file looks like:
""
"n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940"
""
where each line corresponds to one bounding box annotation associated
with an image. Each line can be parsed as:
""
"<JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>"
""
Note that there might exist mulitple bounding box annotations associated
with an image file.
pylint: disable=expression-not-assigned
pylint: enable=expression-not-assigned
Create a single Session to run all image coding calls.
Initializes function that converts PNG to JPEG data.
Initializes function that converts CMYK JPEG data to RGB JPEG data.
Initializes function that decodes RGB JPEG data.
File list from:
https://groups.google.com/forum/embed/?place=forum/torch7#!topic/torch7/fOSTXHIESSU
File list from:
https://github.com/cytsai/ilsvrc-cmyk-image-list
Read the image file.
Clean the dirty data.
1 image is a PNG.
22 JPEG images are in CMYK colorspace.
Decode the RGB JPEG.
Check that image converted to RGB
Each thread produces N shards where N = int(num_shards / num_threads).
"For instance, if num_shards = 128, and the num_threads = 2, then the first"
"thread would produce shards [0, 64)."
"Generate a sharded version of the file name, e.g. 'train-00002-of-00010'"
"Break all images into batches with a [ranges[i][0], ranges[i][1]]."
Launch a thread for each batch.
Create a mechanism for monitoring when all threads are finished.
Create a generic TensorFlow-based utility for converting all image codings.
Wait for all the threads to terminate.
Leave label index 0 empty as a background class.
Construct the list of JPEG files and labels.
Shuffle the ordering of all image files in order to guarantee
random ordering of the images with respect to label in the
saved TFRecord files. Make the randomization repeatable.
Build a map from synset to human-readable label.
Run it!
Allowing None in the signature so that dataset_factory can use the default.
!/usr/bin/python
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Read in the 50000 synsets associated with the validation data set.
Make all sub-directories in the validation data dir.
Move all of the image to the appropriate sub-directory.
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
"The dataset has classes with no images (empty and human), which have ID 0 and 1,"
so we need to specify 49 here despite having only 47 classes with images
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
4 x Inception-A blocks
Reduction-A block
7 x Inception-B blocks
Reduction-A block
3 x Inception-C blocks
Logits and predictions
Force all Variables to reside on the device.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The endpoint shapes must be equal to the original shape even when the
separable convolution is replaced with a normal convolution.
"With the 'NCHW' data format, all endpoint activations have a transposed"
shape from the original shape with the 'NHWC' layout.
'NCWH' data format is not supported.
'NCHW' data format is not supported for separable convolution.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Final pooling and prediction
Global average pooling.
Pooling with a fixed kernel size.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Check graph construction for a number of image size/depths and batch
sizes.
Check layer depths.
Check graph construction for a number of image size/depths and batch
sizes.
Check layer depths.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The current_stride variable keeps track of the effective stride of the
activations. This allows us to invoke atrous convolution whenever applying
the next residual unit would result in the activations having stride larger
than the target output_stride.
The atrous convolution rate parameter.
Move stride from the block's last unit to the end of the block.
"If we have reached the target output_stride, then we need to employ"
atrous convolution with stride=1 and multiply the atrous rate by the
current unit's stride for use in subsequent layers.
Collect activations at the block's end before performing subsampling.
Subsampling of the block's output activations.
"The following implies padding='SAME' for pool1, which makes feature"
alignment easier for dense prediction tasks. This is also used in
https://github.com/facebook/fb.resnet.torch. However the accompanying
code of 'Deep Residual Learning for Image Recognition' uses
padding='VALID' for pool1. You can switch to that choice by setting
"slim.arg_scope([slim.max_pool2d], padding='VALID')."
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Collect outputs for conv2d, fully_connected and max_pool2d"
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Input image.
Convolution kernel.
Input image.
Convolution kernel.
Test both odd and even input dimensions.
Dense feature extraction followed by subsampling.
Make the two networks use the same weights.
Feature extraction at the nominal network rate.
Test both odd and even input dimensions.
Subsampling at the last unit of the block.
Make the two networks use the same weights.
Subsample activations at the end of the blocks.
Make sure that the final output is the same.
Make sure that intermediate block activations in
output_end_points are subsampled versions of the corresponding
ones in expected_end_points.
"Like ResnetUtilsTest.testEndPointsV1(), but for the public API."
Dense feature extraction followed by subsampling.
Make the two networks use the same weights.
Feature extraction at the nominal network rate.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"inputs has shape [batch, 224, 224, 3]"
"inputs has shape [batch, 513, 513, 3]"
We do not include batch normalization or activation functions in
conv1 because the first ResNet unit will perform these. Cf.
Appendix of [2].
This is needed because the pre-activation variant does not have batch
normalization or activation functions in the residual unit output. See
Appendix of [2].
Convert end_points_collection into a dictionary of end_points.
Global average pooling.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
=============================================================================
"These parameters come from the online port, which don't necessarily match"
those in the paper.
TODO(nsilberman): confirm these values with Philip.
##########
Encoder #
##########
"No normalizer for the first encoder layers as per 'Image-to-Image',"
Section 5.1.1
First layer doesn't use normalizer_fn
Last layer doesn't use activation_fn nor normalizer_fn
##########
Decoder #
##########
"Dropout is used at both train and test time as per 'Image-to-Image',"
Section 2.1 (last paragraph).
The Relu comes BEFORE the upsample op:
Explicitly set the normalizer_fn to None to override any default value
"that may come from an arg_scope, such as pix2pix_arg_scope."
No normalization on the input layer.
Stride 1 on the last layer.
"1-dim logits, stride 1, no activation, no normalization."
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Most networks use 224 as their default_image_size
Most networks use 224 as their default_image_size
Most networks use 224 as their default_image_size
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(joelshor): Add a test to check generator endpoints.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Force all Variables to reside on the device.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
Alias
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
pylint: disable=unused-import
pylint: enable=unused-import
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Input image.
Convolution kernel.
Input image.
Convolution kernel.
Test both odd and even input dimensions.
Dense feature extraction followed by subsampling.
Make the two networks use the same weights.
Feature extraction at the nominal network rate.
"Like ResnetUtilsTest.testEndPointsV2(), but for the public API."
Dense feature extraction followed by subsampling.
Make the two networks use the same weights.
Feature extraction at the nominal network rate.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
If we are fine tuning a checkpoint we need to start at a lower learning
rate since we are farther along on training.
We can start quantizing immediately if we are finetuning.
We need to wait for the model to train a bit before we quantize if we are
training from scratch.
Call rewriter to produce graph with fake quant ops and folded batch norms
"quant_delay delays start of quantization till quant_delay steps, allowing"
for better model accuracy.
Configure the learning rate using an exponential decay.
"When restoring from a floating point model, the min/max values for"
quantized weights and activations are not present.
We instruct slim to ignore variables that are missing during restoration
by setting ignore_missing_vars=True
"If we are restoring from a floating point model, we need to initialize"
the global step to zero for the exponential decay to result in
reasonable learning rates.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Orignaly, arg_scope = slim.arg_scope and layers = slim, now switch to more"
update-to-date tf.contrib.* API.
Decay for the moving averages.
epsilon to prevent 0s in variance.
Turns off fused batch norm.
collection containing the moving mean and moving variance.
"For inverted pyramid models, we start with gating switched off."
batch_size x 32 x 112 x 112 x 64
Separable conv is slow when used at first conv layer.
batch_size x 32 x 56 x 56 x 64
batch_size x 32 x 56 x 56 x 64
batch_size x 32 x 56 x 56 x 192
batch_size x 32 x 28 x 28 x 192
batch_size x 32 x 28 x 28 x 256
batch_size x 16 x 14 x 14 x 512
batch_size x 16 x 14 x 14 x 512
batch_size x 16 x 14 x 14 x 512
batch_size x 16 x 14 x 14 x 528
batch_size x 16 x 14 x 14 x 832
batch_size x 8 x 7 x 7 x 832
batch_size x 8 x 7 x 7 x 1024
Final pooling and prediction
Temporal average pooling.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
299 x 299 x 3
149 x 149 x 32
147 x 147 x 32
147 x 147 x 64
73 x 73 x 160
71 x 71 x 192
35 x 35 x 384
4 x Inception-A blocks
35 x 35 x 384
Reduction-A block
17 x 17 x 1024
7 x Inception-B blocks
17 x 17 x 1024
Reduction-B block
8 x 8 x 1536
3 x Inception-C blocks
Auxiliary Head logits
17 x 17 x 1024
Final pooling and prediction
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which"
can be set to False to disable pooling here (as in resnet_*()).
8 x 8 x 1536
1 x 1 x 1536
1536
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN
setups need the gradient of gradient FusedBatchNormGrad.
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN
setups need the gradient of gradient FusedBatchNormGrad.
First upscaling is different because it takes the input vector.
Last layer has different normalizer and activation.
Convert to proper channels.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Use clip_by_value to simulate bandpass activation.
Use clip_by_value to simulate bandpass activation.
Use clip_by_value to simulate bandpass activation.
149 x 149 x 32
147 x 147 x 32
147 x 147 x 64
73 x 73 x 64
73 x 73 x 80
71 x 71 x 192
35 x 35 x 192
35 x 35 x 320
TODO(alemi): Register intermediate endpoints
"17 x 17 x 1088 if output_stride == 8,"
33 x 33 x 1088 if output_stride == 16
TODO(alemi): register intermediate endpoints
TODO(gpapan): Properly support output_stride for the rest of the net.
8 x 8 x 2080
TODO(alemi): register intermediate endpoints
8 x 8 x 1536
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which"
can be set to False to disable pooling here (as in resnet_*()).
Set weight_decay for weights in conv2d and fully_connected layers.
Set activation_fn and parameters for batch_norm.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Decay for the moving averages.
epsilon to prevent 0s in variance.
collection containing update_ops.
use fused batch norm if possible.
Set weight_decay for weights in Conv and FC layers.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"inputs has shape [batch, 224, 224, 3]"
"inputs has shape [batch, 513, 513, 3]"
Use clip_by_value to simulate bandpass activation.
Convert end_points_collection into a dictionary of end_points.
Global average pooling.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"end_points will collect relevant activations for external use, for example"
summaries or losses.
299 x 299 x 3
149 x 149 x 32
147 x 147 x 32
147 x 147 x 64
73 x 73 x 64
73 x 73 x 80.
71 x 71 x 192.
35 x 35 x 192.
Inception blocks
mixed: 35 x 35 x 256.
mixed_1: 35 x 35 x 288.
mixed_2: 35 x 35 x 288.
mixed_3: 17 x 17 x 768.
mixed4: 17 x 17 x 768.
mixed_5: 17 x 17 x 768.
mixed_6: 17 x 17 x 768.
mixed_7: 17 x 17 x 768.
mixed_8: 8 x 8 x 1280.
mixed_9: 8 x 8 x 2048.
mixed_10: 8 x 8 x 2048.
Auxiliary Head logits
Shape of feature map before the final layer.
Final pooling and prediction
Global average pooling.
Pooling with a fixed kernel size.
1 x 1 x 2048
2048
1000
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Reflection pad by 1 in spatial dimensions (axes 1, 2 = h, w) to make a 3x3"
'valid' convolution produce an output with the same dimension as the
input.
This corrects 1 pixel offset for images with even width and height.
conv2d is left aligned and conv2d_transpose is right aligned for even
sized images (while doing 'SAME' padding).
Note: This doesn't reflect actual model in paper.
Neither dropout nor batch norm -> dont need is_training
##########
Encoder #
##########
7x7 input stage
##################
Residual Blocks #
##################
##########
Decoder #
##########
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"end_points will collect relevant activations for external use, for example"
summaries or losses.
Used to find thinned depths for each layer.
Note that sizes in the comments below assume an input spatial size of
"224x224, however, the inputs can be of any size greater 32x32."
224 x 224 x 3
depthwise_multiplier here is different from depth_multiplier.
depthwise_multiplier determines the output channels of the initial
"depthwise conv (see docs for tf.nn.separable_conv2d), while"
depth_multiplier controls the # channels of the subsequent 1x1
convolution. Must have
in_channels * depthwise_multipler <= out_channels
so that the separable convolution is not overparameterized.
Use a normal convolution instead of a separable convolution.
112 x 112 x 64
56 x 56 x 64
56 x 56 x 64
56 x 56 x 192
28 x 28 x 192
Inception module.
28 x 28 x 256
28 x 28 x 320
14 x 14 x 576
14 x 14 x 576
14 x 14 x 576
14 x 14 x 576
14 x 14 x 576
7 x 7 x 1024
7 x 7 x 1024
Final pooling and prediction
Global average pooling.
Pooling with a fixed kernel size.
1 x 1 x 1024
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
=============================================================================
For the Conv2d_0 layer FaceNet has depth=16
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
=============================================================================
Tensorflow mandates these.
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture
Conv defines 3x3 convolution layers
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.
stride is the stride of the convolution
depth is the number of channels or filters in a layer
MOBILENETV1_CONV_DEFS specifies the MobileNet body
Used to find thinned depths for each layer.
The current_stride variable keeps track of the output stride of the
"activations, i.e., the running product of convolution strides up to the"
current network layer. This allows us to invoke atrous convolution
whenever applying the next convolution would result in the activations
having output stride larger than the target output_stride.
The atrous convolution rate parameter.
"If we have reached the target output_stride, then we need to employ"
atrous convolution with stride=1 and multiply the atrous rate by the
current unit's stride for use in subsequent layers.
By passing filters=None
separable_conv2d produces only a depthwise convolution layer
Global average pooling.
Pooling with a fixed kernel size.
1 x 1 x 1024
Set weight_decay for weights in Conv and DepthSepConv layers.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Decay for the moving averages.
epsilon to prevent 0s in variance.
Turns off fused batch norm.
collection containing the moving mean and moving variance.
Final pooling and prediction
Temporal average pooling.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
=============================================================================
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Orignaly, add_arg_scope = slim.add_arg_scope and layers = slim, now switch to"
more update-to-date tf.contrib.* API.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Make sure that round down does not go down by more than 10%.
"Note: want to round down, we adjust each split to match the total."
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts."
which provide numbered scopes.
b1 -> b2 * r -> b2
i -> (o * r) (bottleneck) -> o
"Note in contrast with expansion, we always have"
projection to produce the desired output size.
stride check enforces that we don't add residuals when spatial
dimensions are None
Depth matches
Don't do any splitting if we end up with less than 8 filters
on either side.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Make sure that round down does not go down by more than 10%.
Set conv defs defaults and overrides.
a) Set the tensorflow scope
b) set padding to default: note we might consider removing this
since it is also set by mobilenet_scope
c) set all defaults
d) set all extra overrides.
The current_stride variable keeps track of the output stride of the
"activations, i.e., the running product of convolution strides up to the"
current network layer. This allows us to invoke atrous convolution
whenever applying the next convolution would result in the activations
having output stride larger than the target output_stride.
The atrous convolution rate parameter.
Insert default parameters before the base scope which includes
any custom overrides set in mobilenet.
"If we have reached the target output_stride, then we need to employ"
atrous convolution with stride=1 and multiply the atrous rate by the
current unit's stride for use in subsequent layers.
Update params.
Only insert rate to params if rate > 1.
Set padding
Add all tensors that end with 'output' to
endpoints
1 x 1 x num_classes
Note: legacy scope name.
"Recover output shape, for unknown shape."
the network created will be trainble with dropout/batch norm
initialized appropriately.
Note: do not introduce parameters that would change the inference
"model here (for example whether to use bias), modify conv_def instead."
Set weight_decay for weights in Conv and FC layers.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
This is mostly a sanity test. No deep reason for these particular
constants.
""
"All but first 2 and last one have  two convolutions, and there is one"
extra conv that is not in the spec. (logits)
Check that depthwise are exposed.
"All but 3 op has 3 conv operatore, the remainign 3 have one"
and there is one unaccounted.
Verifies that depth_multiplier arg scope actually works
if no default min_depth is provided.
Verifies that depth_multiplier arg scope actually works
if no default min_depth is provided.
"All convolutions will be 8->48, except for the last one."
Verifies that mobilenet_base returns pre-pooling layer.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
pyformat: disable
Architecture: https://arxiv.org/abs/1801.04381
Note: these parameters of batch norm affect the architecture
that's why they are here and not in training_scope.
pyformat: enable
NB: do not set depth_args unless they are provided to avoid overriding
whatever default depth_multiplier might have thanks to arg_scope.
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that
"'finegrain_classification_mode' is set to True, which means the embedding"
layer will not be shrinked when given a depth-multiplier < 1.0.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Notes for training NASNet Cifar Model
-------------------------------------
batch_size: 32
learning rate: 0.025
cosine (single period) learning rate decay
auxiliary head loss weighting: 0.4
clip global norm of all gradients by 5
600 epochs with a batch size of 32
This is used for the drop path probabilities since it needs to increase
the drop out probability over the course of training.
Notes for training large NASNet model on ImageNet
-------------------------------------
batch size (per replica): 16
learning rate: 0.015 * 100
learning rate decay factor: 0.97
num epochs per decay: 2.4
sync sgd with 100 replicas
auxiliary head loss weighting: 0.4
label smoothing: 0.1
clip global norm of all gradients by 10
Notes for training the mobile NASNet ImageNet model
-------------------------------------
batch size (per replica): 32
learning rate: 0.04 * 50
learning rate scaling factor: 0.97
num epochs per decay: 2.4
sync sgd with 50 replicas
auxiliary head weighting: 0.4
label smoothing: 0.1
clip global norm of all gradients by 10
Decay for the moving averages.
epsilon to prevent 0s in variance.
Decay for the moving averages.
epsilon to prevent 0s in variance.
Decay for the moving averages.
epsilon to prevent 0s in variance.
Shape of feature map before the final layer.
149 x 149 x 32
Run the reduction cells
Calculate the total number of cells in the network
Add 2 for the reduction cells
Calculate the total number of cells in the network
Add 2 for the reduction cells
"If ImageNet, then add an additional two for the stem cells"
Calculate the total number of cells in the network
Add 2 for the reduction cells
"If ImageNet, then add an additional two for the stem cells"
Find where to place the reduction cells or stride normal cells
Setup for building in the auxiliary head.
Run the cells
true_cell_num accounts for the stem cells
Final softmax layer
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Find where to place the reduction cells or stride normal cells
pylint: disable=protected-access
pylint: enable=protected-access
Setup for building in the auxiliary head.
Run the cells
true_cell_num accounts for the stem cells
pylint: disable=protected-access
pylint: enable=protected-access
Final softmax layer
pylint: disable=protected-access
pylint: enable=protected-access
Calculate the total number of cells in the network.
There is no distinction between reduction and normal cells in PNAS so the
total number of cells is equal to the number normal cells plus the number
of stem cells (two by default).
pylint: disable=protected-access
pylint: enable=protected-access
Calculate the total number of cells in the network.
There is no distinction between reduction and normal cells in PNAS so the
total number of cells is equal to the number normal cells plus the number
of stem cells (two by default).
Configuration for the PNASNet-5 model.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Logits and predictions
Logits and predictions
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Logits and predictions
Logits and predictions
Logits and predictions
Force all Variables to reside on the device.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"The cap for tf.clip_by_value, it's hinted from the activation distribution"
"that the majority of activation values are in the range [-6, 6]."
Skip path 1
Skip path 2
"First pad with 0's on the right and bottom, then shift the filter to"
include those 0's that were added.
"If odd number of filters, add an additional one to the second path."
Concat and apply BN
Set the prev layer to the current layer if it is none
Check to be sure prev layer stuff is setup correctly
num_or_size_splits=1
Apply conv operations
Combine hidden states using 'add'.
Add hiddenstate to the list of hiddenstates we can choose from
Dont stride if this is not one of the original hiddenstates
"Check if a stride is needed, then use a strided 1x1 here"
Determine if a reduction should be applied to make the number of
filters match.
Return the concat of all the states
Scale keep prob by layer number
The added 2 is for the reduction cells
Decrease the keep probability over time
""
Script for generating a two-class dataset in COCO format for training an obscured image classifier
""
Requires Python >= 3.6 because of the glob ** expression
""
Collect images and labels
"Labels: clean = 0, obscured = 1"
""
top_folders_to_bottom.py
""
Given a base folder with files like:
""
A/1/2/a.jpg
B/3/4/b.jpg
""
"...moves the top-level folders to the bottom in a new output folder, i.e., creates:"
""
1/2/A/a.jpg
3/4/B/b.jpg
""
"In practice, this is used to make this:"
""
animal/camera01/image01.jpg
""
...look like:
""
camera01/animal/image01.jpg
""
%% Constants and imports
%% Support functions
%% Main functions
Find top-level folder
Find file/folder names
Move or copy
...def process_file()
Enumerate input folder
Convert absolute paths to relative paths
Standardize delimiters
Make sure each input file maps to a unique output file
relative_filename = relative_files[0]
Loop
...def top_folders_to_bottom()
%% Interactive driver
%%
%%
%% Command-line driver
"python top_folders_to_bottom.py ""g:\temp\separated_images"" ""g:\temp\separated_images_inverted"" --n_threads 100"
Convert to an options object
%% Constants and imports
This line was added circa 2018 and it made sense at the time; removing it in 2022
"because matplotlib *mostly* does the right thing now, and overwriting the current"
matplotlib environment is questionable behavior.  Possible breaking change for
some users.
""
import matplotlib; matplotlib.use('agg')
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils)
Assumes the cameratraps repo root is on the path
%% Options
"Make sure there is no overlap between the two sets, because this will cause"
issues in the code
## Required inputs
## Options
Can be a folder or a SAS URL
These apply only when we're doing ground-truth comparisons
"A list of output sets that we should count, but not render images for."
""
"Typically used to preview sets with lots of empties, where you don't want to"
"subset but also don't want to render 100,000 empty images."
""
"detections, non_detections"
"detections_animal, detections_person, detections_vehicle"
Used for summary statistics only
"Number of images to sample, -1 for ""all images"""
"Random seed for sampling, or None"
Optionally separate detections into categories (animal/vehicle/human)
Optionally replace one or more strings in filenames with other strings;
useful for taking a set of results generated for one folder structure
and applying them to a slightly different folder structure.
Allow bypassing API output loading when operating on previously-loaded
results
Should we also split out a separate report about the detections that were
just below our main confidence threshold?
""
Currently only supported when ground truth is unavailable
Control rendering parallelization
...PostProcessingOptions
#%% Helper classes and functions
Anything greater than this isn't clearly positive or negative
image has annotations suggesting both negative and positive
"image is not annotated or is annotated with 'unknown', 'unlabeled', ETC."
image has not yet been assigned a state
"In some analyses, we add an additional class that lets us look at"
detections just below our main confidence threshold
count the # of images with each type of DetectionStatus
Check whether this image has:
- unknown / unassigned-type labels
- negative-type labels
"- positive labels (i.e., labels that are neither unknown nor negative)"
"assert has_unknown_labels is False, '{} has unknown labels'.format(annotations)"
"If there are no image annotations, treat this as unknown"
n_negative += 1
im['_detection_status'] = DetectionStatus.DS_NEGATIVE
"If the image has more than one type of labels, it's ambiguous"
"note: bools are automatically converted to 0/1, so we can sum"
"After the check above, we can be sure it's only one of positive,"
"negative, or unknown."
""
Important: do not merge the following 'unknown' branch with the first
"'unknown' branch above, where we tested 'if len(categories) == 0'"
""
If the image has only unknown labels
If the image has only negative labels
If the images has only positive labels
"Annotate the category, if it is unambiguous"
...for each image
...mark_detection_status()
"Leaving code in place for reading from blob storage, may support this"
in the future.
resize is to display them in this notebook or in the HTML more quickly
os.path.isfile() is slow when mounting remote directories; much faster
to just try/except on the image open.
return ''
Render images to a flat folder... we can use os.sep here because we've
already normalized paths
"errno.ENAMETOOLONG doesn't get thrown properly on Windows, so"
we awkwardly check against a hard-coded limit
Use slashes regardless of os
...render_bounding_boxes
Count items in each category
Optionally sort by filename before writing to html
Write the individual HTML files
...prepare_html_subpages()
%% Main function
#%% Expand some options for convenience
#%% Prepare output dir
#%% Load ground truth if available
Mark images in the ground truth as positive or negative
#%% Load detection (and possibly classification) results
Remove failed rows
Convert keys and values to lowercase
"Add column 'pred_detection_label' to indicate predicted detection status,"
not separating out the classes
#%% Pull out descriptive metadata
This is rare; it only happens during debugging when the caller
is supplying already-loaded API results.
"#%% If we have ground truth, remove images we can't match to ground truth"
#%% Sample images for visualization
#%% Fork here depending on whether or not ground truth is available
"If we have ground truth, we'll compute precision/recall and sample tp/fp/tn/fn."
""
Otherwise we'll just visualize detections/non-detections.
#%% Detection evaluation: compute precision/recall
numpy array of detection probabilities
"numpy array of bools (0.0/1.0), and -1 as null value"
Don't include ambiguous/unknown ground truth in precision/recall analysis
"For completeness, include the result at a confidence threshold of 1.0"
Compute and print summary statistics
Thresholds go up throughout precisions/recalls/thresholds; find the last
value where recall is at or above target.  That's our precision @ target recall.
"np.where returns a tuple of arrays, but in this syntax where we're"
"comparing an array with a scalar, there will only be one element."
Convert back to a list
Flatten the confusion matrix
"#%% Collect classification results, if they exist"
Mapping of classnames to idx for the confusion matrix.
""
"The lambda is actually kind of a hack, because we use assume that"
the following code does not reassign classname_to_idx
Confusion matrix as defaultdict of defaultdict
""
"Rows / first index is ground truth, columns / second index is predicted category"
iDetection = 0; fn = detector_files[iDetection]; print(fn)
"If this image has classification predictions, and an unambiguous class"
"annotated, and is a positive image..."
"The unambiguous category, we make this a set for easier handling afterward"
"Compute the accuracy as intersection of union,"
i.e. (# of categories in both prediciton and GT)
divided by (# of categories in either prediction or GT
""
"In case of only one GT category, the result will be 1.0, if"
prediction is one category and this category matches GT
""
"It is 1.0/(# of predicted top-1 categories), if the GT is"
one of the predicted top-1 categories.
""
"It is 0.0, if none of the predicted categories is correct"
Distribute this accuracy across all predicted categories in the
confusion matrix
...for each file in the detection results
If we have classification results
Build confusion matrix as array from classifier_cm
Print some statistics
Prepare confusion matrix output
Get confusion matrix as string
Get fixed-size classname for each idx
Prepend class name on each line and add to the top
Print formatted confusion matrix
Plot confusion matrix
To manually add more space at bottom: plt.rcParams['figure.subplot.bottom'] = 0.1
""
"Add 0.5 to figsize for every class. For two classes, this will result in"
"fig = plt.figure(figsize=[4,4])"
...if we have classification results
#%% Render output
Write p/r table to .csv file in output directory
Write precision/recall plot to .png file in output directory
#%% Sampling
Sample true/false positives/negatives with correct/incorrect top-1
classification and render to html
Accumulate html image structs (in the format expected by write_html_image_lists)
"for each category, e.g. 'tp', 'fp', ..., 'class_bird', ..."
Add default entries by accessing them for the first time
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]"
"Each element will be a three-tuple with elements file,max_conf,detections"
"Assemble the information we need for rendering, so we can parallelize without"
dealing with Pandas
i_row = 0; row = images_to_visualize.iloc[0]
Filenames should already have been normalized to either '/' or '\'
This should already have been normalized to either '/' or '\'
...def render_image_with_gt(file_info)
file_info = files_to_render[0]
Map all the rendering results in the list rendering_results into the
dictionary images_html
Prepare the individual html image files
Write index.html
Show links to each GT class
""
We could do this without classification results; currently we don't.
Add links to all available classes
Close body and html tags
...for each image
"#%% Otherwise, if we don't have ground truth..."
#%% Sample detections/non-detections
Accumulate html image structs (in the format expected by write_html_image_list)
for each category
Add default entries by accessing them for the first time
"Maps detection categories - e.g. ""human"" - to result set names, e.g."
"""detections_human"""
Add a set of results for each category and combination of categories
Create output directories
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]"
"list of 3-tuples with elements (file, max_conf, detections)"
"Assemble the information we need for rendering, so we can parallelize without"
dealing with Pandas
i_row = 0; row = images_to_visualize.iloc[0]
Filenames should already have been normalized to either '/' or '\'
Get unique categories above the threshold for this image
Local function for parallelization
"This is a list of [class,confidence] pairs, sorted by confidence"
"If we either don't have a confidence threshold, or we've met our"
confidence threshold
...if this detection has classification info
...for each detection
...def render_image_no_gt(file_info):
Map all the rendering results in the list rendering_results into the
dictionary images_html
Prepare the individual html image files
Write index.html
"We can't just sum these, because image_counts includes images in both their"
detection and classification classes
total_images = sum(image_counts.values())
Don't print classification classes here; we'll do that later with a slightly
different structure
Add links to all available classes
os.startfile(output_html_file)
...if we do/don't have ground truth
...process_batch_results
%% Interactive driver(s)
%%
options.unlabeled_classes = ['human']
os.start(ppresults.output_html_file)
%% Command-line driver
""
subset_json_detector_output.py
""
"Creates one or more subsets of a detector API output file (.json), doing either"
"or both of the following (if both are requested, they happen in this order):"
""
"1) Retrieve all elements where filenames contain a specified query string,"
"optionally replacing that query with a replacement token. If the query is blank,"
can also be used to prepend content to all filenames.
""
"2) Create separate .jsons for each unique path, optionally making the filenames"
"in those .json's relative paths.  In this case, you specify an output directory,"
rather than an output path.  All images in the folder blah\foo\bar will end up
in a .json file called blah_foo_bar.json.
""
##
""
Sample invocations (splitting into multiple json's):
""
"Read from ""1800_idfg_statewide_wolf_detections_w_classifications.json"", split up into"
"individual .jsons in 'd:\temp\idfg\output', making filenames relative to their individual"
folders:
""
"python subset_json_detector_output.py ""d:\temp\idfg\1800_idfg_statewide_wolf_detections_w_classifications.json"" ""d:\temp\idfg\output"" --split_folders --make_folder_relative"
""
"Now do the same thing, but instead of writing .json's to d:\temp\idfg\output, write them to *subfolders*"
corresponding to the subfolders for each .json file.
""
"python subset_json_detector_output.py ""d:\temp\idfg\1800_detections_S2.json"" ""d:\temp\idfg\output_to_folders"" --split_folders --make_folder_relative --copy_jsons_to_folders"
""
##
""
Sample invocations (creating a single subset matching a query):
""
"Read from ""1800_detections.json"", write to ""1800_detections_2017.json"""
""
"Include only images matching ""2017"", and change ""2017"" to ""blah"""
""
"python subset_json_detector_output.py ""d:\temp\1800_detections.json"" ""d:\temp\1800_detections_2017_blah.json"" --query 2017 --replacement blah"
""
"Include all images, prepend with ""prefix/"""
""
"python subset_json_detector_output.py ""d:\temp\1800_detections.json"" ""d:\temp\1800_detections_prefix.json"" --replacement ""prefix/"""
""
##
""
"To subset a COCO Camera Traps .json database, see subset_json_db.py"
""
%% Constants and imports
%% Helper classes
Only process files containing the token 'query'
"Replace 'query' with 'replacement' if 'replacement' is not None.  If 'query' is None,"
prepend 'replacement'
Should we split output into individual .json files for each folder?
"Folder level to use for splitting ['bottom','top','n_from_bottom','n_from_top','dict']"
""
'dict' requires 'split_folder_param' to be a dictionary mapping each filename
to a token.
"When using the 'n_from_bottom' parameter to define folder splitting, this"
defines the number of directories from the bottom.  'n_from_bottom' with
a parameter of zero is the same as 'bottom'.
""
Same story with 'n_from_top'.
""
"When 'split_folder_mode' is 'dict', this should be a dictionary mapping each filename"
to a token.
Only meaningful if split_folders is True: should we convert pathnames to be relative
the folder for each .json file?
"Only meaningful if split_folders and make_folder_relative are True: if not None,"
"will copy .json files to their corresponding output directories, relative to"
output_filename
Should we over-write .json files?
"If copy_jsons_to_folders is true, do we require that directories already exist?"
Threshold on confidence
Should we remove failed images?
%% Main function
Format spec:
""
https://github.com/microsoft/CameraTraps/tree/master/api/batch_processing
iImage = 0; im = images_in[0]
Find all detections above threshold for this image
"If there are no detections above threshold, set the max probability"
"to -1, unless it already had a negative probability."
Otherwise find the max confidence
Did this thresholding result in a max-confidence change?
"We should only be *lowering* max confidence values (i.e., making them negative)"
...for each image
i_image = 0; im = images_in[0]
...for each image
i_image = 0; im = images_in[0]
Only take images that match the query
...for each image
"Path('/blah').parts is ('/','blah')"
Handle paths like:
""
"/, \, /stuff, c:, c:\stuff"
Input validation
data = add_missing_detection_results_fields(data)
Map images to unique folders
im = data['images'][0]
"Split string into folders, keeping delimiters"
"Don't use this, it removes delimiters"
tokens = split_path(fn)
Optionally make paths relative
dirname = list(folders_to_images.keys())[0]
im = folders_to_images[dirname][0]
dirname = list(folders_to_images.keys())[0]
"Recycle the 'data' struct, replacing 'images' every time... medium-hacky, but"
forward-compatible in that I don't take dependencies on the other fields
...for each directory
...if we're splitting folders
%% Interactive driver
%%
%% Subset a file without splitting
"%% Subset and split, but don't copy to individual folders"
"%% Subset and split, copying to individual folders"
%% Just do a filename replacement
"python subset_json_detector_output.py ""D:\temp\idfg\detections_idfg_20190625_refiltered.json"" ""D:\temp\idfg\detections_idfg_20190625_refiltered_renamed.json"" --query ""20190625-hddrop/"" --replacement """""
"python subset_json_detector_output.py ""D:\temp\idfg\detections_idfg_20190625_refiltered_renamed.json"" ""D:\temp\idfg\output"" --split_folders --make_folder_relative --copy_jsons_to_folders"
%% Command-line driver
Convert to an options object
%% Constants and imports
%% Merge functions
"Map image filenames to detections, we'll convert to a list later"
Check compatibility of detection categories
Check compatibility of classification categories
"Merge image lists, checking uniqueness"
Replace a previous failure with a success
"Merge info dicts, don't check completion time fields"
...for each dictionary
Convert merged image dictionaries to a sorted list
detection_list = input_lists[0]
d = detection_list[0]
%% Driver
""
load_api_results.py
""
Loads the output of the batch processing API (json) into a pandas dataframe.
""
Also functions to group entries by seq_id.
""
Includes the deprecated functions that worked with the old CSV API output format.
""
%% Constants and imports
%% Functions for grouping by sequence_id
example
input 'file': 'SER/S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG'
output 'id': 'S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG'
%% Functions for loading the result as a Pandas DataFrame
Validate that this is really a detector output file
Fields in the API output json other than 'images'
Normalize paths to simplify comparisons later
"image['file'] = image['file'].replace('\\','/')"
Pack the json output into a Pandas DataFrame
Replace some path tokens to match local paths to original blob structure
string_to_replace = list(filename_replacements.keys())[0]
Sanity-check that this is really a detector output file
Normalize paths to simplify comparisons later
De-serialize detections
Optionally replace some path tokens to match local paths to the original blob structure
string_to_replace = list(options.detector_output_filename_replacements.keys())[0]
"TODO: hit some silly issues with vectorized str() and escaped characters, vectorize"
this later.
""
"detection_results['image_path'].str.replace(string_to_replace,replacement_string)"
iRow = 0
%% Imports
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils)
%% Constants and support classes
We will confirm that this matches what we load from each file
%% Main function
#%% Validate inputs
#%% Load both result sets
#%% Make sure they represent the same set of images
#%% Find differences
"Each of these maps a filename to a two-element list (the image in set A, the image in set B)"
""
"Right now, we only handle a very simple notion of class transition, where the detection"
of maximum confidence changes class *and* both images have an above-threshold detection.
fn = filenames_a[0]
det = im_a['detections'][0]
...for each filename
#%% Sample and plot differences
"Render two sets of results (i.e., a comparison) for a single"
image.
...def render_image_pair()
fn = image_filenames[0]
...def render_detection_comparisons()
"For each category, generate comparison images and the"
comparison HTML page.
""
category = 'common_detections'
Choose detection pairs we're going to render for this category
...for each category
#%% Write the top-level HTML file content
...def compare_batch_results()
%% Interactive driver
%% KRU
%% Command-line driver
# TODO
""
"Merge high-confidence detections from one results file into another file,"
when the target file does not detect anything on an image.
""
Does not currently attempt to merge every detection based on whether individual
detections are missing; only merges detections into images that would otherwise
be considered blank.
""
"If you want to literally merge two .json files, see combine_api_outputs.py."
""
%% Constants and imports
%% Structs
"If you want to merge only certain categories, specify one"
(but not both) of these.
%% Main function
im = output_data['images'][0]
"Determine whether we should be processing all categories, or just a subset"
of categories.
i_source_file = 0; source_file = source_files[i_source_file]
source_im = source_data['images'][0]
detection_category = list(detection_categories)[0]
"This is already a detection, no need to proceed looking for detections to"
transfer
Boxes are x/y/w/h
source_sizes = [det['bbox'][2]*det['bbox'][3] for det in source_detections_this_category_raw]
Only look at boxes below the size threshold
...for each detection category
"print('Adding {} detections to image {}'.format(len(detections_to_transfer),image_filename))"
Update the max_detection_conf field
...for each image
...for each source file
%% Test driver
%%
%% Command-line driver (TODO)
#######
""
convert_output_format.py
""
Converts between file formats output by our batch processing API.  Currently
"supports json <--> csv conversion, but this should be the landing place for any"
conversion - including between future .json versions - that we support in the
future.
""
#######
%% Imports
%% Conversion functions
"We add an output column for each class other than 'empty',"
containing the maximum probability of  that class for each image
Skip sub-threshold detections
Our .json format is xmin/ymin/w/h
""
Our .csv format was ymin/xmin/ymax/xmax
"Category 0 is empty, for which we don't have a column, so the max"
confidence for category N goes in column N-1
...for each detection
...for each image
Format spec:
""
https://github.com/microsoft/CameraTraps/tree/master/api/batch_processing
iFile = 0; row = df.iloc[iFile]
Our .csv format was ymin/xmin/ymax/xmax
""
Our .json format is xmin/ymin/w/h
...for each detection
...for each image
%% Interactive driver
%%
%%
%% Concatenate .csv files from a folder
...for each .csv file
with open(master_csv)
%% Command-line driver
""
separate_detections_into_folders.py
""
"Given a .json file with batch processing results, separate the files in that"
"set of results into folders that contain animals/people/vehicles/nothing,"
according to per-class thresholds.
""
"Image files are copied, not moved."
""
Preserves relative paths within each of those folders; cannot be used with .json
files that have absolute paths in them.
""
"For example, if your .json file has these images:"
""
a/b/c/1.jpg
a/b/d/2.jpg
a/b/e/3.jpg
a/b/f/4.jpg
""
And let's say:
""
"* The results say that the first three images are empty/person/vehicle, respectively"
"* The fourth image is above threshold for ""animal"" and ""person"""
* You specify an output base folder of c:\out
""
You will get the following files:
""
c:\out\empty\a\b\c\1.jpg
c:\out\people\a\b\d\2.jpg
c:\out\vehicles\a\b\e\3.jpg
c:\out\animal_person\a\b\f\4.jpg
""
Hard-coded to work with MDv3 and MDv4 output files.  Not currently future-proofed
"past the classes in MegaDetector v4, not currently ready for species-level classification."
""
%% Constants and imports
Occasionally we have near-zero confidence detections associated with COCO classes that
didn't quite get squeezed out of the model in training.  As long as they're near zero
"confidence, we just ignore them."
%% Options class
"Dictionary mapping categories (plus combinations of categories, and 'empty') to output folders"
%% Support functions
Find the maximum confidence for each category
""
det = detections[0]
"For zero-confidence detections, we occasionally have leftover goop"
from COCO classes
assert det['conf'] < invalid_category_epsilon
Count the number of thresholds exceeded
Do we have a custom threshold for this category?
If this is above multiple thresholds
if this is/isn't a failure case
...def process_detection()
%% Main function
Create output folder if necessary
Load detection results
Map class names to output folders
Create all combinations of categories
Do we have a custom threshold for this category?
Create folder mappings for each category
Create the actual folders
%% Interactive driver
%%
%%
%% Find a particular file
%% Command-line driver
"python api\batch_processing\postprocessing\separate_detections_into_folders.py ""d:\temp\rspb_mini.json"" ""d:\temp\demo_images\rspb_2018_2019_mini"" ""d:\temp\separation_test"" --nthreads 2"
Convert to an options object
For each image...
""
im = images[0]
d = im['detections'][0]
https://github.com/microsoft/CameraTraps/tree/master/api/batch_processing#detector-outputs
...for each detection
...for each image
...def categorize_detections_by_size()
#######
""
remove_repeat_detections.py
""
"Used after running find_repeat_detections, then manually filtering the results,"
to create a final filtered output file.
""
"If you want to use this script, we recommend that you read the user's guide:"
""
https://github.com/microsoft/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms
""
#######
%% Constants and imports
%% Main function
%% Interactive driver
%%
"python remove_repeat_detections.py ""F:\wpz\6714_detections_wpz_all_20191015233705.SUCP_subset.json"" ""F:\wpz\6714_detections_wpz_all_20191015233705.SUCP_subset_filtered.json"" ""F:\wpz\rde\filtering_2019.10.24.16.52.54"""
%% Command-line driver
#######
""
repeat_detections_core.py
""
Core utilities shared by find_repeat_detections and remove_repeat_detections.
""
#######
%% Imports and environment
Note to self: other indexing options:
""
https://rtree.readthedocs.io (not thread- or process-safe)
https://github.com/sergkr/rtreelib
https://github.com/Rhoana/pyrtree
"from ai4eutils; this is assumed to be on the path, as per repo convention"
"""PIL cannot read EXIF metainfo for the images"""
"""Metadata Warning, tag 256 had too many entries: 42, expected 1"""
%% Constants
%% Classes
Relevant for rendering HTML or filtering folder of images
""
"imageBase can also be a SAS URL, in which case some error-checking is"
disabled.
Don't consider detections with confidence lower than this as suspicious
Don't consider detections with confidence higher than this as suspicious
What's the IOU threshold for considering two boxes the same?
How many occurrences of a single location (as defined by the IOU threshold)
are required before we declare it suspicious?
"Ignore ""suspicious"" detections larger than some size; these are often animals"
taking up the whole image.  This is expressed as a fraction of the image size.
"Ignore folders with more than this many images in them, which can stall the process"
A list of classes we don't want to treat as suspicious. Each element is an int.
Load detections from a filter file rather than finding them from the detector output
".json file containing detections, should be called detectionIndex.json in the filtering_* folder"
produced in the first pass
(optional) List of filenames remaining after deletion of identified
repeated detections that are actually animals.  This should be a flat
"text file, one relative filename per line.  See enumerate_images()."
Turn on/off optional outputs
Determines whether bounding-box rendering errors (typically network errors) should
be treated as failures
Box rendering options
State variables
"Replace filename tokens after reading, useful when the directory structure"
has changed relative to the structure the detector saw
How many folders up from the leaf nodes should we be going to aggregate images?
An optional function that takes a string (an image file name) and returns a string (the corresponding
"folder ID), typically used when multiple folders actually correspond to the same camera in a"
manufacturer-specific way (e.g. a/b/c/RECONYX100 and a/b/c/RECONYX101 may really be the same camera).
Include/exclude specific folders... only one of these may be
"specified; ""including"" folders includes *only* those folders."
Sort detections within a directory so nearby detections are adjacent
"in the list, for faster review."
""
"Can be None, 'xsort', or 'clustersort'"
"The data table (Pandas DataFrame), as loaded from the input json file via"
load_api_results()
"The other fields in the input json file, loaded via load_api_results()"
The data table after modification
dict mapping folder names to whole rows from the data table
dict mapping filenames to rows in the master table
"An array of length nDirs, where each element is a list of DetectionLocation"
objects for that directory that have been flagged as suspicious
%% Helper functions
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t"
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t"
%% Sort a list of candidate detections to make them visually easier to review
Just sort by the X location of each box
"Prepare a list of points to represent each box,"
that's what we'll use for clustering
Upper-left
"points.append([det.bbox[0],det.bbox[1]])"
Center
"Labels *could* be any unique labels according to the docs, but in practice"
they are unique integers from 0:nClusters
Make sure the labels are unique incrementing integers
Store the label assigned to each cluster
"Now sort the clusters by their x coordinate, and re-assign labels"
so the labels are sortable
"Compute the centroid for debugging, but we're only going to use the x"
"coordinate.  This is the centroid of points used to represent detections,"
which may be box centers or box corners.
old_cluster_label_to_new_cluster_label[old_cluster_label] =\
new_cluster_labels[old_cluster_label]
%% Look for matches (one directory)
List of DetectionLocations
candidateDetections = []
Create a tree to store candidate detections
"Each image in this folder is a row in ""rows"""
For each image in this directory
""
iDirectoryRow = 0; row = rows.iloc[iDirectoryRow]
""
"iDirectoryRow is a pandas index, so it may not start from zero;"
"for debugging, we maintain i_iteration as a loop index."
"print('Searching row {} of {} (index {}) in dir {}'.format(i_iteration,len(rows),iDirectoryRow,dirName))"
Don't bother checking images with no detections above threshold
"Array of dicts, where each element is"
{
"'category': '1',  # str value, category ID"
"'conf': 0.926,  # confidence of this detections"
""
"# (x_min, y_min) is upper-left, all in relative coordinates"
"'bbox': [x_min, y_min, width_of_box, height_of_box]"
""
}
For each detection in this image
"This is no longer strictly true; I sometimes run RDE in stages, so"
some probabilities have already been made negative
""
assert confidence >= 0.0 and confidence <= 1.0
Optionally exclude some classes from consideration as suspicious
Is this detection too big to be suspicious?
print('Illegal zero-size bounding box on image {}'.format(filename))
These are relative coordinates
print('Ignoring very large detection with area {}'.format(area))
This will return candidates of all classes
For each detection in our candidate list
Don't match across categories
Is this a match?
"If so, add this example to the list for this detection"
We *don't* break here; we allow this instance to possibly
match multiple candidates.  There isn't an obvious right or
wrong here.
...for each detection on our candidate list
"If we found no matches, add this to the candidate list"
candidateDetections.append(candidate)
pyqtree
...for each detection
...for each row
Get all candidate detections
print('Found {} candidate detections for folder {}'.format(
"len(candidateDetections),dirName))"
"For debugging only, it's convenient to have these sorted"
as if they had never gone into a tree structure.  Typically
this is in practce a sort by filename.
...def find_matches_in_directory(dirName)
%% Render candidate repeat detections to html
suspiciousDetectionsThisDir is a list of DetectionLocation objects
For each problematic detection in this directory
""
iDetection = 0; detection = suspiciousDetectionsThisDir[iDetection];
iDetection = 0; detection = suspiciousDetectionsThisDir[0]
_ = pretty_print_object(detection)
Render images
iInstance = 0; instance = detection.instances[iInstance]
...for each instance
Write html for this detection
Use the first image from this detection (arbitrary) as the canonical example
that we'll render for the directory-level page.
...for each detection
Write the html file for this directory
...def render_images_for_directory(iDir)
"%% Update the detection table based on suspicious results, write .csv output"
"An array of length nDirs, where each element is a list of DetectionLocation"
objects for that directory that have been flagged as suspicious
For each directory
For each suspicious detection group in this directory
For each instance of this suspicious detection
This should match the bbox for the detection event
The bbox for this instance should be almost the same as the bbox
"for this detection group, where ""almost"" is defined by the IOU"
threshold.
if iou < options.iouThreshold:
"print('IOU warning: {},{}'.format(iou,options.iouThreshold))"
Make sure the bounding box matches
"Make the probability negative, if it hasn't been switched by"
another bounding box
...for each instance
...for each detection
...for each directory
Update maximum probabilities
For each row...
No longer strictly true; sometimes I run RDE on RDE output
assert maxPOriginal >= 0
We should only be making detections *less* likely
row['max_confidence'] = str(maxP)
"Negative probabilities should be the only reason maxP changed, so"
we should have found at least one negative value
...if there was a meaningful change to the max probability for this row
...for each row
If we're also writing output...
"...def update_detection_table(RepeatDetectionResults,options)"
%% Main function
#%% Input handling
Validate some options
Load the filtering file
Load the same options we used when finding repeat detections
...except for things that explicitly tell this function not to
find repeat detections.
...if we're loading from an existing filtering file
Check early to avoid problems with the output folder
Load file
detectionResults[detectionResults['failure'].notna()]
"Before doing any real work, make sure we can *probably* access images"
"This is just a cursory check on the first image, but it heads off most"
"problems related to incorrect mount points, etc.  Better to do this before"
spending 20 minutes finding repeat detections.
#%% Separate files into directories
This will be a map from a directory name to smaller data frames
This is a mapping back into the rows of the original table
TODO: in the case where we're loading an existing set of FPs after
"manual filtering, we should load these data frames too, rather than"
re-building them from the input.
iRow = 0; row = detectionResults.iloc[0]
Create a new DataFrame with just this row
rowsByDirectory[dirName] = pd.DataFrame(row)
...for each unique detection
Convert lists of rows to proper DataFrames
#% Look for matches (or load them from file)
"Are we actually looking for matches, or just loading from a file?"
length-nDirs list of lists of DetectionLocation objects
We're actually looking for matches...
iDir = 4; dirName = dirsToSearch[iDir]
"for iDir, dirName in enumerate(tqdm(dirsToSearch)):"
#%% Find suspicious locations based on match results
For each directory
""
iDir = 51
A list of DetectionLocation objects
A list of DetectionLocation objects
occurrenceList is a list of file/detection pairs
Sort the above-threshold detections for easier review
...for each directory
If we're just loading detections from a file...
"We're skipping detection-finding, but to see which images are actually legit false"
"positives, we may be looking for physical files or loading from a text file."
For each directory
iDir = 0; detections = suspiciousDetections[0]
""
"suspiciousDetections is an array of DetectionLocation objects,"
one per directory.
For each detection that was present before filtering
iDetection = 0; detection = detections[iDetection]
Are we checking the directory to see whether detections were actually false
"positives, or reading from a list?"
Is the image still there?
"If not, remove this from the list of suspicious detections"
...for each detection
...for each directory
...if we are/aren't finding detections (vs. loading from file)
Render problematic locations with html (loop)
options.pbar = tqdm(total=nDirs)
For each directory
iDir = 51
Add this directory to the master list of html files
...for each directory
Write master html file
Remove unicode characters before formatting
...if we're rendering html
Create filtering directory
iDir = 0; suspiciousDetectionsThisDir = suspiciousDetections[iDir]
suspiciousDetectionsThisDir is a list of DetectionLocation objects
iDetection = 0; detection = suspiciousDetectionsThisDir[0]
Write out the detection index
...if we're writing filtering info
...find_repeat_detections()
#######
""
find_repeat_detections.py
""
"If you want to use this script, we recommend that you read the user's guide:"
""
https://github.com/microsoft/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms
""
"Really, don't try to run this script without reading the user's guide, you'll think"
it's more magical than it is.
""
"This script looks through a sequence of detections in the API output json file, and finds"
"candidates that might be ""repeated false positives"", i.e. that random branch that the"
detector thinks is an animal/person/vehicle.
""
"Typically after running this script, you would do a manual step to remove"
"true positives, then run remove_repeat_detections to produce a final output file."
""
There's no way that statement was self-explanatory; see the user's guide.
""
#######
%% Constants and imports
%% Interactive driver
%%
%% Command-line driver
Convert to an options object
""
If a request has been sent to AML for batch scoring but the monitoring thread of the API was
"interrupted (uncaught exception or having to re-start the API container), we could manually"
"aggregate results from each shard using this script, assuming all jobs submitted to AML have finished."
""
Need to have set environment variables STORAGE_ACCOUNT_NAME and STORAGE_ACCOUNT_KEY to those of the
"storage account backing the API. Also need to adjust the INTERNAL_CONTAINER, AML_CONTAINER and"
AML_CONFIG fields in api_core/orchestrator_api/api_config.py to match the instance of the API that this
request was submitted to.
""
May need to change the import statement in api_core/orchestrator_api/orchestrator.py
"""from sas_blob_utils import SasBlob"" to"
"""from .sas_blob_utils import SasBlob"" to not confuse with the module in AI4Eutils;"
"and change ""import api_config"" to"
"""from api.batch_processing.api_core.orchestrator_api import api_config"""
Execute this script from the root of the repository. You may need to add the repository to PYTHONPATH.
"list_jobs_submitted cannot be serialized (""can't pickle _thread.RLock objects ""), but"
do not need it for aggregating results
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%% helper classes and functions
TODO log exception when we have more telemetry
TODO check that the expiry date of input_container_sas is at least a month
into the future
"if no permission specified explicitly but has an access policy, assumes okay"
TODO - check based on access policy as well
return current UTC time as a string in the ISO 8601 format (so we can query by
timestamp in the Cosmos DB job status table.
example: '2021-02-08T20:02:05.699689Z'
"image_paths will have length at least 1, otherwise would have ended before this step"
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%% instance-specific API settings
you likely need to modify these when deploying a new instance of the API
Azure Batch for batch processing
%% general API settings
"if this number of times the thread wakes up to check is exceeded, stop the monitoring thread"
"quota of active Jobs in our Batch account, which all node pools i.e. API instances share;"
cannot accept job submissions if there are this many active Jobs already
%% MegaDetector info
relative to the `megadetector_copies` folder in the container `models`
TODO add MD versions info to AppConfig
copied from TFDetector class in detection/run_detector.py
%% Azure Batch settings
"%% env variables for service credentials, and info related to these services"
Cosmos DB `batch-api-jobs` table for job status
"Service principal of this ""application"", authorized to use Azure Batch"
Blob storage account for storing Batch tasks' outputs and scoring script
STORAGE_CONTAINER_MODELS = 'models'  # names of the two containers supporting Batch
Azure Container Registry for Docker image used by our Batch node pools
Azure App Configuration instance to get configurations specific to
this instance of the API
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
a job moves from created to running/problem after the Batch Job has been submitted
"job_id should be unique across all instances, and is also the partition key"
TODO do not read the entry first to get the call_params when the Cosmos SDK add a
patching functionality:
https://feedback.azure.com/forums/263030-azure-cosmos-db/suggestions/6693091-be-able-to-do-partial-updates-on-document
need to retain other fields in 'status' to be able to restart monitoring thread
retain existing fields; update as needed
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
sentinel should change if new configurations are available
configs have not changed
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Gunicorn logger handler will get attached if needed in server.py
set for all tasks in the job
cannot execute the scoring script that is in the mounted directory; has to be copied to cwd
not luck giving the commandline arguments via formatted string - set as env vars instead
form shards of images and assign each shard to a Task
for persisting stdout and stderr
persist stdout and stderr (will be removed when node removed)
paths are relative to the Task working directory
can also just upload on failure
first try submitting Tasks
retry submitting Tasks
Change the Job's on_all_tasks_complete option to 'terminateJob' so the Job's status changes automatically
after all submitted tasks are done
This is so that we do not take up the quota for active Jobs in the Batch account.
return type: TaskAddCollectionResult
actually we should probably only re-submit if it's a server_error
docs: # https://docs.microsoft.com/en-us/rest/api/batchservice/odata-filters-in-batch#list-tasks
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
%% Flask app
reference: https://trstringer.com/logging-flask-gunicorn-the-manageable-way/
%% Helper classes
%% Flask endpoints
required params
can be an URL to a file not hosted in an Azure blob storage container
"if use_url, then images_requested_json_sas is required"
optional params
check model_version is among the available model versions
check request_name has only allowed characters
optional params for telemetry collection - logged to status table for now as part of call_params
All API instances / node pools share a quota on total number of active Jobs;
we cannot accept new Job submissions if we are at the quota
required fields
request_status is either completed or failed
the create_batch_job thread will stop when it wakes up the next time
"Fix for Zooniverse - deleting any ""-"" characters in the job_id"
"If the status is running, it could be a Job submitted before the last restart of this"
"API instance. If that is the case, we should start to monitor its progress again."
WARNING model_version could be wrong (a newer version number gets written to the output file) around
"the time that  the model is updated, if this request was submitted before the model update"
and the API restart; this should be quite rare
conform to previous schemes
%% undocumented endpoints
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Gunicorn logger handler will get attached if needed in server.py
request_name and request_submission_timestamp are for appending to
output file names
image_paths can be a list of strings (Azure blob names or public URLs)
"or a list of length-2 lists where each is a [image_id, metadata] pair"
Case 1: listing all images in the container
- not possible to have attached metadata if listing images in a blob
list all images to process
+ 1 so if the number of images listed > MAX_NUMBER_IMAGES_ACCEPTED_PER_JOB
we will know and not proceed
Case 2: user supplied a list of images to process; can include metadata
filter down to those conforming to the provided prefix and accepted suffixes (image file types)
prefix is case-sensitive; suffix is not
"Although urlparse(p).path preserves the extension on local paths, it will not work for"
"blob file names that contains ""#"", which will be treated as indication of a query."
"If the URL is generated via Azure Blob Storage, the ""#"" char will be properly encoded"
apply the first_n and sample_n filters
OK if first_n > total number of images
sample by shuffling image paths and take the first sample_n images
"upload the image list to the container, which is also mounted on all nodes"
all sharding and scoring use the uploaded list
now request_status moves from created to running
an extra field to allow the monitoring thread to restart after an API restart: total number of tasks
also record the number of images to process for reporting
start the monitor thread with the same name
"both succeeded and failed tasks are marked ""completed"" on Batch"
"preserving format from before, but SAS URL to 'failed_images' and 'images' are no longer provided"
"failures should be contained in the output entries, indicated by an 'error' field"
"when people download this, the timestamp will have : replaced by _"
check if the result blob has already been written (could be another instance of the API / worker thread)
"and if so, skip aggregating and uploading the results, and just generate the SAS URL, which"
could be needed still if the previous request_status was `problem`.
upload the output JSON to the Job folder
%% Helper functions *copied* from ct_utils.py and visualization/visualization_utils.py
"Determine the factor, which shifts the decimal point of x"
just behind the last significant digit
"Shift decimal point by multiplicatipon with factor, flooring, and"
division by factor
PIL.Image.convert() returns a converted copy of this image
alter orientation as needed according to EXIF tag 0x112 (274) for Orientation
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28
https://www.media.mit.edu/pia/Research/deepview/exif.html
"%% TFDetector class, an unmodified *copy* of the class in detection/tf_detector.py,"
so we do not have to import the packages required by run_detector.py
Number of decimal places to round to for confidence and bbox coordinates
"MegaDetector was trained with batch size of 1, and the resizing function is a part"
of the inference graph
An enumeration of failure reasons
"change from [y1, x1, y2, x2] to [x1, y1, width, height]"
convert numpy floats to Python floats
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size
"np_images = [np.asarray(image, np.uint8) for image in images]"
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)"
performs inference
our batch size is 1; need to loop the batch dim if supporting batch size > 1
%% Scoring script
determine if there is metadata attached to each image_id
information to determine input and output locations
other parameters for the task
test that we can write to output path; also in case there is no image to process
list images to process
"items in this list can be strings or [image_id, metadata]"
model path
"Path to .pb TensorFlow detector model file, relative to the"
models/megadetector_copies folder in mounted container
score the images
%% Imports
%% Constants
%% Classes
class variables
"instance variables, in order of when they are typically set"
Leaving this commented out to remind us that we don't want this check here; let
the API fail on these images.  It's a huge hassle to remove non-image
files.
""
for path_or_url in images_list:
if not is_image_file_or_url(path_or_url):
raise ValueError('{} is not an image'.format(path_or_url))
Commented out as a reminder: don't check task status (which is a rest API call)
in __repr__; require the caller to explicitly request status
"status=getattr(self, 'status', None))"
estimate # of failed images from failed shards
Download all three JSON urls to memory
Remove files that were submitted but don't appear to be images
assert all(is_image_file_or_url(s) for s in submitted_images)
Diff submitted and processed images
Confirm that the procesed images are a subset of the submitted images
https://www.geeksforgeeks.org/break-list-chunks-size-n-python/
%% Interactive driver
%%
%%
%%
%% Imports and constants
%% Constants I set per script
## Required
base_output_folder_name = os.path.expanduser('~/postprocessing/' + organization_name_short)
Shared Access Signature (SAS) tokens for the Azure Blob Storage container.
Leading question mark is optional.
""
The read-only token is used for accessing images; the write-enabled token is
used for writing file lists.
## Typically left as default
"Pre-pended to all folder names/prefixes, if they're defined below"
"This is how we break the container up into multiple taskgroups, e.g., for"
separate surveys. The typical case is to do the whole container as a single
taskgroup.
"If your ""folders"" are really logical folders corresponding to multiple folders,"
map them here
"A list of .json files to load images from, instead of enumerating.  Formatted as a"
"dictionary, like folder_prefixes."
This is only necessary if you will be performing postprocessing steps that
"don't yet support SAS URLs, specifically the ""subsetting"" step, or in some"
cases the splitting of files into multiple output directories for
empty/animal/vehicle/people.
""
"For those applications, you will need to mount the container to a local drive."
For this case I recommend using rclone whether you are on Windows or Linux;
rclone is much easier than blobfuse for transient mounting.
""
"But most of the time, you can ignore this."
"Supported model_versions: '4', '3', '4_prelim'"
""
Also available at the /supported_model_versions and /default_model_version
endpoints
""
"Unless you have any specific reason to set this to a non-default value, leave"
"it at the default, which as of 2020.04.28 is MegaDetector 4.1"
""
"additional_task_args = {""model_version"":""4_prelim""}"
""
"file_lists_by_folder will contain a list of local JSON file names,"
each JSON file contains a list of blob names corresponding to an API taskgroup
"%% Derived variables, path setup"
local folders
Turn warnings into errors if more than this many images are missing
%% Support functions
"scheme, netloc, path, query, fragment"
%% Read images from lists or enumerate blobs to files
folder_name = folder_names[0]
"Load file lists for this ""folder"""
""
file_list = input_file_lists[folder][0]
Write to file
A flat list of blob paths for each folder
folder_name = folder_names[0]
"If we don't/do have multiple prefixes to enumerate for this ""folder"""
"If this is intended to be a folder, it needs to end in '/', otherwise"
files that start with the same string will match too
...for each prefix
Write to file
...for each folder
%% Some just-to-be-safe double-checking around enumeration
Make sure each folder has at least one image matched; the opposite is usually a sign of a copy/paste issue
...for each image
...for each prefix
...for each folder
Make sure each image comes from one of our folders; the opposite is usually a sign of a bug up above
...for each prefix
...for each folder
...for each image
%% Divide images into chunks for each folder
The JSON file at folder_chunks[i][j] corresponds to task j of taskgroup i
list_file = file_lists_by_folder[0]
"%% Create taskgroups and tasks, and upload image lists to blob storage"
periods not allowed in task names
%% Generate API calls for each task
clipboard.copy(request_strings[0])
clipboard.copy('\n\n'.join(request_strings))
%% Run the tasks (don't run this cell unless you are absolutely sure!)
I really want to make sure I'm sure...
%% Estimate total time
Around 0.8s/image on 16 GPUs
%% Manually create task groups if we ran the tasks manually
%%
"%% Write task information out to disk, in case we need to resume"
%% Status check
print(task.id)
%% Resume jobs if this notebook closes
%% For multiple tasks (use this only when we're merging with another job)
%% For just the one task
%% Load into separate taskgroups
p = task_cache_paths[0]
%% Typically merge everything into one taskgroup
"%% Look for failed shards or missing images, start new tasks if necessary"
List of lists of paths
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup];
"Make a copy, because we append to taskgroup"
i_task = 0; task = tasks[i_task]
Each taskgroup corresponds to one of our folders
Check that we have (almost) all the images
Now look for failed images
Write it out as a flat list as well (without explanation of failures)
...for each task
...for each task group
%% Pull results
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup]; task = taskgroup[0]
Each taskgroup corresponds to one of our folders
...for each task
...for each task group
%% Combine results from task groups into final output files
Check that we have (almost) all the images
The only reason we should ever have a repeated request is the case where an
"image was missing and we reprocessed it, or where it failed and later succeeded"
"There may be non-image files in the request list, ignore those"
Something has gone bonkers if there are images in the results that
aren't in the request
...for each folder
%% Post-processing (no ground truth)
i_folder = 0; folder_name_raw = folder_names[i_folder]
"Omit some pages from the output, useful when animals are rare"
%% Manual processing follows
""
"Everything after this should be considered mostly manual, and no longer includes"
looping over folders.
""
%% RDE (sample directory collapsing)
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images"
"%% Repeat detection elimination, phase 1"
"Deliberately leaving these imports here, rather than at the top, because this cell is not"
typically executed
To invoke custom collapsing of folders for a particular manufacturer's naming scheme
options.customDirNameFunction = remove_overflow_folders
Exclude people and vehicles from RDE
"options.excludeClasses = [2,3]"
%% Manual RDE step
# DELETE THE VALID DETECTIONS ##
%% Re-filtering
%% Post-processing (post-RDE)
i_folder = 0; folder_name_raw = folder_names[i_folder]
"Omit some pages from the output, useful when animals are rare"
"print('No RDE file available for {}, skipping'.format(folder_name))"
continue
%% Create a new category for large boxes
%% Subsetting
i_folder = 0; folder_name = folders[i_folder]
%% String replacement
%% Folder splitting
i_folder = 0; folder_name_raw = folder_names[i_folder]
api_output_file = folder_name_to_combined_output_file[folder_name]
%% Imports and constants
from ai4eutils
Turn warnings into errors if more than this many images are missing
Only relevant when we're using a single GPU
Only relevant when running on CPU
%% Constants I set per script
job_date = '2022-01-01'
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v5.0.0/md_v5b.0.0.pt')
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v4.1.0/md_v4.1.0.pb')
"Number of jobs to split data into, typically equal to the number of available GPUs"
Only used to print out a time estimate
"%% Derived variables, path setup"
local folders
%% Enumerate files
%% Divide images into chunks
%% Estimate total time
%% Write file lists
%% Generate commands
i_task = 0; task = task_info[i_task]
%% Generate combined commands for a handful of tasks
%%
i_task = 8
...for each task
%% Run the tasks
Prefer to run manually
"%% Load results, look for failed or missing images in each task"
i_task = 0; task = task_info[i_task]
im = task_results['images'][0]
...for each task
%% Merge results files and make images relative
im = combined_results['images'][0]
%% Compare results files for different model versions (or before/after RDE)
%% Post-processing (no ground truth)
options.sample_seed = 0
"Omit some pages from the output, useful when animals are rare"
%% Merge in high-confidence detections from another results file
%% RDE (sample directory collapsing)
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images"
%%
%%
"%% Repeat detection elimination, phase 1"
"Deliberately leaving these imports here, rather than at the top, because this cell is not"
typically executed
options.lineThickness = 5
options.boxExpansion = 8
To invoke custom collapsing of folders for a particular manufacturer's naming scheme
options.customDirNameFunction = remove_overflow_folders
Exclude people and vehicles from RDE
"options.excludeClasses = [2,3]"
options.maxImagesPerFolder = 50000
options.includeFolders = ['a/b/c']
options.excludeFolder = ['a/b/c']
"Can be None, 'xsort', or 'clustersort'"
import clipboard; clipboard.copy(os.path.dirname(suspiciousDetectionResults.filterFile))
path_utils.open_file(os.path.dirname(suspiciousDetectionResults.filterFile))
%% Manual RDE step
# DELETE THE VALID DETECTIONS ##
%% Re-filtering
%% Post-processing (post-RDE)
options.sample_seed = 0
"Omit some pages from the output, useful when animals are rare"
"%% Run MegaClassifier (actually, write out a script that runs MegaClassifier)"
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
#%% Set up environment
commands.append('cd CameraTraps/classification\n')
commands.append('conda activate cameratraps-classifier\n')
#%% Crop images
fn = input_files[0]
#%% Run classifier
fn = input_files[0]
#%% Remap classifier outputs
fn = input_files[0]
#%% Merge classification and detection outputs
fn = input_files[0]
#%% Write  out classification script
"%% Run a non-MegaClassifier classifier (i.e., a classifier with no output mapping)"
"This is just passed along to the metadata in the output file, it has no impact"
on how the classification scripts run.
#%% Set up environment
#%% Crop images
fn = input_files[0]
#%% Run classifier
fn = input_files[0]
#%% Merge classification and detection outputs
fn = input_files[0]
#%% Write everything out
%% Create a new category for large boxes
%% Subsetting
i_folder = 0; folder_name = folders[i_folder]
%% String replacement
%% Folder splitting
%% Post-processing (post-classification)
classification_detection_file = classification_detection_files[1]
%% Within-image classification smoothing
Only count detections with a classification confidence threshold above
"*classification_confidence_threshold*, which in practice means we're only"
looking at one category per detection.
""
If an image has at least *min_detections_above_threshold* such detections
"in the most common category, and no more than *max_detections_secondary_class*"
"in the second-most-common category, flip all detections to the most common"
category.
""
"Optionally treat some classes as particularly unreliable, typically used to overwrite an"
"""other"" class."
d['classification_categories']
im['detections']
"path_utils.open_file(os.path.join(input_path,im['file']))"
im = d['images'][0]
...for each classification
...if there are classifications for this detection
...for each detection
If we have at least *min_detections_to_overwrite_other* in a category that isn't
"""other"", change all ""other"" classifications to that category"
...for each classification
...if there are classifications for this detection
...for each detection
"...if we should overwrite all ""other"" classifications"
"At this point, we know we have a dominant category; change all other above-threshold"
"classifications to that category.  That category may have been ""other"", in which case we may have"
already made the relevant changes.
det = detections[0]
...for each image
""
xmp_integration.py
""
"Tools for loading MegaDetector batch API results into XMP metadata, specifically"
for consumption in digiKam:
""
https://cran.r-project.org/web/packages/camtrapR/vignettes/camtrapr2.html
""
%% Imports and constants
%% Class definitions
Folder where images are stored
.json file containing MegaDetector output
"String to remove from all path names, typically representing a"
prefix that was added during MegaDetector processing
Optionally *rename* (not copy) all images that have no detections
above [rename_conf] for the categories in rename_cats from x.jpg to
x.check.jpg
"Comma-deleted list of category names (or ""all"") to apply the rename_conf"
behavior to.
"Minimum detection threshold (applies to all classes, defaults to None,"
i.e. 0.0
%% Functions
Relative image path
Absolute image path
List of categories to write to XMP metadata
Categories with above-threshold detections present for
this image
Maximum confidence for each category
Have we already added this to the list of categories to
write out to this image?
If we're supposed to compare to a threshold...
Else we treat *any* detection as valid...
Keep track of the highest-confidence detection for this class
If we're doing the rename/.check behavior...
Legacy code to rename files where XMP writing failed
%% Interactive/test driver
%%
%% Command-line driver
""
Test script for pushing annotations to the eMammal db
""
%% Imports
%% Database functions
%% Command-line driver
TODO: check project ID ?
No-animal category
%% Imports
%% Main function
if not os.path.exists(directory):
os.makedirs(directory)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Cosmos DB `batch-api-jobs` table for job status
"aggregate the number of images, country and organization names info from each job"
submitted during yesterday (UTC time)
create the card
get rid of formatting differences
compare result
"part is a BodyPart object with b'Content-Type', and b'Content-Disposition', the later"
includes 'name' and 'filename' info
the filename and name info is all in one string with no obvious format
compare result
"part is a BodyPart object with b'Content-Type', and b'Content-Disposition', the later"
includes 'name' and 'filename' info
images[part.headers['filename']] = part.content
the filename and name info is all in one string with no obvious format
"part is a BodyPart object with b'Content-Type', and b'Content-Disposition', the later includes 'name' and 'filename' info"
https://stackoverflow.com/questions/11380413/python-unittest-passing-arguments
"part is a BodyPart object with b'Content-Type', and b'Content-Disposition', the later includes 'name' and 'filename' info"
images[part.headers['filename']] = part.content
For now only requesting on one image at a time
"Attaching more images to the request is possible,"
but you'd have to use callbacks rather than async/await:
https://stackoverflow.com/questions/34403670/superagent-multiple-files-attachment
Make sure there's an API key in the .env file
Minimum detection confidence for showing a bounding box on the output image
Number of top-scoring classes to show at each bounding box
Number of significant float digits in JSON output
remove empty lines
"Determine the factor, which shifts the decimal point of x"
just behind the last significant digit
"Shift decimal point by multiplicatipon with factor, flooring, and"
division by factor
"json_with_classes = self.add_classification_categories(detection_json, class_names)"
Get input and output tensors of classification model
For each image
"Scale pixel values to [0,1]"
For each box
Skip detections with low confidence
Skip if detection category is not in whitelist
"box ymin, xim, ymax, xmax"
"Get current box in relative coordinates and format [x_min, y_min, width_of_box, height_of_box]"
"Convert to [ymin, xmin, ymax, xmax] and"
store it as 1x4 numpy array so we can re-use the generic multi-box padding code
Convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes"
"However, we need to make sure that it box coordinates are still within the image"
Get the first (and only) row as our bbox to classify
Get the image data for that box
Run inference
Add the *num_annotated_classes* top scoring classes
sync API configurations
upper limit on total content length (all images and parameters)
classification configurations
TODO
padding factor used for padding the detected animal
Minimum detection confidence for showing a bounding box on the output image
Number of top-scoring classes to show at each bounding box
Number of significant float digits in JSON output
detection configurations
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"# /ai4e_api_tools has been added to the PYTHONPATH, so we can reference those libraries directly."
from tf_classifer import TFClassifier
Use the AI4EAppInsights library to send log messages.
"Use the APIService to executes your functions within a logging trace, supports long-running/async functions,"
"handles SIGTERM signals from AKS, etc., and handles concurrent requests."
Load the models when the API starts
"TODO classifier = TFClassifier(api_config.CLASSIFICATION_MODEL_PATHS, api_config.CLASSIFICATION_CLASS_NAMES)"
here we make a dict that the request_processing_function can return to the endpoint function
to notify it of an error
check that the content uploaded is not too big
request.content_length is the length of the total payload
"also will not proceed if cannot find content_length, hence in the else we exceed the max limit"
validate detection confidence value
check that the number of images is acceptable
"check if classification is requested and if so, which classifier to use"
read input images and parameters
file of type SpooledTemporaryFile has attributes content_type and a read() method
"if the number of requests exceed this limit, a 503 is returned to the caller."
check if the request_processing_function had an error while parsing user specified parameters
filter the detections by the confidence threshold
"each result is [ymin, xmin, ymax, xmax, confidence, category]"
classification
TODO
try:
if classification:
"print('runserver, classification...')"
tic = datetime.now()
"classification_result = classifier.classify_boxes(images, image_names, result, classification)"
toc = datetime.now()
classification_inference_duration = toc - tic
"print('runserver, classification, classifcation inference duraction: {}' \"
.format({classification_inference_duration}))
""
else:
classification_result = {}
""
except Exception as e:
print('Error performing classification on the images: ' + str(e))
log.log_exception('Error performing classification on the images: ' + str(e))
"abort(500, 'Error performing classification on the images: ' + str(e))"
return results; optionally render the detections on the images and send the annotated images back
"TODO 'classification mean inference time': str(''),"
TODO
"@ai4e_service.api_sync_func(api_path='/supported_classifiers',"
"methods=['GET'],"
"maximum_concurrent_requests=1000,"
trace_name='get:get_supported_classifiers')
"def get_supported_classifiers(*args, **kwargs):"
try:
return list(api_config.CLASSIFICATION_CLASS_NAMES.keys())
except Exception as e:
return 'Supported classifiers unknown. Error: {}'.format(str(e))
img_file = BytesIO(urlopen.urlopen(url).read())
image = Image.open(img_file).convert('RGB')
image = mpimg.imread(url)
Actual detection
calculate the size
img_file = BytesIO(urlopen.urlopen(inputFileName).read())
image = Image.open(img_file).convert('RGB')
Add the patch to the Axes
"plt.savefig(outputFileName, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)"
model configuration
"if(path == ""/""):"
"path = ""index"""
#####
""
run_detector.py
""
"Functions to load a TensorFlow detection model, run inference,"
and render bounding boxes on images.
""
"See the ""test driver"" cell for example invocation."
""
#####
"%% Constants, imports, environment"
%% Core detection functions
image = mpimg.imread(url)
Actual detection
Read the image file
image = mpimg.imread(inputFileName)
Display the image
"top, left, bottom, right"
""
"x,y origin is the upper-left"
Location is the bottom-left of the rect
""
Origin is the upper-left
Add the patch to the Axes
This is magic goop that removes whitespace around image plots (sort of)
"plt.savefig(outputFileName, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)"
%% Test driver
import os
MODEL_FILE = r'/Users/ranjanbalappa/backup/camera-trap/checkpoint/frozen_inference_graph.pb'
TARGET_IMAGES = os.listdir('static/gallery')
TARGET_IMAGES = ['static/gallery/' + f for f in TARGET_IMAGES]
# # Load and run detector on target images
detection_graph = load_model(MODEL_FILE)
startTime = time.time()
"boxes,scores,classes,images = generate_detections(detection_graph,TARGET_IMAGES)"
elapsed = time.time() - startTime
"print(""Done running detector on {} files in {}"".format(len(images),humanfriendly.format_timespan(elapsed)))"
assert len(boxes) == len(TARGET_IMAGES)
inputFileNames = TARGET_IMAGES
outputFileNames=[]
confidenceThreshold=0.9
plt.ioff()
"render_bounding_boxes(boxes, scores, classes, TARGET_IMAGES)"
print(TARGET_IMAGES[0])
output_img = {}
for img_file in TARGET_IMAGES:
"box, score, clss = generate_image_detections(detection_graph, img_file)"
"name, ext = os.path.splitext(img_file.split('/')[-1])"
"num_objects, bboxes = draw_image_detections(box, score, clss, img_file, 'static/results/' + name )"
output_img[img_file.split('/')[-1]] = {
"'num_objects': num_objects,"
"'image_name': img_file.split('/')[-1],"
"'result': 'Animal Detected' if num_objects > 0 else 'No Animal Detected',"
'bboxes': bboxes
}
import json
"with open('static/gallery_results/results.json', 'w') as res:"
"json.dump(output_img, res)"
from . import model
from . import aadConfig as aad
api_url = apiconfig.api['base_url'] + '/camera-trap/detect?confidence={1}&render={1}'
routes for cameratrapassets as these are being loaded
from the cameratrapassets directory instead of the static directory
"def track_images(file, name):"
print(str(e))
resize_images(images)
"bbox points, confidence"
print(img_result)
redirect to home if no images to display
"gallery_images = random.sample(gallery_images, 12)"
from . import aadConfig as aad
api url
Dropzone settings
app.config['AUTHORITY_URL'] =  aad.AUTHORITY_HOST_URL + '/' + aad.TENANT
app.config['DROPZONE_IN_FORM'] = True
app.config['DROPZONE_UPLOAD_ON_CLICK'] = True
app.config['DROPZONE_UPLOAD_BTN_ID'] =  'submit'
app.config[' DROPZONE_UPLOAD_ACTION'] = 'processimages'
Uploads settings
model configuration
# sourceMappingURL=popper.min.js.map
Noty.overrideDefaults({
"layout   : 'topRight',"
"theme    : 'mint',"
"closeWith: ['click', 'button'],"
"timeout: 1500,"
animation: {
"open : 'animated fadeInRight',"
close: 'animated fadeOutRight'
}
});
Initialize
var bLazy = new Blazy({
container: '.scroll-class'
});
timeout: 2500
Defining this global in .eslintrc.json would create a danger of using the global
"unguarded in another place, it seems safer to define global only for this module"
Define a local copy of jQuery
The jQuery object is actually just the init constructor 'enhanced'
Need init if jQuery is called (just allow error to be thrown if not included)
Support: Android <=4.0 only
Make sure we trim BOM and NBSP
The current version of jQuery being used
The default length of a jQuery object is 0
Get the Nth element in the matched element set OR
Get the whole matched element set as a clean array
Return all the elements in a clean array
Return just the one element from the set
Take an array of elements and push it onto the stack
(returning the new matched element set)
Build a new jQuery matched element set
Add the old object onto the stack (as a reference)
Return the newly-formed element set
Execute a callback for every element in the matched set.
For internal use only.
"Behaves like an Array's method, not like a jQuery method."
Handle a deep copy situation
Skip the boolean and the target
Handle case when target is a string or something (possible in deep copy)
Extend jQuery itself if only one argument is passed
Only deal with non-null/undefined values
Extend the base object
Prevent Object.prototype pollution
Prevent never-ending loop
Recurse if we're merging plain objects or arrays
Ensure proper type for the source value
"Never move original objects, clone them"
Don't bring in undefined values
Return the modified object
Unique for each copy of jQuery on the page
Assume jQuery is ready without the ready module
Detect obvious negatives
Use toString instead of jQuery.type to catch host objects
"Objects with no prototype (e.g., `Object.create( null )`) are plain"
Objects with prototype are plain iff they were constructed by a global Object function
Evaluates a script in a global context
Support: Android <=4.0 only
results is for internal usage only
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
"Go through the array, only saving the items"
that pass the validator function
arg is for internal usage only
"Go through the array, translating each of the items to their new values"
"Go through every key on the object,"
Flatten any nested arrays
A global GUID counter for objects
jQuery.support is not used in Core but other projects attach their
properties to it so it needs to exist.
Populate the class2type map
Support: real iOS 8.2 only (not reproducible in simulator)
`in` check used to prevent JIT error (gh-2145)
hasOwn isn't used here due to false negatives
regarding Nodelist length in IE
For CommonJS and CommonJS-like environments where a proper `window`
"is present, execute the factory and get jQuery."
For environments that do not have a `window` with a `document`
"(such as Node.js), expose a factory as module.exports."
This accentuates the need for the creation of a real `window`.
"e.g. var jQuery = require(""jquery"")(window);"
See ticket #14549 for more info.
Pass this if window is not defined yet
"Edge <= 12 - 13+, Firefox <=18 - 45+, IE 10 - 11, Safari 5.1 - 9+, iOS 6 - 9.1"
"throw exceptions when non-strict code (e.g., ASP.NET 4.5) accesses strict mode"
"arguments.callee.caller (trac-13335). But as of jQuery 3.0 (2016), strict mode should be common"
enough that all such attempts are guarded in a try block.
"Support: Chrome <=57, Firefox <=52"
"In some browsers, typeof returns ""function"" for HTML <object> elements"
"(i.e., `typeof document.createElement( ""object"" ) === ""function""`)."
We don't want to classify *any* DOM node as a function.
"Support: Firefox 64+, Edge 18+"
"Some browsers don't support the ""nonce"" property on scripts."
"On the other hand, just using `getAttribute` is not enough as"
the `nonce` attribute is reset to an empty string whenever it
becomes browsing-context connected.
See https://github.com/whatwg/html/issues/2369
See https://html.spec.whatwg.org/#nonce-attributes
The `node.getAttribute` check was added for the sake of
`jQuery.globalEval` so that it can fake a nonce-containing node
via an object.
Support: Android <=2.3 only (functionish RegExp)
Defining this global in .eslintrc.json would create a danger of using the global
"unguarded in another place, it seems safer to define global only for this module"
Define a local copy of jQuery
The jQuery object is actually just the init constructor 'enhanced'
Need init if jQuery is called (just allow error to be thrown if not included)
Support: Android <=4.0 only
Make sure we trim BOM and NBSP
The current version of jQuery being used
The default length of a jQuery object is 0
Get the Nth element in the matched element set OR
Get the whole matched element set as a clean array
Return all the elements in a clean array
Return just the one element from the set
Take an array of elements and push it onto the stack
(returning the new matched element set)
Build a new jQuery matched element set
Add the old object onto the stack (as a reference)
Return the newly-formed element set
Execute a callback for every element in the matched set.
For internal use only.
"Behaves like an Array's method, not like a jQuery method."
Handle a deep copy situation
Skip the boolean and the target
Handle case when target is a string or something (possible in deep copy)
Extend jQuery itself if only one argument is passed
Only deal with non-null/undefined values
Extend the base object
Prevent Object.prototype pollution
Prevent never-ending loop
Recurse if we're merging plain objects or arrays
Ensure proper type for the source value
"Never move original objects, clone them"
Don't bring in undefined values
Return the modified object
Unique for each copy of jQuery on the page
Assume jQuery is ready without the ready module
Detect obvious negatives
Use toString instead of jQuery.type to catch host objects
"Objects with no prototype (e.g., `Object.create( null )`) are plain"
Objects with prototype are plain iff they were constructed by a global Object function
Evaluates a script in a global context
Support: Android <=4.0 only
results is for internal usage only
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
"Go through the array, only saving the items"
that pass the validator function
arg is for internal usage only
"Go through the array, translating each of the items to their new values"
"Go through every key on the object,"
Flatten any nested arrays
A global GUID counter for objects
jQuery.support is not used in Core but other projects attach their
properties to it so it needs to exist.
Populate the class2type map
Support: real iOS 8.2 only (not reproducible in simulator)
`in` check used to prevent JIT error (gh-2145)
hasOwn isn't used here due to false negatives
regarding Nodelist length in IE
Local document vars
Instance-specific data
Instance methods
Use a stripped-down indexOf as it's faster than native
https://jsperf.com/thor-indexof-vs-for/5
Regular expressions
http://www.w3.org/TR/css3-selectors/#whitespace
http://www.w3.org/TR/CSS21/syndata.html#value-def-identifier
Attribute selectors: http://www.w3.org/TR/selectors/#attribute-selectors
Operator (capture 2)
"""Attribute values must be CSS identifiers [capture 5] or strings [capture 3 or capture 4]"""
"To reduce the number of selectors needing tokenize in the preFilter, prefer arguments:"
1. quoted (capture 3; capture 4 or capture 5)
2. simple (capture 6)
3. anything else (capture 2)
"Leading and non-escaped trailing whitespace, capturing some non-whitespace characters preceding the latter"
For use in libraries implementing .is()
We use this for POS matching in `select`
Easily-parseable/retrievable ID or TAG or CLASS selectors
CSS escapes
http://www.w3.org/TR/CSS21/syndata.html#escaped-characters
NaN means non-codepoint
Support: Firefox<24
"Workaround erroneous numeric interpretation of +""0x"""
BMP codepoint
Supplemental Plane codepoint (surrogate pair)
CSS string/identifier serialization
https://drafts.csswg.org/cssom/#common-serializing-idioms
U+0000 NULL becomes U+FFFD REPLACEMENT CHARACTER
Control characters and (dependent upon position) numbers get escaped as code points
Other potentially-special ASCII characters get backslash-escaped
Used for iframes
See setDocument()
"Removing the function wrapper causes a ""Permission Denied"""
error in IE
"Optimize for push.apply( _, NodeList )"
Support: Android<4.0
Detect silently failing push.apply
Leverage slice if possible
Support: IE<9
Otherwise append directly
Can't trust NodeList.length
"nodeType defaults to 9, since context defaults to document"
Return early from calls with invalid selector or context
Try to shortcut find operations (as opposed to filters) in HTML documents
"If the selector is sufficiently simple, try using a ""get*By*"" DOM method"
"(excepting DocumentFragment context, where the methods don't exist)"
ID selector
Document context
"Support: IE, Opera, Webkit"
TODO: identify versions
getElementById can match elements by name instead of ID
Element context
"Support: IE, Opera, Webkit"
TODO: identify versions
getElementById can match elements by name instead of ID
Type selector
Class selector
Take advantage of querySelectorAll
Support: IE 8 only
Exclude object elements
qSA considers elements outside a scoping root when evaluating child or
"descendant combinators, which is not what we want."
"In such cases, we work around the behavior by prefixing every selector in the"
list with an ID selector referencing the scope context.
Thanks to Andrew Dupont for this technique.
"Capture the context ID, setting it first if necessary"
Prefix every selector in the list
Expand context for sibling selectors
All others
"Use (key + "" "") to avoid collision with native prototype properties (see Issue #157)"
Only keep the most recent entries
Remove from its parent by default
release memory in IE
Use IE sourceIndex if available on both nodes
Check if b follows a
Known :disabled false positives: fieldset[disabled] > legend:nth-of-type(n+2) :can-disable
Only certain elements can match :enabled or :disabled
https://html.spec.whatwg.org/multipage/scripting.html#selector-enabled
https://html.spec.whatwg.org/multipage/scripting.html#selector-disabled
Check for inherited disabledness on relevant non-disabled elements:
* listed form-associated elements in a disabled fieldset
https://html.spec.whatwg.org/multipage/forms.html#category-listed
https://html.spec.whatwg.org/multipage/forms.html#concept-fe-disabled
* option elements in a disabled optgroup
https://html.spec.whatwg.org/multipage/forms.html#concept-option-disabled
"All such elements have a ""form"" property."
Option elements defer to a parent optgroup if present
Support: IE 6 - 11
Use the isDisabled shortcut property to check for disabled fieldset ancestors
"Where there is no isDisabled, check manually"
Try to winnow out elements that can't be disabled before trusting the disabled property.
"Some victims get caught in our net (label, legend, menu, track), but it shouldn't"
"even exist on them, let alone have a boolean value."
Remaining elements are neither :enabled nor :disabled
Match elements found at the specified indexes
Expose support vars for convenience
Support: IE <=8
"Assume HTML when documentElement doesn't yet exist, such as inside loading iframes"
https://bugs.jquery.com/ticket/4833
Return early if doc is invalid or already selected
Update global variables
"Support: IE 9-11, Edge"
"Accessing iframe documents after unload throws ""permission denied"" errors (jQuery #13936)"
"Support: IE 11, Edge"
Support: IE 9 - 10 only
Support: IE<8
Verify that getAttribute really returns attributes and not properties
(excepting IE8 booleans)
"Check if getElementsByTagName(""*"") returns only elements"
Support: IE<9
Support: IE<10
Check if getElementById returns elements by name
"The broken getElementById methods don't pick up programmatically-set names,"
so use a roundabout getElementsByName test
ID filter and find
Support: IE 6 - 7 only
getElementById is not reliable as a find shortcut
Verify the id attribute
Fall back on getElementsByName
Tag
DocumentFragment nodes don't have gEBTN
"By happy coincidence, a (broken) gEBTN appears on DocumentFragment nodes too"
Filter out possible comments
Class
QSA and matchesSelector support
matchesSelector(:active) reports false when true (IE9/Opera 11.5)
qSa(:focus) reports false when true (Chrome 21)
We allow this because of a bug in IE8/9 that throws an error
whenever `document.activeElement` is accessed on an iframe
"So, we allow :focus to pass through QSA all the time to avoid the IE error"
See https://bugs.jquery.com/ticket/13378
Build QSA regex
Regex strategy adopted from Diego Perini
Select is set to empty string on purpose
This is to test IE's treatment of not explicitly
"setting a boolean content attribute,"
since its presence should be enough
https://bugs.jquery.com/ticket/12359
"Support: IE8, Opera 11-12.16"
Nothing should be selected when empty strings follow ^= or $= or *=
"The test attribute must be unknown in Opera but ""safe"" for WinRT"
https://msdn.microsoft.com/en-us/library/ie/hh465388.aspx#attribute_section
Support: IE8
"Boolean attributes and ""value"" are not treated correctly"
"Support: Chrome<29, Android<4.4, Safari<7.0+, iOS<7.0+, PhantomJS<1.9.8+"
Webkit/Opera - :checked should return selected option elements
http://www.w3.org/TR/2011/REC-css3-selectors-20110929/#checked
IE8 throws error here and will not see later tests
"Support: Safari 8+, iOS 8+"
https://bugs.webkit.org/show_bug.cgi?id=136851
In-page `selector#id sibling-combinator selector` fails
Support: Windows 8 Native Apps
The type and name attributes are restricted during .innerHTML assignment
Support: IE8
Enforce case-sensitivity of name attribute
FF 3.5 - :enabled/:disabled and hidden elements (hidden elements are still enabled)
IE8 throws error here and will not see later tests
Support: IE9-11+
IE's :disabled selector does not pick up the children of disabled fieldsets
Opera 10-11 does not throw on post-comma invalid pseudos
Check to see if it's possible to do matchesSelector
on a disconnected node (IE 9)
This should fail with an exception
"Gecko does not error, returns false instead"
Element contains another
Purposefully self-exclusive
"As in, an element does not contain itself"
Document order sorting
Flag for duplicate removal
Sort on method existence if only one input has compareDocumentPosition
Calculate position if both inputs belong to the same document
Otherwise we know they are disconnected
Disconnected nodes
Choose the first element that is related to our preferred document
Maintain original order
Exit early if the nodes are identical
Parentless nodes are either documents or disconnected
"If the nodes are siblings, we can do a quick check"
Otherwise we need full lists of their ancestors for comparison
Walk down the tree looking for a discrepancy
Do a sibling check if the nodes have a common ancestor
Otherwise nodes in our document sort first
Set document vars if needed
IE 9's matchesSelector returns false on disconnected nodes
"As well, disconnected nodes are said to be in a document"
fragment in IE 9
Set document vars if needed
Set document vars if needed
Don't get fooled by Object.prototype properties (jQuery #13807)
"Unless we *know* we can detect duplicates, assume their presence"
Clear input after sorting to release objects
See https://github.com/jquery/sizzle/pull/225
"If no nodeType, this is expected to be an array"
Do not traverse comment nodes
Use textContent for elements
innerText usage removed for consistency of new lines (jQuery #11153)
Traverse its children
Do not include comment or processing instruction nodes
Can be adjusted by the user
Move the given value to match[3] whether quoted or unquoted
nth-* requires argument
numeric x and y parameters for Expr.filter.CHILD
remember that false/true cast respectively to 0/1
other types prohibit arguments
Accept quoted arguments as-is
Strip excess characters from unquoted arguments
Get excess from tokenize (recursively)
advance to the next closing parenthesis
excess is a negative index
Return only captures needed by the pseudo filter method (type and argument)
Shortcut for :nth-*(n)
:(first|last|only)-(child|of-type)
Reverse direction for :only-* (if we haven't yet done so)
non-xml :nth-child(...) stores cache data on `parent`
Seek `elem` from a previously-cached index
...in a gzip-friendly way
Support: IE <9 only
Defend against cloned attroperties (jQuery gh-1709)
Fallback to seeking `elem` from the start
"When found, cache indexes on `parent` and break"
Use previously-cached element index if available
...in a gzip-friendly way
Support: IE <9 only
Defend against cloned attroperties (jQuery gh-1709)
xml :nth-child(...)
or :nth-last-child(...) or :nth(-last)?-of-type(...)
Use the same loop as above to seek `elem` from the start
Cache the index of each encountered element
Support: IE <9 only
Defend against cloned attroperties (jQuery gh-1709)
"Incorporate the offset, then check against cycle size"
pseudo-class names are case-insensitive
http://www.w3.org/TR/selectors/#pseudo-classes
Prioritize by case sensitivity in case custom pseudos are added with uppercase letters
Remember that setFilters inherits from pseudos
The user may use createPseudo to indicate that
arguments are needed to create the filter function
just as Sizzle does
But maintain support for old signatures
Potentially complex pseudos
Trim the selector passed to compile
to avoid treating leading and trailing
spaces as combinators
Match elements unmatched by `matcher`
Don't keep the element (issue #299)
"""Whether an element is represented by a :lang() selector"
is based solely on the element's language value
"being equal to the identifier C,"
"or beginning with the identifier C immediately followed by ""-""."
The matching of C against the element's language value is performed case-insensitively.
"The identifier C does not have to be a valid language name."""
http://www.w3.org/TR/selectors/#lang-pseudo
lang value must be a valid identifier
Miscellaneous
Boolean properties
"In CSS3, :checked should return both checked and selected elements"
http://www.w3.org/TR/2011/REC-css3-selectors-20110929/#checked
Accessing this property makes selected-by-default
options in Safari work properly
Contents
http://www.w3.org/TR/selectors/#empty-pseudo
":empty is negated by element (1) or content nodes (text: 3; cdata: 4; entity ref: 5),"
but not by others (comment: 8; processing instruction: 7; etc.)
nodeType < 6 works because attributes (2) do not appear as children
Element/input types
Support: IE<8
"New HTML5 attribute values (e.g., ""search"") appear with elem.type === ""text"""
Position-in-collection
Add button/input type pseudos
Easy API for creating new setFilters
Comma and first run
Don't consume trailing commas as valid
Combinators
Cast descendant combinators to space
Filters
Return the length of the invalid excess
if we're just parsing
"Otherwise, throw an error or return tokens"
Cache the tokens
Check against closest ancestor/preceding element
Check against all ancestor/preceding elements
"We can't set arbitrary data on XML nodes, so they don't benefit from combinator caching"
Support: IE <9 only
Defend against cloned attroperties (jQuery gh-1709)
Assign to newCache so results back-propagate to previous elements
Reuse newcache so results back-propagate to previous elements
A match means we're done; a fail means we have to keep checking
Get initial elements from seed or context
"Prefilter to get matcher input, preserving a map for seed-results synchronization"
"If we have a postFinder, or filtered seed, or non-seed postFilter or preexisting results,"
...intermediate processing is necessary
...otherwise use results directly
Find primary matches
Apply postFilter
Un-match failing elements by moving them back to matcherIn
Get the final matcherOut by condensing this intermediate into postFinder contexts
Restore matcherIn since elem is not yet a final match
Move matched elements from seed to results to keep them synchronized
"Add elements to results, through postFinder if defined"
The foundational matcher ensures that elements are reachable from top-level context(s)
Avoid hanging onto element (issue #299)
Return special upon seeing a positional matcher
Find the next relative operator (if any) for proper handling
"If the preceding token was a descendant combinator, insert an implicit any-element `*`"
We must always have either seed elements or outermost context
Use integer dirruns iff this is the outermost matcher
Add elements passing elementMatchers directly to results
"Support: IE<9, Safari"
"Tolerate NodeList properties (IE: ""length""; Safari: <number>) matching elements by id"
Track unmatched elements for set filters
They will have gone through all possible matchers
"Lengthen the array for every element, matched or not"
"`i` is now the count of elements visited above, and adding it to `matchedCount`"
makes the latter nonnegative.
Apply set filters to unmatched elements
"NOTE: This can be skipped if there are no unmatched elements (i.e., `matchedCount`"
"equals `i`), unless we didn't visit _any_ elements in the above loop because we have"
no element matchers and no seed.
"Incrementing an initially-string ""0"" `i` allows `i` to remain a string only in that"
"case, which will result in a ""00"" `matchedCount` that differs from `i` but is also"
numerically zero.
Reintegrate element matches to eliminate the need for sorting
Discard index placeholder values to get only actual matches
Add matches to results
Seedless set matches succeeding multiple successful matchers stipulate sorting
Override manipulation of globals by nested matchers
Generate a function of recursive functions that can be used to check each element
Cache the compiled function
Save selector and tokenization
Try to minimize operations if there is only one selector in the list and no seed
(the latter of which guarantees us context)
Reduce context if the leading compound selector is an ID
"Precompiled matchers will still verify ancestry, so step up a level"
Fetch a seed set for right-to-left matching
Abort if we hit a combinator
"Search, expanding context for leading sibling combinators"
"If seed is empty or no tokens remain, we can return early"
Compile and execute a filtering function if one is not provided
Provide `match` to avoid retokenization if we modified the selector above
One-time assignments
Sort stability
Support: Chrome 14-35+
Always assume duplicates if they aren't passed to the comparison function
Initialize against the default document
Support: Webkit<537.32 - Safari 6.0.3/Chrome 25 (fixed in Chrome 27)
Detached nodes confoundingly follow *each other*
"Should return 1, but returns 4 (following)"
Support: IE<8
"Prevent attribute/property ""interpolation"""
https://msdn.microsoft.com/en-us/library/ms536429%28VS.85%29.aspx
Support: IE<9
"Use defaultValue in place of getAttribute(""value"")"
Support: IE<9
Use getAttributeNode to fetch booleans when getAttribute lies
Deprecated
Implement the identical functionality for filter and not
Single element
"Arraylike of elements (jQuery, arguments, Array)"
Filtered directly for both simple and complex selectors
"If this is a positional/relative selector, check membership in the returned set"
"so $(""p:first"").is(""p:last"") won't return true for a doc with two ""p""."
Initialize a jQuery object
A central reference to the root jQuery(document)
A simple way to check for HTML strings
Prioritize #id over <tag> to avoid XSS via location.hash (#9521)
Strict HTML recognition (#11290: must start with <)
Shortcut simple #id case for speed
"HANDLE: $(""""), $(null), $(undefined), $(false)"
Method init() accepts an alternate rootjQuery
so migrate can support jQuery.sub (gh-2101)
Handle HTML strings
Assume that strings that start and end with <> are HTML and skip the regex check
Match html or make sure no context is specified for #id
HANDLE: $(html) -> $(array)
Option to run scripts is true for back-compat
Intentionally let the error be thrown if parseHTML is not present
"HANDLE: $(html, props)"
Properties of context are called as methods if possible
...and otherwise set as attributes
HANDLE: $(#id)
Inject the element directly into the jQuery object
"HANDLE: $(expr, $(...))"
"HANDLE: $(expr, context)"
(which is just equivalent to: $(context).find(expr)
HANDLE: $(DOMElement)
HANDLE: $(function)
Shortcut for document ready
Execute immediately if ready is not present
Give the init function the jQuery prototype for later instantiation
Initialize central reference
Methods guaranteed to produce a unique set when starting from a unique set
"Positional selectors never match, since there's no _selection_ context"
Always skip document fragments
Don't pass non-elements to Sizzle
Determine the position of an element within the set
"No argument, return index in parent"
Index in selector
Locate the position of the desired element
"If it receives a jQuery object, the first element is used"
"Support: IE 9 - 11 only, iOS 7 only, Android Browser <=4.3 only"
Treat the template element as a regular one in browsers that
don't support it.
Remove duplicates
Reverse order for parents* and prev-derivatives
Convert String-formatted options into Object-formatted ones
Convert options from String-formatted to Object-formatted if needed
(we check in cache first)
Last fire value for non-forgettable lists
Flag to know if list was already fired
Flag to prevent firing
Actual callback list
Queue of execution data for repeatable lists
Index of currently firing callback (modified by add/remove as needed)
Fire callbacks
Enforce single-firing
"Execute callbacks for all pending executions,"
respecting firingIndex overrides and runtime changes
Run callback and check for early termination
Jump to end and forget the data so .add doesn't re-fire
Forget the data if we're done with it
Clean up if we're done firing for good
Keep an empty list if we have data for future add calls
"Otherwise, this object is spent"
Actual Callbacks object
Add a callback or a collection of callbacks to the list
"If we have memory from a past run, we should fire after adding"
Inspect recursively
Remove a callback from the list
Handle firing indexes
Check if a given callback is in the list.
"If no argument is given, return whether or not list has callbacks attached."
Remove all callbacks from the list
Disable .fire and .add
Abort any current/pending executions
Clear all callbacks and values
Disable .fire
Also disable .add unless we have memory (since it would have no effect)
Abort any pending executions
Call all callbacks with the given context and arguments
Call all the callbacks with the given arguments
To know if the callbacks have already been called at least once
Check for promise aspect first to privilege synchronous behavior
Other thenables
Other non-thenables
Control `resolve` arguments by letting Array#slice cast boolean `noValue` to integer:
* false: [ value ].slice( 0 ) => resolve( value )
* true: [ value ].slice( 1 ) => resolve()
"For Promises/A+, convert exceptions into rejections"
"Since jQuery.when doesn't unwrap thenables, we can skip the extra checks appearing in"
Deferred#then to conditionally suppress rejection.
Support: Android 4.0 only
Strict mode functions invoked without .call/.apply get global-object context
"action, add listener, callbacks,"
"... .then handlers, argument index, [final state]"
Keep pipe for back-compat
"Map tuples (progress, done, fail) to arguments (done, fail, progress)"
deferred.progress(function() { bind to newDefer or newDefer.notify })
deferred.done(function() { bind to newDefer or newDefer.resolve })
deferred.fail(function() { bind to newDefer or newDefer.reject })
Support: Promises/A+ section 2.3.3.3.3
https://promisesaplus.com/#point-59
Ignore double-resolution attempts
Support: Promises/A+ section 2.3.1
https://promisesaplus.com/#point-48
"Support: Promises/A+ sections 2.3.3.1, 3.5"
https://promisesaplus.com/#point-54
https://promisesaplus.com/#point-75
Retrieve `then` only once
Support: Promises/A+ section 2.3.4
https://promisesaplus.com/#point-64
Only check objects and functions for thenability
Handle a returned thenable
Special processors (notify) just wait for resolution
Normal processors (resolve) also hook into progress
...and disregard older resolution values
Handle all other returned values
Only substitute handlers pass on context
and multiple values (non-spec behavior)
Process the value(s)
Default process is resolve
Only normal processors (resolve) catch and reject exceptions
Support: Promises/A+ section 2.3.3.3.4.1
https://promisesaplus.com/#point-61
Ignore post-resolution exceptions
Only substitute handlers pass on context
and multiple values (non-spec behavior)
Support: Promises/A+ section 2.3.3.3.1
https://promisesaplus.com/#point-57
Re-resolve promises immediately to dodge false rejection from
subsequent errors
"Call an optional hook to record the stack, in case of exception"
since it's otherwise lost when execution goes async
progress_handlers.add( ... )
fulfilled_handlers.add( ... )
rejected_handlers.add( ... )
Get a promise for this deferred
"If obj is provided, the promise aspect is added to the object"
Add list-specific methods
promise.progress = list.add
promise.done = list.add
promise.fail = list.add
Handle state
"state = ""resolved"" (i.e., fulfilled)"
"state = ""rejected"""
rejected_callbacks.disable
fulfilled_callbacks.disable
rejected_handlers.disable
fulfilled_handlers.disable
progress_callbacks.lock
progress_handlers.lock
progress_handlers.fire
fulfilled_handlers.fire
rejected_handlers.fire
deferred.notify = function() { deferred.notifyWith(...) }
deferred.resolve = function() { deferred.resolveWith(...) }
deferred.reject = function() { deferred.rejectWith(...) }
deferred.notifyWith = list.fireWith
deferred.resolveWith = list.fireWith
deferred.rejectWith = list.fireWith
Make the deferred a promise
Call given func if any
All done!
Deferred helper
count of uncompleted subordinates
count of unprocessed arguments
subordinate fulfillment data
the master Deferred
subordinate callback factory
Single- and empty arguments are adopted like Promise.resolve
Use .then() to unwrap secondary thenables (cf. gh-3000)
Multiple arguments are aggregated like Promise.all array elements
"These usually indicate a programmer mistake during development,"
warn about them ASAP rather than swallowing them by default.
Support: IE 8 - 9 only
"Console exists when dev tools are open, which can happen at any time"
The deferred used on DOM ready
Wrap jQuery.readyException in a function so that the lookup
happens at the time of error handling instead of callback
registration.
Is the DOM ready to be used? Set to true once it occurs.
A counter to track how many items to wait for before
the ready event fires. See #6781
Handle when the DOM is ready
Abort if there are pending holds or we're already ready
Remember that the DOM is ready
"If a normal DOM Ready event fired, decrement, and wait if need be"
"If there are functions bound, to execute"
The ready event handler and self cleanup method
Catch cases where $(document).ready() is called
after the browser event has already occurred.
Support: IE <=9 - 10 only
"Older IE sometimes signals ""interactive"" too soon"
Handle it asynchronously to allow scripts the opportunity to delay ready
Use the handy event callback
"A fallback to window.onload, that will always work"
Multifunctional method to get and set values of a collection
The value/s can optionally be executed if it's a function
Sets many values
Sets one value
Bulk operations run against the entire set
...except when executing function values
Gets
Matches dashed string for camelizing
Used by camelCase as callback to replace()
Convert dashed to camelCase; used by the css and data modules
"Support: IE <=9 - 11, Edge 12 - 15"
Microsoft forgot to hump their vendor prefix (#9572)
Accepts only:
- Node
- Node.ELEMENT_NODE
- Node.DOCUMENT_NODE
- Object
- Any
Check if the owner object already has a cache
"If not, create one"
"We can accept data for non-element nodes in modern browsers,"
"but we should not, see #8335."
Always return an empty object.
If it is a node unlikely to be stringify-ed or looped over
use plain assignment
Otherwise secure it in a non-enumerable property
configurable must be true to allow the property to be
deleted when data is removed
"Handle: [ owner, key, value ] args"
Always use camelCase key (gh-2257)
"Handle: [ owner, { properties } ] args"
Copy the properties one-by-one to the cache object
Always use camelCase key (gh-2257)
In cases where either:
""
1. No key was specified
"2. A string key was specified, but no value provided"
""
"Take the ""read"" path and allow the get method to determine"
"which value to return, respectively either:"
""
1. The entire cache object
2. The data stored at the key
""
"When the key is not a string, or both a key and value"
"are specified, set or extend (existing objects) with either:"
""
1. An object of properties
2. A key and value
""
"Since the ""set"" path can have two possible entry points"
return the expected data based on which path was taken[*]
Support array or space separated string of keys
If key is an array of keys...
"We always set camelCase keys, so remove that."
"If a key with the spaces exists, use it."
"Otherwise, create an array by matching non-whitespace"
Remove the expando if there's no more data
Support: Chrome <=35 - 45
Webkit & Blink performance suffers when deleting properties
"from DOM nodes, so set to undefined instead"
https://bugs.chromium.org/p/chromium/issues/detail?id=378607 (bug restricted)
Implementation Summary
""
1. Enforce API surface and semantic compatibility with 1.9.x branch
2. Improve the module's maintainability by reducing the storage
paths to a single mechanism.
"3. Use the same single mechanism to support ""private"" and ""user"" data."
"4. _Never_ expose ""private"" data to user code (TODO: Drop _data, _removeData)"
5. Avoid exposing implementation details on user objects (eg. expando properties)
6. Provide a clear path for implementation upgrade to WeakMap in 2014
Only convert to a number if it doesn't change the string
"If nothing was found internally, try to fetch any"
data from the HTML5 data-* attribute
Make sure we set the data so it isn't changed later
TODO: Now that all calls to _data and _removeData have been replaced
"with direct calls to dataPriv methods, these can be deprecated."
Gets all values
Support: IE 11 only
The attrs elements can be null (#14894)
Sets multiple values
The calling jQuery object (element matches) is not empty
(and therefore has an element appears at this[ 0 ]) and the
`value` parameter was not undefined. An empty jQuery object
will result in `undefined` for elem = this[ 0 ] which will
throw an exception if an attempt to read a data cache is made.
Attempt to get data from the cache
The key will always be camelCased in Data
"Attempt to ""discover"" the data in"
HTML5 custom data-* attrs
"We tried really hard, but the data doesn't exist."
Set the data...
We always store the camelCased key
Speed up dequeue by getting out quickly if this is just a lookup
"If the fx queue is dequeued, always remove the progress sentinel"
Add a progress sentinel to prevent the fx queue from being
automatically dequeued
Clear up the last queue stop function
"Not public - generate a queueHooks object, or return the current one"
Ensure a hooks for this queue
Get a promise resolved when queues of a certain type
are emptied (fx is the type by default)
Check attachment across shadow DOM boundaries when possible (gh-3504)
isHiddenWithinTree might be called from jQuery#filter function;
"in that case, element will be second argument"
Inline style trumps all
"Otherwise, check computed style"
Support: Firefox <=43 - 45
"Disconnected elements can have computed display: none, so first confirm that elem is"
in the document.
"Remember the old values, and insert the new ones"
Revert the old values
Starting value computation is required for potential unit mismatches
Support: Firefox <=54
Halve the iteration target value to prevent interference from CSS upper bounds (gh-2144)
Trust units reported by jQuery.css
Iteratively approximate from a nonzero starting point
Evaluate and update our best guess (doubling guesses that zero out).
Finish if the scale equals or crosses 1 (making the old*new product non-positive).
Make sure we update the tween properties later on
Apply relative offset (+=/-=) if specified
Determine new display value for elements that need to change
"Since we force visibility upon cascade-hidden elements, an immediate (and slow)"
check is required in this first loop unless we have a nonempty display value (either
inline or about-to-be-restored)
Remember what we're overwriting
Set the display of the elements in a second loop to avoid constant reflow
We have to close these tags to support XHTML (#13200)
Support: IE <=9 only
XHTML parsers do not magically insert elements in the
same way that tag soup parsers do. So we cannot shorten
this by omitting <tbody> or other required elements.
Support: IE <=9 only
Support: IE <=9 - 11 only
Use typeof to avoid zero-argument method invocation on host objects (#15151)
Mark scripts as having already been evaluated
Add nodes directly
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
Convert non-html into a text node
Convert html into DOM nodes
Deserialize a standard representation
Descend through wrappers to the right content
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
Remember the top-level container
Ensure the created nodes are orphaned (#12392)
Remove wrapper from fragment
Skip elements already in the context collection (trac-4087)
Append to fragment
Preserve script evaluation history
Capture executables
Support: Android 4.0 - 4.3 only
Check state lost if the name is set (#11217)
Support: Windows Web Apps (WWA)
`name` and `type` must use .setAttribute for WWA (#14901)
Support: Android <=4.1 only
Older WebKit doesn't clone checked state correctly in fragments
Support: IE <=11 only
Make sure textarea (and checkbox) defaultValue is properly cloned
Support: IE <=9 - 11+
"focus() and blur() are asynchronous, except when they are no-op."
"So expect focus to be synchronous when the element is already active,"
and blur to be synchronous when the element is not already active.
"(focus and blur are always synchronous in other supported browsers,"
this just defines when we can count on it).
Support: IE <=9 only
Accessing document.activeElement can throw unexpectedly
https://bugs.jquery.com/ticket/13393
Types can be a map of types/handlers
"( types-Object, selector, data )"
"( types-Object, data )"
"( types, fn )"
"( types, selector, fn )"
"( types, data, fn )"
"Can use an empty set, since event contains the info"
Use same guid so caller can remove using origFn
Don't attach events to noData or text/comment nodes (but allow plain objects)
Caller can pass in an object of custom data in lieu of the handler
Ensure that invalid selectors throw exceptions at attach time
"Evaluate against documentElement in case elem is a non-element node (e.g., document)"
"Make sure that the handler has a unique ID, used to find/remove it later"
"Init the element's event structure and main handler, if this is the first"
Discard the second event of a jQuery.event.trigger() and
when an event is called after a page has unloaded
Handle multiple events separated by a space
"There *must* be a type, no attaching namespace-only handlers"
"If event changes its type, use the special event handlers for the changed type"
"If selector defined, determine special event api type, otherwise given type"
Update special based on newly reset type
handleObj is passed to all event handlers
Init the event handler queue if we're the first
Only use addEventListener if the special events handler returns false
"Add to the element's handler list, delegates in front"
"Keep track of which events have ever been used, for event optimization"
Detach an event or set of events from an element
Once for each type.namespace in types; type may be omitted
"Unbind all events (on this namespace, if provided) for the element"
Remove matching events
Remove generic event handler if we removed something and no more handlers exist
(avoids potential for endless recursion during removal of special event handlers)
Remove data and the expando if it's no longer used
Make a writable jQuery.Event from the native event object
Use the fix-ed jQuery.Event rather than the (read-only) native event
"Call the preDispatch hook for the mapped type, and let it bail if desired"
Determine handlers
Run delegates first; they may want to stop propagation beneath us
"If the event is namespaced, then each handler is only invoked if it is"
specially universal or its namespaces are a superset of the event's.
Call the postDispatch hook for the mapped type
Find delegate handlers
Support: IE <=9
Black-hole SVG <use> instance trees (trac-13180)
Support: Firefox <=42
Suppress spec-violating clicks indicating a non-primary pointer button (trac-3861)
https://www.w3.org/TR/DOM-Level-3-Events/#event-type-click
Support: IE 11 only
"...but not arrow key ""clicks"" of radio inputs, which can have `button` -1 (gh-2343)"
Don't check non-elements (#13208)
"Don't process clicks on disabled elements (#6911, #8165, #11382, #11764)"
Don't conflict with Object.prototype properties (#13203)
Add the remaining (directly-bound) handlers
Prevent triggered image.load events from bubbling to window.load
Utilize native event to ensure correct state for checkable inputs
"For mutual compressibility with _default, replace `this` access with a local var."
`|| data` is dead code meant only to preserve the variable through minification.
Claim the first handler
"dataPriv.set( el, ""click"", ... )"
Return false to allow normal processing in the caller
"For mutual compressibility with _default, replace `this` access with a local var."
`|| data` is dead code meant only to preserve the variable through minification.
Force setup before triggering a click
Return non-false to allow normal event-path propagation
"For cross-browser consistency, suppress native .click() on links"
Also prevent it if we're currently inside a leveraged native-event stack
Support: Firefox 20+
Firefox doesn't alert if the returnValue field is not set.
Ensure the presence of an event listener that handles manually-triggered
synthetic events by interrupting progress until reinvoked in response to
"*native* events that it fires directly, ensuring that state changes have"
already occurred before other listeners are invoked.
"Missing expectSync indicates a trigger call, which must force setup through jQuery.event.add"
Register the controller as a special universal handler for all event namespaces
Interrupt processing of the outer synthetic .trigger()ed event
Store arguments for use when handling the inner native event
Trigger the native event and capture its result
Support: IE <=9 - 11+
focus() and blur() are asynchronous
Cancel the outer synthetic event
If this is an inner synthetic event for an event with a bubbling surrogate
"(focus or blur), assume that the surrogate already propagated from triggering the"
native event and prevent that from happening again here.
This technically gets the ordering wrong w.r.t. to `.trigger()` (in which the
"bubbling surrogate propagates *after* the non-bubbling base), but that seems"
less bad than duplication.
"If this is a native event triggered above, everything is now in order"
Fire an inner synthetic event with the original arguments
...and capture the result
Support: IE <=9 - 11+
Extend with the prototype to reset the above stopImmediatePropagation()
Abort handling of the native event
"This ""if"" is needed for plain objects"
Allow instantiation without the 'new' keyword
Event object
Events bubbling up the document may have been marked as prevented
by a handler lower down the tree; reflect the correct value.
Support: Android <=2.3 only
Create target properties
Support: Safari <=6 - 7 only
"Target should not be a text node (#504, #13143)"
Event type
Put explicitly provided properties onto the event object
Create a timestamp if incoming event doesn't have one
Mark it as fixed
jQuery.Event is based on DOM3 Events as specified by the ECMAScript Language Binding
https://www.w3.org/TR/2003/WD-DOM-Level-3-Events-20030331/ecma-script-binding.html
Includes all common event props including KeyEvent and MouseEvent specific props
Add which for key events
Add which for click: 1 === left; 2 === middle; 3 === right
Utilize native event if possible so blur/focus sequence is correct
Claim the first handler
"dataPriv.set( this, ""focus"", ... )"
"dataPriv.set( this, ""blur"", ... )"
Return false to allow normal processing in the caller
Force setup before trigger
Return non-false to allow normal event-path propagation
Create mouseenter/leave events using mouseover/out and event-time checks
so that event delegation works in jQuery.
Do the same for pointerenter/pointerleave and pointerover/pointerout
""
Support: Safari 7 only
Safari sends mouseenter too often; see:
https://bugs.chromium.org/p/chromium/issues/detail?id=470258
for the description of the bug (it existed in older Chrome versions as well).
For mouseenter/leave call the handler if related is outside the target.
NB: No relatedTarget if the mouse left/entered the browser window
( event )  dispatched jQuery.Event
"( types-object [, selector] )"
"( types [, fn] )"
See https://github.com/eslint/eslint/issues/3229
"Support: IE <=10 - 11, Edge 12 - 13 only"
In IE/Edge using regex groups here causes severe slowdowns.
See https://connect.microsoft.com/IE/feedback/details/1736512/
"checked=""checked"" or checked"
Prefer a tbody over its parent table for containing new rows
Replace/restore the type attribute of script elements for safe DOM manipulation
"1. Copy private data: events, handlers, etc."
2. Copy user data
"Fix IE bugs, see support tests"
Fails to persist the checked state of a cloned checkbox or radio button.
Fails to return the selected option to the default selected state when cloning options
Flatten any nested arrays
"We can't cloneNode fragments that contain checked, in WebKit"
Require either new content or an interest in ignored elements to invoke the callback
Use the original fragment for the last item
instead of the first because it can end up
being emptied incorrectly in certain situations (#8070).
Keep references to cloned scripts for later restoration
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
Reenable scripts
Evaluate executable scripts on first document insertion
"Optional AJAX dependency, but won't run scripts if not present"
Fix IE cloning issues
We eschew Sizzle here for performance reasons: https://jsperf.com/getall-vs-sizzle/2
Copy the events from the original to the clone
Preserve script evaluation history
Return the cloned set
This is a shortcut to avoid jQuery.event.remove's overhead
Support: Chrome <=35 - 45+
"Assign undefined instead of using delete, see Data#remove"
Support: Chrome <=35 - 45+
"Assign undefined instead of using delete, see Data#remove"
Prevent memory leaks
Remove any remaining nodes
See if we can take a shortcut and just use innerHTML
Remove element nodes and prevent memory leaks
"If using innerHTML throws an exception, use the fallback method"
"Make the changes, replacing each non-ignored context element with the new content"
Force callback invocation
"Support: Android <=4.0 only, PhantomJS 1 only"
".get() because push.apply(_, arraylike) throws on ancient WebKit"
"Support: IE <=11 only, Firefox <=30 (#15098, #14150)"
IE throws on elements created in popups
"FF meanwhile throws on frame elements through ""defaultView.getComputedStyle"""
Executing both pixelPosition & boxSizingReliable tests require only one layout
so they're executed at the same time to save the second computation.
"This is a singleton, we need to execute it only once"
"Support: Android 4.0 - 4.3 only, Firefox <=3 - 44"
"Support: Android 4.0 - 4.3 only, Safari <=9.1 - 10.1, iOS <=7.0 - 9.3"
"Some styles come back with percentage values, even though they shouldn't"
Support: IE 9 - 11 only
Detect misreporting of content dimensions for box-sizing:border-box elements
Support: IE 9 only
Detect overflow:scroll screwiness (gh-3699)
Support: Chrome <=64
Don't get tricked when zoom affects offsetWidth (gh-4029)
Nullify the div so it wouldn't be stored in the memory and
it will also be a sign that checks already performed
Finish early in limited (non-browser) environments
Support: IE <=9 - 11 only
Style of cloned element affects source element cloned (#8908)
Support: Firefox 51+
Retrieving style before computed somehow
fixes an issue with getting wrong values
on detached elements
getPropertyValue is needed for:
".css('filter') (IE 9 only, #12537)"
.css('--customProperty) (#3144)
"A tribute to the ""awesome hack by Dean Edwards"""
"Android Browser returns percentage for some values,"
but width seems to be reliably pixels.
This is against the CSSOM draft spec:
https://drafts.csswg.org/cssom/#resolved-values
Remember the original values
Put in the new values to get a computed value out
Revert the changed values
Support: IE <=9 - 11 only
IE returns zIndex value as an integer.
"Define the hook, we'll check on the first run if it's really needed."
Hook not needed (or it's not possible to use it due
"to missing dependency), remove it."
Hook needed; redefine it so that the support test is not executed again.
Return a vendor-prefixed property or undefined
Check for vendor prefixed names
Return a potentially-mapped jQuery.cssProps or vendor prefixed property
Swappable if display is none or starts with table
"except ""table"", ""table-cell"", or ""table-caption"""
See here for display values: https://developer.mozilla.org/en-US/docs/CSS/display
Any relative (+/-) values have already been
normalized at this point
"Guard against undefined ""subtract"", e.g., when used as in cssHooks"
Adjustment may not be necessary
Both box models exclude margin
"If we get here with a content-box, we're seeking ""padding"" or ""border"" or ""margin"""
Add padding
"For ""border"" or ""margin"", add border"
But still keep track of it otherwise
"If we get here with a border-box (content + padding + border), we're seeking ""content"" or"
"""padding"" or ""margin"""
"For ""content"", subtract padding"
"For ""content"" or ""padding"", subtract border"
Account for positive content-box scroll gutter when requested by providing computedVal
"offsetWidth/offsetHeight is a rounded sum of content, padding, scroll gutter, and border"
"Assuming integer scroll gutter, subtract the rest and round down"
"If offsetWidth/offsetHeight is unknown, then we can't determine content-box scroll gutter"
Use an explicit zero to avoid NaN (gh-3964)
Start with computed style
"To avoid forcing a reflow, only fetch boxSizing if we need it (gh-4322)."
Fake content-box until we know it's needed to know the true value.
Support: Firefox <=54
"Return a confounding non-pixel value or feign ignorance, as appropriate."
"Fall back to offsetWidth/offsetHeight when value is ""auto"""
This happens for inline elements with no explicit setting (gh-3571)
Support: Android <=4.1 - 4.3 only
Also use offsetWidth/offsetHeight for misreported inline dimensions (gh-3602)
Support: IE 9-11 only
Also use offsetWidth/offsetHeight for when box sizing is unreliable
We use getClientRects() to check for hidden/disconnected.
"In those cases, the computed value can be trusted to be border-box"
"Where available, offsetWidth/offsetHeight approximate border box dimensions."
"Where not available (e.g., SVG), assume unreliable box-sizing and interpret the"
retrieved value as a content box dimension.
"Normalize """" and auto"
Adjust for the element's box model
Provide the current computed size to request scroll gutter calculation (gh-3589)
Add in style property hooks for overriding the default
behavior of getting and setting a style property
We should always get a number back from opacity
"Don't automatically add ""px"" to these possibly-unitless properties"
Add in properties whose names you wish to fix before
setting or getting the value
Get and set the style property on a DOM Node
Don't set styles on text and comment nodes
Make sure that we're working with the right name
Make sure that we're working with the right name. We don't
want to query the value if it is a CSS custom property
since they are user-defined.
"Gets hook for the prefixed version, then unprefixed version"
Check if we're setting a value
"Convert ""+="" or ""-="" to relative numbers (#7345)"
Fixes bug #9237
Make sure that null and NaN values aren't set (#7116)
"If a number was passed in, add the unit (except for certain CSS properties)"
The isCustomProp check can be removed in jQuery 4.0 when we only auto-append
"""px"" to a few hardcoded values."
background-* props affect original clone's values
"If a hook was provided, use that value, otherwise just set the specified value"
If a hook was provided get the non-computed value from there
Otherwise just get the value from the style object
Make sure that we're working with the right name. We don't
want to modify the value if it is a CSS custom property
since they are user-defined.
Try prefixed name followed by the unprefixed name
If a hook was provided get the computed value from there
"Otherwise, if a way to get the computed value exists, use that"
"Convert ""normal"" to computed value"
Make numeric if forced or a qualifier was provided and val looks numeric
Certain elements can have dimension info if we invisibly show them
but it must have a current display style that would benefit
Support: Safari 8+
Table columns in Safari have non-zero offsetWidth & zero
getBoundingClientRect().width unless display is changed.
Support: IE <=11 only
Running getBoundingClientRect on a disconnected node
in IE throws an error.
Only read styles.position if the test has a chance to fail
to avoid forcing a reflow.
"To avoid forcing a reflow, only fetch boxSizing if we need it (gh-3991)"
Account for unreliable border-box dimensions by comparing offset* to computed and
faking a content-box to get border and padding (gh-3699)
Convert to pixels if value adjustment is needed
These hooks are used by animate to expand properties
Assumes a single number if not a string
"Use a property on the element directly when it is not a DOM element,"
or when there is no matching style property that exists.
Passing an empty string as a 3rd parameter to .css will automatically
attempt a parseFloat and fallback to a string if the parse fails.
"Simple values such as ""10px"" are parsed to Float;"
"complex values such as ""rotate(1rad)"" are returned as-is."
"Empty strings, null, undefined and ""auto"" are converted to 0."
Use step hook for back compat.
Use cssHook if its there.
Use .style if available and use plain properties where available.
Support: IE <=9 only
Panic based approach to setting things on disconnected nodes
Back compat <1.8 extension point
Animations created synchronously will run synchronously
Generate parameters to create a standard animation
"If we include width, step value is 1 to do all cssExpand values,"
otherwise step value is 2 to skip over Left and Right
We're done with this property
Queue-skipping animations hijack the fx hooks
Ensure the complete handler is called before this completes
Detect show/hide animations
"Pretend to be hidden if this is a ""show"" and"
there is still data from a stopped show/hide
Ignore all other no-op show/hide data
Bail out if this is a no-op like .hide().hide()
"Restrict ""overflow"" and ""display"" styles during box animations"
"Support: IE <=9 - 11, Edge 12 - 15"
Record all 3 overflow attributes because IE does not infer the shorthand
from identically-valued overflowX and overflowY and Edge just mirrors
the overflowX value there.
"Identify a display type, preferring old show/hide data over the CSS cascade"
Get nonempty value(s) by temporarily forcing visibility
Animate inline elements as inline-block
Restore the original display value at the end of pure show/hide animations
Implement show/hide animations
General show/hide setup for this element animation
"Store hidden/visible for toggle so `.stop().toggle()` ""reverses"""
Show elements before animating them
"The final step of a ""hide"" animation is actually hiding the element"
Per-property setup
"camelCase, specialEasing and expand cssHook pass"
"Not quite $.extend, this won't overwrite existing keys."
"Reusing 'index' because we have the correct ""name"""
Don't match elem in the :animated selector
Support: Android 2.3 only
Archaic crash bug won't allow us to use `1 - ( 0.5 || 0 )` (#12497)
"If there's more to do, yield"
"If this was an empty animation, synthesize a final progress notification"
Resolve the animation and report its conclusion
"If we are going to the end, we want to run all the tweens"
otherwise we skip this part
"Resolve when we played the last frame; otherwise, reject"
Attach callbacks from options
Go to the end state if fx are off
"Normalize opt.queue - true/undefined/null -> ""fx"""
Queueing
Show any hidden elements after setting opacity to 0
Animate to the value specified
Operate on a copy of prop so per-property easing won't be lost
"Empty animations, or finishing resolves immediately"
Start the next in the queue if the last step wasn't forced.
"Timers currently will call their complete callbacks, which"
will dequeue but only if they were gotoEnd.
Enable finishing flag on private data
Empty the queue first
"Look for any active animations, and finish them"
Look for any animations in the old queue and finish them
Turn off finishing flag
Generate shortcuts for custom animations
Run the timer and safely remove it when done (allowing for external removal)
Default speed
"Based off of the plugin by Clint Helfers, with permission."
https://web.archive.org/web/20100324014747/http://blindsignals.com/index.php/2009/07/jquery-delay/
Support: Android <=4.3 only
"Default value for a checkbox should be ""on"""
Support: IE <=11 only
Must access selectedIndex to make default options select
Support: IE <=11 only
An input loses its value after becoming a radio
"Don't get/set attributes on text, comment and attribute nodes"
Fallback to prop when attributes are not supported
Attribute hooks are determined by the lowercase version
Grab necessary hook if one is defined
"Non-existent attributes return null, we normalize to undefined"
Attribute names can contain non-HTML whitespace characters
https://html.spec.whatwg.org/multipage/syntax.html#attributes-2
Hooks for boolean attributes
Remove boolean attributes when set to false
Avoid an infinite loop by temporarily removing this function from the getter
"Don't get/set properties on text, comment and attribute nodes"
Fix name and attach hooks
Support: IE <=9 - 11 only
elem.tabIndex doesn't always return the
correct value when it hasn't been explicitly set
https://web.archive.org/web/20141116233347/http://fluidproject.org/blog/2008/01/09/getting-setting-and-removing-tabindex-values-with-javascript/
Use proper attribute retrieval(#12072)
Support: IE <=11 only
Accessing the selectedIndex property
forces the browser to respect setting selected
on the option
The getter ensures a default option is selected
when in an optgroup
"eslint rule ""no-unused-expressions"" is disabled for this code"
since it considers such accessions noop
Strip and collapse whitespace according to HTML spec
https://infra.spec.whatwg.org/#strip-and-collapse-ascii-whitespace
Only assign if different to avoid unneeded rendering.
This expression is here for better compressibility (see addClass)
Remove *all* instances
Only assign if different to avoid unneeded rendering.
Toggle individual class names
"Check each className given, space separated list"
Toggle whole class name
Store className if set
"If the element has a class name or if we're passed `false`,"
"then remove the whole classname (if there was one, the above saved it)."
"Otherwise bring back whatever was previously saved (if anything),"
falling back to the empty string if nothing was stored.
Handle most common string cases
Handle cases where value is null/undef or number
"Treat null/undefined as """"; convert numbers to string"
"If set returns undefined, fall back to normal setting"
Support: IE <=10 - 11 only
"option.text throws exceptions (#14686, #14858)"
Strip and collapse whitespace
https://html.spec.whatwg.org/#strip-and-collapse-whitespace
Loop through all the selected options
Support: IE <=9 only
IE8-9 doesn't update selected after form reset (#2551)
Don't return options that are disabled or in a disabled optgroup
Get the specific value for the option
We don't need an array for one selects
Multi-Selects return an array
Force browsers to behave consistently when non-matching value is set
Radios and checkboxes getter/setter
Return jQuery for attributes-only inclusion
Don't do events on text and comment nodes
focus/blur morphs to focusin/out; ensure we're not firing them right now
Namespaced trigger; create a regexp to match event type in handle()
"Caller can pass in a jQuery.Event object, Object, or just an event type string"
Trigger bitmask: & 1 for native handlers; & 2 for jQuery (always true)
Clean up the event in case it is being reused
"Clone any incoming data and prepend the event, creating the handler arg list"
Allow special events to draw outside the lines
"Determine event propagation path in advance, per W3C events spec (#9951)"
"Bubble up to document, then to window; watch for a global ownerDocument var (#9724)"
"Only add window if we got to document (e.g., not plain obj or detached DOM)"
Fire handlers on the event path
jQuery handler
Native handler
"If nobody prevented the default action, do it now"
Call a native DOM method on the target with the same name as the event.
"Don't do default actions on window, that's where global variables be (#6170)"
Don't re-trigger an onFOO event when we call its FOO() method
"Prevent re-triggering of the same event, since we already bubbled it above"
Piggyback on a donor event to simulate a different one
Used only for `focus(in | out)` events
Support: Firefox <=44
Firefox doesn't have focus(in | out) events
Related ticket - https://bugzilla.mozilla.org/show_bug.cgi?id=687787
""
"Support: Chrome <=48 - 49, Safari <=9.0 - 9.1"
"focus(in | out) events fire after focus & blur events,"
which is spec violation - http://www.w3.org/TR/DOM-Level-3-Events/#events-focusevent-event-order
Related ticket - https://bugs.chromium.org/p/chromium/issues/detail?id=449857
Attach a single capturing handler on the document while someone wants focusin/focusout
Cross-browser xml parsing
Support: IE 9 - 11 only
IE throws on parseFromString with invalid input.
Serialize array item.
Treat each array item as a scalar.
"Item is non-scalar (array or object), encode its numeric index."
Serialize object item.
Serialize scalar item.
Serialize an array of form elements or a set of
key/values into a query string
"If value is a function, invoke it and use its return value"
"If an array was passed in, assume that it is an array of form elements."
Serialize the form elements
"If traditional, encode the ""old"" way (the way 1.3.2 or older"
"did it), otherwise encode params recursively."
Return the resulting serialization
"Can add propHook for ""elements"" to filter or add form elements"
"Use .is( "":disabled"" ) so that fieldset[disabled] works"
"#7653, #8125, #8152: local protocol detection"
Avoid comment-prolog char sequence (#10098); must appease lint and evade compression
Anchor tag for parsing the document origin
"Base ""constructor"" for jQuery.ajaxPrefilter and jQuery.ajaxTransport"
"dataTypeExpression is optional and defaults to ""*"""
For each dataType in the dataTypeExpression
Prepend if requested
Otherwise append
Base inspection function for prefilters and transports
A special extend for ajax options
"that takes ""flat"" options (not to be deep extended)"
Fixes #9887
Remove auto dataType and get content-type in the process
Check if we're dealing with a known content-type
Check to see if we have a response for the expected dataType
Try convertible dataTypes
Or just use first one
If we found a dataType
We add the dataType to the list if needed
and return the corresponding response
Work with a copy of dataTypes in case we need to modify it for conversion
Create converters map with lowercased keys
Convert to each sequential dataType
Apply the dataFilter if provided
There's only work to do if current dataType is non-auto
Convert response if prev dataType is non-auto and differs from current
Seek a direct converter
"If none found, seek a pair"
If conv2 outputs current
If prev can be converted to accepted input
Condense equivalence converters
"Otherwise, insert the intermediate dataType"
Apply converter (if not an equivalence)
"Unless errors are allowed to bubble, catch and return them"
Counter for holding the number of active queries
Last-Modified header cache for next request
Data converters
"Keys separate source (or catchall ""*"") and destination types with a single space"
Convert anything to text
Text to html (true = no transformation)
Evaluate text as a json expression
Parse text as xml
For options that shouldn't be deep extended:
you can add your own custom options here if
and when you create one that shouldn't be
deep extended (see ajaxExtend)
Creates a full fledged settings object into target
with both ajaxSettings and settings fields.
"If target is omitted, writes into ajaxSettings."
Building a settings object
Extending ajaxSettings
Main method
"If url is an object, simulate pre-1.5 signature"
Force options to be an object
URL without anti-cache param
Response headers
timeout handle
Url cleanup var
Request state (becomes false upon send and true upon completion)
To know if global events are to be dispatched
Loop variable
uncached part of the url
Create the final options object
Callbacks context
Context for global events is callbackContext if it is a DOM node or jQuery collection
Deferreds
Status-dependent callbacks
Headers (they are sent all at once)
Default abort message
Fake xhr
Builds headers hashtable if needed
Raw string
Caches the header
Overrides response content-type header
Status-dependent callbacks
Execute the appropriate callbacks
Lazy-add the new callbacks in a way that preserves old ones
Cancel the request
Attach deferreds
Add protocol if not provided (prefilters might expect it)
Handle falsy url in the settings object (#10093: consistency with old signature)
We also use the url parameter if available
Alias method option to type as per ticket #12004
Extract dataTypes list
A cross-domain request is in order when the origin doesn't match the current origin.
"Support: IE <=8 - 11, Edge 12 - 15"
"IE throws exception on accessing the href property if url is malformed,"
e.g. http://example.com:80x/
Support: IE <=8 - 11 only
Anchor's host property isn't correctly set when s.url is relative
"If there is an error parsing the URL, assume it is crossDomain,"
it can be rejected by the transport if it is invalid
Convert data if not already a string
Apply prefilters
"If request was aborted inside a prefilter, stop there"
We can fire global events as of now if asked to
Don't fire events if jQuery.event is undefined in an AMD-usage scenario (#15118)
Watch for a new set of requests
Uppercase the type
Determine if request has content
Save the URL in case we're toying with the If-Modified-Since
and/or If-None-Match header later on
Remove hash to simplify url manipulation
More options handling for requests with no content
Remember the hash so we can put it back
"If data is available and should be processed, append data to url"
#9682: remove data so that it's not used in an eventual retry
Add or update anti-cache param if needed
Put hash and anti-cache on the URL that will be requested (gh-1732)
Change '%20' to '+' if this is encoded form body content (gh-2658)
"Set the If-Modified-Since and/or If-None-Match header, if in ifModified mode."
"Set the correct header, if data is being sent"
"Set the Accepts header for the server, depending on the dataType"
Check for headers option
Allow custom headers/mimetypes and early abort
Abort if not done already and return
Aborting is no longer a cancellation
Install callbacks on deferreds
Get transport
"If no transport, we auto-abort"
Send global event
"If request was aborted inside ajaxSend, stop there"
Timeout
Rethrow post-completion exceptions
Propagate others as results
Callback for when everything is done
Ignore repeat invocations
Clear timeout if it exists
Dereference transport for early garbage collection
(no matter how long the jqXHR object will be used)
Cache response headers
Set readyState
Determine if successful
Get response data
Convert no matter what (that way responseXXX fields are always set)
"If successful, handle type chaining"
"Set the If-Modified-Since and/or If-None-Match header, if in ifModified mode."
if no content
if not modified
"If we have data, let's convert it"
Extract error from statusText and normalize for non-aborts
Set data for the fake xhr object
Success/Error
Status-dependent callbacks
Complete
Handle the global AJAX counter
Shift arguments if data argument was omitted
The url can be an options object (which then must have .url)
"Make this explicit, since user can override this through ajaxSetup (#11264)"
Only evaluate the response if it is successful (gh-4126)
"dataFilter is not invoked for failure responses, so using it instead"
of the default converter is kludgy but it works.
The elements to wrap the target around
"File protocol always yields status code 0, assume 200"
Support: IE <=9 only
#1450: sometimes IE returns 1223 when it should be 204
Cross domain only allowed if supported through XMLHttpRequest
Apply custom fields if provided
Override mime type if needed
X-Requested-With header
"For cross-domain requests, seeing as conditions for a preflight are"
"akin to a jigsaw puzzle, we simply never set it to be sure."
(it can always be set on a per-request basis or even using ajaxSetup)
"For same-domain requests, won't change header if already provided."
Set headers
Callback
Support: IE <=9 only
"On a manual native abort, IE9 throws"
errors on any property access that is not readyState
"File: protocol always yields status 0; see #8605, #14207"
Support: IE <=9 only
IE9 has no XHR2 but throws on binary (trac-11426)
"For XHR2 non-text, let the caller handle it (gh-2498)"
Listen to events
Support: IE 9 only
Use onreadystatechange to replace onabort
to handle uncaught aborts
Check readyState before timeout as it changes
"Allow onerror to be called first,"
but that will not handle a native abort
"Also, save errorCallback to a variable"
as xhr.onerror cannot be accessed
Create the abort callback
Do send the request (this may raise an exception)
#14683: Only rethrow if this hasn't been notified as an error yet
Prevent auto-execution of scripts when no explicit dataType was provided (See gh-2432)
Install script dataType
Handle cache's special case and crossDomain
Bind script tag hack transport
This transport only deals with cross domain or forced-by-attrs requests
Use native DOM manipulation to avoid our domManip AJAX trickery
Default jsonp settings
"Detect, normalize options and install callbacks for jsonp requests"
"Handle iff the expected data type is ""jsonp"" or we have a parameter to set"
"Get callback name, remembering preexisting value associated with it"
Insert callback into url or form data
Use data converter to retrieve json after script execution
Force json dataType
Install callback
Clean-up function (fires after converters)
If previous value didn't exist - remove it
Otherwise restore preexisting value
Save back as free
Make sure that re-using the options doesn't screw things around
Save the callback name for future use
Call if it was a function and we have a response
Delegate to script
Support: Safari 8 only
In Safari 8 documents created via document.implementation.createHTMLDocument
collapse sibling forms: the second one becomes a child of the first one.
"Because of that, this security measure has to be disabled in Safari 8."
https://bugs.webkit.org/show_bug.cgi?id=137337
"Argument ""data"" should be string of html"
"context (optional): If specified, the fragment will be created in this context,"
defaults to document
"keepScripts (optional): If true, will include scripts passed in the html string"
Stop scripts or inline event handlers from being executed immediately
by using document.implementation
Set the base href for the created document
so any parsed elements with URLs
are based on the document's URL (gh-2965)
Single tag
If it's a function
We assume that it's the callback
"Otherwise, build a param string"
"If we have elements to modify, make the request"
"If ""type"" variable is undefined, then ""GET"" method will be used."
Make value of this field explicit since
user can override it through ajaxSetup method
Save response for use in complete callback
"If a selector was specified, locate the right elements in a dummy div"
Exclude scripts to avoid IE 'Permission Denied' errors
Otherwise use the full result
"If the request succeeds, this function gets ""data"", ""status"", ""jqXHR"""
but they are ignored because response was set above.
"If it fails, this function gets ""jqXHR"", ""status"", ""error"""
Attach a bunch of functions for handling common AJAX events
"Set position first, in-case top/left are set even on static elem"
Need to be able to calculate position if either
top or left is auto and position is either absolute or fixed
Use jQuery.extend here to allow modification of coordinates argument (gh-1848)
offset() relates an element's border box to the document origin
Preserve chaining for setter
Return zeros for disconnected and hidden (display: none) elements (gh-2310)
Support: IE <=11 only
Running getBoundingClientRect on a
disconnected node in IE throws an error
Get document-relative position by adding viewport scroll to viewport-relative gBCR
position() relates an element's margin box to its offset parent's padding box
This corresponds to the behavior of CSS absolute positioning
"position:fixed elements are offset from the viewport, which itself always has zero offset"
Assume position:fixed implies availability of getBoundingClientRect
"Account for the *real* offset parent, which can be the document or its root element"
when a statically positioned element is identified
"Incorporate borders into its offset, since they are outside its content origin"
Subtract parent offsets and element margins
This method will return documentElement in the following cases:
"1) For the element inside the iframe without offsetParent, this method will return"
documentElement of the parent window
2) For the hidden or detached element
"3) For body or html element, i.e. in case of the html node - it will return itself"
""
but those exceptions were never presented as a real life use-cases
and might be considered as more preferable results.
""
"This logic, however, is not guaranteed and can change at any point in the future"
Create scrollLeft and scrollTop methods
Coalesce documents and windows
"Support: Safari <=7 - 9.1, Chrome <=37 - 49"
Add the top/left cssHooks using jQuery.fn.position
Webkit bug: https://bugs.webkit.org/show_bug.cgi?id=29084
Blink bug: https://bugs.chromium.org/p/chromium/issues/detail?id=589347
getComputedStyle returns percent when specified for top/left/bottom/right;
"rather than make the css module depend on the offset module, just check for it here"
"If curCSS returns percentage, fallback to offset"
"Create innerHeight, innerWidth, height, width, outerHeight and outerWidth methods"
"Margin is only for outerHeight, outerWidth"
$( window ).outerWidth/Height return w/h including scrollbars (gh-1729)
Get document width or height
"Either scroll[Width/Height] or offset[Width/Height] or client[Width/Height],"
whichever is greatest
"Get width or height on the element, requesting but not forcing parseFloat"
Set width or height on the element
Handle event binding
"( namespace ) or ( selector, types [, fn] )"
"Bind a function to a context, optionally partially applying any"
arguments.
jQuery.proxy is deprecated to promote standards (specifically Function#bind)
"However, it is not slated for removal any time soon"
"Quick check to determine if target is callable, in the spec"
"this throws a TypeError, but we will just return undefined."
Simulated bind
"Set the guid of unique handler to the same of original handler, so it can be removed"
"As of jQuery 3.0, isNumeric is limited to"
strings and numbers (primitives or objects)
that can be coerced to finite numbers (gh-2662)
"parseFloat NaNs numeric-cast false positives ("""")"
"...but misinterprets leading-number strings, particularly hex literals (""0x..."")"
subtraction forces infinities to NaN
"Register as a named AMD module, since jQuery can be concatenated with other"
"files that may use define, but not via a proper concatenation script that"
understands anonymous AMD modules. A named AMD is safest and most robust
way to register. Lowercase jquery is used because AMD module names are
"derived from file names, and jQuery is normally delivered in a lowercase"
file name. Do this after creating the global so that if an AMD module wants
"to call noConflict to hide this version of jQuery, it will work."
"Note that for maximum portability, libraries that are not jQuery should"
"declare themselves as anonymous modules, and avoid setting a global if an"
"AMD loader is present. jQuery is a special case. For more information, see"
https://github.com/jrburke/requirejs/wiki/Updating-existing-libraries#wiki-anon
Map over jQuery in case of overwrite
Map over the $ in case of overwrite
"Expose jQuery and $ identifiers, even in AMD"
"(#7102#comment:10, https://github.com/jquery/jquery/pull/557)"
and CommonJS for browser emulators (#13566)
For CommonJS and CommonJS-like environments where a proper `window`
"is present, execute the factory and get jQuery."
For environments that do not have a `window` with a `document`
"(such as Node.js), expose a factory as module.exports."
This accentuates the need for the creation of a real `window`.
"e.g. var jQuery = require(""jquery"")(window);"
See ticket #14549 for more info.
Pass this if window is not defined yet
"Edge <= 12 - 13+, Firefox <=18 - 45+, IE 10 - 11, Safari 5.1 - 9+, iOS 6 - 9.1"
"throw exceptions when non-strict code (e.g., ASP.NET 4.5) accesses strict mode"
"arguments.callee.caller (trac-13335). But as of jQuery 3.0 (2016), strict mode should be common"
enough that all such attempts are guarded in a try block.
"Support: Chrome <=57, Firefox <=52"
"In some browsers, typeof returns ""function"" for HTML <object> elements"
"(i.e., `typeof document.createElement( ""object"" ) === ""function""`)."
We don't want to classify *any* DOM node as a function.
"Support: Firefox 64+, Edge 18+"
"Some browsers don't support the ""nonce"" property on scripts."
"On the other hand, just using `getAttribute` is not enough as"
the `nonce` attribute is reset to an empty string whenever it
becomes browsing-context connected.
See https://github.com/whatwg/html/issues/2369
See https://html.spec.whatwg.org/#nonce-attributes
The `node.getAttribute` check was added for the sake of
`jQuery.globalEval` so that it can fake a nonce-containing node
via an object.
Support: Android <=2.3 only (functionish RegExp)
Defining this global in .eslintrc.json would create a danger of using the global
"unguarded in another place, it seems safer to define global only for this module"
Define a local copy of jQuery
The jQuery object is actually just the init constructor 'enhanced'
Need init if jQuery is called (just allow error to be thrown if not included)
Support: Android <=4.0 only
Make sure we trim BOM and NBSP
The current version of jQuery being used
The default length of a jQuery object is 0
Get the Nth element in the matched element set OR
Get the whole matched element set as a clean array
Return all the elements in a clean array
Return just the one element from the set
Take an array of elements and push it onto the stack
(returning the new matched element set)
Build a new jQuery matched element set
Add the old object onto the stack (as a reference)
Return the newly-formed element set
Execute a callback for every element in the matched set.
For internal use only.
"Behaves like an Array's method, not like a jQuery method."
Handle a deep copy situation
Skip the boolean and the target
Handle case when target is a string or something (possible in deep copy)
Extend jQuery itself if only one argument is passed
Only deal with non-null/undefined values
Extend the base object
Prevent Object.prototype pollution
Prevent never-ending loop
Recurse if we're merging plain objects or arrays
Ensure proper type for the source value
"Never move original objects, clone them"
Don't bring in undefined values
Return the modified object
Unique for each copy of jQuery on the page
Assume jQuery is ready without the ready module
Detect obvious negatives
Use toString instead of jQuery.type to catch host objects
"Objects with no prototype (e.g., `Object.create( null )`) are plain"
Objects with prototype are plain iff they were constructed by a global Object function
Evaluates a script in a global context
Support: Android <=4.0 only
results is for internal usage only
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
"Go through the array, only saving the items"
that pass the validator function
arg is for internal usage only
"Go through the array, translating each of the items to their new values"
"Go through every key on the object,"
Flatten any nested arrays
A global GUID counter for objects
jQuery.support is not used in Core but other projects attach their
properties to it so it needs to exist.
Populate the class2type map
Support: real iOS 8.2 only (not reproducible in simulator)
`in` check used to prevent JIT error (gh-2145)
hasOwn isn't used here due to false negatives
regarding Nodelist length in IE
Local document vars
Instance-specific data
Instance methods
Use a stripped-down indexOf as it's faster than native
https://jsperf.com/thor-indexof-vs-for/5
Regular expressions
http://www.w3.org/TR/css3-selectors/#whitespace
http://www.w3.org/TR/CSS21/syndata.html#value-def-identifier
Attribute selectors: http://www.w3.org/TR/selectors/#attribute-selectors
Operator (capture 2)
"""Attribute values must be CSS identifiers [capture 5] or strings [capture 3 or capture 4]"""
"To reduce the number of selectors needing tokenize in the preFilter, prefer arguments:"
1. quoted (capture 3; capture 4 or capture 5)
2. simple (capture 6)
3. anything else (capture 2)
"Leading and non-escaped trailing whitespace, capturing some non-whitespace characters preceding the latter"
For use in libraries implementing .is()
We use this for POS matching in `select`
Easily-parseable/retrievable ID or TAG or CLASS selectors
CSS escapes
http://www.w3.org/TR/CSS21/syndata.html#escaped-characters
NaN means non-codepoint
Support: Firefox<24
"Workaround erroneous numeric interpretation of +""0x"""
BMP codepoint
Supplemental Plane codepoint (surrogate pair)
CSS string/identifier serialization
https://drafts.csswg.org/cssom/#common-serializing-idioms
U+0000 NULL becomes U+FFFD REPLACEMENT CHARACTER
Control characters and (dependent upon position) numbers get escaped as code points
Other potentially-special ASCII characters get backslash-escaped
Used for iframes
See setDocument()
"Removing the function wrapper causes a ""Permission Denied"""
error in IE
"Optimize for push.apply( _, NodeList )"
Support: Android<4.0
Detect silently failing push.apply
Leverage slice if possible
Support: IE<9
Otherwise append directly
Can't trust NodeList.length
"nodeType defaults to 9, since context defaults to document"
Return early from calls with invalid selector or context
Try to shortcut find operations (as opposed to filters) in HTML documents
"If the selector is sufficiently simple, try using a ""get*By*"" DOM method"
"(excepting DocumentFragment context, where the methods don't exist)"
ID selector
Document context
"Support: IE, Opera, Webkit"
TODO: identify versions
getElementById can match elements by name instead of ID
Element context
"Support: IE, Opera, Webkit"
TODO: identify versions
getElementById can match elements by name instead of ID
Type selector
Class selector
Take advantage of querySelectorAll
Support: IE 8 only
Exclude object elements
qSA considers elements outside a scoping root when evaluating child or
"descendant combinators, which is not what we want."
"In such cases, we work around the behavior by prefixing every selector in the"
list with an ID selector referencing the scope context.
Thanks to Andrew Dupont for this technique.
"Capture the context ID, setting it first if necessary"
Prefix every selector in the list
Expand context for sibling selectors
All others
"Use (key + "" "") to avoid collision with native prototype properties (see Issue #157)"
Only keep the most recent entries
Remove from its parent by default
release memory in IE
Use IE sourceIndex if available on both nodes
Check if b follows a
Known :disabled false positives: fieldset[disabled] > legend:nth-of-type(n+2) :can-disable
Only certain elements can match :enabled or :disabled
https://html.spec.whatwg.org/multipage/scripting.html#selector-enabled
https://html.spec.whatwg.org/multipage/scripting.html#selector-disabled
Check for inherited disabledness on relevant non-disabled elements:
* listed form-associated elements in a disabled fieldset
https://html.spec.whatwg.org/multipage/forms.html#category-listed
https://html.spec.whatwg.org/multipage/forms.html#concept-fe-disabled
* option elements in a disabled optgroup
https://html.spec.whatwg.org/multipage/forms.html#concept-option-disabled
"All such elements have a ""form"" property."
Option elements defer to a parent optgroup if present
Support: IE 6 - 11
Use the isDisabled shortcut property to check for disabled fieldset ancestors
"Where there is no isDisabled, check manually"
Try to winnow out elements that can't be disabled before trusting the disabled property.
"Some victims get caught in our net (label, legend, menu, track), but it shouldn't"
"even exist on them, let alone have a boolean value."
Remaining elements are neither :enabled nor :disabled
Match elements found at the specified indexes
Expose support vars for convenience
Support: IE <=8
"Assume HTML when documentElement doesn't yet exist, such as inside loading iframes"
https://bugs.jquery.com/ticket/4833
Return early if doc is invalid or already selected
Update global variables
"Support: IE 9-11, Edge"
"Accessing iframe documents after unload throws ""permission denied"" errors (jQuery #13936)"
"Support: IE 11, Edge"
Support: IE 9 - 10 only
Support: IE<8
Verify that getAttribute really returns attributes and not properties
(excepting IE8 booleans)
"Check if getElementsByTagName(""*"") returns only elements"
Support: IE<9
Support: IE<10
Check if getElementById returns elements by name
"The broken getElementById methods don't pick up programmatically-set names,"
so use a roundabout getElementsByName test
ID filter and find
Support: IE 6 - 7 only
getElementById is not reliable as a find shortcut
Verify the id attribute
Fall back on getElementsByName
Tag
DocumentFragment nodes don't have gEBTN
"By happy coincidence, a (broken) gEBTN appears on DocumentFragment nodes too"
Filter out possible comments
Class
QSA and matchesSelector support
matchesSelector(:active) reports false when true (IE9/Opera 11.5)
qSa(:focus) reports false when true (Chrome 21)
We allow this because of a bug in IE8/9 that throws an error
whenever `document.activeElement` is accessed on an iframe
"So, we allow :focus to pass through QSA all the time to avoid the IE error"
See https://bugs.jquery.com/ticket/13378
Build QSA regex
Regex strategy adopted from Diego Perini
Select is set to empty string on purpose
This is to test IE's treatment of not explicitly
"setting a boolean content attribute,"
since its presence should be enough
https://bugs.jquery.com/ticket/12359
"Support: IE8, Opera 11-12.16"
Nothing should be selected when empty strings follow ^= or $= or *=
"The test attribute must be unknown in Opera but ""safe"" for WinRT"
https://msdn.microsoft.com/en-us/library/ie/hh465388.aspx#attribute_section
Support: IE8
"Boolean attributes and ""value"" are not treated correctly"
"Support: Chrome<29, Android<4.4, Safari<7.0+, iOS<7.0+, PhantomJS<1.9.8+"
Webkit/Opera - :checked should return selected option elements
http://www.w3.org/TR/2011/REC-css3-selectors-20110929/#checked
IE8 throws error here and will not see later tests
"Support: Safari 8+, iOS 8+"
https://bugs.webkit.org/show_bug.cgi?id=136851
In-page `selector#id sibling-combinator selector` fails
Support: Windows 8 Native Apps
The type and name attributes are restricted during .innerHTML assignment
Support: IE8
Enforce case-sensitivity of name attribute
FF 3.5 - :enabled/:disabled and hidden elements (hidden elements are still enabled)
IE8 throws error here and will not see later tests
Support: IE9-11+
IE's :disabled selector does not pick up the children of disabled fieldsets
Opera 10-11 does not throw on post-comma invalid pseudos
Check to see if it's possible to do matchesSelector
on a disconnected node (IE 9)
This should fail with an exception
"Gecko does not error, returns false instead"
Element contains another
Purposefully self-exclusive
"As in, an element does not contain itself"
Document order sorting
Flag for duplicate removal
Sort on method existence if only one input has compareDocumentPosition
Calculate position if both inputs belong to the same document
Otherwise we know they are disconnected
Disconnected nodes
Choose the first element that is related to our preferred document
Maintain original order
Exit early if the nodes are identical
Parentless nodes are either documents or disconnected
"If the nodes are siblings, we can do a quick check"
Otherwise we need full lists of their ancestors for comparison
Walk down the tree looking for a discrepancy
Do a sibling check if the nodes have a common ancestor
Otherwise nodes in our document sort first
Set document vars if needed
IE 9's matchesSelector returns false on disconnected nodes
"As well, disconnected nodes are said to be in a document"
fragment in IE 9
Set document vars if needed
Set document vars if needed
Don't get fooled by Object.prototype properties (jQuery #13807)
"Unless we *know* we can detect duplicates, assume their presence"
Clear input after sorting to release objects
See https://github.com/jquery/sizzle/pull/225
"If no nodeType, this is expected to be an array"
Do not traverse comment nodes
Use textContent for elements
innerText usage removed for consistency of new lines (jQuery #11153)
Traverse its children
Do not include comment or processing instruction nodes
Can be adjusted by the user
Move the given value to match[3] whether quoted or unquoted
nth-* requires argument
numeric x and y parameters for Expr.filter.CHILD
remember that false/true cast respectively to 0/1
other types prohibit arguments
Accept quoted arguments as-is
Strip excess characters from unquoted arguments
Get excess from tokenize (recursively)
advance to the next closing parenthesis
excess is a negative index
Return only captures needed by the pseudo filter method (type and argument)
Shortcut for :nth-*(n)
:(first|last|only)-(child|of-type)
Reverse direction for :only-* (if we haven't yet done so)
non-xml :nth-child(...) stores cache data on `parent`
Seek `elem` from a previously-cached index
...in a gzip-friendly way
Support: IE <9 only
Defend against cloned attroperties (jQuery gh-1709)
Fallback to seeking `elem` from the start
"When found, cache indexes on `parent` and break"
Use previously-cached element index if available
...in a gzip-friendly way
Support: IE <9 only
Defend against cloned attroperties (jQuery gh-1709)
xml :nth-child(...)
or :nth-last-child(...) or :nth(-last)?-of-type(...)
Use the same loop as above to seek `elem` from the start
Cache the index of each encountered element
Support: IE <9 only
Defend against cloned attroperties (jQuery gh-1709)
"Incorporate the offset, then check against cycle size"
pseudo-class names are case-insensitive
http://www.w3.org/TR/selectors/#pseudo-classes
Prioritize by case sensitivity in case custom pseudos are added with uppercase letters
Remember that setFilters inherits from pseudos
The user may use createPseudo to indicate that
arguments are needed to create the filter function
just as Sizzle does
But maintain support for old signatures
Potentially complex pseudos
Trim the selector passed to compile
to avoid treating leading and trailing
spaces as combinators
Match elements unmatched by `matcher`
Don't keep the element (issue #299)
"""Whether an element is represented by a :lang() selector"
is based solely on the element's language value
"being equal to the identifier C,"
"or beginning with the identifier C immediately followed by ""-""."
The matching of C against the element's language value is performed case-insensitively.
"The identifier C does not have to be a valid language name."""
http://www.w3.org/TR/selectors/#lang-pseudo
lang value must be a valid identifier
Miscellaneous
Boolean properties
"In CSS3, :checked should return both checked and selected elements"
http://www.w3.org/TR/2011/REC-css3-selectors-20110929/#checked
Accessing this property makes selected-by-default
options in Safari work properly
Contents
http://www.w3.org/TR/selectors/#empty-pseudo
":empty is negated by element (1) or content nodes (text: 3; cdata: 4; entity ref: 5),"
but not by others (comment: 8; processing instruction: 7; etc.)
nodeType < 6 works because attributes (2) do not appear as children
Element/input types
Support: IE<8
"New HTML5 attribute values (e.g., ""search"") appear with elem.type === ""text"""
Position-in-collection
Add button/input type pseudos
Easy API for creating new setFilters
Comma and first run
Don't consume trailing commas as valid
Combinators
Cast descendant combinators to space
Filters
Return the length of the invalid excess
if we're just parsing
"Otherwise, throw an error or return tokens"
Cache the tokens
Check against closest ancestor/preceding element
Check against all ancestor/preceding elements
"We can't set arbitrary data on XML nodes, so they don't benefit from combinator caching"
Support: IE <9 only
Defend against cloned attroperties (jQuery gh-1709)
Assign to newCache so results back-propagate to previous elements
Reuse newcache so results back-propagate to previous elements
A match means we're done; a fail means we have to keep checking
Get initial elements from seed or context
"Prefilter to get matcher input, preserving a map for seed-results synchronization"
"If we have a postFinder, or filtered seed, or non-seed postFilter or preexisting results,"
...intermediate processing is necessary
...otherwise use results directly
Find primary matches
Apply postFilter
Un-match failing elements by moving them back to matcherIn
Get the final matcherOut by condensing this intermediate into postFinder contexts
Restore matcherIn since elem is not yet a final match
Move matched elements from seed to results to keep them synchronized
"Add elements to results, through postFinder if defined"
The foundational matcher ensures that elements are reachable from top-level context(s)
Avoid hanging onto element (issue #299)
Return special upon seeing a positional matcher
Find the next relative operator (if any) for proper handling
"If the preceding token was a descendant combinator, insert an implicit any-element `*`"
We must always have either seed elements or outermost context
Use integer dirruns iff this is the outermost matcher
Add elements passing elementMatchers directly to results
"Support: IE<9, Safari"
"Tolerate NodeList properties (IE: ""length""; Safari: <number>) matching elements by id"
Track unmatched elements for set filters
They will have gone through all possible matchers
"Lengthen the array for every element, matched or not"
"`i` is now the count of elements visited above, and adding it to `matchedCount`"
makes the latter nonnegative.
Apply set filters to unmatched elements
"NOTE: This can be skipped if there are no unmatched elements (i.e., `matchedCount`"
"equals `i`), unless we didn't visit _any_ elements in the above loop because we have"
no element matchers and no seed.
"Incrementing an initially-string ""0"" `i` allows `i` to remain a string only in that"
"case, which will result in a ""00"" `matchedCount` that differs from `i` but is also"
numerically zero.
Reintegrate element matches to eliminate the need for sorting
Discard index placeholder values to get only actual matches
Add matches to results
Seedless set matches succeeding multiple successful matchers stipulate sorting
Override manipulation of globals by nested matchers
Generate a function of recursive functions that can be used to check each element
Cache the compiled function
Save selector and tokenization
Try to minimize operations if there is only one selector in the list and no seed
(the latter of which guarantees us context)
Reduce context if the leading compound selector is an ID
"Precompiled matchers will still verify ancestry, so step up a level"
Fetch a seed set for right-to-left matching
Abort if we hit a combinator
"Search, expanding context for leading sibling combinators"
"If seed is empty or no tokens remain, we can return early"
Compile and execute a filtering function if one is not provided
Provide `match` to avoid retokenization if we modified the selector above
One-time assignments
Sort stability
Support: Chrome 14-35+
Always assume duplicates if they aren't passed to the comparison function
Initialize against the default document
Support: Webkit<537.32 - Safari 6.0.3/Chrome 25 (fixed in Chrome 27)
Detached nodes confoundingly follow *each other*
"Should return 1, but returns 4 (following)"
Support: IE<8
"Prevent attribute/property ""interpolation"""
https://msdn.microsoft.com/en-us/library/ms536429%28VS.85%29.aspx
Support: IE<9
"Use defaultValue in place of getAttribute(""value"")"
Support: IE<9
Use getAttributeNode to fetch booleans when getAttribute lies
Deprecated
Implement the identical functionality for filter and not
Single element
"Arraylike of elements (jQuery, arguments, Array)"
Filtered directly for both simple and complex selectors
"If this is a positional/relative selector, check membership in the returned set"
"so $(""p:first"").is(""p:last"") won't return true for a doc with two ""p""."
Initialize a jQuery object
A central reference to the root jQuery(document)
A simple way to check for HTML strings
Prioritize #id over <tag> to avoid XSS via location.hash (#9521)
Strict HTML recognition (#11290: must start with <)
Shortcut simple #id case for speed
"HANDLE: $(""""), $(null), $(undefined), $(false)"
Method init() accepts an alternate rootjQuery
so migrate can support jQuery.sub (gh-2101)
Handle HTML strings
Assume that strings that start and end with <> are HTML and skip the regex check
Match html or make sure no context is specified for #id
HANDLE: $(html) -> $(array)
Option to run scripts is true for back-compat
Intentionally let the error be thrown if parseHTML is not present
"HANDLE: $(html, props)"
Properties of context are called as methods if possible
...and otherwise set as attributes
HANDLE: $(#id)
Inject the element directly into the jQuery object
"HANDLE: $(expr, $(...))"
"HANDLE: $(expr, context)"
(which is just equivalent to: $(context).find(expr)
HANDLE: $(DOMElement)
HANDLE: $(function)
Shortcut for document ready
Execute immediately if ready is not present
Give the init function the jQuery prototype for later instantiation
Initialize central reference
Methods guaranteed to produce a unique set when starting from a unique set
"Positional selectors never match, since there's no _selection_ context"
Always skip document fragments
Don't pass non-elements to Sizzle
Determine the position of an element within the set
"No argument, return index in parent"
Index in selector
Locate the position of the desired element
"If it receives a jQuery object, the first element is used"
"Support: IE 9 - 11 only, iOS 7 only, Android Browser <=4.3 only"
Treat the template element as a regular one in browsers that
don't support it.
Remove duplicates
Reverse order for parents* and prev-derivatives
Convert String-formatted options into Object-formatted ones
Convert options from String-formatted to Object-formatted if needed
(we check in cache first)
Last fire value for non-forgettable lists
Flag to know if list was already fired
Flag to prevent firing
Actual callback list
Queue of execution data for repeatable lists
Index of currently firing callback (modified by add/remove as needed)
Fire callbacks
Enforce single-firing
"Execute callbacks for all pending executions,"
respecting firingIndex overrides and runtime changes
Run callback and check for early termination
Jump to end and forget the data so .add doesn't re-fire
Forget the data if we're done with it
Clean up if we're done firing for good
Keep an empty list if we have data for future add calls
"Otherwise, this object is spent"
Actual Callbacks object
Add a callback or a collection of callbacks to the list
"If we have memory from a past run, we should fire after adding"
Inspect recursively
Remove a callback from the list
Handle firing indexes
Check if a given callback is in the list.
"If no argument is given, return whether or not list has callbacks attached."
Remove all callbacks from the list
Disable .fire and .add
Abort any current/pending executions
Clear all callbacks and values
Disable .fire
Also disable .add unless we have memory (since it would have no effect)
Abort any pending executions
Call all callbacks with the given context and arguments
Call all the callbacks with the given arguments
To know if the callbacks have already been called at least once
Check for promise aspect first to privilege synchronous behavior
Other thenables
Other non-thenables
Control `resolve` arguments by letting Array#slice cast boolean `noValue` to integer:
* false: [ value ].slice( 0 ) => resolve( value )
* true: [ value ].slice( 1 ) => resolve()
"For Promises/A+, convert exceptions into rejections"
"Since jQuery.when doesn't unwrap thenables, we can skip the extra checks appearing in"
Deferred#then to conditionally suppress rejection.
Support: Android 4.0 only
Strict mode functions invoked without .call/.apply get global-object context
"action, add listener, callbacks,"
"... .then handlers, argument index, [final state]"
Keep pipe for back-compat
"Map tuples (progress, done, fail) to arguments (done, fail, progress)"
deferred.progress(function() { bind to newDefer or newDefer.notify })
deferred.done(function() { bind to newDefer or newDefer.resolve })
deferred.fail(function() { bind to newDefer or newDefer.reject })
Support: Promises/A+ section 2.3.3.3.3
https://promisesaplus.com/#point-59
Ignore double-resolution attempts
Support: Promises/A+ section 2.3.1
https://promisesaplus.com/#point-48
"Support: Promises/A+ sections 2.3.3.1, 3.5"
https://promisesaplus.com/#point-54
https://promisesaplus.com/#point-75
Retrieve `then` only once
Support: Promises/A+ section 2.3.4
https://promisesaplus.com/#point-64
Only check objects and functions for thenability
Handle a returned thenable
Special processors (notify) just wait for resolution
Normal processors (resolve) also hook into progress
...and disregard older resolution values
Handle all other returned values
Only substitute handlers pass on context
and multiple values (non-spec behavior)
Process the value(s)
Default process is resolve
Only normal processors (resolve) catch and reject exceptions
Support: Promises/A+ section 2.3.3.3.4.1
https://promisesaplus.com/#point-61
Ignore post-resolution exceptions
Only substitute handlers pass on context
and multiple values (non-spec behavior)
Support: Promises/A+ section 2.3.3.3.1
https://promisesaplus.com/#point-57
Re-resolve promises immediately to dodge false rejection from
subsequent errors
"Call an optional hook to record the stack, in case of exception"
since it's otherwise lost when execution goes async
progress_handlers.add( ... )
fulfilled_handlers.add( ... )
rejected_handlers.add( ... )
Get a promise for this deferred
"If obj is provided, the promise aspect is added to the object"
Add list-specific methods
promise.progress = list.add
promise.done = list.add
promise.fail = list.add
Handle state
"state = ""resolved"" (i.e., fulfilled)"
"state = ""rejected"""
rejected_callbacks.disable
fulfilled_callbacks.disable
rejected_handlers.disable
fulfilled_handlers.disable
progress_callbacks.lock
progress_handlers.lock
progress_handlers.fire
fulfilled_handlers.fire
rejected_handlers.fire
deferred.notify = function() { deferred.notifyWith(...) }
deferred.resolve = function() { deferred.resolveWith(...) }
deferred.reject = function() { deferred.rejectWith(...) }
deferred.notifyWith = list.fireWith
deferred.resolveWith = list.fireWith
deferred.rejectWith = list.fireWith
Make the deferred a promise
Call given func if any
All done!
Deferred helper
count of uncompleted subordinates
count of unprocessed arguments
subordinate fulfillment data
the master Deferred
subordinate callback factory
Single- and empty arguments are adopted like Promise.resolve
Use .then() to unwrap secondary thenables (cf. gh-3000)
Multiple arguments are aggregated like Promise.all array elements
"These usually indicate a programmer mistake during development,"
warn about them ASAP rather than swallowing them by default.
Support: IE 8 - 9 only
"Console exists when dev tools are open, which can happen at any time"
The deferred used on DOM ready
Wrap jQuery.readyException in a function so that the lookup
happens at the time of error handling instead of callback
registration.
Is the DOM ready to be used? Set to true once it occurs.
A counter to track how many items to wait for before
the ready event fires. See #6781
Handle when the DOM is ready
Abort if there are pending holds or we're already ready
Remember that the DOM is ready
"If a normal DOM Ready event fired, decrement, and wait if need be"
"If there are functions bound, to execute"
The ready event handler and self cleanup method
Catch cases where $(document).ready() is called
after the browser event has already occurred.
Support: IE <=9 - 10 only
"Older IE sometimes signals ""interactive"" too soon"
Handle it asynchronously to allow scripts the opportunity to delay ready
Use the handy event callback
"A fallback to window.onload, that will always work"
Multifunctional method to get and set values of a collection
The value/s can optionally be executed if it's a function
Sets many values
Sets one value
Bulk operations run against the entire set
...except when executing function values
Gets
Matches dashed string for camelizing
Used by camelCase as callback to replace()
Convert dashed to camelCase; used by the css and data modules
"Support: IE <=9 - 11, Edge 12 - 15"
Microsoft forgot to hump their vendor prefix (#9572)
Accepts only:
- Node
- Node.ELEMENT_NODE
- Node.DOCUMENT_NODE
- Object
- Any
Check if the owner object already has a cache
"If not, create one"
"We can accept data for non-element nodes in modern browsers,"
"but we should not, see #8335."
Always return an empty object.
If it is a node unlikely to be stringify-ed or looped over
use plain assignment
Otherwise secure it in a non-enumerable property
configurable must be true to allow the property to be
deleted when data is removed
"Handle: [ owner, key, value ] args"
Always use camelCase key (gh-2257)
"Handle: [ owner, { properties } ] args"
Copy the properties one-by-one to the cache object
Always use camelCase key (gh-2257)
In cases where either:
""
1. No key was specified
"2. A string key was specified, but no value provided"
""
"Take the ""read"" path and allow the get method to determine"
"which value to return, respectively either:"
""
1. The entire cache object
2. The data stored at the key
""
"When the key is not a string, or both a key and value"
"are specified, set or extend (existing objects) with either:"
""
1. An object of properties
2. A key and value
""
"Since the ""set"" path can have two possible entry points"
return the expected data based on which path was taken[*]
Support array or space separated string of keys
If key is an array of keys...
"We always set camelCase keys, so remove that."
"If a key with the spaces exists, use it."
"Otherwise, create an array by matching non-whitespace"
Remove the expando if there's no more data
Support: Chrome <=35 - 45
Webkit & Blink performance suffers when deleting properties
"from DOM nodes, so set to undefined instead"
https://bugs.chromium.org/p/chromium/issues/detail?id=378607 (bug restricted)
Implementation Summary
""
1. Enforce API surface and semantic compatibility with 1.9.x branch
2. Improve the module's maintainability by reducing the storage
paths to a single mechanism.
"3. Use the same single mechanism to support ""private"" and ""user"" data."
"4. _Never_ expose ""private"" data to user code (TODO: Drop _data, _removeData)"
5. Avoid exposing implementation details on user objects (eg. expando properties)
6. Provide a clear path for implementation upgrade to WeakMap in 2014
Only convert to a number if it doesn't change the string
"If nothing was found internally, try to fetch any"
data from the HTML5 data-* attribute
Make sure we set the data so it isn't changed later
TODO: Now that all calls to _data and _removeData have been replaced
"with direct calls to dataPriv methods, these can be deprecated."
Gets all values
Support: IE 11 only
The attrs elements can be null (#14894)
Sets multiple values
The calling jQuery object (element matches) is not empty
(and therefore has an element appears at this[ 0 ]) and the
`value` parameter was not undefined. An empty jQuery object
will result in `undefined` for elem = this[ 0 ] which will
throw an exception if an attempt to read a data cache is made.
Attempt to get data from the cache
The key will always be camelCased in Data
"Attempt to ""discover"" the data in"
HTML5 custom data-* attrs
"We tried really hard, but the data doesn't exist."
Set the data...
We always store the camelCased key
Speed up dequeue by getting out quickly if this is just a lookup
"If the fx queue is dequeued, always remove the progress sentinel"
Add a progress sentinel to prevent the fx queue from being
automatically dequeued
Clear up the last queue stop function
"Not public - generate a queueHooks object, or return the current one"
Ensure a hooks for this queue
Get a promise resolved when queues of a certain type
are emptied (fx is the type by default)
Check attachment across shadow DOM boundaries when possible (gh-3504)
isHiddenWithinTree might be called from jQuery#filter function;
"in that case, element will be second argument"
Inline style trumps all
"Otherwise, check computed style"
Support: Firefox <=43 - 45
"Disconnected elements can have computed display: none, so first confirm that elem is"
in the document.
"Remember the old values, and insert the new ones"
Revert the old values
Starting value computation is required for potential unit mismatches
Support: Firefox <=54
Halve the iteration target value to prevent interference from CSS upper bounds (gh-2144)
Trust units reported by jQuery.css
Iteratively approximate from a nonzero starting point
Evaluate and update our best guess (doubling guesses that zero out).
Finish if the scale equals or crosses 1 (making the old*new product non-positive).
Make sure we update the tween properties later on
Apply relative offset (+=/-=) if specified
Determine new display value for elements that need to change
"Since we force visibility upon cascade-hidden elements, an immediate (and slow)"
check is required in this first loop unless we have a nonempty display value (either
inline or about-to-be-restored)
Remember what we're overwriting
Set the display of the elements in a second loop to avoid constant reflow
We have to close these tags to support XHTML (#13200)
Support: IE <=9 only
XHTML parsers do not magically insert elements in the
same way that tag soup parsers do. So we cannot shorten
this by omitting <tbody> or other required elements.
Support: IE <=9 only
Support: IE <=9 - 11 only
Use typeof to avoid zero-argument method invocation on host objects (#15151)
Mark scripts as having already been evaluated
Add nodes directly
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
Convert non-html into a text node
Convert html into DOM nodes
Deserialize a standard representation
Descend through wrappers to the right content
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
Remember the top-level container
Ensure the created nodes are orphaned (#12392)
Remove wrapper from fragment
Skip elements already in the context collection (trac-4087)
Append to fragment
Preserve script evaluation history
Capture executables
Support: Android 4.0 - 4.3 only
Check state lost if the name is set (#11217)
Support: Windows Web Apps (WWA)
`name` and `type` must use .setAttribute for WWA (#14901)
Support: Android <=4.1 only
Older WebKit doesn't clone checked state correctly in fragments
Support: IE <=11 only
Make sure textarea (and checkbox) defaultValue is properly cloned
Support: IE <=9 - 11+
"focus() and blur() are asynchronous, except when they are no-op."
"So expect focus to be synchronous when the element is already active,"
and blur to be synchronous when the element is not already active.
"(focus and blur are always synchronous in other supported browsers,"
this just defines when we can count on it).
Support: IE <=9 only
Accessing document.activeElement can throw unexpectedly
https://bugs.jquery.com/ticket/13393
Types can be a map of types/handlers
"( types-Object, selector, data )"
"( types-Object, data )"
"( types, fn )"
"( types, selector, fn )"
"( types, data, fn )"
"Can use an empty set, since event contains the info"
Use same guid so caller can remove using origFn
Don't attach events to noData or text/comment nodes (but allow plain objects)
Caller can pass in an object of custom data in lieu of the handler
Ensure that invalid selectors throw exceptions at attach time
"Evaluate against documentElement in case elem is a non-element node (e.g., document)"
"Make sure that the handler has a unique ID, used to find/remove it later"
"Init the element's event structure and main handler, if this is the first"
Discard the second event of a jQuery.event.trigger() and
when an event is called after a page has unloaded
Handle multiple events separated by a space
"There *must* be a type, no attaching namespace-only handlers"
"If event changes its type, use the special event handlers for the changed type"
"If selector defined, determine special event api type, otherwise given type"
Update special based on newly reset type
handleObj is passed to all event handlers
Init the event handler queue if we're the first
Only use addEventListener if the special events handler returns false
"Add to the element's handler list, delegates in front"
"Keep track of which events have ever been used, for event optimization"
Detach an event or set of events from an element
Once for each type.namespace in types; type may be omitted
"Unbind all events (on this namespace, if provided) for the element"
Remove matching events
Remove generic event handler if we removed something and no more handlers exist
(avoids potential for endless recursion during removal of special event handlers)
Remove data and the expando if it's no longer used
Make a writable jQuery.Event from the native event object
Use the fix-ed jQuery.Event rather than the (read-only) native event
"Call the preDispatch hook for the mapped type, and let it bail if desired"
Determine handlers
Run delegates first; they may want to stop propagation beneath us
"If the event is namespaced, then each handler is only invoked if it is"
specially universal or its namespaces are a superset of the event's.
Call the postDispatch hook for the mapped type
Find delegate handlers
Support: IE <=9
Black-hole SVG <use> instance trees (trac-13180)
Support: Firefox <=42
Suppress spec-violating clicks indicating a non-primary pointer button (trac-3861)
https://www.w3.org/TR/DOM-Level-3-Events/#event-type-click
Support: IE 11 only
"...but not arrow key ""clicks"" of radio inputs, which can have `button` -1 (gh-2343)"
Don't check non-elements (#13208)
"Don't process clicks on disabled elements (#6911, #8165, #11382, #11764)"
Don't conflict with Object.prototype properties (#13203)
Add the remaining (directly-bound) handlers
Prevent triggered image.load events from bubbling to window.load
Utilize native event to ensure correct state for checkable inputs
"For mutual compressibility with _default, replace `this` access with a local var."
`|| data` is dead code meant only to preserve the variable through minification.
Claim the first handler
"dataPriv.set( el, ""click"", ... )"
Return false to allow normal processing in the caller
"For mutual compressibility with _default, replace `this` access with a local var."
`|| data` is dead code meant only to preserve the variable through minification.
Force setup before triggering a click
Return non-false to allow normal event-path propagation
"For cross-browser consistency, suppress native .click() on links"
Also prevent it if we're currently inside a leveraged native-event stack
Support: Firefox 20+
Firefox doesn't alert if the returnValue field is not set.
Ensure the presence of an event listener that handles manually-triggered
synthetic events by interrupting progress until reinvoked in response to
"*native* events that it fires directly, ensuring that state changes have"
already occurred before other listeners are invoked.
"Missing expectSync indicates a trigger call, which must force setup through jQuery.event.add"
Register the controller as a special universal handler for all event namespaces
Interrupt processing of the outer synthetic .trigger()ed event
Store arguments for use when handling the inner native event
Trigger the native event and capture its result
Support: IE <=9 - 11+
focus() and blur() are asynchronous
Cancel the outer synthetic event
If this is an inner synthetic event for an event with a bubbling surrogate
"(focus or blur), assume that the surrogate already propagated from triggering the"
native event and prevent that from happening again here.
This technically gets the ordering wrong w.r.t. to `.trigger()` (in which the
"bubbling surrogate propagates *after* the non-bubbling base), but that seems"
less bad than duplication.
"If this is a native event triggered above, everything is now in order"
Fire an inner synthetic event with the original arguments
...and capture the result
Support: IE <=9 - 11+
Extend with the prototype to reset the above stopImmediatePropagation()
Abort handling of the native event
"This ""if"" is needed for plain objects"
Allow instantiation without the 'new' keyword
Event object
Events bubbling up the document may have been marked as prevented
by a handler lower down the tree; reflect the correct value.
Support: Android <=2.3 only
Create target properties
Support: Safari <=6 - 7 only
"Target should not be a text node (#504, #13143)"
Event type
Put explicitly provided properties onto the event object
Create a timestamp if incoming event doesn't have one
Mark it as fixed
jQuery.Event is based on DOM3 Events as specified by the ECMAScript Language Binding
https://www.w3.org/TR/2003/WD-DOM-Level-3-Events-20030331/ecma-script-binding.html
Includes all common event props including KeyEvent and MouseEvent specific props
Add which for key events
Add which for click: 1 === left; 2 === middle; 3 === right
Utilize native event if possible so blur/focus sequence is correct
Claim the first handler
"dataPriv.set( this, ""focus"", ... )"
"dataPriv.set( this, ""blur"", ... )"
Return false to allow normal processing in the caller
Force setup before trigger
Return non-false to allow normal event-path propagation
Create mouseenter/leave events using mouseover/out and event-time checks
so that event delegation works in jQuery.
Do the same for pointerenter/pointerleave and pointerover/pointerout
""
Support: Safari 7 only
Safari sends mouseenter too often; see:
https://bugs.chromium.org/p/chromium/issues/detail?id=470258
for the description of the bug (it existed in older Chrome versions as well).
For mouseenter/leave call the handler if related is outside the target.
NB: No relatedTarget if the mouse left/entered the browser window
( event )  dispatched jQuery.Event
"( types-object [, selector] )"
"( types [, fn] )"
See https://github.com/eslint/eslint/issues/3229
"Support: IE <=10 - 11, Edge 12 - 13 only"
In IE/Edge using regex groups here causes severe slowdowns.
See https://connect.microsoft.com/IE/feedback/details/1736512/
"checked=""checked"" or checked"
Prefer a tbody over its parent table for containing new rows
Replace/restore the type attribute of script elements for safe DOM manipulation
"1. Copy private data: events, handlers, etc."
2. Copy user data
"Fix IE bugs, see support tests"
Fails to persist the checked state of a cloned checkbox or radio button.
Fails to return the selected option to the default selected state when cloning options
Flatten any nested arrays
"We can't cloneNode fragments that contain checked, in WebKit"
Require either new content or an interest in ignored elements to invoke the callback
Use the original fragment for the last item
instead of the first because it can end up
being emptied incorrectly in certain situations (#8070).
Keep references to cloned scripts for later restoration
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
Reenable scripts
Evaluate executable scripts on first document insertion
"Optional AJAX dependency, but won't run scripts if not present"
Fix IE cloning issues
We eschew Sizzle here for performance reasons: https://jsperf.com/getall-vs-sizzle/2
Copy the events from the original to the clone
Preserve script evaluation history
Return the cloned set
This is a shortcut to avoid jQuery.event.remove's overhead
Support: Chrome <=35 - 45+
"Assign undefined instead of using delete, see Data#remove"
Support: Chrome <=35 - 45+
"Assign undefined instead of using delete, see Data#remove"
Prevent memory leaks
Remove any remaining nodes
See if we can take a shortcut and just use innerHTML
Remove element nodes and prevent memory leaks
"If using innerHTML throws an exception, use the fallback method"
"Make the changes, replacing each non-ignored context element with the new content"
Force callback invocation
"Support: Android <=4.0 only, PhantomJS 1 only"
".get() because push.apply(_, arraylike) throws on ancient WebKit"
"Support: IE <=11 only, Firefox <=30 (#15098, #14150)"
IE throws on elements created in popups
"FF meanwhile throws on frame elements through ""defaultView.getComputedStyle"""
Executing both pixelPosition & boxSizingReliable tests require only one layout
so they're executed at the same time to save the second computation.
"This is a singleton, we need to execute it only once"
"Support: Android 4.0 - 4.3 only, Firefox <=3 - 44"
"Support: Android 4.0 - 4.3 only, Safari <=9.1 - 10.1, iOS <=7.0 - 9.3"
"Some styles come back with percentage values, even though they shouldn't"
Support: IE 9 - 11 only
Detect misreporting of content dimensions for box-sizing:border-box elements
Support: IE 9 only
Detect overflow:scroll screwiness (gh-3699)
Support: Chrome <=64
Don't get tricked when zoom affects offsetWidth (gh-4029)
Nullify the div so it wouldn't be stored in the memory and
it will also be a sign that checks already performed
Finish early in limited (non-browser) environments
Support: IE <=9 - 11 only
Style of cloned element affects source element cloned (#8908)
Support: Firefox 51+
Retrieving style before computed somehow
fixes an issue with getting wrong values
on detached elements
getPropertyValue is needed for:
".css('filter') (IE 9 only, #12537)"
.css('--customProperty) (#3144)
"A tribute to the ""awesome hack by Dean Edwards"""
"Android Browser returns percentage for some values,"
but width seems to be reliably pixels.
This is against the CSSOM draft spec:
https://drafts.csswg.org/cssom/#resolved-values
Remember the original values
Put in the new values to get a computed value out
Revert the changed values
Support: IE <=9 - 11 only
IE returns zIndex value as an integer.
"Define the hook, we'll check on the first run if it's really needed."
Hook not needed (or it's not possible to use it due
"to missing dependency), remove it."
Hook needed; redefine it so that the support test is not executed again.
Return a vendor-prefixed property or undefined
Check for vendor prefixed names
Return a potentially-mapped jQuery.cssProps or vendor prefixed property
Swappable if display is none or starts with table
"except ""table"", ""table-cell"", or ""table-caption"""
See here for display values: https://developer.mozilla.org/en-US/docs/CSS/display
Any relative (+/-) values have already been
normalized at this point
"Guard against undefined ""subtract"", e.g., when used as in cssHooks"
Adjustment may not be necessary
Both box models exclude margin
"If we get here with a content-box, we're seeking ""padding"" or ""border"" or ""margin"""
Add padding
"For ""border"" or ""margin"", add border"
But still keep track of it otherwise
"If we get here with a border-box (content + padding + border), we're seeking ""content"" or"
"""padding"" or ""margin"""
"For ""content"", subtract padding"
"For ""content"" or ""padding"", subtract border"
Account for positive content-box scroll gutter when requested by providing computedVal
"offsetWidth/offsetHeight is a rounded sum of content, padding, scroll gutter, and border"
"Assuming integer scroll gutter, subtract the rest and round down"
"If offsetWidth/offsetHeight is unknown, then we can't determine content-box scroll gutter"
Use an explicit zero to avoid NaN (gh-3964)
Start with computed style
"To avoid forcing a reflow, only fetch boxSizing if we need it (gh-4322)."
Fake content-box until we know it's needed to know the true value.
Support: Firefox <=54
"Return a confounding non-pixel value or feign ignorance, as appropriate."
"Fall back to offsetWidth/offsetHeight when value is ""auto"""
This happens for inline elements with no explicit setting (gh-3571)
Support: Android <=4.1 - 4.3 only
Also use offsetWidth/offsetHeight for misreported inline dimensions (gh-3602)
Support: IE 9-11 only
Also use offsetWidth/offsetHeight for when box sizing is unreliable
We use getClientRects() to check for hidden/disconnected.
"In those cases, the computed value can be trusted to be border-box"
"Where available, offsetWidth/offsetHeight approximate border box dimensions."
"Where not available (e.g., SVG), assume unreliable box-sizing and interpret the"
retrieved value as a content box dimension.
"Normalize """" and auto"
Adjust for the element's box model
Provide the current computed size to request scroll gutter calculation (gh-3589)
Add in style property hooks for overriding the default
behavior of getting and setting a style property
We should always get a number back from opacity
"Don't automatically add ""px"" to these possibly-unitless properties"
Add in properties whose names you wish to fix before
setting or getting the value
Get and set the style property on a DOM Node
Don't set styles on text and comment nodes
Make sure that we're working with the right name
Make sure that we're working with the right name. We don't
want to query the value if it is a CSS custom property
since they are user-defined.
"Gets hook for the prefixed version, then unprefixed version"
Check if we're setting a value
"Convert ""+="" or ""-="" to relative numbers (#7345)"
Fixes bug #9237
Make sure that null and NaN values aren't set (#7116)
"If a number was passed in, add the unit (except for certain CSS properties)"
The isCustomProp check can be removed in jQuery 4.0 when we only auto-append
"""px"" to a few hardcoded values."
background-* props affect original clone's values
"If a hook was provided, use that value, otherwise just set the specified value"
If a hook was provided get the non-computed value from there
Otherwise just get the value from the style object
Make sure that we're working with the right name. We don't
want to modify the value if it is a CSS custom property
since they are user-defined.
Try prefixed name followed by the unprefixed name
If a hook was provided get the computed value from there
"Otherwise, if a way to get the computed value exists, use that"
"Convert ""normal"" to computed value"
Make numeric if forced or a qualifier was provided and val looks numeric
Certain elements can have dimension info if we invisibly show them
but it must have a current display style that would benefit
Support: Safari 8+
Table columns in Safari have non-zero offsetWidth & zero
getBoundingClientRect().width unless display is changed.
Support: IE <=11 only
Running getBoundingClientRect on a disconnected node
in IE throws an error.
Only read styles.position if the test has a chance to fail
to avoid forcing a reflow.
"To avoid forcing a reflow, only fetch boxSizing if we need it (gh-3991)"
Account for unreliable border-box dimensions by comparing offset* to computed and
faking a content-box to get border and padding (gh-3699)
Convert to pixels if value adjustment is needed
These hooks are used by animate to expand properties
Assumes a single number if not a string
"Based off of the plugin by Clint Helfers, with permission."
https://web.archive.org/web/20100324014747/http://blindsignals.com/index.php/2009/07/jquery-delay/
Support: Android <=4.3 only
"Default value for a checkbox should be ""on"""
Support: IE <=11 only
Must access selectedIndex to make default options select
Support: IE <=11 only
An input loses its value after becoming a radio
"Don't get/set attributes on text, comment and attribute nodes"
Fallback to prop when attributes are not supported
Attribute hooks are determined by the lowercase version
Grab necessary hook if one is defined
"Non-existent attributes return null, we normalize to undefined"
Attribute names can contain non-HTML whitespace characters
https://html.spec.whatwg.org/multipage/syntax.html#attributes-2
Hooks for boolean attributes
Remove boolean attributes when set to false
Avoid an infinite loop by temporarily removing this function from the getter
"Don't get/set properties on text, comment and attribute nodes"
Fix name and attach hooks
Support: IE <=9 - 11 only
elem.tabIndex doesn't always return the
correct value when it hasn't been explicitly set
https://web.archive.org/web/20141116233347/http://fluidproject.org/blog/2008/01/09/getting-setting-and-removing-tabindex-values-with-javascript/
Use proper attribute retrieval(#12072)
Support: IE <=11 only
Accessing the selectedIndex property
forces the browser to respect setting selected
on the option
The getter ensures a default option is selected
when in an optgroup
"eslint rule ""no-unused-expressions"" is disabled for this code"
since it considers such accessions noop
Strip and collapse whitespace according to HTML spec
https://infra.spec.whatwg.org/#strip-and-collapse-ascii-whitespace
Only assign if different to avoid unneeded rendering.
This expression is here for better compressibility (see addClass)
Remove *all* instances
Only assign if different to avoid unneeded rendering.
Toggle individual class names
"Check each className given, space separated list"
Toggle whole class name
Store className if set
"If the element has a class name or if we're passed `false`,"
"then remove the whole classname (if there was one, the above saved it)."
"Otherwise bring back whatever was previously saved (if anything),"
falling back to the empty string if nothing was stored.
Handle most common string cases
Handle cases where value is null/undef or number
"Treat null/undefined as """"; convert numbers to string"
"If set returns undefined, fall back to normal setting"
Support: IE <=10 - 11 only
"option.text throws exceptions (#14686, #14858)"
Strip and collapse whitespace
https://html.spec.whatwg.org/#strip-and-collapse-whitespace
Loop through all the selected options
Support: IE <=9 only
IE8-9 doesn't update selected after form reset (#2551)
Don't return options that are disabled or in a disabled optgroup
Get the specific value for the option
We don't need an array for one selects
Multi-Selects return an array
Force browsers to behave consistently when non-matching value is set
Radios and checkboxes getter/setter
Return jQuery for attributes-only inclusion
Don't do events on text and comment nodes
focus/blur morphs to focusin/out; ensure we're not firing them right now
Namespaced trigger; create a regexp to match event type in handle()
"Caller can pass in a jQuery.Event object, Object, or just an event type string"
Trigger bitmask: & 1 for native handlers; & 2 for jQuery (always true)
Clean up the event in case it is being reused
"Clone any incoming data and prepend the event, creating the handler arg list"
Allow special events to draw outside the lines
"Determine event propagation path in advance, per W3C events spec (#9951)"
"Bubble up to document, then to window; watch for a global ownerDocument var (#9724)"
"Only add window if we got to document (e.g., not plain obj or detached DOM)"
Fire handlers on the event path
jQuery handler
Native handler
"If nobody prevented the default action, do it now"
Call a native DOM method on the target with the same name as the event.
"Don't do default actions on window, that's where global variables be (#6170)"
Don't re-trigger an onFOO event when we call its FOO() method
"Prevent re-triggering of the same event, since we already bubbled it above"
Piggyback on a donor event to simulate a different one
Used only for `focus(in | out)` events
Support: Firefox <=44
Firefox doesn't have focus(in | out) events
Related ticket - https://bugzilla.mozilla.org/show_bug.cgi?id=687787
""
"Support: Chrome <=48 - 49, Safari <=9.0 - 9.1"
"focus(in | out) events fire after focus & blur events,"
which is spec violation - http://www.w3.org/TR/DOM-Level-3-Events/#events-focusevent-event-order
Related ticket - https://bugs.chromium.org/p/chromium/issues/detail?id=449857
Attach a single capturing handler on the document while someone wants focusin/focusout
Serialize array item.
Treat each array item as a scalar.
"Item is non-scalar (array or object), encode its numeric index."
Serialize object item.
Serialize scalar item.
Serialize an array of form elements or a set of
key/values into a query string
"If value is a function, invoke it and use its return value"
"If an array was passed in, assume that it is an array of form elements."
Serialize the form elements
"If traditional, encode the ""old"" way (the way 1.3.2 or older"
"did it), otherwise encode params recursively."
Return the resulting serialization
"Can add propHook for ""elements"" to filter or add form elements"
"Use .is( "":disabled"" ) so that fieldset[disabled] works"
The elements to wrap the target around
Support: Safari 8 only
In Safari 8 documents created via document.implementation.createHTMLDocument
collapse sibling forms: the second one becomes a child of the first one.
"Because of that, this security measure has to be disabled in Safari 8."
https://bugs.webkit.org/show_bug.cgi?id=137337
"Argument ""data"" should be string of html"
"context (optional): If specified, the fragment will be created in this context,"
defaults to document
"keepScripts (optional): If true, will include scripts passed in the html string"
Stop scripts or inline event handlers from being executed immediately
by using document.implementation
Set the base href for the created document
so any parsed elements with URLs
are based on the document's URL (gh-2965)
Single tag
"Set position first, in-case top/left are set even on static elem"
Need to be able to calculate position if either
top or left is auto and position is either absolute or fixed
Use jQuery.extend here to allow modification of coordinates argument (gh-1848)
offset() relates an element's border box to the document origin
Preserve chaining for setter
Return zeros for disconnected and hidden (display: none) elements (gh-2310)
Support: IE <=11 only
Running getBoundingClientRect on a
disconnected node in IE throws an error
Get document-relative position by adding viewport scroll to viewport-relative gBCR
position() relates an element's margin box to its offset parent's padding box
This corresponds to the behavior of CSS absolute positioning
"position:fixed elements are offset from the viewport, which itself always has zero offset"
Assume position:fixed implies availability of getBoundingClientRect
"Account for the *real* offset parent, which can be the document or its root element"
when a statically positioned element is identified
"Incorporate borders into its offset, since they are outside its content origin"
Subtract parent offsets and element margins
This method will return documentElement in the following cases:
"1) For the element inside the iframe without offsetParent, this method will return"
documentElement of the parent window
2) For the hidden or detached element
"3) For body or html element, i.e. in case of the html node - it will return itself"
""
but those exceptions were never presented as a real life use-cases
and might be considered as more preferable results.
""
"This logic, however, is not guaranteed and can change at any point in the future"
Create scrollLeft and scrollTop methods
Coalesce documents and windows
"Support: Safari <=7 - 9.1, Chrome <=37 - 49"
Add the top/left cssHooks using jQuery.fn.position
Webkit bug: https://bugs.webkit.org/show_bug.cgi?id=29084
Blink bug: https://bugs.chromium.org/p/chromium/issues/detail?id=589347
getComputedStyle returns percent when specified for top/left/bottom/right;
"rather than make the css module depend on the offset module, just check for it here"
"If curCSS returns percentage, fallback to offset"
"Create innerHeight, innerWidth, height, width, outerHeight and outerWidth methods"
"Margin is only for outerHeight, outerWidth"
$( window ).outerWidth/Height return w/h including scrollbars (gh-1729)
Get document width or height
"Either scroll[Width/Height] or offset[Width/Height] or client[Width/Height],"
whichever is greatest
"Get width or height on the element, requesting but not forcing parseFloat"
Set width or height on the element
Handle event binding
"( namespace ) or ( selector, types [, fn] )"
"Bind a function to a context, optionally partially applying any"
arguments.
jQuery.proxy is deprecated to promote standards (specifically Function#bind)
"However, it is not slated for removal any time soon"
"Quick check to determine if target is callable, in the spec"
"this throws a TypeError, but we will just return undefined."
Simulated bind
"Set the guid of unique handler to the same of original handler, so it can be removed"
"As of jQuery 3.0, isNumeric is limited to"
strings and numbers (primitives or objects)
that can be coerced to finite numbers (gh-2662)
"parseFloat NaNs numeric-cast false positives ("""")"
"...but misinterprets leading-number strings, particularly hex literals (""0x..."")"
subtraction forces infinities to NaN
"Register as a named AMD module, since jQuery can be concatenated with other"
"files that may use define, but not via a proper concatenation script that"
understands anonymous AMD modules. A named AMD is safest and most robust
way to register. Lowercase jquery is used because AMD module names are
"derived from file names, and jQuery is normally delivered in a lowercase"
file name. Do this after creating the global so that if an AMD module wants
"to call noConflict to hide this version of jQuery, it will work."
"Note that for maximum portability, libraries that are not jQuery should"
"declare themselves as anonymous modules, and avoid setting a global if an"
"AMD loader is present. jQuery is a special case. For more information, see"
https://github.com/jrburke/requirejs/wiki/Updating-existing-libraries#wiki-anon
Map over jQuery in case of overwrite
Map over the $ in case of overwrite
"Expose jQuery and $ identifiers, even in AMD"
"(#7102#comment:10, https://github.com/jquery/jquery/pull/557)"
and CommonJS for browser emulators (#13566)
Implementation Summary
""
1. Enforce API surface and semantic compatibility with 1.9.x branch
2. Improve the module's maintainability by reducing the storage
paths to a single mechanism.
"3. Use the same single mechanism to support ""private"" and ""user"" data."
"4. _Never_ expose ""private"" data to user code (TODO: Drop _data, _removeData)"
5. Avoid exposing implementation details on user objects (eg. expando properties)
6. Provide a clear path for implementation upgrade to WeakMap in 2014
Only convert to a number if it doesn't change the string
"If nothing was found internally, try to fetch any"
data from the HTML5 data-* attribute
Make sure we set the data so it isn't changed later
TODO: Now that all calls to _data and _removeData have been replaced
"with direct calls to dataPriv methods, these can be deprecated."
Gets all values
Support: IE 11 only
The attrs elements can be null (#14894)
Sets multiple values
The calling jQuery object (element matches) is not empty
(and therefore has an element appears at this[ 0 ]) and the
`value` parameter was not undefined. An empty jQuery object
will result in `undefined` for elem = this[ 0 ] which will
throw an exception if an attempt to read a data cache is made.
Attempt to get data from the cache
The key will always be camelCased in Data
"Attempt to ""discover"" the data in"
HTML5 custom data-* attrs
"We tried really hard, but the data doesn't exist."
Set the data...
We always store the camelCased key
Convert String-formatted options into Object-formatted ones
Convert options from String-formatted to Object-formatted if needed
(we check in cache first)
Last fire value for non-forgettable lists
Flag to know if list was already fired
Flag to prevent firing
Actual callback list
Queue of execution data for repeatable lists
Index of currently firing callback (modified by add/remove as needed)
Fire callbacks
Enforce single-firing
"Execute callbacks for all pending executions,"
respecting firingIndex overrides and runtime changes
Run callback and check for early termination
Jump to end and forget the data so .add doesn't re-fire
Forget the data if we're done with it
Clean up if we're done firing for good
Keep an empty list if we have data for future add calls
"Otherwise, this object is spent"
Actual Callbacks object
Add a callback or a collection of callbacks to the list
"If we have memory from a past run, we should fire after adding"
Inspect recursively
Remove a callback from the list
Handle firing indexes
Check if a given callback is in the list.
"If no argument is given, return whether or not list has callbacks attached."
Remove all callbacks from the list
Disable .fire and .add
Abort any current/pending executions
Clear all callbacks and values
Disable .fire
Also disable .add unless we have memory (since it would have no effect)
Abort any pending executions
Call all callbacks with the given context and arguments
Call all the callbacks with the given arguments
To know if the callbacks have already been called at least once
Support: IE <=9 - 11+
"focus() and blur() are asynchronous, except when they are no-op."
"So expect focus to be synchronous when the element is already active,"
and blur to be synchronous when the element is not already active.
"(focus and blur are always synchronous in other supported browsers,"
this just defines when we can count on it).
Support: IE <=9 only
Accessing document.activeElement can throw unexpectedly
https://bugs.jquery.com/ticket/13393
Types can be a map of types/handlers
"( types-Object, selector, data )"
"( types-Object, data )"
"( types, fn )"
"( types, selector, fn )"
"( types, data, fn )"
"Can use an empty set, since event contains the info"
Use same guid so caller can remove using origFn
Don't attach events to noData or text/comment nodes (but allow plain objects)
Caller can pass in an object of custom data in lieu of the handler
Ensure that invalid selectors throw exceptions at attach time
"Evaluate against documentElement in case elem is a non-element node (e.g., document)"
"Make sure that the handler has a unique ID, used to find/remove it later"
"Init the element's event structure and main handler, if this is the first"
Discard the second event of a jQuery.event.trigger() and
when an event is called after a page has unloaded
Handle multiple events separated by a space
"There *must* be a type, no attaching namespace-only handlers"
"If event changes its type, use the special event handlers for the changed type"
"If selector defined, determine special event api type, otherwise given type"
Update special based on newly reset type
handleObj is passed to all event handlers
Init the event handler queue if we're the first
Only use addEventListener if the special events handler returns false
"Add to the element's handler list, delegates in front"
"Keep track of which events have ever been used, for event optimization"
Detach an event or set of events from an element
Once for each type.namespace in types; type may be omitted
"Unbind all events (on this namespace, if provided) for the element"
Remove matching events
Remove generic event handler if we removed something and no more handlers exist
(avoids potential for endless recursion during removal of special event handlers)
Remove data and the expando if it's no longer used
Make a writable jQuery.Event from the native event object
Use the fix-ed jQuery.Event rather than the (read-only) native event
"Call the preDispatch hook for the mapped type, and let it bail if desired"
Determine handlers
Run delegates first; they may want to stop propagation beneath us
"If the event is namespaced, then each handler is only invoked if it is"
specially universal or its namespaces are a superset of the event's.
Call the postDispatch hook for the mapped type
Find delegate handlers
Support: IE <=9
Black-hole SVG <use> instance trees (trac-13180)
Support: Firefox <=42
Suppress spec-violating clicks indicating a non-primary pointer button (trac-3861)
https://www.w3.org/TR/DOM-Level-3-Events/#event-type-click
Support: IE 11 only
"...but not arrow key ""clicks"" of radio inputs, which can have `button` -1 (gh-2343)"
Don't check non-elements (#13208)
"Don't process clicks on disabled elements (#6911, #8165, #11382, #11764)"
Don't conflict with Object.prototype properties (#13203)
Add the remaining (directly-bound) handlers
Prevent triggered image.load events from bubbling to window.load
Utilize native event to ensure correct state for checkable inputs
"For mutual compressibility with _default, replace `this` access with a local var."
`|| data` is dead code meant only to preserve the variable through minification.
Claim the first handler
"dataPriv.set( el, ""click"", ... )"
Return false to allow normal processing in the caller
"For mutual compressibility with _default, replace `this` access with a local var."
`|| data` is dead code meant only to preserve the variable through minification.
Force setup before triggering a click
Return non-false to allow normal event-path propagation
"For cross-browser consistency, suppress native .click() on links"
Also prevent it if we're currently inside a leveraged native-event stack
Support: Firefox 20+
Firefox doesn't alert if the returnValue field is not set.
Ensure the presence of an event listener that handles manually-triggered
synthetic events by interrupting progress until reinvoked in response to
"*native* events that it fires directly, ensuring that state changes have"
already occurred before other listeners are invoked.
"Missing expectSync indicates a trigger call, which must force setup through jQuery.event.add"
Register the controller as a special universal handler for all event namespaces
Interrupt processing of the outer synthetic .trigger()ed event
Store arguments for use when handling the inner native event
Trigger the native event and capture its result
Support: IE <=9 - 11+
focus() and blur() are asynchronous
Cancel the outer synthetic event
If this is an inner synthetic event for an event with a bubbling surrogate
"(focus or blur), assume that the surrogate already propagated from triggering the"
native event and prevent that from happening again here.
This technically gets the ordering wrong w.r.t. to `.trigger()` (in which the
"bubbling surrogate propagates *after* the non-bubbling base), but that seems"
less bad than duplication.
"If this is a native event triggered above, everything is now in order"
Fire an inner synthetic event with the original arguments
...and capture the result
Support: IE <=9 - 11+
Extend with the prototype to reset the above stopImmediatePropagation()
Abort handling of the native event
"This ""if"" is needed for plain objects"
Allow instantiation without the 'new' keyword
Event object
Events bubbling up the document may have been marked as prevented
by a handler lower down the tree; reflect the correct value.
Support: Android <=2.3 only
Create target properties
Support: Safari <=6 - 7 only
"Target should not be a text node (#504, #13143)"
Event type
Put explicitly provided properties onto the event object
Create a timestamp if incoming event doesn't have one
Mark it as fixed
jQuery.Event is based on DOM3 Events as specified by the ECMAScript Language Binding
https://www.w3.org/TR/2003/WD-DOM-Level-3-Events-20030331/ecma-script-binding.html
Includes all common event props including KeyEvent and MouseEvent specific props
Add which for key events
Add which for click: 1 === left; 2 === middle; 3 === right
Utilize native event if possible so blur/focus sequence is correct
Claim the first handler
"dataPriv.set( this, ""focus"", ... )"
"dataPriv.set( this, ""blur"", ... )"
Return false to allow normal processing in the caller
Force setup before trigger
Return non-false to allow normal event-path propagation
Create mouseenter/leave events using mouseover/out and event-time checks
so that event delegation works in jQuery.
Do the same for pointerenter/pointerleave and pointerover/pointerout
""
Support: Safari 7 only
Safari sends mouseenter too often; see:
https://bugs.chromium.org/p/chromium/issues/detail?id=470258
for the description of the bug (it existed in older Chrome versions as well).
For mouseenter/leave call the handler if related is outside the target.
NB: No relatedTarget if the mouse left/entered the browser window
( event )  dispatched jQuery.Event
"( types-object [, selector] )"
"( types [, fn] )"
Defining this global in .eslintrc.json would create a danger of using the global
"unguarded in another place, it seems safer to define global only for this module"
Define a local copy of jQuery
The jQuery object is actually just the init constructor 'enhanced'
Need init if jQuery is called (just allow error to be thrown if not included)
Support: Android <=4.0 only
Make sure we trim BOM and NBSP
The current version of jQuery being used
The default length of a jQuery object is 0
Get the Nth element in the matched element set OR
Get the whole matched element set as a clean array
Return all the elements in a clean array
Return just the one element from the set
Take an array of elements and push it onto the stack
(returning the new matched element set)
Build a new jQuery matched element set
Add the old object onto the stack (as a reference)
Return the newly-formed element set
Execute a callback for every element in the matched set.
For internal use only.
"Behaves like an Array's method, not like a jQuery method."
Handle a deep copy situation
Skip the boolean and the target
Handle case when target is a string or something (possible in deep copy)
Extend jQuery itself if only one argument is passed
Only deal with non-null/undefined values
Extend the base object
Prevent Object.prototype pollution
Prevent never-ending loop
Recurse if we're merging plain objects or arrays
Ensure proper type for the source value
"Never move original objects, clone them"
Don't bring in undefined values
Return the modified object
Unique for each copy of jQuery on the page
Assume jQuery is ready without the ready module
Detect obvious negatives
Use toString instead of jQuery.type to catch host objects
"Objects with no prototype (e.g., `Object.create( null )`) are plain"
Objects with prototype are plain iff they were constructed by a global Object function
Evaluates a script in a global context
Support: Android <=4.0 only
results is for internal usage only
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
"Go through the array, only saving the items"
that pass the validator function
arg is for internal usage only
"Go through the array, translating each of the items to their new values"
"Go through every key on the object,"
Flatten any nested arrays
A global GUID counter for objects
jQuery.support is not used in Core but other projects attach their
properties to it so it needs to exist.
Populate the class2type map
Support: real iOS 8.2 only (not reproducible in simulator)
`in` check used to prevent JIT error (gh-2145)
hasOwn isn't used here due to false negatives
regarding Nodelist length in IE
"Create innerHeight, innerWidth, height, width, outerHeight and outerWidth methods"
"Margin is only for outerHeight, outerWidth"
$( window ).outerWidth/Height return w/h including scrollbars (gh-1729)
Get document width or height
"Either scroll[Width/Height] or offset[Width/Height] or client[Width/Height],"
whichever is greatest
"Get width or height on the element, requesting but not forcing parseFloat"
Set width or height on the element
Methods guaranteed to produce a unique set when starting from a unique set
"Positional selectors never match, since there's no _selection_ context"
Always skip document fragments
Don't pass non-elements to Sizzle
Determine the position of an element within the set
"No argument, return index in parent"
Index in selector
Locate the position of the desired element
"If it receives a jQuery object, the first element is used"
"Support: IE 9 - 11 only, iOS 7 only, Android Browser <=4.3 only"
Treat the template element as a regular one in browsers that
don't support it.
Remove duplicates
Reverse order for parents* and prev-derivatives
Return jQuery for attributes-only inclusion
Speed up dequeue by getting out quickly if this is just a lookup
"If the fx queue is dequeued, always remove the progress sentinel"
Add a progress sentinel to prevent the fx queue from being
automatically dequeued
Clear up the last queue stop function
"Not public - generate a queueHooks object, or return the current one"
Ensure a hooks for this queue
Get a promise resolved when queues of a certain type
are emptied (fx is the type by default)
The elements to wrap the target around
Serialize array item.
Treat each array item as a scalar.
"Item is non-scalar (array or object), encode its numeric index."
Serialize object item.
Serialize scalar item.
Serialize an array of form elements or a set of
key/values into a query string
"If value is a function, invoke it and use its return value"
"If an array was passed in, assume that it is an array of form elements."
Serialize the form elements
"If traditional, encode the ""old"" way (the way 1.3.2 or older"
"did it), otherwise encode params recursively."
Return the resulting serialization
"Can add propHook for ""elements"" to filter or add form elements"
"Use .is( "":disabled"" ) so that fieldset[disabled] works"
"Set position first, in-case top/left are set even on static elem"
Need to be able to calculate position if either
top or left is auto and position is either absolute or fixed
Use jQuery.extend here to allow modification of coordinates argument (gh-1848)
offset() relates an element's border box to the document origin
Preserve chaining for setter
Return zeros for disconnected and hidden (display: none) elements (gh-2310)
Support: IE <=11 only
Running getBoundingClientRect on a
disconnected node in IE throws an error
Get document-relative position by adding viewport scroll to viewport-relative gBCR
position() relates an element's margin box to its offset parent's padding box
This corresponds to the behavior of CSS absolute positioning
"position:fixed elements are offset from the viewport, which itself always has zero offset"
Assume position:fixed implies availability of getBoundingClientRect
"Account for the *real* offset parent, which can be the document or its root element"
when a statically positioned element is identified
"Incorporate borders into its offset, since they are outside its content origin"
Subtract parent offsets and element margins
This method will return documentElement in the following cases:
"1) For the element inside the iframe without offsetParent, this method will return"
documentElement of the parent window
2) For the hidden or detached element
"3) For body or html element, i.e. in case of the html node - it will return itself"
""
but those exceptions were never presented as a real life use-cases
and might be considered as more preferable results.
""
"This logic, however, is not guaranteed and can change at any point in the future"
Create scrollLeft and scrollTop methods
Coalesce documents and windows
"Support: Safari <=7 - 9.1, Chrome <=37 - 49"
Add the top/left cssHooks using jQuery.fn.position
Webkit bug: https://bugs.webkit.org/show_bug.cgi?id=29084
Blink bug: https://bugs.chromium.org/p/chromium/issues/detail?id=589347
getComputedStyle returns percent when specified for top/left/bottom/right;
"rather than make the css module depend on the offset module, just check for it here"
"If curCSS returns percentage, fallback to offset"
"#7653, #8125, #8152: local protocol detection"
Avoid comment-prolog char sequence (#10098); must appease lint and evade compression
Anchor tag for parsing the document origin
"Base ""constructor"" for jQuery.ajaxPrefilter and jQuery.ajaxTransport"
"dataTypeExpression is optional and defaults to ""*"""
For each dataType in the dataTypeExpression
Prepend if requested
Otherwise append
Base inspection function for prefilters and transports
A special extend for ajax options
"that takes ""flat"" options (not to be deep extended)"
Fixes #9887
Remove auto dataType and get content-type in the process
Check if we're dealing with a known content-type
Check to see if we have a response for the expected dataType
Try convertible dataTypes
Or just use first one
If we found a dataType
We add the dataType to the list if needed
and return the corresponding response
Work with a copy of dataTypes in case we need to modify it for conversion
Create converters map with lowercased keys
Convert to each sequential dataType
Apply the dataFilter if provided
There's only work to do if current dataType is non-auto
Convert response if prev dataType is non-auto and differs from current
Seek a direct converter
"If none found, seek a pair"
If conv2 outputs current
If prev can be converted to accepted input
Condense equivalence converters
"Otherwise, insert the intermediate dataType"
Apply converter (if not an equivalence)
"Unless errors are allowed to bubble, catch and return them"
Counter for holding the number of active queries
Last-Modified header cache for next request
Data converters
"Keys separate source (or catchall ""*"") and destination types with a single space"
Convert anything to text
Text to html (true = no transformation)
Evaluate text as a json expression
Parse text as xml
For options that shouldn't be deep extended:
you can add your own custom options here if
and when you create one that shouldn't be
deep extended (see ajaxExtend)
Creates a full fledged settings object into target
with both ajaxSettings and settings fields.
"If target is omitted, writes into ajaxSettings."
Building a settings object
Extending ajaxSettings
Main method
"If url is an object, simulate pre-1.5 signature"
Force options to be an object
URL without anti-cache param
Response headers
timeout handle
Url cleanup var
Request state (becomes false upon send and true upon completion)
To know if global events are to be dispatched
Loop variable
uncached part of the url
Create the final options object
Callbacks context
Context for global events is callbackContext if it is a DOM node or jQuery collection
Deferreds
Status-dependent callbacks
Headers (they are sent all at once)
Default abort message
Fake xhr
Builds headers hashtable if needed
Raw string
Caches the header
Overrides response content-type header
Status-dependent callbacks
Execute the appropriate callbacks
Lazy-add the new callbacks in a way that preserves old ones
Cancel the request
Attach deferreds
Add protocol if not provided (prefilters might expect it)
Handle falsy url in the settings object (#10093: consistency with old signature)
We also use the url parameter if available
Alias method option to type as per ticket #12004
Extract dataTypes list
A cross-domain request is in order when the origin doesn't match the current origin.
"Support: IE <=8 - 11, Edge 12 - 15"
"IE throws exception on accessing the href property if url is malformed,"
e.g. http://example.com:80x/
Support: IE <=8 - 11 only
Anchor's host property isn't correctly set when s.url is relative
"If there is an error parsing the URL, assume it is crossDomain,"
it can be rejected by the transport if it is invalid
Convert data if not already a string
Apply prefilters
"If request was aborted inside a prefilter, stop there"
We can fire global events as of now if asked to
Don't fire events if jQuery.event is undefined in an AMD-usage scenario (#15118)
Watch for a new set of requests
Uppercase the type
Determine if request has content
Save the URL in case we're toying with the If-Modified-Since
and/or If-None-Match header later on
Remove hash to simplify url manipulation
More options handling for requests with no content
Remember the hash so we can put it back
"If data is available and should be processed, append data to url"
#9682: remove data so that it's not used in an eventual retry
Add or update anti-cache param if needed
Put hash and anti-cache on the URL that will be requested (gh-1732)
Change '%20' to '+' if this is encoded form body content (gh-2658)
"Set the If-Modified-Since and/or If-None-Match header, if in ifModified mode."
"Set the correct header, if data is being sent"
"Set the Accepts header for the server, depending on the dataType"
Check for headers option
Allow custom headers/mimetypes and early abort
Abort if not done already and return
Aborting is no longer a cancellation
Install callbacks on deferreds
Get transport
"If no transport, we auto-abort"
Send global event
"If request was aborted inside ajaxSend, stop there"
Timeout
Rethrow post-completion exceptions
Propagate others as results
Callback for when everything is done
Ignore repeat invocations
Clear timeout if it exists
Dereference transport for early garbage collection
(no matter how long the jqXHR object will be used)
Cache response headers
Set readyState
Determine if successful
Get response data
Convert no matter what (that way responseXXX fields are always set)
"If successful, handle type chaining"
"Set the If-Modified-Since and/or If-None-Match header, if in ifModified mode."
if no content
if not modified
"If we have data, let's convert it"
Extract error from statusText and normalize for non-aborts
Set data for the fake xhr object
Success/Error
Status-dependent callbacks
Complete
Handle the global AJAX counter
Shift arguments if data argument was omitted
The url can be an options object (which then must have .url)
"( namespace ) or ( selector, types [, fn] )"
"Bind a function to a context, optionally partially applying any"
arguments.
jQuery.proxy is deprecated to promote standards (specifically Function#bind)
"However, it is not slated for removal any time soon"
"Quick check to determine if target is callable, in the spec"
"this throws a TypeError, but we will just return undefined."
Simulated bind
"Set the guid of unique handler to the same of original handler, so it can be removed"
"As of jQuery 3.0, isNumeric is limited to"
strings and numbers (primitives or objects)
that can be coerced to finite numbers (gh-2662)
"parseFloat NaNs numeric-cast false positives ("""")"
"...but misinterprets leading-number strings, particularly hex literals (""0x..."")"
subtraction forces infinities to NaN
CSS string/identifier serialization
https://drafts.csswg.org/cssom/#common-serializing-idioms
U+0000 NULL becomes U+FFFD REPLACEMENT CHARACTER
Control characters and (dependent upon position) numbers get escaped as code points
Other potentially-special ASCII characters get backslash-escaped
Flag for duplicate removal
Sort on method existence if only one input has compareDocumentPosition
Calculate position if both inputs belong to the same document
Otherwise we know they are disconnected
Disconnected nodes
Choose the first element that is related to our preferred document
Maintain original order
Clear input after sorting to release objects
See https://github.com/jquery/sizzle/pull/225
Same basic safeguard as Sizzle
Early return if context is not an element or document
"If no nodeType, this is expected to be an array"
Do not traverse comment nodes
Use textContent for elements
Do not include comment or processing instruction nodes
documentElement is verified for cases where it doesn't yet exist
(such as loading iframes in IE - #4833)
Don't get fooled by Object.prototype properties (jQuery #13807)
Deprecated
Animations created synchronously will run synchronously
Generate parameters to create a standard animation
"If we include width, step value is 1 to do all cssExpand values,"
otherwise step value is 2 to skip over Left and Right
We're done with this property
Queue-skipping animations hijack the fx hooks
Ensure the complete handler is called before this completes
Detect show/hide animations
"Pretend to be hidden if this is a ""show"" and"
there is still data from a stopped show/hide
Ignore all other no-op show/hide data
Bail out if this is a no-op like .hide().hide()
"Restrict ""overflow"" and ""display"" styles during box animations"
"Support: IE <=9 - 11, Edge 12 - 15"
Record all 3 overflow attributes because IE does not infer the shorthand
from identically-valued overflowX and overflowY and Edge just mirrors
the overflowX value there.
"Identify a display type, preferring old show/hide data over the CSS cascade"
Get nonempty value(s) by temporarily forcing visibility
Animate inline elements as inline-block
Restore the original display value at the end of pure show/hide animations
Implement show/hide animations
General show/hide setup for this element animation
"Store hidden/visible for toggle so `.stop().toggle()` ""reverses"""
Show elements before animating them
"The final step of a ""hide"" animation is actually hiding the element"
Per-property setup
"camelCase, specialEasing and expand cssHook pass"
"Not quite $.extend, this won't overwrite existing keys."
"Reusing 'index' because we have the correct ""name"""
Don't match elem in the :animated selector
Support: Android 2.3 only
Archaic crash bug won't allow us to use `1 - ( 0.5 || 0 )` (#12497)
"If there's more to do, yield"
"If this was an empty animation, synthesize a final progress notification"
Resolve the animation and report its conclusion
"If we are going to the end, we want to run all the tweens"
otherwise we skip this part
"Resolve when we played the last frame; otherwise, reject"
Attach callbacks from options
Go to the end state if fx are off
"Normalize opt.queue - true/undefined/null -> ""fx"""
Queueing
Show any hidden elements after setting opacity to 0
Animate to the value specified
Operate on a copy of prop so per-property easing won't be lost
"Empty animations, or finishing resolves immediately"
Start the next in the queue if the last step wasn't forced.
"Timers currently will call their complete callbacks, which"
will dequeue but only if they were gotoEnd.
Enable finishing flag on private data
Empty the queue first
"Look for any active animations, and finish them"
Look for any animations in the old queue and finish them
Turn off finishing flag
Generate shortcuts for custom animations
Run the timer and safely remove it when done (allowing for external removal)
Default speed
Swappable if display is none or starts with table
"except ""table"", ""table-cell"", or ""table-caption"""
See here for display values: https://developer.mozilla.org/en-US/docs/CSS/display
Any relative (+/-) values have already been
normalized at this point
"Guard against undefined ""subtract"", e.g., when used as in cssHooks"
Adjustment may not be necessary
Both box models exclude margin
"If we get here with a content-box, we're seeking ""padding"" or ""border"" or ""margin"""
Add padding
"For ""border"" or ""margin"", add border"
But still keep track of it otherwise
"If we get here with a border-box (content + padding + border), we're seeking ""content"" or"
"""padding"" or ""margin"""
"For ""content"", subtract padding"
"For ""content"" or ""padding"", subtract border"
Account for positive content-box scroll gutter when requested by providing computedVal
"offsetWidth/offsetHeight is a rounded sum of content, padding, scroll gutter, and border"
"Assuming integer scroll gutter, subtract the rest and round down"
"If offsetWidth/offsetHeight is unknown, then we can't determine content-box scroll gutter"
Use an explicit zero to avoid NaN (gh-3964)
Start with computed style
"To avoid forcing a reflow, only fetch boxSizing if we need it (gh-4322)."
Fake content-box until we know it's needed to know the true value.
Support: Firefox <=54
"Return a confounding non-pixel value or feign ignorance, as appropriate."
"Fall back to offsetWidth/offsetHeight when value is ""auto"""
This happens for inline elements with no explicit setting (gh-3571)
Support: Android <=4.1 - 4.3 only
Also use offsetWidth/offsetHeight for misreported inline dimensions (gh-3602)
Support: IE 9-11 only
Also use offsetWidth/offsetHeight for when box sizing is unreliable
We use getClientRects() to check for hidden/disconnected.
"In those cases, the computed value can be trusted to be border-box"
"Where available, offsetWidth/offsetHeight approximate border box dimensions."
"Where not available (e.g., SVG), assume unreliable box-sizing and interpret the"
retrieved value as a content box dimension.
"Normalize """" and auto"
Adjust for the element's box model
Provide the current computed size to request scroll gutter calculation (gh-3589)
Add in style property hooks for overriding the default
behavior of getting and setting a style property
We should always get a number back from opacity
"Don't automatically add ""px"" to these possibly-unitless properties"
Add in properties whose names you wish to fix before
setting or getting the value
Get and set the style property on a DOM Node
Don't set styles on text and comment nodes
Make sure that we're working with the right name
Make sure that we're working with the right name. We don't
want to query the value if it is a CSS custom property
since they are user-defined.
"Gets hook for the prefixed version, then unprefixed version"
Check if we're setting a value
"Convert ""+="" or ""-="" to relative numbers (#7345)"
Fixes bug #9237
Make sure that null and NaN values aren't set (#7116)
"If a number was passed in, add the unit (except for certain CSS properties)"
The isCustomProp check can be removed in jQuery 4.0 when we only auto-append
"""px"" to a few hardcoded values."
background-* props affect original clone's values
"If a hook was provided, use that value, otherwise just set the specified value"
If a hook was provided get the non-computed value from there
Otherwise just get the value from the style object
Make sure that we're working with the right name. We don't
want to modify the value if it is a CSS custom property
since they are user-defined.
Try prefixed name followed by the unprefixed name
If a hook was provided get the computed value from there
"Otherwise, if a way to get the computed value exists, use that"
"Convert ""normal"" to computed value"
Make numeric if forced or a qualifier was provided and val looks numeric
Certain elements can have dimension info if we invisibly show them
but it must have a current display style that would benefit
Support: Safari 8+
Table columns in Safari have non-zero offsetWidth & zero
getBoundingClientRect().width unless display is changed.
Support: IE <=11 only
Running getBoundingClientRect on a disconnected node
in IE throws an error.
Only read styles.position if the test has a chance to fail
to avoid forcing a reflow.
"To avoid forcing a reflow, only fetch boxSizing if we need it (gh-3991)"
Account for unreliable border-box dimensions by comparing offset* to computed and
faking a content-box to get border and padding (gh-3699)
Convert to pixels if value adjustment is needed
These hooks are used by animate to expand properties
Assumes a single number if not a string
Check for promise aspect first to privilege synchronous behavior
Other thenables
Other non-thenables
Control `resolve` arguments by letting Array#slice cast boolean `noValue` to integer:
* false: [ value ].slice( 0 ) => resolve( value )
* true: [ value ].slice( 1 ) => resolve()
"For Promises/A+, convert exceptions into rejections"
"Since jQuery.when doesn't unwrap thenables, we can skip the extra checks appearing in"
Deferred#then to conditionally suppress rejection.
Support: Android 4.0 only
Strict mode functions invoked without .call/.apply get global-object context
"action, add listener, callbacks,"
"... .then handlers, argument index, [final state]"
Keep pipe for back-compat
"Map tuples (progress, done, fail) to arguments (done, fail, progress)"
deferred.progress(function() { bind to newDefer or newDefer.notify })
deferred.done(function() { bind to newDefer or newDefer.resolve })
deferred.fail(function() { bind to newDefer or newDefer.reject })
Support: Promises/A+ section 2.3.3.3.3
https://promisesaplus.com/#point-59
Ignore double-resolution attempts
Support: Promises/A+ section 2.3.1
https://promisesaplus.com/#point-48
"Support: Promises/A+ sections 2.3.3.1, 3.5"
https://promisesaplus.com/#point-54
https://promisesaplus.com/#point-75
Retrieve `then` only once
Support: Promises/A+ section 2.3.4
https://promisesaplus.com/#point-64
Only check objects and functions for thenability
Handle a returned thenable
Special processors (notify) just wait for resolution
Normal processors (resolve) also hook into progress
...and disregard older resolution values
Handle all other returned values
Only substitute handlers pass on context
and multiple values (non-spec behavior)
Process the value(s)
Default process is resolve
Only normal processors (resolve) catch and reject exceptions
Support: Promises/A+ section 2.3.3.3.4.1
https://promisesaplus.com/#point-61
Ignore post-resolution exceptions
Only substitute handlers pass on context
and multiple values (non-spec behavior)
Support: Promises/A+ section 2.3.3.3.1
https://promisesaplus.com/#point-57
Re-resolve promises immediately to dodge false rejection from
subsequent errors
"Call an optional hook to record the stack, in case of exception"
since it's otherwise lost when execution goes async
progress_handlers.add( ... )
fulfilled_handlers.add( ... )
rejected_handlers.add( ... )
Get a promise for this deferred
"If obj is provided, the promise aspect is added to the object"
Add list-specific methods
promise.progress = list.add
promise.done = list.add
promise.fail = list.add
Handle state
"state = ""resolved"" (i.e., fulfilled)"
"state = ""rejected"""
rejected_callbacks.disable
fulfilled_callbacks.disable
rejected_handlers.disable
fulfilled_handlers.disable
progress_callbacks.lock
progress_handlers.lock
progress_handlers.fire
fulfilled_handlers.fire
rejected_handlers.fire
deferred.notify = function() { deferred.notifyWith(...) }
deferred.resolve = function() { deferred.resolveWith(...) }
deferred.reject = function() { deferred.rejectWith(...) }
deferred.notifyWith = list.fireWith
deferred.resolveWith = list.fireWith
deferred.rejectWith = list.fireWith
Make the deferred a promise
Call given func if any
All done!
Deferred helper
count of uncompleted subordinates
count of unprocessed arguments
subordinate fulfillment data
the master Deferred
subordinate callback factory
Single- and empty arguments are adopted like Promise.resolve
Use .then() to unwrap secondary thenables (cf. gh-3000)
Multiple arguments are aggregated like Promise.all array elements
See https://github.com/eslint/eslint/issues/3229
"Support: IE <=10 - 11, Edge 12 - 13 only"
In IE/Edge using regex groups here causes severe slowdowns.
See https://connect.microsoft.com/IE/feedback/details/1736512/
"checked=""checked"" or checked"
Prefer a tbody over its parent table for containing new rows
Replace/restore the type attribute of script elements for safe DOM manipulation
"1. Copy private data: events, handlers, etc."
2. Copy user data
"Fix IE bugs, see support tests"
Fails to persist the checked state of a cloned checkbox or radio button.
Fails to return the selected option to the default selected state when cloning options
Flatten any nested arrays
"We can't cloneNode fragments that contain checked, in WebKit"
Require either new content or an interest in ignored elements to invoke the callback
Use the original fragment for the last item
instead of the first because it can end up
being emptied incorrectly in certain situations (#8070).
Keep references to cloned scripts for later restoration
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
Reenable scripts
Evaluate executable scripts on first document insertion
"Optional AJAX dependency, but won't run scripts if not present"
Fix IE cloning issues
We eschew Sizzle here for performance reasons: https://jsperf.com/getall-vs-sizzle/2
Copy the events from the original to the clone
Preserve script evaluation history
Return the cloned set
This is a shortcut to avoid jQuery.event.remove's overhead
Support: Chrome <=35 - 45+
"Assign undefined instead of using delete, see Data#remove"
Support: Chrome <=35 - 45+
"Assign undefined instead of using delete, see Data#remove"
Prevent memory leaks
Remove any remaining nodes
See if we can take a shortcut and just use innerHTML
Remove element nodes and prevent memory leaks
"If using innerHTML throws an exception, use the fallback method"
"Make the changes, replacing each non-ignored context element with the new content"
Force callback invocation
"Support: Android <=4.0 only, PhantomJS 1 only"
".get() because push.apply(_, arraylike) throws on ancient WebKit"
"Define the hook, we'll check on the first run if it's really needed."
Hook not needed (or it's not possible to use it due
"to missing dependency), remove it."
Hook needed; redefine it so that the support test is not executed again.
Executing both pixelPosition & boxSizingReliable tests require only one layout
so they're executed at the same time to save the second computation.
"This is a singleton, we need to execute it only once"
"Support: Android 4.0 - 4.3 only, Firefox <=3 - 44"
"Support: Android 4.0 - 4.3 only, Safari <=9.1 - 10.1, iOS <=7.0 - 9.3"
"Some styles come back with percentage values, even though they shouldn't"
Support: IE 9 - 11 only
Detect misreporting of content dimensions for box-sizing:border-box elements
Support: IE 9 only
Detect overflow:scroll screwiness (gh-3699)
Support: Chrome <=64
Don't get tricked when zoom affects offsetWidth (gh-4029)
Nullify the div so it wouldn't be stored in the memory and
it will also be a sign that checks already performed
Finish early in limited (non-browser) environments
Support: IE <=9 - 11 only
Style of cloned element affects source element cloned (#8908)
Support: Firefox 51+
Retrieving style before computed somehow
fixes an issue with getting wrong values
on detached elements
getPropertyValue is needed for:
".css('filter') (IE 9 only, #12537)"
.css('--customProperty) (#3144)
"A tribute to the ""awesome hack by Dean Edwards"""
"Android Browser returns percentage for some values,"
but width seems to be reliably pixels.
This is against the CSSOM draft spec:
https://drafts.csswg.org/cssom/#resolved-values
Remember the original values
Put in the new values to get a computed value out
Revert the changed values
Support: IE <=9 - 11 only
IE returns zIndex value as an integer.
Determine new display value for elements that need to change
"Since we force visibility upon cascade-hidden elements, an immediate (and slow)"
check is required in this first loop unless we have a nonempty display value (either
inline or about-to-be-restored)
Remember what we're overwriting
Set the display of the elements in a second loop to avoid constant reflow
Starting value computation is required for potential unit mismatches
Support: Firefox <=54
Halve the iteration target value to prevent interference from CSS upper bounds (gh-2144)
Trust units reported by jQuery.css
Iteratively approximate from a nonzero starting point
Evaluate and update our best guess (doubling guesses that zero out).
Finish if the scale equals or crosses 1 (making the old*new product non-positive).
Make sure we update the tween properties later on
Apply relative offset (+=/-=) if specified
"Support: IE <=11 only, Firefox <=30 (#15098, #14150)"
IE throws on elements created in popups
"FF meanwhile throws on frame elements through ""defaultView.getComputedStyle"""
css is assumed
"isHiddenWithinTree reports if an element has a non-""none"" display style (inline and/or"
"through the CSS cascade), which is useful in deciding whether or not to make it visible."
It differs from the :hidden selector (jQuery.expr.pseudos.hidden) in two important ways:
* A hidden ancestor does not force an element to be classified as hidden.
* Being disconnected from the document does not force an element to be classified as hidden.
These differences improve the behavior of .toggle() et al. when applied to elements that are
"detached or contained within hidden ancestors (gh-2404, gh-2863)."
isHiddenWithinTree might be called from jQuery#filter function;
"in that case, element will be second argument"
Inline style trumps all
"Otherwise, check computed style"
Support: Firefox <=43 - 45
"Disconnected elements can have computed display: none, so first confirm that elem is"
in the document.
A method for quickly swapping in/out CSS properties to get correct calculations.
"Remember the old values, and insert the new ones"
Revert the old values
Don't do events on text and comment nodes
focus/blur morphs to focusin/out; ensure we're not firing them right now
Namespaced trigger; create a regexp to match event type in handle()
"Caller can pass in a jQuery.Event object, Object, or just an event type string"
Trigger bitmask: & 1 for native handlers; & 2 for jQuery (always true)
Clean up the event in case it is being reused
"Clone any incoming data and prepend the event, creating the handler arg list"
Allow special events to draw outside the lines
"Determine event propagation path in advance, per W3C events spec (#9951)"
"Bubble up to document, then to window; watch for a global ownerDocument var (#9724)"
"Only add window if we got to document (e.g., not plain obj or detached DOM)"
Fire handlers on the event path
jQuery handler
Native handler
"If nobody prevented the default action, do it now"
Call a native DOM method on the target with the same name as the event.
"Don't do default actions on window, that's where global variables be (#6170)"
Don't re-trigger an onFOO event when we call its FOO() method
"Prevent re-triggering of the same event, since we already bubbled it above"
Piggyback on a donor event to simulate a different one
Used only for `focus(in | out)` events
Attach a bunch of functions for handling common AJAX events
Support: Firefox <=44
Firefox doesn't have focus(in | out) events
Related ticket - https://bugzilla.mozilla.org/show_bug.cgi?id=687787
""
"Support: Chrome <=48 - 49, Safari <=9.0 - 9.1"
"focus(in | out) events fire after focus & blur events,"
which is spec violation - http://www.w3.org/TR/DOM-Level-3-Events/#events-focusevent-event-order
Related ticket - https://bugs.chromium.org/p/chromium/issues/detail?id=449857
Attach a single capturing handler on the document while someone wants focusin/focusout
Handle event binding
"Based off of the plugin by Clint Helfers, with permission."
https://web.archive.org/web/20100324014747/http://blindsignals.com/index.php/2009/07/jquery-delay/
Support: IE <=9 - 11 only
Use typeof to avoid zero-argument method invocation on host objects (#15151)
Mark scripts as having already been evaluated
Support: Android 4.0 - 4.3 only
Check state lost if the name is set (#11217)
Support: Windows Web Apps (WWA)
`name` and `type` must use .setAttribute for WWA (#14901)
Support: Android <=4.1 only
Older WebKit doesn't clone checked state correctly in fragments
Support: IE <=11 only
Make sure textarea (and checkbox) defaultValue is properly cloned
Add nodes directly
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
Convert non-html into a text node
Convert html into DOM nodes
Deserialize a standard representation
Descend through wrappers to the right content
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
Remember the top-level container
Ensure the created nodes are orphaned (#12392)
Remove wrapper from fragment
Skip elements already in the context collection (trac-4087)
Append to fragment
Preserve script evaluation history
Capture executables
We have to close these tags to support XHTML (#13200)
Support: IE <=9 only
XHTML parsers do not magically insert elements in the
same way that tag soup parsers do. So we cannot shorten
this by omitting <tbody> or other required elements.
Support: IE <=9 only
"Make this explicit, since user can override this through ajaxSetup (#11264)"
Only evaluate the response if it is successful (gh-4126)
"dataFilter is not invoked for failure responses, so using it instead"
of the default converter is kludgy but it works.
rtagName captures the name from the first start tag in a string of HTML
https://html.spec.whatwg.org/multipage/syntax.html#tag-open-state
https://html.spec.whatwg.org/multipage/syntax.html#tag-name-state
"These usually indicate a programmer mistake during development,"
warn about them ASAP rather than swallowing them by default.
Support: IE 8 - 9 only
"Console exists when dev tools are open, which can happen at any time"
Check if the owner object already has a cache
"If not, create one"
"We can accept data for non-element nodes in modern browsers,"
"but we should not, see #8335."
Always return an empty object.
If it is a node unlikely to be stringify-ed or looped over
use plain assignment
Otherwise secure it in a non-enumerable property
configurable must be true to allow the property to be
deleted when data is removed
"Handle: [ owner, key, value ] args"
Always use camelCase key (gh-2257)
"Handle: [ owner, { properties } ] args"
Copy the properties one-by-one to the cache object
Always use camelCase key (gh-2257)
In cases where either:
""
1. No key was specified
"2. A string key was specified, but no value provided"
""
"Take the ""read"" path and allow the get method to determine"
"which value to return, respectively either:"
""
1. The entire cache object
2. The data stored at the key
""
"When the key is not a string, or both a key and value"
"are specified, set or extend (existing objects) with either:"
""
1. An object of properties
2. A key and value
""
"Since the ""set"" path can have two possible entry points"
return the expected data based on which path was taken[*]
Support array or space separated string of keys
If key is an array of keys...
"We always set camelCase keys, so remove that."
"If a key with the spaces exists, use it."
"Otherwise, create an array by matching non-whitespace"
Remove the expando if there's no more data
Support: Chrome <=35 - 45
Webkit & Blink performance suffers when deleting properties
"from DOM nodes, so set to undefined instead"
https://bugs.chromium.org/p/chromium/issues/detail?id=378607 (bug restricted)
Accepts only:
- Node
- Node.ELEMENT_NODE
- Node.DOCUMENT_NODE
- Object
- Any
Only count HTML whitespace
Other whitespace should count in values
https://infra.spec.whatwg.org/#ascii-whitespace
[[Class]] -> type pairs
All support tests are defined in their respective modules.
"Support: Chrome <=57, Firefox <=52"
"In some browsers, typeof returns ""function"" for HTML <object> elements"
"(i.e., `typeof document.createElement( ""object"" ) === ""function""`)."
We don't want to classify *any* DOM node as a function.
"Support: Firefox 64+, Edge 18+"
"Some browsers don't support the ""nonce"" property on scripts."
"On the other hand, just using `getAttribute` is not enough as"
the `nonce` attribute is reset to an empty string whenever it
becomes browsing-context connected.
See https://github.com/whatwg/html/issues/2369
See https://html.spec.whatwg.org/#nonce-attributes
The `node.getAttribute` check was added for the sake of
`jQuery.globalEval` so that it can fake a nonce-containing node
via an object.
Support: Safari 8 only
In Safari 8 documents created via document.implementation.createHTMLDocument
collapse sibling forms: the second one becomes a child of the first one.
"Because of that, this security measure has to be disabled in Safari 8."
https://bugs.webkit.org/show_bug.cgi?id=137337
This is the only module that needs core/support
"Argument ""data"" should be string of html"
"context (optional): If specified, the fragment will be created in this context,"
defaults to document
"keepScripts (optional): If true, will include scripts passed in the html string"
Stop scripts or inline event handlers from being executed immediately
by using document.implementation
Set the base href for the created document
so any parsed elements with URLs
are based on the document's URL (gh-2965)
Single tag
Matches dashed string for camelizing
Used by camelCase as callback to replace()
Convert dashed to camelCase; used by the css and data modules
"Support: IE <=9 - 11, Edge 12 - 15"
Microsoft forgot to hump their vendor prefix (#9572)
Multifunctional method to get and set values of a collection
The value/s can optionally be executed if it's a function
Sets many values
Sets one value
Bulk operations run against the entire set
...except when executing function values
Gets
Strip and collapse whitespace according to HTML spec
https://infra.spec.whatwg.org/#strip-and-collapse-ascii-whitespace
Initialize a jQuery object
A central reference to the root jQuery(document)
A simple way to check for HTML strings
Prioritize #id over <tag> to avoid XSS via location.hash (#9521)
Strict HTML recognition (#11290: must start with <)
Shortcut simple #id case for speed
"HANDLE: $(""""), $(null), $(undefined), $(false)"
Method init() accepts an alternate rootjQuery
so migrate can support jQuery.sub (gh-2101)
Handle HTML strings
Assume that strings that start and end with <> are HTML and skip the regex check
Match html or make sure no context is specified for #id
HANDLE: $(html) -> $(array)
Option to run scripts is true for back-compat
Intentionally let the error be thrown if parseHTML is not present
"HANDLE: $(html, props)"
Properties of context are called as methods if possible
...and otherwise set as attributes
HANDLE: $(#id)
Inject the element directly into the jQuery object
"HANDLE: $(expr, $(...))"
"HANDLE: $(expr, context)"
(which is just equivalent to: $(context).find(expr)
HANDLE: $(DOMElement)
HANDLE: $(function)
Shortcut for document ready
Execute immediately if ready is not present
Give the init function the jQuery prototype for later instantiation
Initialize central reference
Support: Android <=2.3 only (functionish RegExp)
Prevent errors from freezing future callback execution (gh-1823)
Not backwards-compatible as this does not execute sync
Is the DOM ready to be used? Set to true once it occurs.
A counter to track how many items to wait for before
the ready event fires. See #6781
Abort if there are pending holds or we're already ready
Remember that the DOM is ready
"If a normal DOM Ready event fired, decrement, and wait if need be"
Make jQuery.ready Promise consumable (gh-1778)
Catch cases where $(document).ready() is called
after the browser event has already occurred.
Support: IE9-10 only
"Older IE sometimes signals ""interactive"" too soon"
Handle it asynchronously to allow scripts the opportunity to delay ready
Use the handy event callback
"A fallback to window.onload, that will always work"
The deferred used on DOM ready
Wrap jQuery.readyException in a function so that the lookup
happens at the time of error handling instead of callback
registration.
Is the DOM ready to be used? Set to true once it occurs.
A counter to track how many items to wait for before
the ready event fires. See #6781
Handle when the DOM is ready
Abort if there are pending holds or we're already ready
Remember that the DOM is ready
"If a normal DOM Ready event fired, decrement, and wait if need be"
"If there are functions bound, to execute"
The ready event handler and self cleanup method
Catch cases where $(document).ready() is called
after the browser event has already occurred.
Support: IE <=9 - 10 only
"Older IE sometimes signals ""interactive"" too soon"
Handle it asynchronously to allow scripts the opportunity to delay ready
Use the handy event callback
"A fallback to window.onload, that will always work"
rsingleTag matches a string consisting of a single HTML element with no attributes
and captures the element's name
"Register as a named AMD module, since jQuery can be concatenated with other"
"files that may use define, but not via a proper concatenation script that"
understands anonymous AMD modules. A named AMD is safest and most robust
way to register. Lowercase jquery is used because AMD module names are
"derived from file names, and jQuery is normally delivered in a lowercase"
file name. Do this after creating the global so that if an AMD module wants
"to call noConflict to hide this version of jQuery, it will work."
"Note that for maximum portability, libraries that are not jQuery should"
"declare themselves as anonymous modules, and avoid setting a global if an"
"AMD loader is present. jQuery is a special case. For more information, see"
https://github.com/jrburke/requirejs/wiki/Updating-existing-libraries#wiki-anon
Map over jQuery in case of overwrite
Map over the $ in case of overwrite
"Expose jQuery and $ identifiers, even in AMD"
"(#7102#comment:10, https://github.com/jquery/jquery/pull/557)"
and CommonJS for browser emulators (#13566)
"Use a property on the element directly when it is not a DOM element,"
or when there is no matching style property that exists.
Passing an empty string as a 3rd parameter to .css will automatically
attempt a parseFloat and fallback to a string if the parse fails.
"Simple values such as ""10px"" are parsed to Float;"
"complex values such as ""rotate(1rad)"" are returned as-is."
"Empty strings, null, undefined and ""auto"" are converted to 0."
Use step hook for back compat.
Use cssHook if its there.
Use .style if available and use plain properties where available.
Support: IE <=9 only
Panic based approach to setting things on disconnected nodes
Back compat <1.8 extension point
Handle most common string cases
Handle cases where value is null/undef or number
"Treat null/undefined as """"; convert numbers to string"
"If set returns undefined, fall back to normal setting"
Support: IE <=10 - 11 only
"option.text throws exceptions (#14686, #14858)"
Strip and collapse whitespace
https://html.spec.whatwg.org/#strip-and-collapse-whitespace
Loop through all the selected options
Support: IE <=9 only
IE8-9 doesn't update selected after form reset (#2551)
Don't return options that are disabled or in a disabled optgroup
Get the specific value for the option
We don't need an array for one selects
Multi-Selects return an array
Force browsers to behave consistently when non-matching value is set
Radios and checkboxes getter/setter
"Don't get/set attributes on text, comment and attribute nodes"
Fallback to prop when attributes are not supported
Attribute hooks are determined by the lowercase version
Grab necessary hook if one is defined
"Non-existent attributes return null, we normalize to undefined"
Attribute names can contain non-HTML whitespace characters
https://html.spec.whatwg.org/multipage/syntax.html#attributes-2
Hooks for boolean attributes
Remove boolean attributes when set to false
Avoid an infinite loop by temporarily removing this function from the getter
Support: Android <=4.3 only
"Default value for a checkbox should be ""on"""
Support: IE <=11 only
Must access selectedIndex to make default options select
Support: IE <=11 only
An input loses its value after becoming a radio
Only assign if different to avoid unneeded rendering.
This expression is here for better compressibility (see addClass)
Remove *all* instances
Only assign if different to avoid unneeded rendering.
Toggle individual class names
"Check each className given, space separated list"
Toggle whole class name
Store className if set
"If the element has a class name or if we're passed `false`,"
"then remove the whole classname (if there was one, the above saved it)."
"Otherwise bring back whatever was previously saved (if anything),"
falling back to the empty string if nothing was stored.
"Don't get/set properties on text, comment and attribute nodes"
Fix name and attach hooks
Support: IE <=9 - 11 only
elem.tabIndex doesn't always return the
correct value when it hasn't been explicitly set
https://web.archive.org/web/20141116233347/http://fluidproject.org/blog/2008/01/09/getting-setting-and-removing-tabindex-values-with-javascript/
Use proper attribute retrieval(#12072)
Support: IE <=11 only
Accessing the selectedIndex property
forces the browser to respect setting selected
on the option
The getter ensures a default option is selected
when in an optgroup
"eslint rule ""no-unused-expressions"" is disabled for this code"
since it considers such accessions noop
If it's a function
We assume that it's the callback
"Otherwise, build a param string"
"If we have elements to modify, make the request"
"If ""type"" variable is undefined, then ""GET"" method will be used."
Make value of this field explicit since
user can override it through ajaxSetup method
Save response for use in complete callback
"If a selector was specified, locate the right elements in a dummy div"
Exclude scripts to avoid IE 'Permission Denied' errors
Otherwise use the full result
"If the request succeeds, this function gets ""data"", ""status"", ""jqXHR"""
but they are ignored because response was set above.
"If it fails, this function gets ""jqXHR"", ""status"", ""error"""
Cross-browser xml parsing
Support: IE 9 - 11 only
IE throws on parseFromString with invalid input.
Prevent auto-execution of scripts when no explicit dataType was provided (See gh-2432)
Install script dataType
Handle cache's special case and crossDomain
Bind script tag hack transport
This transport only deals with cross domain or forced-by-attrs requests
Use native DOM manipulation to avoid our domManip AJAX trickery
Default jsonp settings
"Detect, normalize options and install callbacks for jsonp requests"
"Handle iff the expected data type is ""jsonp"" or we have a parameter to set"
"Get callback name, remembering preexisting value associated with it"
Insert callback into url or form data
Use data converter to retrieve json after script execution
Force json dataType
Install callback
Clean-up function (fires after converters)
If previous value didn't exist - remove it
Otherwise restore preexisting value
Save back as free
Make sure that re-using the options doesn't screw things around
Save the callback name for future use
Call if it was a function and we have a response
Delegate to script
"File protocol always yields status code 0, assume 200"
Support: IE <=9 only
#1450: sometimes IE returns 1223 when it should be 204
Cross domain only allowed if supported through XMLHttpRequest
Apply custom fields if provided
Override mime type if needed
X-Requested-With header
"For cross-domain requests, seeing as conditions for a preflight are"
"akin to a jigsaw puzzle, we simply never set it to be sure."
(it can always be set on a per-request basis or even using ajaxSetup)
"For same-domain requests, won't change header if already provided."
Set headers
Callback
Support: IE <=9 only
"On a manual native abort, IE9 throws"
errors on any property access that is not readyState
"File: protocol always yields status 0; see #8605, #14207"
Support: IE <=9 only
IE9 has no XHR2 but throws on binary (trac-11426)
"For XHR2 non-text, let the caller handle it (gh-2498)"
Listen to events
Support: IE 9 only
Use onreadystatechange to replace onabort
to handle uncaught aborts
Check readyState before timeout as it changes
"Allow onerror to be called first,"
but that will not handle a native abort
"Also, save errorCallback to a variable"
as xhr.onerror cannot be accessed
Create the abort callback
Do send the request (this may raise an exception)
#14683: Only rethrow if this hasn't been notified as an error yet
Implement the identical functionality for filter and not
Single element
"Arraylike of elements (jQuery, arguments, Array)"
Filtered directly for both simple and complex selectors
"If this is a positional/relative selector, check membership in the returned set"
"so $(""p:first"").is(""p:last"") won't return true for a doc with two ""p""."
# sourceMappingURL=sizzle.min.map
Local document vars
Instance-specific data
Instance methods
Use a stripped-down indexOf as it's faster than native
https://jsperf.com/thor-indexof-vs-for/5
Regular expressions
http://www.w3.org/TR/css3-selectors/#whitespace
http://www.w3.org/TR/CSS21/syndata.html#value-def-identifier
Attribute selectors: http://www.w3.org/TR/selectors/#attribute-selectors
Operator (capture 2)
"""Attribute values must be CSS identifiers [capture 5] or strings [capture 3 or capture 4]"""
"To reduce the number of selectors needing tokenize in the preFilter, prefer arguments:"
1. quoted (capture 3; capture 4 or capture 5)
2. simple (capture 6)
3. anything else (capture 2)
"Leading and non-escaped trailing whitespace, capturing some non-whitespace characters preceding the latter"
For use in libraries implementing .is()
We use this for POS matching in `select`
Easily-parseable/retrievable ID or TAG or CLASS selectors
CSS escapes
http://www.w3.org/TR/CSS21/syndata.html#escaped-characters
NaN means non-codepoint
Support: Firefox<24
"Workaround erroneous numeric interpretation of +""0x"""
BMP codepoint
Supplemental Plane codepoint (surrogate pair)
CSS string/identifier serialization
https://drafts.csswg.org/cssom/#common-serializing-idioms
U+0000 NULL becomes U+FFFD REPLACEMENT CHARACTER
Control characters and (dependent upon position) numbers get escaped as code points
Other potentially-special ASCII characters get backslash-escaped
Used for iframes
See setDocument()
"Removing the function wrapper causes a ""Permission Denied"""
error in IE
"Optimize for push.apply( _, NodeList )"
Support: Android<4.0
Detect silently failing push.apply
Leverage slice if possible
Support: IE<9
Otherwise append directly
Can't trust NodeList.length
"nodeType defaults to 9, since context defaults to document"
Return early from calls with invalid selector or context
Try to shortcut find operations (as opposed to filters) in HTML documents
"If the selector is sufficiently simple, try using a ""get*By*"" DOM method"
"(excepting DocumentFragment context, where the methods don't exist)"
ID selector
Document context
"Support: IE, Opera, Webkit"
TODO: identify versions
getElementById can match elements by name instead of ID
Element context
"Support: IE, Opera, Webkit"
TODO: identify versions
getElementById can match elements by name instead of ID
Type selector
Class selector
Take advantage of querySelectorAll
Support: IE 8 only
Exclude object elements
qSA considers elements outside a scoping root when evaluating child or
"descendant combinators, which is not what we want."
"In such cases, we work around the behavior by prefixing every selector in the"
list with an ID selector referencing the scope context.
Thanks to Andrew Dupont for this technique.
"Capture the context ID, setting it first if necessary"
Prefix every selector in the list
Expand context for sibling selectors
All others
"Use (key + "" "") to avoid collision with native prototype properties (see Issue #157)"
Only keep the most recent entries
Remove from its parent by default
release memory in IE
Use IE sourceIndex if available on both nodes
Check if b follows a
Known :disabled false positives: fieldset[disabled] > legend:nth-of-type(n+2) :can-disable
Only certain elements can match :enabled or :disabled
https://html.spec.whatwg.org/multipage/scripting.html#selector-enabled
https://html.spec.whatwg.org/multipage/scripting.html#selector-disabled
Check for inherited disabledness on relevant non-disabled elements:
* listed form-associated elements in a disabled fieldset
https://html.spec.whatwg.org/multipage/forms.html#category-listed
https://html.spec.whatwg.org/multipage/forms.html#concept-fe-disabled
* option elements in a disabled optgroup
https://html.spec.whatwg.org/multipage/forms.html#concept-option-disabled
"All such elements have a ""form"" property."
Option elements defer to a parent optgroup if present
Support: IE 6 - 11
Use the isDisabled shortcut property to check for disabled fieldset ancestors
"Where there is no isDisabled, check manually"
Try to winnow out elements that can't be disabled before trusting the disabled property.
"Some victims get caught in our net (label, legend, menu, track), but it shouldn't"
"even exist on them, let alone have a boolean value."
Remaining elements are neither :enabled nor :disabled
Match elements found at the specified indexes
Expose support vars for convenience
Support: IE <=8
"Assume HTML when documentElement doesn't yet exist, such as inside loading iframes"
https://bugs.jquery.com/ticket/4833
Return early if doc is invalid or already selected
Update global variables
"Support: IE 9-11, Edge"
"Accessing iframe documents after unload throws ""permission denied"" errors (jQuery #13936)"
"Support: IE 11, Edge"
Support: IE 9 - 10 only
Support: IE<8
Verify that getAttribute really returns attributes and not properties
(excepting IE8 booleans)
"Check if getElementsByTagName(""*"") returns only elements"
Support: IE<9
Support: IE<10
Check if getElementById returns elements by name
"The broken getElementById methods don't pick up programmatically-set names,"
so use a roundabout getElementsByName test
ID filter and find
Support: IE 6 - 7 only
getElementById is not reliable as a find shortcut
Verify the id attribute
Fall back on getElementsByName
Tag
DocumentFragment nodes don't have gEBTN
"By happy coincidence, a (broken) gEBTN appears on DocumentFragment nodes too"
Filter out possible comments
Class
QSA and matchesSelector support
matchesSelector(:active) reports false when true (IE9/Opera 11.5)
qSa(:focus) reports false when true (Chrome 21)
We allow this because of a bug in IE8/9 that throws an error
whenever `document.activeElement` is accessed on an iframe
"So, we allow :focus to pass through QSA all the time to avoid the IE error"
See https://bugs.jquery.com/ticket/13378
Build QSA regex
Regex strategy adopted from Diego Perini
Select is set to empty string on purpose
This is to test IE's treatment of not explicitly
"setting a boolean content attribute,"
since its presence should be enough
https://bugs.jquery.com/ticket/12359
"Support: IE8, Opera 11-12.16"
Nothing should be selected when empty strings follow ^= or $= or *=
"The test attribute must be unknown in Opera but ""safe"" for WinRT"
https://msdn.microsoft.com/en-us/library/ie/hh465388.aspx#attribute_section
Support: IE8
"Boolean attributes and ""value"" are not treated correctly"
"Support: Chrome<29, Android<4.4, Safari<7.0+, iOS<7.0+, PhantomJS<1.9.8+"
Webkit/Opera - :checked should return selected option elements
http://www.w3.org/TR/2011/REC-css3-selectors-20110929/#checked
IE8 throws error here and will not see later tests
"Support: Safari 8+, iOS 8+"
https://bugs.webkit.org/show_bug.cgi?id=136851
In-page `selector#id sibling-combinator selector` fails
Support: Windows 8 Native Apps
The type and name attributes are restricted during .innerHTML assignment
Support: IE8
Enforce case-sensitivity of name attribute
FF 3.5 - :enabled/:disabled and hidden elements (hidden elements are still enabled)
IE8 throws error here and will not see later tests
Support: IE9-11+
IE's :disabled selector does not pick up the children of disabled fieldsets
Opera 10-11 does not throw on post-comma invalid pseudos
Check to see if it's possible to do matchesSelector
on a disconnected node (IE 9)
This should fail with an exception
"Gecko does not error, returns false instead"
Element contains another
Purposefully self-exclusive
"As in, an element does not contain itself"
Document order sorting
Flag for duplicate removal
Sort on method existence if only one input has compareDocumentPosition
Calculate position if both inputs belong to the same document
Otherwise we know they are disconnected
Disconnected nodes
Choose the first element that is related to our preferred document
Maintain original order
Exit early if the nodes are identical
Parentless nodes are either documents or disconnected
"If the nodes are siblings, we can do a quick check"
Otherwise we need full lists of their ancestors for comparison
Walk down the tree looking for a discrepancy
Do a sibling check if the nodes have a common ancestor
Otherwise nodes in our document sort first
Set document vars if needed
IE 9's matchesSelector returns false on disconnected nodes
"As well, disconnected nodes are said to be in a document"
fragment in IE 9
Set document vars if needed
Set document vars if needed
Don't get fooled by Object.prototype properties (jQuery #13807)
"Unless we *know* we can detect duplicates, assume their presence"
Clear input after sorting to release objects
See https://github.com/jquery/sizzle/pull/225
"If no nodeType, this is expected to be an array"
Do not traverse comment nodes
Use textContent for elements
innerText usage removed for consistency of new lines (jQuery #11153)
Traverse its children
Do not include comment or processing instruction nodes
Can be adjusted by the user
Move the given value to match[3] whether quoted or unquoted
nth-* requires argument
numeric x and y parameters for Expr.filter.CHILD
remember that false/true cast respectively to 0/1
other types prohibit arguments
Accept quoted arguments as-is
Strip excess characters from unquoted arguments
Get excess from tokenize (recursively)
advance to the next closing parenthesis
excess is a negative index
Return only captures needed by the pseudo filter method (type and argument)
Shortcut for :nth-*(n)
:(first|last|only)-(child|of-type)
Reverse direction for :only-* (if we haven't yet done so)
non-xml :nth-child(...) stores cache data on `parent`
Seek `elem` from a previously-cached index
...in a gzip-friendly way
Support: IE <9 only
Defend against cloned attroperties (jQuery gh-1709)
Fallback to seeking `elem` from the start
"When found, cache indexes on `parent` and break"
Use previously-cached element index if available
...in a gzip-friendly way
Support: IE <9 only
Defend against cloned attroperties (jQuery gh-1709)
xml :nth-child(...)
or :nth-last-child(...) or :nth(-last)?-of-type(...)
Use the same loop as above to seek `elem` from the start
Cache the index of each encountered element
Support: IE <9 only
Defend against cloned attroperties (jQuery gh-1709)
"Incorporate the offset, then check against cycle size"
pseudo-class names are case-insensitive
http://www.w3.org/TR/selectors/#pseudo-classes
Prioritize by case sensitivity in case custom pseudos are added with uppercase letters
Remember that setFilters inherits from pseudos
The user may use createPseudo to indicate that
arguments are needed to create the filter function
just as Sizzle does
But maintain support for old signatures
Potentially complex pseudos
Trim the selector passed to compile
to avoid treating leading and trailing
spaces as combinators
Match elements unmatched by `matcher`
Don't keep the element (issue #299)
"""Whether an element is represented by a :lang() selector"
is based solely on the element's language value
"being equal to the identifier C,"
"or beginning with the identifier C immediately followed by ""-""."
The matching of C against the element's language value is performed case-insensitively.
"The identifier C does not have to be a valid language name."""
http://www.w3.org/TR/selectors/#lang-pseudo
lang value must be a valid identifier
Miscellaneous
Boolean properties
"In CSS3, :checked should return both checked and selected elements"
http://www.w3.org/TR/2011/REC-css3-selectors-20110929/#checked
Accessing this property makes selected-by-default
options in Safari work properly
Contents
http://www.w3.org/TR/selectors/#empty-pseudo
":empty is negated by element (1) or content nodes (text: 3; cdata: 4; entity ref: 5),"
but not by others (comment: 8; processing instruction: 7; etc.)
nodeType < 6 works because attributes (2) do not appear as children
Element/input types
Support: IE<8
"New HTML5 attribute values (e.g., ""search"") appear with elem.type === ""text"""
Position-in-collection
Add button/input type pseudos
Easy API for creating new setFilters
Comma and first run
Don't consume trailing commas as valid
Combinators
Cast descendant combinators to space
Filters
Return the length of the invalid excess
if we're just parsing
"Otherwise, throw an error or return tokens"
Cache the tokens
Check against closest ancestor/preceding element
Check against all ancestor/preceding elements
"We can't set arbitrary data on XML nodes, so they don't benefit from combinator caching"
Support: IE <9 only
Defend against cloned attroperties (jQuery gh-1709)
Assign to newCache so results back-propagate to previous elements
Reuse newcache so results back-propagate to previous elements
A match means we're done; a fail means we have to keep checking
Get initial elements from seed or context
"Prefilter to get matcher input, preserving a map for seed-results synchronization"
"If we have a postFinder, or filtered seed, or non-seed postFilter or preexisting results,"
...intermediate processing is necessary
...otherwise use results directly
Find primary matches
Apply postFilter
Un-match failing elements by moving them back to matcherIn
Get the final matcherOut by condensing this intermediate into postFinder contexts
Restore matcherIn since elem is not yet a final match
Move matched elements from seed to results to keep them synchronized
"Add elements to results, through postFinder if defined"
The foundational matcher ensures that elements are reachable from top-level context(s)
Avoid hanging onto element (issue #299)
Return special upon seeing a positional matcher
Find the next relative operator (if any) for proper handling
"If the preceding token was a descendant combinator, insert an implicit any-element `*`"
We must always have either seed elements or outermost context
Use integer dirruns iff this is the outermost matcher
Add elements passing elementMatchers directly to results
"Support: IE<9, Safari"
"Tolerate NodeList properties (IE: ""length""; Safari: <number>) matching elements by id"
Track unmatched elements for set filters
They will have gone through all possible matchers
"Lengthen the array for every element, matched or not"
"`i` is now the count of elements visited above, and adding it to `matchedCount`"
makes the latter nonnegative.
Apply set filters to unmatched elements
"NOTE: This can be skipped if there are no unmatched elements (i.e., `matchedCount`"
"equals `i`), unless we didn't visit _any_ elements in the above loop because we have"
no element matchers and no seed.
"Incrementing an initially-string ""0"" `i` allows `i` to remain a string only in that"
"case, which will result in a ""00"" `matchedCount` that differs from `i` but is also"
numerically zero.
Reintegrate element matches to eliminate the need for sorting
Discard index placeholder values to get only actual matches
Add matches to results
Seedless set matches succeeding multiple successful matchers stipulate sorting
Override manipulation of globals by nested matchers
Generate a function of recursive functions that can be used to check each element
Cache the compiled function
Save selector and tokenization
Try to minimize operations if there is only one selector in the list and no seed
(the latter of which guarantees us context)
Reduce context if the leading compound selector is an ID
"Precompiled matchers will still verify ancestry, so step up a level"
Fetch a seed set for right-to-left matching
Abort if we hit a combinator
"Search, expanding context for leading sibling combinators"
"If seed is empty or no tokens remain, we can return early"
Compile and execute a filtering function if one is not provided
Provide `match` to avoid retokenization if we modified the selector above
One-time assignments
Sort stability
Support: Chrome 14-35+
Always assume duplicates if they aren't passed to the comparison function
Initialize against the default document
Support: Webkit<537.32 - Safari 6.0.3/Chrome 25 (fixed in Chrome 27)
Detached nodes confoundingly follow *each other*
"Should return 1, but returns 4 (following)"
Support: IE<8
"Prevent attribute/property ""interpolation"""
https://msdn.microsoft.com/en-us/library/ms536429%28VS.85%29.aspx
Support: IE<9
"Use defaultValue in place of getAttribute(""value"")"
Support: IE<9
Use getAttributeNode to fetch booleans when getAttribute lies
EXPOSE
Sizzle requires that there be a global window in Common-JS like environments
EXPOSE
# sourceMappingURL=noty.min.js.map
Trim the opening space.
Replace the class name.
Trim the opening and closing spaces.
Opera 12.10 and Firefox 18 and later support
fix for Chrome < 45
"If len is 2, that means that we need to schedule an async flush."
"If additional callbacks are queued before the queue is flushed, they"
will be processed by this flush that we are scheduling.
test for web worker but not in IE10
node
node version 0.10.x displays a deprecation warning when nextTick is used recursively
see https://github.com/cujojs/when/issues/410 for details
vertx
web worker
Store setTimeout reference so es6-promise will be unaffected by
other code modifying setTimeout (like sinon.useFakeTimers())
Decide what async method to use to triggering processing of queued callbacks:
value === 1
value === 1
noop
"The array here would be [ 1, 2, 3 ];"
Code here never runs because there are rejected promises!
"error.message === ""2"""
result === 'promise 2' because it was resolved before promise1
was resolved.
Code here never runs
reason.message === 'promise 2' because promise 2 became rejected before
promise 1 became fulfilled
Code here doesn't run because the promise is rejected!
reason.message === 'WHOOPS'
Code here doesn't run because the promise is rejected!
reason.message === 'WHOOPS'
on success
on failure
on fulfillment
on rejection
on fulfillment
on rejection
user is available
"user is unavailable, and you are given the reason why"
"If `findUser` fulfilled, `userName` will be the user's name, otherwise it"
will be `'default name'`
never reached
"if `findUser` fulfilled, `reason` will be 'Found user, but still unhappy'."
"If `findUser` rejected, `reason` will be '`findUser` rejected and we're unhappy'."
never reached
never reached
The `PedgagocialException` is propagated all the way down to here
The user's comments are now available
"If `findCommentsByAuthor` fulfills, we'll have the value here"
"If `findCommentsByAuthor` rejects, we'll have the reason here"
success
failure
failure
success
success
failure
success
failure
failure
success
found books
something went wrong
synchronous
something went wrong
async with promises
something went wrong
silently ignored
Strange compat..
# sourceMappingURL=es6-promise.map
removed by extract-text-webpack-plugin
bind button events if any
ugly fix for progressbar display bug
it's in the queue
API functions
Document visibility change controller
shim for using process in browser
cached from whatever global is present so that test runners that stub it
don't break things.  But we need to wrap it in a try catch in case it is
wrapped in strict mode code which doesn't define any globals.  It's inside a
function because try/catches deoptimize in certain engines.
normal enviroments in sane situations
if setTimeout wasn't available but was latter defined
when when somebody has screwed with setTimeout but no I.E. maddness
When we are in I.E. but the script has been evaled so I.E. doesn't trust the global object when called normally
"same as above but when it's a version of I.E. that must have the global object for 'this', hopfully our context correct otherwise it will throw a global error"
normal enviroments in sane situations
if clearTimeout wasn't available but was latter defined
when when somebody has screwed with setTimeout but no I.E. maddness
When we are in I.E. but the script has been evaled so I.E. doesn't  trust the global object when called normally
"same as above but when it's a version of I.E. that must have the global object for 'this', hopfully our context correct otherwise it will throw a global error."
Some versions of I.E. have different rules for clearTimeout vs setTimeout
v8 likes predictible objects
This works in non-strict mode
This works if eval is allowed (see CSP)
This works if the window reference is available
"g can still be undefined, but nothing to do about it..."
"We return undefined, instead of nothing here, so it's"
easier to handle this case. if(!global) { ...}
# sourceMappingURL=noty.js.map
Trim the opening space.
Replace the class name.
Trim the opening and closing spaces.
Opera 12.10 and Firefox 18 and later support
fix for Chrome < 45
bind button events if any
ugly fix for progressbar display bug
it's in the queue
API functions
Document visibility change controller
# sourceMappingURL=bootstrap.bundle.min.js.map
# sourceMappingURL=bootstrap.min.js.map
eslint-disable-next-line no-bitwise
TODO: Remove in v5
Public
Public
Public
Don't call next when the page isn't visible
or the carousel or its parent isn't visible
"If it's a touch-enabled device, mouseenter/leave are fired as"
part of the mouse compatibility events on first tap - the carousel
would stop cycling until user tapped out of it;
"here, we listen for touchend, explicitly pause the carousel"
"(as if it's the second time we tap on it, mouseenter compat event"
is NOT fired) and after a timeout (to allow for mouse compatibility
events to fire) we explicitly restart cycling
"Some weirdness is happening, so we bail"
Public
preventDefault only for <a> elements (which change the URL) not inside the collapsible element
NOTE: 1 DOM access here
"Return body, `getScroll` will take care to get the correct `scrollTop` from it"
Firefox want us to check `-x` and `-y` variations as well
NOTE: 1 DOM access here
Skip hidden elements which don't have an offsetParent
.offsetParent will return the closest TD or TABLE in case
"no offsetParent is present, I hate this job..."
This check is needed to avoid errors in case one of the elements isn't defined for any reason
"Here we make sure to give as ""start"" the element that comes first in the DOM"
Get common ancestor container
Both nodes are inside #document
"one of the nodes is inside shadowDOM, find which one"
"IE10 10 FIX: Please, don't ask, the element isn't"
considered in DOM in some circumstances...
This isn't reproducible in IE10 compatibility mode of IE11
subtract scrollbar size from sizes
"if an hypothetical scrollbar is detected, we must be sure it's not a `border`"
we make this check conditional for performance reasons
"In cases where the parent is fixed, we must ignore negative scroll in offset calc"
Subtract margins of documentElement in case it's being used as parent
we do this only on HTML because it's the only element that behaves
differently when margins are applied to it. The margins are included in
"the box of the documentElement, in the other cases not."
Attach marginTop and marginLeft because in some circumstances we may need them
This check is needed to avoid errors in case one of the elements isn't defined for any reason
NOTE: 1 DOM access here
Handle viewport case
Handle other cases based on DOM element used as boundaries
"In case of HTML, we need a different computation"
"for all the other DOM elements, this one is good"
Add paddings
Get popper node sizes
"Add position, width and height to our offsets object"
depending by the popper placement we have to compute its offsets slightly differently
use native find if supported
use `filter` to obtain the same behavior of `find`
use native findIndex if supported
use `find` + `indexOf` if `findIndex` isn't supported
eslint-disable-line dot-notation
Add properties to offsets to make them a complete clientRect object
we do this before each modifier to make sure the previous one doesn't
mess with these values
"if popper is destroyed, don't perform any further update"
compute reference element offsets
"compute auto placement, store placement inside the data object,"
modifiers will be able to edit `placement` if needed
and refer to originalPlacement to know the original value
store the computed placement inside `originalPlacement`
compute the popper offsets
run the modifiers
the first `update` will call `onCreate` callback
the other ones will call `onUpdate` callback
touch DOM only if `applyStyle` modifier is enabled
remove the popper if user explicity asked for the deletion on destroy
do not use `remove` because IE11 doesn't support it
Resize event listener on window
Scroll event listener on scroll parents
Remove resize event listener on window
Remove scroll event listener on scroll parents
Reset state
add unit if the value is numeric and is one of the following
"any property present in `data.styles` will be applied to the popper,"
in this way we can make the 3rd party modifiers add custom styles to it
"Be aware, modifiers could override the properties defined in the previous"
lines of this modifier!
"any property present in `data.attributes` will be applied to the popper,"
they will be set as HTML attributes of the element
if arrowElement is defined and arrowStyles has some properties
compute reference element offsets
"compute auto placement, store placement inside the data object,"
modifiers will be able to edit `placement` if needed
and refer to originalPlacement to know the original value
Apply `position` to popper before anything else because
without the position applied we can't guarantee correct computations
Remove this legacy support in Popper.js v2
Styles
Avoid blurry text by using full pixel integers.
"For pixel-perfect positioning, top/bottom prefers rounded"
"values, while left/right prefers floored values."
"if gpuAcceleration is set to `true` and transform is supported,"
we use `translate3d` to apply the position to the popper we
automatically use the supported prefixed version if needed
"now, let's make a step back and look at this code closely (wtf?)"
"If the content of the popper grows once it's been positioned, it"
may happen that the popper gets misplaced because of the new content
overflowing its reference element
"To avoid this problem, we provide two options (x and y), which allow"
the consumer to define the offset origin.
"If we position a popper on top of a reference element, we can set"
`x` to `top` to make the popper grow towards its top instead of
its bottom.
"othwerise, we use the standard `top`, `left`, `bottom` and `right` properties"
Attributes
"Update `data` attributes, styles and arrowStyles"
arrow depends on keepTogether in order to work
"if arrowElement is a string, suppose it's a CSS selector"
"if arrowElement is not found, don't run the modifier"
if the arrowElement isn't a query selector we must check that the
provided DOM node is child of its popper node
""
extends keepTogether behavior making sure the popper and its
reference have enough pixels in conjuction
""
top/left side
bottom/right side
compute center of the popper
Compute the sideValue using the updated popper offsets
take popper margin in account because we don't have this info available
prevent arrowElement from being placed not contiguously to its popper
Get rid of `auto` `auto-start` and `auto-end`
"if `inner` modifier is enabled, we can't use the `flip` modifier"
"seems like flip is trying to loop, probably there's not enough space on any of the flippable sides"
using floor because the reference offsets may contain decimals we are not going to consider here
flip the variation if required
this boolean to detect any flip loop
"this object contains `position`, we want to preserve it along with"
any additional property we may add in the future
separate value from unit
"If it's not a number it's an operator, I guess"
"if is a vh or vw, we calculate the size based on the viewport"
"if is an explicit pixel unit, we get rid of the unit and keep the value"
"if is an implicit unit, it's px, and we return just the value"
Use height if placement is left or right and index is 0 otherwise use width
in this way the first offset will use an axis and the second one
will use the other one
Split the offset string to obtain a list of values and operands
"The regex addresses values with the plus or minus sign in front (+10, -20, etc)"
Detect if the offset string contains a pair of values or a single one
they could be separated by comma or space
"If divider is found, we divide the list of values and operands to divide"
them by ofset X and Y.
Convert the values with units to absolute pixels to allow our computations
Most of the units rely on the orientation of the popper
This aggregates any `+` or `-` sign that aren't considered operators
"e.g.: 10 + +5 => [10, +, +5]"
Here we convert the string values into number values (in px)
Loop trough the offsets arrays and execute the operations
"If offsetParent is the reference element, we really want to"
go one step up and use the next offsetParent as reference to
avoid to make this modifier completely useless and look like broken
NOTE: DOM access here
resets the popper's position so that the document size can be calculated excluding
the size of the popper element itself
NOTE: DOM access here
restores the original style properties after the offsets have been computed
"if shift shiftvariation is specified, run the modifier"
Avoid unnecessary DOM access if visibility hasn't changed
Avoid unnecessary DOM access if visibility hasn't changed
Utils
Methods
"make update() debounced, so that it only runs at most once-per-tick"
with {} we create a new object with the options inside it
init state
get reference and popper elements (allow jQuery wrappers)
Deep merge modifiers options
Refactoring modifiers' list (Object => Array)
sort the modifiers by order
modifiers have the ability to execute arbitrary code when Popper.js get inited
such code is executed in the same order of its modifier
they could add new properties to their options configuration
BE AWARE: don't add options to `options.modifiers.name` but to `modifierOptions`!
fire the first update to position the popper in the right place
"setup event listeners, they will take care of update the position in specific situations"
We can't use class properties because they don't get listed in the
class prototype and break stuff like Sinon stubs
Public
"to allow the menu to ""escape"" the scroll parent's boundaries"
https://github.com/twbs/bootstrap/issues/24251
empty mouseover listeners to the body's immediate children;
only needed because of broken event delegation on iOS
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html
empty mouseover listeners we added for iOS support
If not input/textarea:
- And not a key in REGEXP_KEYDOWN => not a dropdown command
If input/textarea:
- If space key => not a dropdown command
- If key is other than escape
- If key is not up or down => not a dropdown command
- If trigger inside the menu => not a dropdown command
Up
Down
Public
Don't move modal's DOM position
the following methods are used to handle overflowing modals
todo (fat): these should probably be refactored out of modal.js
----------------------------------------------------------------------
Note: DOMNode.style.paddingRight returns the actual value or '' if not set
while $(DOMNode).css('padding-right') returns the calculated value or 0 if not set
Restore fixed content padding
thx d.walsh
Only register focus restorer if modal will actually get shown
Public
empty mouseover listeners to the body's immediate children;
only needed because of broken event delegation on iOS
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html
empty mouseover listeners we added for iOS support
Content is a DOM node or a jQuery
Overrides
Getters
Public
TODO (fat): remove sketch reliance on jQuery position/offset
Set triggered link as active
With both <ul> and <nav> markup a parent is the previous sibling of any nav ancestor
Public
# sourceMappingURL=bootstrap.bundle.js.map
eslint-disable-next-line no-bitwise
TODO: Remove in v5
Public
Public
Public
Don't call next when the page isn't visible
or the carousel or its parent isn't visible
"If it's a touch-enabled device, mouseenter/leave are fired as"
part of the mouse compatibility events on first tap - the carousel
would stop cycling until user tapped out of it;
"here, we listen for touchend, explicitly pause the carousel"
"(as if it's the second time we tap on it, mouseenter compat event"
is NOT fired) and after a timeout (to allow for mouse compatibility
events to fire) we explicitly restart cycling
"Some weirdness is happening, so we bail"
Public
preventDefault only for <a> elements (which change the URL) not inside the collapsible element
Public
"to allow the menu to ""escape"" the scroll parent's boundaries"
https://github.com/twbs/bootstrap/issues/24251
empty mouseover listeners to the body's immediate children;
only needed because of broken event delegation on iOS
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html
empty mouseover listeners we added for iOS support
If not input/textarea:
- And not a key in REGEXP_KEYDOWN => not a dropdown command
If input/textarea:
- If space key => not a dropdown command
- If key is other than escape
- If key is not up or down => not a dropdown command
- If trigger inside the menu => not a dropdown command
Up
Down
Public
Don't move modal's DOM position
the following methods are used to handle overflowing modals
todo (fat): these should probably be refactored out of modal.js
----------------------------------------------------------------------
Note: DOMNode.style.paddingRight returns the actual value or '' if not set
while $(DOMNode).css('padding-right') returns the calculated value or 0 if not set
Restore fixed content padding
thx d.walsh
Only register focus restorer if modal will actually get shown
Public
empty mouseover listeners to the body's immediate children;
only needed because of broken event delegation on iOS
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html
empty mouseover listeners we added for iOS support
Content is a DOM node or a jQuery
Overrides
Getters
Public
TODO (fat): remove sketch reliance on jQuery position/offset
Set triggered link as active
With both <ul> and <nav> markup a parent is the previous sibling of any nav ancestor
Public
# sourceMappingURL=bootstrap.js.map
Public
empty mouseover listeners to the body's immediate children;
only needed because of broken event delegation on iOS
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html
empty mouseover listeners we added for iOS support
Content is a DOM node or a jQuery
# sourceMappingURL=tooltip.js.map
Public
preventDefault only for <a> elements (which change the URL) not inside the collapsible element
# sourceMappingURL=collapse.js.map
Public
Don't move modal's DOM position
the following methods are used to handle overflowing modals
todo (fat): these should probably be refactored out of modal.js
----------------------------------------------------------------------
Note: DOMNode.style.paddingRight returns the actual value or '' if not set
while $(DOMNode).css('padding-right') returns the calculated value or 0 if not set
Restore fixed content padding
thx d.walsh
Only register focus restorer if modal will actually get shown
# sourceMappingURL=modal.js.map
Overrides
Getters
# sourceMappingURL=popover.js.map
Public
# sourceMappingURL=tab.js.map
Public
"to allow the menu to ""escape"" the scroll parent's boundaries"
https://github.com/twbs/bootstrap/issues/24251
empty mouseover listeners to the body's immediate children;
only needed because of broken event delegation on iOS
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html
empty mouseover listeners we added for iOS support
If not input/textarea:
- And not a key in REGEXP_KEYDOWN => not a dropdown command
If input/textarea:
- If space key => not a dropdown command
- If key is other than escape
- If key is not up or down => not a dropdown command
- If trigger inside the menu => not a dropdown command
Up
Down
# sourceMappingURL=dropdown.js.map
Public
Don't call next when the page isn't visible
or the carousel or its parent isn't visible
"If it's a touch-enabled device, mouseenter/leave are fired as"
part of the mouse compatibility events on first tap - the carousel
would stop cycling until user tapped out of it;
"here, we listen for touchend, explicitly pause the carousel"
"(as if it's the second time we tap on it, mouseenter compat event"
is NOT fired) and after a timeout (to allow for mouse compatibility
events to fire) we explicitly restart cycling
"Some weirdness is happening, so we bail"
# sourceMappingURL=carousel.js.map
Public
# sourceMappingURL=alert.js.map
# sourceMappingURL=index.js.map
Public
# sourceMappingURL=button.js.map
Public
TODO (fat): remove sketch reliance on jQuery position/offset
Set triggered link as active
With both <ul> and <nav> markup a parent is the previous sibling of any nav ancestor
# sourceMappingURL=scrollspy.js.map
eslint-disable-next-line no-bitwise
TODO: Remove in v5
# sourceMappingURL=util.js.map
private
Protected
Getters
Public
If this is a touch-enabled device we add extra
empty mouseover listeners to the body's immediate children;
only needed because of broken event delegation on iOS
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html
If this is a touch-enabled device we remove the extra
empty mouseover listeners we added for iOS support
Protected
Content is a DOM node or a jQuery
Private
Static
Getters
Public
Private
It's a jQuery object
Static
preventDefault only for <a> elements (which change the URL) not inside the collapsible element
Getters
Public
Private
Don't move modal's DOM position
----------------------------------------------------------------------
the following methods are used to handle overflowing modals
todo (fat): these should probably be refactored out of modal.js
----------------------------------------------------------------------
Note: DOMNode.style.paddingRight returns the actual value or '' if not set
while $(DOMNode).css('padding-right') returns the calculated value or 0 if not set
Adjust fixed content padding
Adjust sticky content margin
Adjust body padding
Restore fixed content padding
Restore sticky content
Restore body padding
Static
Only register focus restorer if modal will actually get shown
Getters
Overrides
We use append for html objects to maintain js events
Private
Static
Getters
Public
Private
Static
Getters
Public
Disable totally Popper.js for Dropdown in Navbar
Check if it's jQuery element
"If boundary is not `scrollParent`, then set position to `static`"
"to allow the menu to ""escape"" the scroll parent's boundaries"
https://github.com/twbs/bootstrap/issues/24251
If this is a touch-enabled device we add extra
empty mouseover listeners to the body's immediate children;
only needed because of broken event delegation on iOS
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html
Private
Handle dropup
Disable Popper.js if we have a static display
Static
If this is a touch-enabled device we remove the extra
empty mouseover listeners we added for iOS support
eslint-disable-next-line complexity
If not input/textarea:
- And not a key in REGEXP_KEYDOWN => not a dropdown command
If input/textarea:
- If space key => not a dropdown command
- If key is other than escape
- If key is not up or down => not a dropdown command
- If trigger inside the menu => not a dropdown command
Getters
Public
Don't call next when the page isn't visible
or the carousel or its parent isn't visible
Private
"If it's a touch-enabled device, mouseenter/leave are fired as"
part of the mouse compatibility events on first tap - the carousel
would stop cycling until user tapped out of it;
"here, we listen for touchend, explicitly pause the carousel"
"(as if it's the second time we tap on it, mouseenter compat event"
is NOT fired) and after a timeout (to allow for mouse compatibility
events to fire) we explicitly restart cycling
"Some weirdness is happening, so we bail"
Static
Getters
Public
Private
Static
Getters
Public
Static
Getters
Public
TODO (fat): remove sketch reliance on jQuery position/offset
Private
eslint-disable-next-line arrow-body-style
Set triggered link as active
Set triggered links parents as active
With both <ul> and <nav> markup a parent is the previous sibling of any nav ancestor
Handle special case when .nav-link is inside .nav-item
Static
Shoutout AngusCroll (https://goo.gl/pxwQGp)
eslint-disable-next-line no-bitwise
Get transition-duration of the element
Return 0 if element or transition duration is not found
"If multiple durations are defined, take the first"
TODO: Remove in v5
"Determine the factor, which shifts the decimal point of x"
just behind the last significant digit
"Shift decimal point by multiplicatipon with factor, flooring, and"
division by factor
_ = pretty_print_object(obj)
Sloppy that I'm making a module-wide change here...
Determine the coordinates of the intersection rectangle
The intersection of two axis-aligned bounding boxes is always an
axis-aligned bounding box
Compute the area of both AABBs
Compute the intersection over union by taking the intersection
area and dividing it by the sum of prediction + ground-truth
areas - the intersection area.
######
""
cct_json_utils.py
""
Utilities for working with COCO Camera Traps .json databases
""
Format spec:
""
https://github.com/Microsoft/CameraTraps/blob/master/data_management/README.md#coco-cameratraps-format
""
######
%% Constants and imports
%% Classes
Collect all names
Make names unique and sort
cast location to string as the entries in locations are strings
Convert classnames to lowercase to simplify comparisons later
Normalize paths to simplify comparisons later
"Make custom replacements in filenames, typically used to"
accommodate changes in root paths between DB construction and use
## Build useful mappings to facilitate working with the DB
Category ID <--> name
Image filename --> ID
"Each image can potentially multiple annotations, hence using lists"
Image ID --> image object
Image ID --> annotations
...__init__
...class IndexedJsonDb
""
cct_json_to_filename_json.py
""
Converts a .json file in COCO Camera Traps format to a .json-formatted list of
relative file names.
""
%% Constants and environment
%% Main function
"json.dump(s,open(outputFilename,'w'))"
%% Command-line driver
%% Interactive driver
%%
""
download_lila_subset.py
""
"Example of how to download a list of files from LILA, e.g. all the files"
in a data set corresponding to a particular species.
""
%% Constants and imports
SAS URLs come from:
""
http://lila.science/?attachment_id=792
""
"In this example, we're using the Missouri Camera Traps data set"
This assumes you've downloaded the metadata file from LILA
"We will demonstrate two approaches to downloading, one that loops over files"
"and downloads directly in Python, another that uses AzCopy."
""
AzCopy will generally be more performant and supports resuming if the
transfers are interrupted.  It assumes that azcopy is on the system path.
Number of concurrent download threads (when not using AzCopy)
%% Environment prep and derived constants
%% Open the metadata file
%% Build a list of image files (relative path names) that match the target species
Retrieve the category ID we're interested in
Retrieve all the images that match that category
Retrieve image file names
%% Support functions
print('Skipping file {}'.format(fn))
"print('Downloading {} to {}'.format(url,target_file))"
%% Download those image files
"Write out a list of files, and use the azcopy ""list-of-files"" option to download those files"
this azcopy feature is unofficially documented at https://github.com/Azure/azure-storage-azcopy/wiki/Listing-specific-files-to-transfer
Loop over files
""
add_bounding_boxes_to_megadb.py
""
"Given pseudo-jsons containing the bounding box annotations made by iMerit, add them to"
a list of entries from the MegaDB sequences table.
the category map that comes in the pseudo-jsons
annotation_path points to a directory containing annotation pseudo-jsons
annotation_path points to a single annotation pseudo-json
each row in this pseudo-json is a COCO formatted entry for an image sequence
iMerit calls this field image_id; some of these are URL encoded
dataset = image_ref.split('dataset')[1].split('.')[0]  # prior to batch 10
lower-case all image filenames !
image_filename = image_ref.split('.img')[1].lower()  # prior to batch 10
dataset = image_ref.split('dataset')[1].split('.')[0]  # prior to batch 10
image_filename = image_ref.split('.img')[1].lower()  # prior to batch 10
how many boxes of each category?
check that all sequences are for a single dataset; each may need adjustment to how image
identifiers are mapped
""
add_bounding_boxes_to_json.py
""
This script takes a image database in the COCO Camera Traps format and merges in a set of bounding
box annotations in the format that iMerit uses (a .json where actually only each row is a valid json).
""
"If you need to update an existing bbox database, please get all the original annotation files and"
re-generate from scratch
""
%% Imports
%% Configurations and paths
images database
output bboxes database
annotation files (pseudo json) obtained from our annotation vendor that contain annotations for this dataset
None or a string or tuple of strings that is the prefix to all file_name of interest / in this dataset in the annotation files
functions for mapping the image_id in the annotation files (pseudo jsons) to the image_id used in the image DB
our img_id doesn't contain frame info
"batch3 - ""file_name"":""ASG0000019_0_S1_B06_R1_PICT0007.JPG"""
"batch5 and 7 - ""file_name"":""datasetsnapshotserengeti.seqASG000002m-frame0.imgS1_B06_R1_PICT0056.JPG"""
sometimes - 'datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG'
"id in DB (old_token): 'S6/J01/J01_R1/S6_J01_R1_IMAG0001', 'S1/B05/B05_R1/S1_B05_R1_PICT0036'"
specify which one to use for your dataset here
%% Load the image database and fill in DB info for the output bbox database
load the images database
%% Find the height and width of images from the annotation files
""
...if they are not available in the images DB
each row in this pseudo-json is a COCO formatted entry for an image sequence
%% Other functions required by specific datasets
the IDFG image database does not include images from unlabeled folders that were annotated with bounding boxes
%% Create the bbox database from all annotation files pertaining to this dataset
"for the incoming annotations, look up by category name (common) and convert to the numerical id used in our databases"
"for each annotation pseudo-json, check that the image it refers to exists in the original database"
each row in this pseudo-json is a COCO formatted entry for an image sequence
check that entry is for this dataset
category map for this entry in the annotation file - usually the same across all entries but just in case
rspb_add_image_entry(img_id)
use the image length and width in the image DB
"[top left x, top left y, width, height] in relative coordinates"
"add all images that have been sent to annotation, some of which may be empty of bounding boxes"
rspb_add_image_entry(db_image_id)
print('Image not found in origin dir at {}'.format(origin_path))
currently only supports one endpoint
if need to re-download a dataset's images in case of corruption
file_list_to_download = [i for i in file_list_to_download if i['dataset'] == 'rspb_gola']
need to create a new blob service for this dataset
"the SAS token can be just for the container, not the storage account"
- will be fine for accessing files in that container later
%% Common queries
This query is used when preparing tfrecords for object detector training
We do not want to get the whole seq obj where at least one image has bbox because some images in that sequence
will not be bbox labeled so will be confusing
"For public datasets to be converted to the CCT format, we get the whole seq object because"
sequence level attributes need to be included too. megadb/converters/megadb_to_cct.py handles
the case of bbox-only JSONs with the flag exclude_species_class
%% Parameters
use False for when the results file will be too big to store in memory or in a single JSON.
%% Script
initialize Cosmos DB client
execute the query
loop through and save the results
schema already checks that the min possible value of frame_num is 0
"if there are more than one image item, each needs a frame_num"
checks across all sequence items
per sequence item checks
load the schema
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory
load the schema
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory
checks across all datasets items
check for expiry date of the SAS keys
update the sequences
"`id` is from the database, as well as all attributes starting with _"
"if valuable sequence information is available, add them to the image"
required fields for an image object
add seq-level class labels for this image
add other sequence-level properties to each image too
add other image-level properties
... for im in seq['images']
... for seq in mega_db
consolidate categories
some property names have changed in the new schema
a dummy sequence ID will be generated if the image entry does not have a seq_id field
seq_id only needs to be unique within this dataset; MegaDB does not rely on it as the _id field
"""annotations"" fields are opened and have its sub-field surfaced one level up"
set the `dataset` property on each sequence to the provided dataset_name
check that the location field is the same for all images in a sequence
check which fields in a CCT image entry are sequence-level
image-level properties that really should be sequence-level
need to add (misidentified) seq properties not present for each image in a sequence to img_level_properties
"(some properties act like flags - all have the same value, but not present on each img)"
add the sequence-level properties to the sequence objects
not every sequence have to have all the seq_level_properties
get the value of this sequence-level property from the first image entry
check which fields are really dataset-level and should be included in the dataset table instead.
delete sequence-level properties that should be dataset-level
make all `class` fields lower-case; cast `seq_id` to type string in case they're integers
%% validation
"at first a dict of image_id: image_obj with annotations embedded, then it becomes"
an array of image objects
%% integrate the image DB
takes in image entries and species and other annotations in the image DB
convert the species category to explicit string name
there may be other fields in the annotation object
these fields should already be gotten from the image object
%% integrate the bbox DB
add any images that are not in the image DB
also add any fields in the image object that are not present already
add bbox to the annotations field
for any newly added images
"'bbox_abs': bbox_anno['bbox'],"
not keeping height and width
""
jb_csv_to_json.py
""
Convert a particular .csv file to CCT format.  Images were not available at
"the time I wrote this script, so this is much shorter than other scripts"
in this folder.
""
%% Constants and environment
%% Read source data
%% Confirm filename uniqueness (this data set has one label per image)
"Build up a map from filenames to a list of rows, checking image existence as we go"
%% Create CCT dictionaries
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
imageName = imageFilenames[0]
"As per above, this is convenient and appears to be true; asserting to be safe"
"Filenames look like ""290716114012001a1116.jpg"""
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
%% Create info struct
%% Write output
%% Sanity-check
%% Imports and constants
%% Load data
"with open(image_json,'r') as f:"
data = json.load(f)
%% Sanity-check data
%% Label previews
%% Collect images to annotate
%% Sort by sequence and frame
%% Copy to a folder by GUID
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension
im = images_to_annotate[0]
...for each image
%% Write out the annotation list
""
"Import a Snapshot Safari project (one project, one season)"
""
Before running this script:
""
"* Mount the blob container where the images live, or copy the"
images to local storage
""
What this script does:
""
* Creates a .json file
* Creates zip archives of the season without humans.
* Copies animals and humans to separate folders
""
After running this script:
""
* Create or update LILA page
* Push zipfile and unzipped images to LILA
* Push unzipped humans to wildlifeblobssc
* Delete images from UMN uplaod storage
%% Imports
From ai4eutils
From CameraTraps
%% Constants
project_name = 'XXX'; season_name = 'S1'; project_friendly_name = 'Snapshot Unknown'
project_name = 'KRU'; season_name = 'S1'; project_friendly_name = 'Snapshot Kruger'
project_name = 'CDB'; season_name = 'S1'; project_friendly_name = 'Snapshot Camdeboo'
project_name = 'MTZ'; season_name = 'S1'; project_friendly_name = 'Snapshot Mountain Zebra'
project_name = 'ENO'; season_name = 'S1'; project_friendly_name = 'Snapshot Enonkishu'
project_name = 'KAR'; season_name = 'S1'; project_friendly_name = 'Snapshot Karoo'
project_name = 'KGA'; season_name = 'S1'; project_friendly_name = 'Snapshot Kgalagadi'
%% Folder/file creation
E.g. KRU_S1
E.g. Z:\KRU
E.g. Z:\KRU\KRU_S1
Contains annotations for each capture event (sequence)
Maps image IDs to filenames; each line looks like:
""
"KRU_S1#1#1#2,3,KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0004.JPG"
"Total number of each answer to each question, e.g. total number of times each species was identified"
""
Not used here
Create output folders
Images will be placed in a season-specific folder inside this (the source data includes
this in path names)
%% Load metadata files
%% Convert to dictionaries (prep)
%% Convert to dictionaries (loops)
"TODO: iterrows() is a terrible way to do this, but this is one of those days"
"where I want to get this done, not get better at Python."
irow = 0; row = image_table.iloc[0]
"Loaded as an int64, converting to int here"
...for each row in the image table
Make sure image IDs are what we think they are
...for each row in the annotation table
%% Take a look at categories (just sanity-checking)
print('\nCategories by species:')
pp.pprint(categories_by_species)
%% Fill in some image fields we didn't have when we created the image table
"width, height, corrupt, seq_num_frames, location, datetime"
Every annotation in this list should have the same sequence ID
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;"
there's no reason to do this redundantly for every image
Every image in this sequence should point back to the same equence
Every annotation in this list should have the same location
Every annotation in this list should have the same datetime
Is this image on disk?
iImage = 0; im = images[0]
...for each image
"images_processed = pool.map(process_image, images)"
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))"
%% Count missing/corrupted images
%% Print distribution of sequence lengths (sanity-check)
%% Replicate annotations across images
iAnn = 0; ann = annotations[iAnn]
%% See what files are on disk but not annotated
%% Sanity-check image and annotation uniqueness
%% Minor updates to fields
%% Write .json file
%% Create a list of human files
ann = annotations[0]
%% Create public archive and public/private folders
im = images[0]
E.g. KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0001.JPG
Copy to private output folder
Add to zipfile
Possibly start a new archive
Copy to public output folder
...for each image
%% Sanity-check .json file
"This will produce some validation errors, because this zipfile doesn't include humans"
%% Zip up .json and .csv files
%% When I skip to this part (using a pre-rendered .json file)
%%
%%
ann = annotations[0]
%% Summary prep for LILA
"%% Generate preview, sanity-check labels"
"viz_options.classes_to_include = ['jackalblackbacked','bustardkori']"
%% Scrap
%% Find annotations for a particular image
%% Write a list of missing images
""
save_the_elephants_survey_A.py
""
Convert the .csv file provided for the Save the Elephants Survey A data set to a
COCO-camera-traps .json file
""
%% Constants and environment
Handle all unstructured fields in the source data as extra fields in the annotations
"photo_type really should be an image property, but there are a few conflicts"
that forced me to handle it as an annotation proprerty
%% Read source data
"%% Map filenames to rows, verify image existence"
Maps relative paths to row indices in input_metadata
"Build up a map from filenames to a list of rows, checking image existence as we go"
Ignore directories
%% Make sure the multiple-annotation cases make sense
%%
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
Force the empty category to be ID 0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image]
Example filename:
""
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG'
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG'
i_row = row_indices[0]
There are a small handful of datetime mismatches across annotations
for the same image
assert im['datetime'] == timestamp
Special cases based on the 'photo type' field
Various spellings of 'community'
Have we seen this category before?
Create an annotation
fieldname = list(mapped_fields.keys())[0]
...for each row
...for each image
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
%% Scrap
%% Find unique photo types
##
""
timelapse_csv_set_to_json.py
""
Given a directory full of reasonably-consistent Timelapse-exported
".csvs, assemble a CCT .json."
""
"Assumes that you have a list of all files in the directory tree, including"
image and .csv files.
""
##
%% Constants and imports
Text file with relative paths to all files (images and .csv files)
"%% Read file list, make a list of all image files and all .csv files"
"%% Verify column consistency, create a giant array with all rows from all .csv files"
i_csv = 0; csv_filename = csv_files[0]
%% Prepare some data structures we'll need for mapping image rows in .csv files to actual image files
Enumerate all folders containing image files
"In this data set, a site folder looks like:"
""
Processed Images\\site_name
%% Map .csv files to candidate camera folders
fn = valid_csv_files[0]
"Some site folders appear as ""XXNNNN"", some appear as ""XXNNNN_complete"""
...for each .csv file
%% Map camera folders to candidate image folders
%% Main loop over labels (prep)
Force the empty category to be ID 0
Images that are marked empty and also have a species label
%% Main loop over labels (loop)
i_row = 0; row = input_metadata.iloc[i_row]
"for i_row,row in input_metadata.iterrows():"
"Usually this is just a single folder name, sometimes it's a full path,"
which we don't want
Check whether this file exists on disk
Retrieve image width and height
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each row in the big table of concatenated .csv files
%% Fix cases where an image was annotated as 'unlabeled' and as something else
This annotation is 'unlabeled'
Was there another category associated with this image?
%% Check for un-annnotated images
Enumerate all images
list(relative_path_to_image.keys())[0]
%% Create info struct
%% Write output
%% Sanity-check the database's integrity
%% Render a bunch of images to make sure the labels got carried along correctly
""
auckland_doc_to_json.py
""
Convert Auckland DOC data set to COCO camera traps format
""
%% Constants and imports
Filenames will be stored in the output .json relative to this base dir
%% Enumerate files
%% Assemble dictionaries
Force the empty category to be ID 0
fn = image_files[0]; print(fn)
Typically y:\Maukahuka_Auckland_Island\1_Training\Winter_Trial_2019\cat\cat\eat\20190903_IDdY_34_E3_tmp_201908240051.JPG
"This data set has two top-level folders, ""1_Training"" (which has class names encoded"
"in paths) and ""2_Testing"" (which has no class information)."
...for each image
%% Write output .json
%% Write train/test .jsons
%% Validate .json files
%% Preview labels
checkpoint
""
awc_to_json.py
""
Convert a particular .csv file to CCT format.
""
%% Constants and environment
%% Read source data
%% Main loop over labels
Force the empty category to be ID 0
iRow = 0; row = input_metadata.iloc[iRow]
"ImageID,FileName,FilePath,SpeciesID,CommonName"
"This makes an assumption of one annotation per image, which happens to be"
true in this data set.
Retrieve image width and height
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
%% Check for images that aren't included in the metadata file
Enumerate all images
list(relativePathToImage.keys())[0]
%% Create info struct
%% Write output
%% Sanity-check the database's integrity
%% Render a bunch of images to make sure the labels got carried along correctly
""
wellington_to_json.py
""
Convert the .csv file provided for the Wellington data set to a
COCO-camera-traps .json file
""
%% Constants and environment
%% Read source data
"Filenames were provided as *.jpg, but images were *.JPG, converting here"
"%% Map filenames to rows, verify image existence"
"Takes ~30 seconds, since it's checking the existence of ~270k images"
"Build up a map from filenames to a list of rows, checking image existence as we go"
"I didn't expect this to be true a priori, but it appears to be true, and"
it saves us the trouble of checking consistency across multiple occurrences
of an image.
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
imageName = imageFilenames[0]
"As per above, this is convenient and appears to be true; asserting to be safe"
"Filenames look like ""290716114012001a1116.jpg"""
This gets imported as an int64
"These appear as ""image1"", ""image2"", etc."
"In the form ""001a"""
Can be in the form '111' or 's46'
"In the form ""7/29/2016 11:40"""
Check image height and width
"Use 'empty', to be consistent with other data on lila"
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
""
mcgill_to_json.py
""
Convert the .csv file provided for the McGill test data set to a
COCO-camera-traps .json file
""
%% Constants and environment
%% Read source data
"%% Map filenames to rows, verify image existence"
Create an additional column for concatenated filenames
Maps relative filenames to rows
"Build up a map from filenames to a list of rows, checking image existence as we go"
row = input_metadata.iloc[0]
"I didn't expect this to be true a priori, but it appears to be true, and"
it saves us the trouble of checking consistency across multiple occurrences
of an image.
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
row = input_metadata.iloc[0]
"Filenames look like ""290716114012001a1116.jpg"""
"In the form ""001a"""
Can be in the form '111' or 's46'
"In the form ""7/29/2016 11:40"""
Check image height and width
NaN is the only thing we should see that's not a string
NaN is the only thing we should see that's not a string
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
""
missouri_to_json.py
""
Create .json files from the original source files for the Missouri Camera Traps
data set.  Metadata was provided here in two formats:
""
"1) In one subset of the data, folder names indicated species names.  In Set 1,"
there are no empty sequences.  Set 1 has a metadata file to indicate image-level
bounding boxes.
""
2) A subset of the data (overlapping with (1)) was annotated with bounding
"boxes, specified in a whitespace-delimited text file.  In set 2, there are"
"some sequences omitted from the metadata file, which implied emptiness."
""
"In the end, set 2 labels were not reliable enough to publish, so LILA includes only set 1."
""
%% Constants and imports
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Filenames will be stored in the output .json relative to this base dir
"%% Enumerate files, read image sizes (both sets)"
"Takes a few minutes, since we're reading image sizes."
Each element will be a list of relative path/full path/width/height
"Only process leaf nodes corresponding to sequences, which look like:"
""
E:\wildlife_data\missouri_camera_traps\Set1\1.02-Agouti\SEQ75583
E:\wildlife_data\missouri_camera_traps\Set2\p1d101
""
assert len(files) <= 2
Read the image
Not an image...
Store file info
"...if we didn't hit the max file limit, keep going"
...for each file
%% Add sequence lengths (both sets)
%% Load the set 1 metadata file
"List of lists, length varies according to number of bounding boxes"
""
Preserves original ordering
iLine = 0; line = metadataSet1Lines[0]
"Lines should be filename, number of bounding boxes, boxes (four values per box)"
Make sure we have image info for this image
%% Print missing files from Set 1 metadata
Manual changes I made to the metadata file:
""
'IMG' --> 'IMG_'
Red_Brocket_Deer --> Red_Deer
European-Hare --> European_Hare
Wood-Mouse --> Wood_Mouse
Coiban-Agouti --> Coiban_Agouti
%% Load the set 2 metadata file
"This metadata file contains most (but not all) images, and a class label (person/animal/empty)"
"for each, plus bounding boxes."
"List of lists, length varies according to number of bounding boxes"
""
Preserves original ordering
"Create class IDs for each *sequence*, which we'll use to attach classes to"
images for which we don't have metadata
""
This only contains mappings for sequences that appear in the metadata.
iLine = 0; line = metadataSet2Lines[0]
"Lines should be filename, number of bounding boxes, labeled boxes (five values per box)"
""
Empty images look like filename\t0\t0
E.g. 'Set2\\p1d101\\p1d101s100i10.JPG'
Make sure we don't have mixed classes within an image
"Figure out what class this *sequence* is, so we know how to handle unlabeled"
images from this sequence
Can't un-do a mixed sequence
Previously-empty sequences get the image class label
"If the sequence has a non-empty class, possibly change it"
Make sure we have image info for this image
...for each line in the set 2 metadata file
%% What Set 2 images do I not have metadata for?
These are *mostly* empty images
iImage = 0; imageID = set2ImageIDs[iImage]
%% Create categories and annotations for set 1
"Though we have no empty sequences, we do have empty images in this set"
For each image
""
iImage = 0; imageID = set1ImageIDs[iImage]
E.g. Set1\\1.80-Coiban_Agouti\\SEQ83155\\SEQ83155_IMG_0010.JPG
Find the species name
This image may still be empty...
category['count'] = category['count'] + 1
"If we have bounding boxes, create image-level annotations"
"filename, number of bounding boxes, boxes (four values per box)"
"Make sure the relative filename matches, allowing for the fact that"
some of the filenames in the metadata aren't quite right
This image is non-empty
"Some redundant bounding boxes crept in, don't add them twice"
Check this bbox against previous bboxes
""
Inefficient?  Yes.  In an important way?  No.
"Bounding box values are in absolute coordinates, with the origin"
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1]."
""
"Convert to floats and to x/y/w/h, as per CCT standard"
...for each box
if we do/don't have boxes for this image
Else create a sequence-level annotation
...for each image
%% Create categories and annotations for set 2
For each image
""
iImage = 0; imageID = set2ImageIDs[iImage]
E.g. 'Set2\\p1d100\\p1d100s10i1.JPG'
"Find the sequence ID, sanity check filename against what we stored"
"If we have bounding boxes or an explicit empty label, create image-level annotations"
"filename, number of bounding boxes, labeled boxes (five values per box)"
"Make sure the relative filename matches, allowing for the fact that"
some of the filenames in the metadata aren't quite right
"Bounding box values are in absolute coordinates, with the origin"
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1]."
""
Convert to floats and to x/y/w/h
...for each box
...if we do/don't have boxes for this image
Else create a sequence-level annotation
...for each image
"%% The 'count' field isn't really meaningful, delete it"
"It's really the count of image-level annotations, not total images assigned to a class"
%% Write output .json files
%% Sanity-check final set 1 .json file
"python sanity_check_json_db.py --bCheckImageSizes --baseDir ""E:\wildlife_data\missouri_camera_traps"" ""E:\wildlife_data\missouri_camera_traps\missouri_camera_traps_set1.json"""
%% Generate previews
"Generate previewse:\wildlife_data\missouri_camera_traps\missouri_camera_traps_set1.json"" ""e:\wildlife_data\missouri_camera_traps\preview"" ""e:\wildlife_data\missouri_camera_traps"" --num_to_visualize 1000"
""
filenames_to_json.py
""
Take a directory of images in which species labels are encoded by folder
"names, and produces a COCO-style .json file"
""
%% Constants and imports
from the ai4eutils repo
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Filenames will be stored in the output .json relative to this base dir
"rawClassListFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_classes.csv')"
"classMappingsFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_class_mapping.csv')"
"%% Enumerate files, read image sizes"
Each element will be a list of relative path/full path/width/height
Read the image
Corrupt or not an image
Store file info
Write to output file
...for each image file
...csv file output
...if the file list is/isn't available
%% Enumerate classes
Maps classes to counts
We like 'empty' to be class 0
%% Assemble dictionaries
...for each category
Each element is a list of relative path/full path/width/height/className
...for each image
%% External class mapping
%% Write raw class table
cat = categories[0]
%% Read the mapped class table
"id, source, count, target"
"%% Make classMappings contain *all* classes, not just remapped classes"
cat = categories[0]
%% Create new class list
"Start at 1, explicitly assign 0 to ""empty"""
One-off issue with character encoding
%% Re-map annotations
ann = annotations[0]
%% Write output .json
%% Utilities
%%
Find images with a particular tag
%% Randomly sample annotations
""
nacti_fieldname_adjustments.py
""
"NACTI metadata was posted with ""filename"" in images instead of ""file_name"", and"
"used string (rather than int) category IDs (in categories, but not in annotations)."
""
This script fixes those issues and rev's the version number.
""
%% Constants and environment
%% Read .json file
"%% Rev version number, update field names and types"
%% Write json file
""
cct_field_adjustments.py
""
CCT metadata was posted with int locations instead of strings.
""
This script fixes those issues and rev's the version number.
""
%% Constants and environment
%% Read .json file
"%% Rev version number, update field names and types"
%% Write json file
%% Check output data file
""
carrizo_shrubfree_2018.py
""
Convert the .csv file provided for the Carrizo Mojave data set to a
COCO-camera-traps .json file
""
%% Constants and environment
%% Read source data
Original .csv file had superfluous spaces in column names
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
58 missing files (of 17652)
%% Check for images that aren't included in the metadata file
3012 of 20606 files are not in metadata
%% Create CCT dictionaries
Map categories to integer IDs
""
The category '0' is reserved for 'empty'
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
Each filename should just match one row
Don't include images that don't exist on disk
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
""
rspb_to_json.py
""
Convert the .csv file provided for the RSPB data set to a
COCO-camera-traps .json file
""
%% Constants and environment
[location] is an obfuscation
%% Create info struct
%% Read source data
metadataTable.columns.values
""
"array(['Project', 'inDir', 'FileName', 'Station', 'Camera',"
"'StationCameraFileName', 'DateTimeOriginal', 'DateReadable',"
"'outDir', 'filename_new', 'fileExistsAlready', 'CopyStatus',"
"'Species'], dtype=object)"
We'll populate these later
keys should be lowercase
"%% Enumerate images, confirm filename uniqueness"
"%% Update metadata filenames to include site and camera folders, check existence"
""
Takes ~1min
iRow = 0; row = metadataTable.iloc[iRow]
There's a bug in the metadata; the 'camera' column isn't correct.
camera = row['Camera']
"These appear as, e.g., '3.22e12'"
camera = str(int(float(camera)))
Let's pull this out of the file name instead
""
Filenames look like one of the following:
""
A1__03224850850507__2015-11-28__10-45-04(1).JPG
Bayama2PH__C05__NA(NA).JPG
assert(os.path.isfile(fullPath))
metadataTable.iloc[iRow] = row
Re-assemble into an updated table
%% Check for images that aren't included in the metadata file
Enumerate all images
Write to a text file
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
iRow = 0; row = metadataTable.iloc[iRow]
A1__03224850850507__2015-11-28__10-45-04(1).JPG
'A1\\03224850850507\\A1__03224850850507__2015-11-28__10-45-04(1).JPG'
Not currently populated
"Often -1, sometimes a semi-meaningful int"
A1
03224850850507
"In variable form, but sometimes '28/11/2015 10:45'"
Check image height and width
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Write output
%% Check database integrity
%% Preview a few images to make sure labels were passed along sensibly
%% One-time processing step: copy images to a flat directory for annotation
%%
""
pc_to_json.py
""
Convert a particular collection of .csv files to CCT format.
""
%% Constants and environment
%% Read and concatenate source data
List files
"List of dataframes, one per .csv file; we'll concatenate later"
i_file = 87; fn = input_files[i_file]
Concatenate into a giant data frame
%% List files
%% Main loop over labels (prep)
Force the empty category to be ID 0
%% Main loop over labels (loop)
iRow = 0; row = input_metadata.iloc[iRow]
"ImageID,FileName,FilePath,SpeciesID,CommonName"
assert os.path.isfile(full_path)
Retrieve image width and height
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
%% See what's up with missing files
s = list(image_relative_paths)[0]
s = missing_files[0]
%% Check for images that aren't included in the metadata file
%% Sample the database
%%
Collect the images we want
%% Create info struct
%% Write output
%% Sanity-check the database's integrity
%% Render a bunch of images to make sure the labels got carried along correctly
options.classes_to_exclude = ['unlabeled']
%% Write out a list of files to annotate
""
save_the_elephants_survey_B.py
""
Convert the .csv file provided for the Save the Elephants Survey B data set to a
COCO-camera-traps .json file
""
%% Constants and environment
input_base = r'/mnt/blobfuse/wildlifeblobssc/ste_2019_08_drop'
output_base = r'/home/gramener/survey_b'
Handle all unstructured fields in the source data as extra fields in the annotations
"photo_type really should be an image property, but there are a few conflicts"
that forced me to handle it as an annotation proprerty
%% Read source data
"%% Map filenames to rows, verify image existence"
"%% Map filenames to rows, verify image existence"
Maps relative paths to row indices in input_metadata
"Build up a map from filenames to a list of rows, checking image existence as we go"
Ignore directories
%% Make sure the multiple-annotation cases make sense
%%
%% Check for images that aren't included in the metadata file
Enumerate all images
%% Create CCT dictionaries
Force the empty category to be ID 0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image]
Example filename:
""
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG'
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG'
i_row = row_indices[0]
timestamp = row['Date']
There are a small handful of datetime mismatches across annotations
for the same image
assert im['datetime'] == timestamp
Special cases based on the 'photo type' field
Various spellings of 'community'
Have we seen this category before?
Create an annotation
fieldname = list(mapped_fields.keys())[0]
...for each row
...for each image
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
%% Scrap
%% Find unique photo types
""
carrizo_trail_cam_2017.py
""
"Convert the .csv files provided for the ""Trail Cam Carrizo"" 2017 data set to"
a COCO-camera-traps .json file.
""
%% Constants and environment
%% Read source data
Removing the empty records
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
908 missing files (of 60562)
%% Check for images that aren't included in the metadata file
105329 of 164983 files are not in metadata
%% Create CCT dictionaries
Map categories to integer IDs
""
The category '0' is reserved for 'empty'
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
Each filename should just match one row
Don't include images that don't exist on disk
Have we seen this category before?
Create an annotation
"The Internet tells me this guarantees uniqueness to a reasonable extent, even"
beyond the sheer improbability of collisions.
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Validate output
%% Preview labels
""
ena24_to_json_2017.py
""
Convert the ENA24 data set to a COCO-camera-traps .json file
""
%% Constants and environment
Temporary folders for human and non-human images
Clean existing output folders/zipfiles
%% Support functions
%% Read source data
"%% Map filenames to rows, verify image existence"
"Build up a map from filenames to a list of rows, checking image existence as we go"
%% Create CCT dictionaries
"Also gets image sizes, so this takes ~6 minutes"
""
"Implicitly checks images for overt corruptness, i.e. by not crashing."
Map categories to integer IDs (that's what COCO likes)
For each image
""
"Because in practice images are 1:1 with annotations in this data set,"
this is also a loop over annotations.
Check image height and width
"Each row is category, [box coordinates]"
"If there's just one row, loadtxt reads it as a 1d array; make it a 2d array"
with one row
Each row is a bounding box
Have we seen this category before?
Create an annotation
...for each bounding box
"This was here for debugging; nearly every instance is Human+Horse, Human+Vehicle,"
"or Human+Dog, but there is one Rabbit+Opossium, and a few Deer+Chicken!"
...for each image
Convert categories to a CCT-style dictionary
%% Create info struct
%% Write output
%% Create ZIP files for human and non human
%% Validate output
%% Preview labels
%% Imports and constants
configurations and paths
%% Helper functions
"dest_path = copy(source_path, dest_folder)"
num_workers = multiprocessing.cpu_count()
pool = ThreadPool(num_workers)
"results = pool.starmap(_copy_unzip, zip(sources, itertools.repeat(dest_folder)))"
""
print('Waiting for processes to finish...')
pool.close()
pool.join()
sequential
%% Command-line driver
if the blob container is already mounted on the VM
or you can download them using the storage Python SDK
key to the storage account should be stored in the environment variable AZ_STORAGE_KEY
""
eMammal_helpers.py
""
Support functions for processing eMammal metadata
""
%% Constants and imports
%% Support functions
"pad to a total of 3 digits if < 1000, or 4 digits otherwise"
img_frame is a string from the xml tree
"length 4 frame order is returned as is, others are left padded to be 3 digit long"
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels
pad to a total of 4 digits
img_frame is a string from the xml tree
"length 4 frame order is returned as is, others are left padded to be 3 digit long"
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels
others column
summer day hours: 6am - 7pm
others day hours: 7am - 6pm
""
make_eMammal_json.py
""
"Produces the COCO-formatted json database for an eMammal dataset, i.e. a"
"collection of folders, each of which contains a deployment_manifest.xml file."
""
"In this process, each image needs to be loaded to size it."
""
"To add bounding box annotations to the resulting database, use"
add_annotations_to_eMammal_json.py.
""
%% Constants and imports
"Either add the eMammal directory to your path, or run from there"
os.chdir(r'd:\git\CameraTraps\database_tools\eMammal')
import warnings
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
warnings.filterwarnings('ignore')
Should we run the image size retrieval in parallel?
%% Support functions
%% Main loop (metadata processing; image sizes are retrieved later)
deployment = folders[0]
sequence = image_sequences[0]
get species info for this sequence
add each image's info to database
img = images[0]
"some manifests don't have the ImageOrder info, but the info is in the file name"
full_img_id has no frame info
""
frame number only used in requests to iMerit for ordering
...for each image
...for each sequence
...for each deployment
%% Get image sizes
"'tasks' is currently a list of 2-tuples, with each entry as [image dictionary,path]."
""
"Go through that and copy just the image dictionaries to 'db_images', adding size"
information to each entry.  Takes a couple hours.
opening each image seems too fast for this multi-threaded version to be faster than sequential code.
%% Assemble top-level dictionaries
%% Write out .json
""
make_full_SS_json.py
""
Create a COCO-camera-traps .json file for Snapshot Serengeti data from
the original .csv files provided on Dryad.
""
%% Imports and constants
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays"
Count the number of images with multiple species
"%% Read image .csv file, format into a dictionary mapping images to capture events"
%% Create CCT-style .json
still need image width and height
...for each image
%% Write output files
""
make_per_season_SS_json.py
""
Create a COCO-camera-traps .json file for each Snapshot Serengeti season from
the original .csv files provided on Dryad.
""
%% Imports and constants
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays"
"%% Read image .csv file, format into a dictionary mapping images to capture events"
%% Create CCT-style .json
still need image width and height
...for each image ID
%% Write output files
...for each season
%% Configurations
"if the following two files are provided, only the tfrecord generation section will be executed"
proceed to generate tfrecords if True
approximate fraction to split the new entries by
categories in the database to include
"in addition, any images with a 'group' label will not be included to avoid confusion"
see 'image_contains_group' in create_tfrecords_format.py
%% Input validation
%% Convert the COCO Camera Trap format data to another json
that aligns with the fields in the resulting tfrecords
%% Make train/val/test splits
%% Write the tfrecords
want the file names of all tfrecords to be of the same format and length in each part
%% Parameters
%% Load the annotations queried from megadb
"%% Make the ""dataset"" required by create_tfrecords.py"
%% Create tfrecords
Construct a Reader to read examples from the .tfrecords file
Basic info
Print out the per class image counts
Can we detect if there any missing classes?
"We expect class id for each value in the range [0, max_class_id]"
So lets see if we are missing any of these values
Construct a Reader to read examples from the .tfrecords file
Reversed coordinates?
Too small of an area?
Basic info
"print(""Images with areas < 10:"")"
for img_id in images_with_small_bboxes:
print(img_id)
"print(""Images with reversed coordinates:"")"
for img_id in images_with_reversed_coords:
print(img_id)
for img_id in images_with_bbox_count_mismatch:
print(img_id)
""
read_from_tf_records.py
""
"Reads detection results from a tfrecords file of the style generated by the TFODAPI inference script,"
"and converts it to a .p file that's friendly to other tools in this repo, e.g. detection/detector_eval."
""
"Detection and ground truth bounding box coordinates are in the format of [ymin, xmin, ymax, xmax]."
""
coding: utf-8
In[1]:
In[2]:
In[ ]:
Defaults are not specified since both keys are required.
"image, label, height, width"
"print(sess.run([output['image/filename'], output['image/class/text'], output['image/class/label'], output['image/height'], output['image/width']]))"
tasks = list(glob.glob('/data/lila/nacti/cropped_tfrecords/t*'))
"for _ in tqdm.tqdm(p.imap(analyze_record, tasks, chunksize=10), total=len(tasks)):"
pass
Parallel(n_jobs=8)(delayed(analyze_record)(path) for path in list(glob.glob('/data/lila/nacti/cropped_tfrecords/t*')))
""
iterate_tf_records.py
""
Inherited from Visipedia tfrecords repo.
""
print(feature_key)
return a dictionary of the features
Construct a Reader to read examples from the .tfrecords file
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Required
Class label for the whole image
Objects
Bounding Boxes
Parts
Areas
Ids
Any extra data (e.g. stringified json)
Additional fields for the format needed by the Object Detection repository
"For explanation of the fields, see https://github.com/visipedia/tfrecords"
Additional fields for the format needed by the Object Detection repository
Create a single Session to run all image coding calls.
Initializes function that converts PNG to JPEG data.
Initializes function that decodes RGB JPEG data.
Convert the image data from png to jpg
Decode the image data as a jpeg image
Read the image file.
Clean the dirty data.
Decode the RGB JPEG.
Check that image converted to RGB
Each thread produces N shards where N = int(num_shards / num_threads).
"For instance, if num_shards = 128, and the num_threads = 2, then the first"
"thread would produce shards [0, 64)."
"Generate a sharded version of the file name, e.g. 'train-00002-of-00010'"
raise
Images in the tfrecords set must be shuffled properly
"Break all images into batches with a [ranges[i][0], ranges[i][1]]."
Launch a thread for each batch.
Create a mechanism for monitoring when all threads are finished.
Create a generic TensorFlow-based utility for converting all image codings.
A Queue to hold the image examples that fail to process.
Wait for all the threads to terminate.
Collect the errors
""
create_splits.py
""
"Based on a tfrecords_format json file of the database, creates 3 splits according to"
the specified fractions based on location (images from the same location should be in
one split) or based on images.
""
"If a previous split is provided (append_to_previous_split is True), the entries in"
"each split will be preserved, and new entries will be appended, so that new models"
can warm start with a model trained on the original splits.
""
%% Imports
%% Main function
"code below references 'locatioin' as the attribute to split on, but it works for any split_by attribute"
present in the image entryes
"find new locations and assign them to a split, without reassigning any previous locations"
do NOT sort the IDs to keep the shuffled order
""
create_tfrecords_format.py
""
This script converts a COCO formatted json database to another json file that would be the
input to create_tfrecords.py or similar scripts to create tf_records
""
%% Imports and environment
%% Main tfrecord generation function
Remap category IDs; TF needs consecutive category ids
Sanity-check number of empty annotations and annotation-less images
Images without annotations don't have bounding boxes
Images with annotations *may* have bounding boxes
"prepend the dataset name to image_id because after inference on val set, records from"
different datasets are stored in one tfrecord
Propagate optional metadata to tfrecords
checking to ignore any images that contain 'group' needs to happen before ignoring non-valid categories!
Only include valid categories
...for each annotation for the current image
...for each image
""
create_tfrecords_from_coco.py
""
This script creates a tfrecords file from a classification dataset in COCO format.
%% Imports and environment
%% Main tfrecord generation function
We remap all category IDs such that they are consecutive starting from zero
"If this is already the case for the input dataset, then the remapping will not"
"have any effect, i.e. the order of the classes will remain unchanged"
Propagate optional metadata to tfrecords
endfor each annotation for the current image
endfor each image
"Calculate number of shards to get the desired number of images per record,"
ensure it is evenly divisible by the number of threads
%% Command-line driver
%% Driver
#######
""
sanity_check_json_db.py
""
"Does some sanity-checking and computes basic statistics on a db, specifically:"
""
* Verifies that required fields are present and have the right types
* Verifies that annotations refer to valid images
* Verifies that annotations refer to valid categories
"* Verifies that image, category, and annotation IDs are unique"
""
* Optionally checks file existence
""
* Finds un-annotated images
* Finds unused categories
""
* Prints a list of categories sorted by count
""
#######
%% Constants and environment
%% Functions
"If baseDir is non-empty, checks image existence"
This is used in a medium-hacky way to share modified options across threads
print('Image path {} does not exist'.format(filePath))
"#%% Read .json file if necessary, sanity-check fields"
info = data['info']
"#%% Build dictionaries, checking ID uniqueness and internal validity as we go"
Confirm that required fields are present
Confirm ID uniqueness
...for each category
image = images[0]
Confirm that required fields are present
Confirm ID uniqueness
We previously supported ints here; this should be strings now
"assert isinstance(image['location'], str) or isinstance(image['location'], int), 'Illegal image location type'"
Are we checking for unused images?
Recursively enumerate images
print('Image {} is unused'.format(p))
Are we checking file existence and/or image size?
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)"
...for each image
Confirm that required fields are present
Confirm ID uniqueness
Confirm validity
...for each annotation
#%% Print statistics
Find un-annotated images and multi-annotation images
Find unused categories
Prints a list of categories sorted by count
https://stackoverflow.com/questions/72899/how-do-i-sort-a-list-of-dictionaries-by-a-value-of-the-dictionary
...def sanity_check_json_db()
%% Command-line driver
"python sanity_check_json_db.py ""e:\wildlife_data\wellington_data\wellington_camera_traps.json"" --baseDir ""e:\wildlife_data\wellington_data\images"" --bFindUnusedImages --bCheckImageSizes"
"python sanity_check_json_db.py ""D:/wildlife_data/mcgill_test/mcgill_test.json"" --baseDir ""D:/wildlife_data/mcgill_test"" --bFindUnusedImages --bCheckImageSizes"
"Here the '-u' prevents buffering, which makes tee happier"
""
python -u sanity_check_json_db.py '/datadrive1/nacti_metadata.json' --baseDir '/datadrive1/nactiUnzip/' --bFindUnusedImages --bCheckImageSizes | tee ~/nactiTest.out
%% Interactive driver(s)
%%
Sanity-check .json files for LILA
Sanity-check one file with all the bells and whistles
options.iMaxNumImages = 10
""
add_url_to_database.py
""
"Adds a ""url"" field to a coco-camera-traps .json database, specifically to allow the db to"
be reviewed in the Visipedia annotation tool.
""
""
remove_corrupted_images_from_database.py
""
"Given a coco-camera-traps .json file, checks all images for TF-friendliness and generates"
a new .json file that only contains the non-corrupted images.
""
%% Imports and constants
import multiprocessing
"I leave this at an annoying low number, since by definition weird stuff will"
"be happening in the TF kernel, and it's useful to keep having content in the console."
%% Function definitions
"I sometimes pass in a list of images, sometimes a dict with a single"
element mapping a job ID to the list of images
"We're about to start a lot of TF sessions, and we don't want gobs"
of debugging information printing out for every session.
At some point we were creating a single session and looping over images
"within that session, but the only way I found to reliably not run out"
of GPU memory was to create a session per image and gc.collect() after
each session.
Map Image IDs to boolean (should I keep this image?)
"Convert to lists, append job numbers to the image lists"
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)"
Merge results
%% Interactive driver
%%
base_dir = r'D:\temp\snapshot_serengeti_tfrecord_generation'
Load annotations
Check for corruption
Write out only the uncorrupted data
%% Command-line driver
""
combine_two_json_files.py
""
"Merges two coco-camera-traps .json files. In particular, categories are combined and re-numbered."
""
Combined Info
Combined Images
Combined Categories
## categories to merge
Combined Annotations
""
make_detection_db_for_viewing.py
""
"Given a .json file with ground truth bounding boxes, and a .p file containing detections for the same images,"
"creates a new .json file with separate classes for ground truth and detection, suitable for viewing in the Visipedia"
annotation tool.
""
%% Imports and constants
%% Main function
im_id_to_im = {im['id']:im for im in images}
make new categories to distinguish between ground truth and detections
"update all gt annotations to be class ""gt"""
collect all detections by image
keep any detection with score above det_thresh
"need to convert bbox from [x1,y1,x2,y2] to [x,y,w,h]"
"add ""info"" and ""licenses"" for annotation tools to function"
create new db
%% Command-line handling
""
analyze_json_database.py
""
Plots location/class/etc. distributions for classes in a coco-camera-traps .json file.
""
Currently includes some one-off code for specific species.
""
%% Constants and imports
%% Path configuration
%% Load source data
%% Build image/category dictionaries
%% Make plot of category distribution
%% make plots of location distribution
"plt.title('Number of images per location, by category')"
"plt.tight_layout(rect=[0,0,1,0.9])"
#make plot of images per season
%% Make plot of lions per location
%% Make plot of elephants per location
for loc in sorted_by_total[:25]:
"print('Location:' + loc[0] +', Lions: ' + str(len(loc_to_lion_ims[loc[0]])) + ', Elephants: ' + str(len(loc_to_elephant_ims[loc[0]])) + ', Total ims: ' + str(len(loc_to_ims[loc[0]])))"
""
subset_json_db.py
""
Select a subset of images (and associated annotations) from a .json file
in COCO Camera Traps format.
""
"To subset the .json files produced by our batch processing API, see"
subset_json_detector_output.py
""
Sample invocation:
""
"python subset_json_db.py ""E:\Statewide_wolf_container\idfg_20190409.json"" ""E:\Statewide_wolf_container\idfg_20190409_clearcreek.json"" ""clearcreek"" --ignore_case"
""
%% Constants and imports
%% Functions
Load the input file if necessary
Find images matching the query
Find annotations referring to those images
Write the output file if requested
%% Interactive driver
%%
%% Command-line driver
""
add_width_and_height_to_database.py
""
Grabs width and height from actual image files for a .json database that is missing w/h.
""
Originally used when we created a .json file for snapshot serengeti from .csv.
""
"This file converts the JSON output of the batch processing API to a pickle file, which can be used by the"
script ./make_classification_dataset.py
"Convert boxes from JSON   [x_min, y_min, width_of_box, height_of_box]"
"to PICKLE [ymin,  xmin,  ymax,         xmax]"
Write detections to file with pickle
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the"
script ./make_classification_dataset.py
Write detections to file with pickle
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the"
script ./make_classification_dataset.py
Assumes that the root of the CameraTrap repo is in the PYTHONPATH
Minimum threshold to put a detection into the output JSON file
Parameter check
Load detections from input
Load COCO style annotations
Build output JSON in format version 1.0
Adding the only known metadata info
"The pickle file does not contain category information, so we assume the default"
For each image with detections
for each detection
"Convert boxes from [ymin, xmin, ymax, xmax] format to"
"[x_min, y_min, width_of_box, height_of_box]"
Write output json
"Batch file for applying an object detection graph to a COCO style dataset,"
cropping images to the detected animals inside and creating a COCO-
style classification dataset out of it. It also saves the detections
to a file using pickle
#########################################################
## Configuration
Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing `PATH_TO_FROZEN_GRAPH` to point to a new .pb file.
"parser.add_argument('detections_output', type=str, default='detections_final.pkl',"
"help='Pickle file with the detections, which can be used for cropping later on.')"
#########################################################
## The actual code
Check arguments
/ai4edevfs/models/object_detection/faster_rcnn_inception_resnet_v2_atrous/megadetector/frozen_inference_graph.pb
"Detection threshold should be in [0,1]"
Padding around the detected objects when cropping
1.3 for the cropping during test time and 1.3 for
the context that the CNN requires in the left-over
image
Fraction of locations used for testing
Create output directories
Load a (frozen) Tensorflow model into memory.
Load COCO style annotations from the input dataset
"Get all categories, their names, and create an updated ID for the json file"
Prepare the coco-style json files
Split the dataset by locations
Load detections
TFRecords variables
The detection part
## Preparations: get all the output tensors
The following processing is only for single image
Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
For all images listed in the annotations file
Path to the input image
Skip the image if it is annotated with more than one category
Get category ID for this image
... and the corresponding category name
The remapped category ID for our json file
Whether it belongs to a training or testing location
Skip excluded categories
"If we already have detection results, we can use them"
Otherwise run detector
"We allow to skip images, which we do not have available right now"
This is useful for processing parts of large datasets
Load image
Run inference
"all outputs are float32 numpy arrays, so convert types as appropriate"
Add detections to the collection
Select detections with a confidence larger DETECTION_THRESHOLD
Skip if no detection selected
Get these boxes and convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes"
"However, we need to make sure that it box coordinates are still within the image"
"For each detected bounding box with high confidence, we will"
crop the image to the padded box and save it
"bbox is the detected box, crop_box the padded / enlarged box"
The file path as it will appear in the annotation json
Add numbering to the original file name if there are multiple boxes
The absolute file path where we will store the image
Only used if an coco-style dataset is created
Create the category directories if necessary
"if COCO_OUTPUT_DIR is set, then we will only use the shape"
of cropped_img in the following code. So instead of reading
cropped_img = np.array(Image.open(out_file))
we can speed everything up by reading only the size of the image
Read the image
Add annotations to the appropriate json
Propagate optional metadata to tfrecords
Write out COCO-style json files to the output directory
Write detections to file with pickle
json_file = TEST_JSON
js_keys = ['/'.join(im['file_name'].split('/')[1:])[:-4] for im in js['images']]
for tk in js_keys:
"assert np.isclose(1, np.sum(detections[tk]['detection_scores'] > 0.5))"
""
ocr_sandbox.py
""
sandbox for experimenting with using OCR to pull metadata from camera trap images
""
The general approach is:
""
"* Crop a fixed percentage from the top and bottom of an image, slightly larger"
than the largest examples we've seen of how much space is used for metadata.
""
"* Refine that crop by blurring a little, then looking for huge peaks in the"
"color histogram suggesting a solid background, then finding rows that are"
mostly that color.
""
"* Crop to the refined crop, then run pytesseract to extract text"
""
"* Use regular expressions to find time and date, in the future can add, e.g.,"
"temperature (which is often present *only* in the images, unlike time/date which"
are also usually in EXIF but often wrong or lost in processing)
""
"The metadata extraction (EXIF, IPTC) here is just sample code that seemed to"
belong in this file.
""
Contact: Dan Morris (dan@microsoft.com)
""
%% Constants and imports
pip install pytesseract
""
"Also intall tesseract from: https://github.com/UB-Mannheim/tesseract/wiki, and add"
"the installation dir to your path (on Windows, typically C:\Program Files (x86)\Tesseract-OCR)"
pip install IPTCInfo3
from the ai4eutils repo: https://github.com/Microsoft/ai4eutils
""
"Only used for writing out a summary, not important for the core metadata extraction"
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Using a semi-arbitrary metric of how much it feels like we found the
"text-containing region, discard regions that appear to be extraction failures"
Pad each crop with a few pixels to make tesseract happy
Discard text from the top
"When we're looking for pixels that match the background color, allow some"
tolerance around the dominant color
We need to see a consistent color in at least this fraction of pixels in our rough
crop to believe that we actually found a candidate metadata region.
"What fraction of the [top,bottom] of the image should we use for our rough crop?"
A row is considered a probable metadata row if it contains at least this fraction
"of the background color.  This is used only to find the top and bottom of the crop area,"
"so it's not that *every* row needs to hit this criteria, only the rows that are generally"
above and below the text.
%% Support functions
"%% Load some images, pull EXIF and IPTC data for fun"
%% Rough crop
"This will be an nImages x 1 list of 2 x 1 lists (image top, image bottom)"
image = images[0]
"l,t,r,b"
""
"0,0 is upper-left"
"%% Close-crop around the text, return a revised image and success metric"
Did we find a sensible mode that looks like a background value?
"This looks very scientific, right?  Definitely a probability?"
"print('Failed min background fraction test: {} of {}'.format(pBackGroundValue,minBackgroundFraction))"
"Notes to self, things I tried that didn't really go anywhere..."
"analysisImage = cv2.blur(analysisImage, (3,3))"
"analysisImage = cv2.medianBlur(analysisImage,5)"
"analysisImage = cv2.Canny(analysisImage,100,100)"
imagePil = Image.fromarray(analysisImage); imagePil
Use row heuristics to refine the crop
""
This egregious block of code makes me miss my fluency in Matlab.
"print('Cropping to {},{},{},{}'.format(x,y,w,h))"
Crop the image
"For some reason, tesseract doesn't like characters really close to the edge"
imagePil = Image.fromarray(croppedImage); imagePil
%% Go to OCR-town
"An nImages x 2 list of strings, extracted from the top and bottom of each image"
An nImages x 2 list of cropped images
iImage = 0; iRegion = 1; regionSet = imageRegions[iImage]; region = regionSet[iRegion]
text = pytesseract.image_to_string(region)
pil --> cv2
"image = cv2.medianBlur(image, 3)"
"image = cv2.erode(image, None, iterations=2)"
"image = cv2.dilate(image, None, iterations=4)"
"image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]"
"image = cv2.blur(image, (3,3))"
"image = cv2.copyMakeBorder(image,10,10,10,10,cv2.BORDER_CONSTANT,value=[0,0,0])"
"text = pytesseract.image_to_string(imagePil, lang='eng')"
https://github.com/tesseract-ocr/tesseract/wiki/Command-Line-Usage
"psm 6: ""assume a single uniform block of text"""
""
...for each cropped region
...for each image
%% Extract dates and times
s = '1:22 pm'
s = '1:23:44 pm'
%% Write results to a handy html file
Add image name and resized image
Add results and individual region images
"textStyle = ""font-family:calibri,verdana,arial;font-weight:bold;font-size:150%;text-align:left;margin-left:50px;"""
%% Scrap
Alternative approaches to finding the text/background  region
Using findCountours()
imagePil = Image.fromarray(analysisImage); imagePil
"analysisImage = cv2.erode(analysisImage, None, iterations=3)"
"analysisImage = cv2.dilate(analysisImage, None, iterations=3)"
"analysisImage = cv2.threshold(analysisImage, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]"
Find object with the biggest bounding box
Using connectedComponents()
analysisImage = image
print('Found {} components'.format(nb_components))
We just want the *background* image
open the file
read it
do the substitution
matplotlib.use('Agg')
from UIComponents.DBObjects import *
Initialize Database
# database connection credentials
# try to connect as USER to database DB_NAME through peewee
Load the saved embedding model
dataset_query = Detection.select().limit(5)
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})"
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getalllabels()) , dataset.getallpaths(), {})"
Random examples to start
"random_ids = np.random.choice(dataset.current_set, 1000, replace=False).tolist()"
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)"
print(random_ids)
Move Records
"moveRecords(dataset, DetectionKind.ModelDetection.value, DetectionKind.UserDetection.value, random_ids)"
#print([len(x) for x in dataset.set_indices])
# Finetune the embedding model
#dataset.set_kind(DetectionKind.UserDetection.value)
#dataset.train()
"#train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)"
"#finetune_embedding(model, checkpoint['loss_type'], dataset, 32, 4, 100)"
#save_checkpoint({
"#        'arch': model.arch,"
"#        'state_dict': model.state_dict(),"
"#        'optimizer' : optimizer.state_dict(),"
"#        'loss_type' : loss_type,"
"#        }, False, ""%s%s_%s_%04d.tar""%('finetuned', loss_type, model.arch, len(dataset.set_indices[DetectionKind.UserDetection.value])))"
Get indices of samples to get user to label
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
"kwargs[""already_selected""].extend(indices)"
Train on samples that have been labeled so far
Test on the samples that have not been labeled
"'optimizer' : optimizer.state_dict(),"
num_classes= len(train_dataset.getClassesInfo()[0])
"print(""Num Classes= ""+str(num_classes))"
define loss function (criterion) and optimizer
"optimizer = torch.optim.SGD(params, momentum = 0.9, lr = args.lr, weight_decay = args.weight_decay)"
optimizer.load_state_dict(checkpoint['optimizer'])
train for one epoch
"adjust_lr(optimizer,epoch)"
if epoch % 1 == 0 and epoch > 0:
"a, b, c = e.predict(train_embd_loader, load_info = True, dim = args.feat_dim)"
"plot_embedding(reduce_dimensionality(a), b, c, {})"
evaluate on validation set
"embedding_net = EmbeddingNet('resnet50', 256, True)"
compute output
val_loader = train_dataset.getSingleLoader(batch_size = 8)
"for a, b , c in val_loader:"
print(b[0])
"plt.imshow(np.rollaxis(np.rollaxis(a[0].numpy(), 1, 0), 2, 1))"
plt.show()
"print(np.rollaxis(a[0].numpy() , 1, 0).shape)"
--------some stuff needed to get AJAX to work with bottle?--------#
-------------------------------------------------------------------------------- #
PREPARE TO QUEUE IMAGES FOR LABELING
-------------------------------------------------------------------------------- #
# Connect as USER to database DB_NAME through peewee and initialize database proxy
# Load embedding model
---------------------------------------------------------------------- #
CREATE QUEUE OF IMAGES TO LABEL
---------------------------------------------------------------------- #
Use classifier to generate predictions
# Update model predicted class in PostgreSQL database
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
-------------------------------------------------------------------------------- #
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI
-------------------------------------------------------------------------------- #
"# static routes (to serve CSS, etc.)"
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps"
"return bottle.static_file(filename, root='../../../../../../../../../../../.')"
# dynamic routes
if data['display_grayscale']:
indices_to_exclude.update(set(color_indices))
elif not data['display_grayscale']:
indices_to_exclude.update(set(grayscale_indices))
data['display_images'] = {}
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices]
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices]
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices]
data['display_images']['detection_categories'] = []
for i in indices:
if str(dataset.samples[i][1]) == 'None':
data['display_images']['detection_categories'].append('None')
else:
existing_category_entries = {cat.id: cat.name for cat in Category.select()}
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()"
data['display_images']['detection_categories'].append(cat_name)
Use image ids in images_to_label to get the corresponding dataset indices
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
print(set(dataset.set_indices[4]).update(set(indices_to_label)))
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Use image id images_to_label to get the corresponding dataset index
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Train on samples that have been labeled so far
dataset.set_kind(DetectionKind.UserDetection.value)
print(y_train)
Predict on the samples that have not been labeled
print(y_pred)
Update model predicted class in PostgreSQL database
timer = time.time()
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
print('Updating the database took %0.2f seconds'%(time.time() - timer))
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
"once the classifier has been trained the first time, switch to AL sampling"
# sourceMappingURL=bootstrap.bundle.min.js.map
"print(ap_distances.size(),an_distances.size())"
losses = -(((-ap_distances)/128)+1+1e-16).log() - (((-(128-an_distances))/128)+1+1e-16).log()
import pdb
pdb.set_trace()
losses = ap_distances - an_distances + self.margin
print(losses.size())
from UIComponents.DBObjects import *
TODO: should this also change self.kind?
get the embedding representations for all samples (i.e. set current_set to all indices)
"return PILImage.open(os.path.join(self.img_base,path)).convert('RGB')"
"print(self.labels_set, self.n_classes)"
from sklearn.manifold import TSNE
embedding= TSNE(n_components=2).fit_transform(X)
embedding= PCA(n_components=2).fit_transform(X)
return X
"print(dir(event), type(sc))"
"print(label,bgcolor)"
"patches.append(mpatches.Patch(color=indexcolors[i], label=label))"
"plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,"
"ncol=12, mode=""expand"", borderaxespad=0., handles=patches)"
plt.legend(handles=patches)
"plt.xlabel('Dim 1', fontsize=12)"
"plt.ylabel('Dim 2', fontsize=12)"
plt.grid(True)
"plt.xlabel('Dim 1', fontsize=12)"
"plt.ylabel('Dim 2', fontsize=12)"
plt.grid(True)
print(thumb)
"img.thumbnail((16, 12), PILImage.ANTIALIAS)"
plt.show()
Add all negatives for all positive pairs
print(triplets.shape[0])
compute output
measure accuracy
compute loss on this batch
"train on a batch, record loss, and measure accuracy (if calc_accuracy)"
compute output
measure accuracy and record loss
switch to evaluate mode
compute output
switch to evaluate mode
"self.fc13 = nn.Linear(128, 64)"
self.bn2 = nn.BatchNorm1d(64)
x = F.relu(self.fc12(x))
x = F.relu(self.bn1(self.fc13(x)))
x = F.relu(self.fc13(x))
"x = F.dropout(x, training=self.training)"
save features last FC layer
x = F.relu(x)
save features last FC layer
import matplotlib.pyplot as plt
plt.switch_backend('agg')
class EmbeddingNet(nn.Module):
"def __init__(self, architecture, feat_dim, use_pretrained=False):"
"super(EmbeddingNet, self).__init__()"
self.feat_dim= feat_dim
self.inner_model = models.__dict__[architecture](pretrained=use_pretrained)
if architecture.startswith('resnet'):
in_feats= self.inner_model.fc.in_features
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)"
elif architecture.startswith('inception'):
in_feats= self.inner_model.fc.in_features
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)"
if architecture.startswith('densenet'):
in_feats= self.inner_model.classifier.in_features
"self.inner_model.classifier = nn.Linear(in_feats, feat_dim)"
if architecture.startswith('vgg'):
in_feats= self.inner_model.classifier._modules['6'].in_features
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)"
if architecture.startswith('alexnet'):
in_feats= self.inner_model.classifier._modules['6'].in_features
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)"
"def forward(self, x):"
return self.inner_model.forward(x)
class NormalizedEmbeddingNet(EmbeddingNet):
"def __init__(self, architecture, feat_dim, use_pretrained=False):"
"EmbeddingNet.__init__(self, architecture, feat_dim, use_pretrained = use_pretrained)"
"def forward(self, x):"
embedding =  F.normalize(self.inner_model.forward(x))*10.0
"return embedding, embedding"
"def get_random_images(num, image_dir, test_transforms):"
"data = datasets.ImageFolder(image_dir, transform=test_transforms) # slight abuse; this expects subfolders corresponding to classes but we have no classes here"
indices = list(range(len(data)))
np.random.shuffle(indices)
idx = indices[:num]
from torch.utils.data.sampler import SubsetRandomSampler
sampler = SubsetRandomSampler(idx)
"loader = torch.utils.data.DataLoader(data,"
"sampler=sampler, batch_size=num)"
dataiter = iter(loader)
"images, labels = dataiter.next()"
"return images, labels"
"def predict_image(image, model, test_transforms):"
"device = torch.device(""cuda"" if torch.cuda.is_available()"
"else ""cpu"")"
image_tensor = test_transforms(image).float()
image_tensor = image_tensor.unsqueeze_(0)
input = Variable(image_tensor)
input = input.to(device)
output = model(input)[0]
return output.data.cpu().numpy()
Connect to database and initialize db_proxy
# database connection credentials
# load the dataset
Load the saved embedding model from the checkpoint
# update the dataset embedding
# Create a folder for saving embedding visualizations with this model checkpoint
model_emb_dirname = os.path.basename(args.base_model).split('.')[0]
"os.makedirs(model_emb_dirname, exist_ok=True)"
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')"
dataset.embedding_mode()
"assert 2==3, 'break'"
datasetindices = list(range(len(dataset)))
np.random.shuffle(datasetindices)
random_indices = datasetindices[:args.num]
print(random_indices)
"selected_sample_features = np.array([]).reshape(0, 256)"
selected_sample_labels = []
for idx in random_indices:
"selected_sample_features = np.vstack([selected_sample_features, X_train[idx]])"
selected_sample_labels.append(y_train[idx])
img_path = imagepaths[idx].split('.JPG')[0]
image = dataset.loader(img_path)
selected_sample_images.append(image)
# TRY NEAREST NEIGHBORS WALK THROUGH EMBEDDING
nbrs = NearestNeighbors(n_neighbors=args.num).fit(selected_sample_features)
"distances, indices = nbrs.kneighbors(selected_sample_features)"
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')"
"idx_w_closest_nbr = np.where(distances[:,1] == min(distances[:,1]))[0][0]"
order = [idx_w_closest_nbr]
for ii in range(len(distances)):
"distances[ii, 0] = np.inf"
while len(order)<args.num:
curr_idx = order[-1]
curr_neighbors = indices[curr_idx]
curr_dists = list(distances[curr_idx])
# print(min(curr_dists))
next_closest_pos = curr_dists.index(min(curr_dists))
next_closest = curr_neighbors[next_closest_pos]
order.append(next_closest)
# make sure you can't revisit past nodes
for vi in order:
vi_pos = list(indices[next_closest]).index(vi)
"distances[next_closest, vi_pos] = np.inf"
for ii in range(len(order)):
imgidx = order[ii]
image = selected_sample_images[imgidx]
"image.save(model_emb_dirname+""/img""+str(ii)+""_""+str(selected_sample_labels[imgidx])+"".png"")"
# Specify the transformations on the input images before inference
"# test_transforms = transforms.Compose([transforms.Resize([224, 224]), transforms.ToTensor()])"
"test_transforms = transforms.Compose([transforms.Resize([256, 256]), transforms.RandomCrop([224, 224]), transforms.RandomHorizontalFlip(), transforms.ColorJitter(), transforms.ToTensor(), transforms.Normalize([0.407328, 0.407328, 0.407328], [0.118641, 0.118641, 0.118641])])"
"images, labels = get_random_images(num, image_dir, test_transforms)"
"all_features = np.array([]).reshape(0, 256)"
for ii in range(len(images)):
image = to_pil(images[ii])
"features = predict_image(image, model, test_transforms)"
"all_features = np.vstack([all_features, features])"
# for ii in range(len(images)):
#     image = to_pil(images[ii])
"#     image.save(""img""+str(ii)+"".png"")"
# TRY CLUSTERING
kmeans1 = KMeans(n_clusters=5).fit(StandardScaler().fit_transform(all_features))
print(kmeans1.labels_)
for ii in range(len(images)):
image = to_pil(images[ii])
"filename = str(kmeans1.labels_[ii])+""/img""+str(ii)+"".png"""
if not os.path.exists(os.path.dirname(filename)):
os.makedirs(os.path.dirname(filename))
image.save(filename)
class SaveFeatures():
"def __init__(self, module):"
self.hook = module.register_forward_hook(self.hook_fn)
"def hook_fn(self, module, input, output):"
"self.features = torch.tensor(output, requires_grad=True).cuda()"
def close(self):
self.hook.remove()
Load the saved embedding model from the checkpoint
"Get a sample from the database, with eval transforms applied, etc."
Connect to database and sample a dataset
output = model.forward(sample_image.unsqueeze(0))
print(output)
with torch.no_grad():
sample_image_input = sample_image.cuda(non_blocking=True)
"_, output = model(sample_image_input) # compute output"
print(output)
sample_image = PILImage.open(sample_image_path).convert('RGB')
"sample_image = transforms.Compose([Resize([256, 256]), CenterCrop(([[224,224]])), ToTensor(), Normalize([0.369875, 0.388726, 0.347536], [0.136821, 0.143952, 0.145229])])(sample_image)"
print(list(model_inner_resnet.children()))
print(model_inner_resnet.fc)
print(model_inner_resnet.fc0)
# print(model_inner_resnet.layer4[0].conv2)
# print(type(model))
# print(len(list(model_inner_resnet.children())))
# print(list(model.children()))
# print(list(list(model.children())[0].children()))
"img = np.uint8(np.random.uniform(150, 180, (56, 56, 3)))/255"
"img_tensor = torch.unsqueeze(torch.from_numpy(img), 0)"
full_out = model_inner_resnet.forward(img_tensor)
print(full_out)
model(img_tensor)
activations = SaveFeatures(model_inner_resnet.layer4[0].conv2)
print(activations.features)
print(type(activations.features))
activations.close()
Connect to database and sample a dataset
Load the saved embedding model from the checkpoint
Update the dataset embedding
Get a random query image
# # # IMAGES IN THE SAME SEQUENCE # # # #
"assert 2==3, 'break'"
# # # CLOSEST IN (EMBEDDING) FEATURE SPACE # # # #
Connect to database DB_NAME as USER and initialize tables
Populate Info table
Populate Category table
"# For now, we have a predefined list of species we expect to see in the camera trap database (e.g. maybe from a quick look through the images)"
# TODO: allow user to update the class list through the labeling tool UI as they see different species
Populate Image and Detection tables
Image entry data
Detection entry data
Connect to database DB_NAME as USER and initialize tables
Populate Info table
Populate Category table
# Create category records from species present in the COCO camera trap classification dataset (COCO classes minus any excluded classes)
# TODO: allow user to update the class list through the labeling tool UI as they see different species
Populate Image and Detection tables
"with open(os.path.join(args.crop_dir,'crops.json'), 'r') as infile:"
crops_json = json.load(infile)
counter = 0
timer = time.time()
num_detections = len(crops_json)
for detectionid in crops_json:
counter += 1
detection_data = crops_json[detectionid]
# Image entry data
existing_image_entries = Image.select().where((Image.file_name == detection_data['file_name']))
try:
existing_image_entry = existing_image_entries.get()
except:
"image_entry = Image.create(id=detectionid, file_name=detection_data['file_name'], width=detection_data['width'], height=detection_data['height'], grayscale=detection_data['grayscale'],"
"source_file_name=detection_data['source_file_name'], relative_size=detection_data['relative_size'],"
"seq_id=detection_data['seq_id'], seq_num_frames=detection_data['seq_num_frames'], frame_num=detection_data['frame_num'])"
image_entry.save()
# Detection entry data
"detection_entry = Detection.create(id=detectionid, image=detectionid, bbox_confidence=detection_data['bbox_confidence'],"
"bbox_X1=detection_data['bbox_X1'], bbox_Y1=detection_data['bbox_Y1'], bbox_X2=detection_data['bbox_X2'], bbox_Y2=detection_data['bbox_Y2'],"
kind=DetectionKind.ModelDetection.value)
detection_entry.save()
if counter%100 == 0:
"print('Updated database with Image and Detection table entries for %d out of %d crops in %0.2f seconds'%(counter, num_detections, time.time() - timer))"
# data related to original image
Get class names from .txt list
Initialize Oracle table
Map filenames to classes (NOTE: we assume a single image does not contain more than one class)
"For each detection, use source image path to get class"
TODO update: Assumes that crops have already
been generated for the images using make_active_learning_classification_dataset.py. The created DB contains tables:
- info: information about the dataset
- image: images present in the dataset
- detections: crops of images with detections with confidence greater than a specified threshold
Initialize Database
# database connection credentials
HOST = 'localhost'
PORT = 5432
"# first, make sure the (user, password) has been created"
"# sudo -u postgres psql -c ""CREATE USER <db_user> WITH PASSWORD <db_password>;"""
"# sudo -u postgres psql -c ""CREATE DATABASE <db_name> WITH OWNER <db_user> CONNECTION LIMIT -1;"""
"# sudo -u postgres psql -c ""GRANT CONNECT ON DATABASE <db_name> TO <db_user>;"""
"# sudo -u postgres psql -d <db_name> -c ""CREATE EXTENSION IF NOT EXISTS \""uuid-ossp\"";"""
# Try to connect as USER to database DB_NAME through peewee
Populate Tables
# create Info table
# get class names for Category table
Faster anD available in Python 3.5 and above
# iterate through images in each class folder
killing this process after over 38 hours adding over 500k white-tailed deer crops from emammal
resuming for remaining classes
# get cropped image data for Image table
"if mean of each channel is about the same, image is likely grayscale"
# still have no info on these:
seq_id = CharField(null= True)                # sequence identifier for the original image
seq_num_frames = IntegerField(null = True)    # number of frames in sequence
frame_num = IntegerField(null = True)         # which frame number in sequence
location = CharField(null = True)             # location of camera trap
datetime = DateTimeField(null = True)
# store info about the detection corresponding to this image
# store info about the true labels for the detection
#  - for pretrain dataset this is the same as the detection_category if the detection categories
print(classes)
Connect to database and sample a dataset
Load the saved embedding model from the checkpoint
Update the dataset embedding
save the images
save the features
"with open(os.path.join(args.output_dir, 'lastlayer_features.mat'), 'wb') as f:"
"pickle.dump(sample_features, f)"
"with open(os.path.join(args.output_dir, 'labels.mat'), 'wb') as f:"
"pickle.dump(sample_labels, f)"
"parser.add_argument('--db_name', default='missouricameratraps', type=str, help='Name of the training (target) data Postgres DB.')"
"parser.add_argument('--db_user', default='user', type=str, help='Name of the user accessing the Postgres DB.')"
"parser.add_argument('--db_password', default='password', type=str, help='Password of the user accessing the Postgres DB.')"
"parser.add_argument('--base_model', type=str, help='Path to latest embedding model checkpoint.')"
"parser.add_argument('--output_dir', type=str, help='Output directory for subset of crops')"
Add json entry for this crop
Copy file for this crop to subset dataset crop dir
Copy file for its full-size source image to subset dataset image dir
Write crops.json to subset dataset crop dir
store info about the crops produced in a JSON file
------------------------------------------------------------------------------------------------------------#
COMMENT OUT IF NOT USING A SPECIFIC PROJECT WITHIN ROBERT LONG EMAMMAL DATASET
------------------------------------------------------------------------------------------------------------#
get some information about the source image
------------------------------------------------------------------------------------------------------------#
NOTE: EDIT THIS SECTION BASED ON DATASET SOURCE
get info about sequence the source image belongs to from path and directory
# missouricameratraps:
imgframenum = int(os.path.basename(imgfile).split('.JPG')[0].split('_')[-1])
imgseqid = int(os.path.split(os.path.dirname(imgfile))[-1])
"imgseqnumframes = len([name for name in os.listdir(os.path.dirname(imgfile)) if os.path.isfile(os.path.join(os.path.dirname(imgfile), name))])"
# emammal:
------------------------------------------------------------------------------------------------------------#
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Variables for the hierarchical cluster
Variables for the AL algorithm
Data variables
"connectivity = kneighbors_graph(self.transformed_X,max_features)"
Fit cluster and update cluster variables
The sklearn hierarchical clustering algo numbers leaves which correspond
to actual datapoints 0 to n_points - 1 and all internal nodes have
ids greater than n_points - 1 with the root having the highest node id
"If no labels have been observed, simply return uniform distribution"
"If no observations, return worst possible upper lower bounds"
Loop through generations from bottom to top
Update admissible labels for node
Calculate score
Determine if node should be split
Make sure label set for node so that we can flow to children
if necessary
Only split if all ancestors are admissible nodes
This is part  of definition of admissible pruning
Check that pruning covers all leave nodes
Fill in labels
Observe labels for previously recommended batches
TODO(lishal): implement multiple selection methods
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
A list of initialized samplers is allowed as an input because
"for AL_methods that search over different mixtures, may want mixtures to"
have shared AL_methods so that initialization is only performed once for
computation intensive methods like HierarchicalClusteringAL and
states are shared between mixtures.
"If initialized samplers are not provided, initialize them ourselves."
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copy these objects to make sure they are not modified while simulating
trajectories as they are used later by the main run_experiment script.
Assumes that model has already by fit using all labeled data so
the probabilities can be used immediately to hallucinate labels
All models need to have predict_proba method
Hallucinate labels for selected datapoints to be label
using class probabilities from model
"Not saving already_selected here, if saving then should sort"
only for the input to fit but preserve ordering of indices in
already_selected
Useful to know how accuracy compares for model trained on hallucinated
labels vs trained on true labels.  But can remove this train to speed
up simulations.  Won't speed up significantly since many more models
are being trained inside the loop above.
Save trajectory for reference
Delete created copies
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER
Should check model but kernel_svm does not have coef_ so need better
handling here
Make sure that model object fed in did not change during simulations
Return indices based on return type specified
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Set gamma for gaussian kernel to be equal to 1/n_features
kneighbors graph is constructed using k=10
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of"
"another point, make it vice versa"
Graph edges are weighted by applying gaussian kernel to manhattan dist.
"By default, gamma for rbf kernel is equal to 1/n_features but may"
get better results if gamma is tuned.
Define graph density for an observation to be sum of weights for all
edges to the node representing the datapoint.  Normalize sum weights
by total number of neighbors.
"If a neighbor has already been sampled, reduce the graph density"
for its direct neighbors to promote diversity.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update observed reward and arm probabilities
Sample an arm
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
y only used for determining how many clusters there should be
probably not practical to assume we know # of classes before hand
should also probably scale with dimensionality of data
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
TODO(lishal): have MarginSampler and this share margin function
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Probably okay to always use MiniBatchKMeans
Should standardize data before clustering
Can cluster on standardized data but train on raw features if desired
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Update min_distances for all examples given new cluster center.
Assumes that the transform function takes in original data and not
flattened data.
Initialize centers with a randomly selected datapoint
New examples should not be in already selected since those points
should have min_distance of zero to a cluster center.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
This is uniform given the remaining pool but biased wrt the entire pool.
sample = [i for i in range(self.X.shape[0]) if i not in already_selected]
return sample[0:N]
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Fields for hierarchical clustering AL
Setting parent and storing nodes in dict for fast access
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Copyright 2017 Google Inc.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
Faster and available in Python 3.5 and above
transform_list.append(CenterCrop((processed_size)))
transform_list.append(Lambda(lambda X: normalize(X)))
"print(self.labels_set, self.n_classes)"
"from PyQt5 import QtCore, QtWidgets,QtGui"
from collections import deque
from peewee import *
from UIComponents.Tag import Tag
policy.setHeightForWidth(True)
"print(self.tab1.parentWidget(),self)"
self.tab4.add.clicked.connect(self.addSpecies)
self.tab4.update.clicked.connect(self.updateSpecies)
checkpoint= load_checkpoint('../merge/triplet_model_0054.tar')
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))"
"embedding_net = EmbeddingNet(checkpoint['arch'], checkpoint['feat_dim'])"
if checkpoint['loss_type'].lower()=='center':
"model = torch.nn.DataParallel(ClassificationNet(embedding_net, n_classes=14)).cuda()"
else:
model= torch.nn.DataParallel(embedding_net).cuda()
model.load_state_dict(checkpoint['state_dict'])
self.parentWidget().progressBar.setMaximum(len(run_dataset)//2048)
"e=Engine(model,None,None, verbose=True,progressBar= self.parentWidget().progressBar)"
label = [ x[1] for x in run_dataset.samples]
"print(indices,selected_set)"
print(query.sql())
src.delete().where(src.image_id<<rList))
"det= UserDetection.create(category_id=0, id=str(index+label[1][2]),image_id=final[0], bbox_X=label[1][0], bbox_Y=label[1][1], bbox_W=label[1][2], bbox_H=label[1][3])"
for x in self.tab1.grid.tags:
x.delete_instance()
db.create_tables([Detection])
This is simply to show the bar
"p = Process(target=ex.active, args=())"
p.start()
p.join()
ex.active()
ex.centralWidget().setCurrentIndex(1)
main()
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))"
print(row)
"unq_id= ""crops_""+str(uuid.uuid1())"
"print(line,imageWidth,imageHeight)"
"print(""%s,%s,%s,%0.6f,%0.6f,%0.6f,%0.6f,%0.6f""%(line[0], line[1],line[2],float(line[3]),topRel,leftRel,bottomRel,rightRel))"
if not os.path.exists(dest):
os.mkdir(dest)
raise
out.close()
"print length,(i-1)*length,i*length"
matplotlib.use('Agg')
get the inputs
zero the parameter gradients
forward + backward + optimize
print statistics
get the inputs
forward + backward + optimize
print statistics
get the inputs
zero the parameter gradients
forward + backward + optimize
get the inputs
forward + backward + optimize
print statistics
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
"conf.reset()"""""""
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
conf.reset()
for clf in classifiers:
"clf.fit(X_train, y_train)"
preds= clf.predict_proba(X)
uncertainty+= preds.max(axis=1)
print(uncertainty[ind])
print(uncertainty[ind])
true_labels = y[unlabeled_indices]
select up to 5 digit examples that the classifier is most uncertain about
remember best acc@1 and save checkpoint
"completeClassificationLoop(run_dataset, model,num_classes)"
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)"
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd)
labels = db.labels_
"mapp=(find_probablemap(label,labels, K=args.K))"
"print(""Clusters"")"
"for i,x in enumerate(labels):"
labels[i]= mapp[x]
print(np.sum(labels == label)/labels.size)
"print(""Confidence Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning(embd, label, idx)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(""Entropy Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning_entropy(embd, label, idx)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(idx,idx.shape)"
for i in idx:
print(paths[i])
"print(""Silohette active learning"")"
"idx= active_learning2(embd, 1000, args.num_clusters)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
embd= reduce_dimensionality(embd)#[0:10000])
labels= labels[0:10000]
label= label[0:10000]
paths= paths[0:10000]
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])"
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])"
plt.show()
"np.save(args.name_prefix+""_embeddings.npy"",embd)"
"np.save(args.name_prefix+""_labels.npy"",label)"
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")"
del output
define loss function (criterion) and optimizer
"optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay= args.weight_decay)"
optimizer.load_state_dict(checkpoint['optimizer'])
train for one epoch
evaluate on validation set
matplotlib.use('Agg')
selected_set.add(rand_ind[i])
"embedding_net = EmbeddingNet('resnet50', 256, True)"
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)"
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)"
print('Embedding Done')
sys.stdout.flush()
Random examples to start
"random_ids = noveltySamples(unlabeled_dataset.em, unlabeled_dataset.getIDs(), 1000)"
print(random_ids)
Move Records
Finetune the embedding model
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)"
unlabeled_dataset.updateEmbedding(model)
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})"
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()"
matplotlib.use('Agg')
"uncertainty= np.apply_along_axis(stats.entropy,1,probs) * (1 - probs.max(axis=1))"
"copy(paths[srt[i]], ""active"")"
"plot_together( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()), preds, base_ind, dataset.getpaths(), {})"
"return np.random.choice(range(0,prob_list[0].shape[0]), 100, replace=False).tolist()"
selected_set.add(rand_ind[i])
"query= Detection.update(kind = destKind.value).where(Detection.id.in_(rList), Detection.kind == srcKind.value)"
query.execute()
"embedding_net = EmbeddingNet('resnet50', 256, True)"
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)"
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)"
print('Embedding Done')
sys.stdout.flush()
"plot_embedding(dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})"
Random examples to start
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)"
print(random_ids)
Move Records
Finetune the embedding model
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)"
unlabeled_dataset.updateEmbedding(model)
"print(dataset.em[dataset.current_set].shape, np.asarray(dataset.getlabels()).shape, len(dataset.getpaths()))"
"plot_embedding( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})"
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})"
train_eval_classifier()
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()"
"names = [""Linear SVM"", ""RBF SVM"", ""Random Forest"", ""Neural Net"", ""Naive Bayes""]"
"classifiers = [SVC(kernel=""linear"", C=0.025, probability= True, class_weight='balanced'),"
"SVC(gamma=2, C=1, probability= True, class_weight='balanced'),"
"RandomForestClassifier(max_depth=None, n_estimators=100, class_weight='balanced'),"
"MLPClassifier(alpha=1),"
GaussianNB()]
estimators= []
"for name, clf in zip(names, classifiers):"
"estimators.append((name, clf))"
"eclf1 = VotingClassifier(estimators= estimators, voting='hard')"
"eclf2 = VotingClassifier(estimators= estimators, voting='soft')"
"names.append(""ensemble hard"")"
classifiers.append(eclf1)
"names.append(""ensemble soft"")"
classifiers.append(eclf2)
dataset.image_mode()
dataset.updateEmbedding(model)
y_pred= clf.predict(X_test)
"print(confusion_matrix(y_test, y_pred))"
paths= dataset.getpaths()
"for i, (yp, yt) in enumerate(zip(y_pred, y_test)):"
if yp != yt:
"copy(paths[i],""mistakes"")"
"print(yt, yp, paths[i],i)"
"clf_output= clf_e.embedding(eval_loader, dim=48)"
"self.train_transform = transforms.Compose([Resize(raw_size), CenterCrop((processed_size)), ToTensor(), Normalize(mean, std)])"
"print(self.labels_set, self.n_classes)"
"print(line,imageWidth,imageHeight)"
"print length,(i-1)*length,i*length"
#########################################################
## Configuration
#########################################################
## The actual code
Check arguments
Create output directories
Padding around the detected objects when cropping
1.3 for the cropping during test time and 1.3 for
the context that the CNN requires in the left-over
image
Load a (frozen) Tensorflow model into memory.
## Preparations: get all the output tensors
The following processing is only for single image
Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
Follow the convention by adding back the batch dimension
For all images in the image directoryig
Load image
Run inference
"all outputs are float32 numpy arrays, so convert types as appropriate"
Add detections to the collection
Get info about the image
Select detections with a confidence larger than DETECTION_CONFIDENCE
Get these boxes and convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes"
"However, we need to make sure that it box coordinates are still within the image"
"For each detected bounding box with high confidence, we will"
crop the image to the padded box and save it
generate a unique identifier for the detection
"bbox is the detected box, crop_box the padded / enlarged box"
Add numbering to the original file name if there are multiple boxes
The absolute file path where we will store the image
"if COCO_OUTPUT_DIR is set, then we will only use the shape"
of cropped_img in the following code. So instead of reading
cropped_img = np.array(Image.open(out_file))
we can speed everything up by reading only the size of the image
matplotlib.use('Agg')
conf= ConfusionMatrix(24)
get the inputs
zero the parameter gradients
forward + backward + optimize
print statistics
get the inputs
forward + backward + optimize
print statistics
get the inputs
zero the parameter gradients
forward + backward + optimize
get the inputs
forward + backward + optimize
print statistics
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
"conf.reset()"""""""
define loss function (criterion) and optimizer
conf= NPConfusionMatrix(10)
all_indices= set(range(len(y)))
diff= all_indices.difference(base_ind)
"conf.update(preds_tr,y_train)"
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))"
conf.reset()
for clf in classifiers:
"clf.fit(X_train, y_train)"
preds= clf.predict_proba(X)
uncertainty+= preds.max(axis=1)
print(uncertainty[ind])
print(uncertainty[ind])
true_labels = y[unlabeled_indices]
select up to 5 digit examples that the classifier is most uncertain about
"print(indices,selected_set)"
print(query.sql())
remember best acc@1 and save checkpoint
"completeClassificationLoop(run_dataset, model,num_classes)"
"embd, label, paths = extract_embeddings(run_loader, model)"
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)"
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd)
labels = db.labels_
"mapp=(find_probablemap(label,labels, K=args.K))"
"print(""Clusters"")"
"for i,x in enumerate(labels):"
labels[i]= mapp[x]
print(np.sum(labels == label)/labels.size)
"print(""Confidence Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning(embd, label, idx)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(""Entropy Active Learning"")"
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)"
for i in range(9):
"idx= active_learning_entropy(embd, label, idx)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
"new_selected= selectSamples(embd,paths,3000)"
"print(idx,idx.shape)"
for i in idx:
print(paths[i])
"print(""Silohette active learning"")"
"idx= active_learning2(embd, 1000, args.num_clusters)"
print(idx.shape)
"apply_different_methods(embd[idx], label[idx], embd, label)"
"print(""Random"")"
"idx = np.random.choice(np.arange(len(paths)), 1000, replace=False)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
"apply_different_methods(embd[idx], label[idx], embd, label)"
embd= reduce_dimensionality(embd)#[0:10000])
labels= labels[0:10000]
label= label[0:10000]
paths= paths[0:10000]
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])"
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])"
plt.show()
"np.save(args.name_prefix+""_embeddings.npy"",embd)"
"np.save(args.name_prefix+""_labels.npy"",label)"
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")"
combo.setEnabled(not finalized)
print(self.bbox)
self.addWidget(self.child)
Moving container with arrows
Left - Bottom
Right-Bottom
Left-Top
Right-Top
Left - Bottom
Right - Bottom
Left - Top
Right - Top
check cursor horizontal position
check cursor vertical position
self.resizeEvent=self.onResize
"print(""Parent"", parent, parent.width(), parent.height())"
"self.setGeometry(0,0,410,307)"
"print(w,h,""w,h"")"
"print(""final"",tag.getFinal())"
pass
"self.tags.append(TContainer(self,Category.get(-1),[0,0,0.1,0.1],True, Qt.red))"
label= CharField()
fullname=str(self.model)
"self.name= (fullname[fullname.find("":"")+2:fullname.find("">"")].strip()+'_set').lower()"
"print(""Parent"", self.parentWidget().width(), self.parentWidget().height())"
"print(self.model,self.name,query.sql())"
self.tab4.speciesList.setModel(species)
"self.tab4.speciesList.setRowHidden(len(species.stringList())-1, True)"
self.speciesList.itemChanged.connect(self.itemChanged)
###############
""
iwildcam_dataset.py
""
Loader for the iWildCam detection data set.
""
###############
loads the taxonomy data and converts to ints
set up dummy data
create a dictionary of lists containing taxonomic labels
set up the filenames and annotations
This loop reads the bboxes and corresponding labels and assigns them
the correct image. Kind of slow at the moment...
"To speed up the loop, creating mapping for image_id to list index"
"check that the image contains an animal, if not, don't append a box or label to the"
image list
"Bboxes should have ('ymin', 'xmin', 'ymax', 'xmax') format"
"Currently we take the label from the annotation file, non-consecutive-"
label-support would be great
"self.bboxes[idx].append([-1.,-1.,0.,0.])"
self.labels[idx].append(30)
load classes
print out some stats
To make sure we loaded the bboxes correctly:
self.validate_bboxes()
Make sure all are greater equal 0
We have to add 1 as the framework assumes that labels start from 0
print(bboxes)
###############
""
util.py
""
Image utilities used in the FasterRCNN framework.
""
###############
"reshape (H, W) -> (1, H, W)"
"transpose (H, W, C) -> (C, H, W)"
pass in list of files
print(im_id)
print(images[im_id]['seq_id'])
get unique colors in segmentation
get box for each color
this is the background class
get a box around this color
"x1,y1 is top left corner, x2,y2 is bottom right corner"
create coco-style json
In settings.json first activate computer vision mode:
https://github.com/Microsoft/AirSim/blob/master/docs/image_apis.md#computer-vision-mode
import setup_path
load animal class name lookup
set segmentation values for everything to 0
set segmentation for each animal to a different value
"client.simSetCameraOrientation(""0"", airsim.to_quaternion(-0.161799, 0, 0)); #radians"
pose = client.simGetVehiclePose()
pp.pprint(pose)
print('Pose ' + str(cam_num))
print(pose)
"print(""Type %d, size %d, pos %s"" % (response.image_type, len(response.image_data_float), pprint.pformat(response.camera_position)))"
"print(""Type %d, size %d, pos %s"" % (response.image_type, len(response.image_data_uint8), pprint.pformat(response.camera_position)))"
pose = client.simGetVehiclePose()
pp.pprint(pose)
currently reset() doesn't work in CV mode. Below is the workaround
"client.simSetPose(airsim.Pose(airsim.Vector3r(0, 0, 0), airsim.to_quaternion(0, 0, 0)), True)"
environment_lookup = {}
print(len(list(env_list)))
save environment dict every time so you don't lose the info if airsim crashes
"%% Constants, imports, environment"
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
Numpy FutureWarnings from tensorflow import
%% Classes
Stick this into filenames before the extension for the rendered result
Number of decimal places to round to for confidence and bbox coordinates
"MegaDetector was trained with batch size of 1, and the resizing function is a part"
of the inference graph
An enumeration of failure reasons
"change from [y1, x1, y2, x2] to [x1, y1, width_box, height_box]"
convert numpy floats to Python floats
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size
"np_images = [np.asarray(image, np.uint8) for image in images]"
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)"
performs inference
our batch size is 1; need to loop the batch dim if supporting batch size > 1
%% Main function
"load and run detector on target images, and visualize the results"
"since we'll be writing a bunch of files to the same folder, rename"
as necessary to avoid collisions
"the error code and message is written by generate_detections_one_image,"
which is wrapped in a big try catch
image is modified in place
%% Command-line driver
"but for a single image, args.image_dir is also None"
#####
""
process_video.py
""
"Split a video into frames, run the frames through run_tf_detector_batch.py, and"
optionally stitch together results into a new video with detection boxes.
""
#####
"%% Constants, imports, environment"
%% Function for rendering frames to video and vice-versa
http://tsaith.github.io/combine-images-into-a-video-with-python-3-and-opencv-3.html
Determine the width and height from the first image
Define the codec and create VideoWriter object
https://stackoverflow.com/questions/33311153/python-extracting-and-saving-video-frames
%% Main function
Render detections to images
Combine into a video
%% Interactive driver
%% Load video and split into frames
"python process_video.py ""c:\temp\models\md_v4.0.0.pb"" ""c:\temp\LIFT0003.MP4"" --debug_max_frames=10 --render_output_video=True"
%% Command-line driver
"%% Constants, imports, environment"
from multiprocessing.pool import ThreadPool as workerpool
Numpy FutureWarnings from tensorflow import
%% Support functions for multiprocessing
Split a list into chunks of size n
Split a list into n even chunks
%% Main function
If we're not using multiprocessing...
Load the detector
"If we're using multiprocessing, let the workers load the model, just store"
the model filename.
Does not count those already processed
Will not add additional entries not in the starter checkpoint
checkpoint
"This was modified in place, but we also return it for backwards-compatibility."
%% Command-line driver
Load the checkpoint if available
""
Relative file names are only output at the end; all file paths in the checkpoint are
still full paths.
"Find the images to score; images can be a directory, may need to recurse"
A json list of image paths
A single image file
Test that we can write to the output_file's dir if checkpointing requested
This script is taken from https://github.com/tensorflow/models/blob/master/research/object_detection/model_main.py
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
substitute the AML data reference mount points for relevant parts in the pipeline.config
substitute the AML data reference mount points for relevant parts in the pipeline.config and overwrite
The first eval input will be evaluated.
Currently only a single Eval Spec is allowed.
throttle_secs is not documented in eval.proto. This replaces eval_interval_secs somewhat
""
copy_checkpoints.py
""
Run this script with specified source_dir and target_dir while the model is training to make a copy
of every checkpoint (checkpoints are kept once an hour by default and is difficult to adjust)
""
do not copy event or evaluation results
"labels input to compute_object_detection_metrics() needs to start at 0, not 1"
num_detections = len(dets['boxes'])
to prevent 'Invalid dimensions for box data.' error
this box will not match any detections
compute one-class precision/recall/average precision (if every box is just of an object class)
"iterate through each image in the gt file, not the detection file"
ground truth
convert gt box coordinates to TFODAPI format
detections
only include a detection entry if that image had detections
%% Empty and non-empty classification at image level
%% Empty and non-empty classification at sequence level
TODO move detector_output_path specific code out so that this function evaluates only on classification results (confidences)
evaluate on sequences that are present in both gt and the detector output file
%% Utilities
#####
""
detect_and_predict_image.py
""
"Functions to load a TensorFlow detection and a classification model, run inference,"
"render bounding boxes on images, and write out the resulting"
images (with bounding boxes and classes).
""
"See the ""test driver"" cell for example invocation."
""
""
#####
"%% Constants, imports, environment"
Minimum detection confidence for showing a bounding box on the output image
Stick this into filenames before the extension for the rendered result
Number of top-scoring classes to show at each bounding box
%% Core detection functions
Load images if they're not already numpy arrays
iImage = 0; image = images[iImage]
"Load the image as an nparray of size h,w,nChannels"
"There was a time when I was loading with PIL and switched to mpimg,"
"but I can't remember why, and converting to RGB is a very good reason"
"to load with PIL, since mpimg doesn't give any indication of color"
"order, which basically breaks all .png files."
""
"So if you find a bug related to using PIL, update this comment"
"to indicate what it was, but also disable .png support."
image = mpimg.imread(image)
This shouldn't be necessarily when loading with PIL and converting to RGB
Actual detection
...for each image
"Currently ""boxes"" is a list of length nImages, where each element is shaped as"
""
"1,nDetections,4"
""
"This implicitly banks on TF giving us back a fixed number of boxes, let's assert on this"
to make sure this doesn't silently break in the future.
iBox = 0; box = boxes[iBox]
"""scores"" is a length-nImages list of elements with size 1,nDetections"
"""classes"" is a length-nImages list of elements with size 1,nDetections"
""
"Still as floats, but really representing ints"
Squeeze out the empty axis
boxes is nImages x nDetections x 4
scores and classes are both nImages x nDetections
Get input and output tensors of classification model
"imsize = cur_image['width'], cur_image['height']"
Select detections with a confidence larger 0.5
Get these boxes and convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes"
"However, we need to make sure that it box coordinates are still within the image"
For convenience:
Create an array with contains the index of the corresponding crop_box for each selected box
i.e. [False False 0 False 1 2 3 False False]
For each box
If this box should be classified
Run inference
if box should not be classified
...for each box
species_scores should have shape len(images) x len(boxes) x num_species
...for each image
...with tf.Session
with classification_graph
species_scores should have shape len(images) x len(boxes) x num_species
%% Rendering functions
Display the image
plt.show()
"top, left, bottom, right"
""
"x,y origin is the upper-left"
Generate bounding box text
Choose color based on class
Location is the bottom-left of the rect
""
Origin is the upper-left
iRight = x + w
iTop = y + h
Add the patch to the Axes
Add class description
First determine best location by finding the corner that is closest to the image center
relative corner coordinates
relative coordinates of image center
Compute pair-wise squared distance and get the index of the one with minimal distance
Get the corresponding coordinates ...
... and alignment for the text box
Plot the text box with background
...for each box
This is magic goop that removes whitespace around image plots (sort of)
"plt.savefig(outputFileName, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)"
os.startfile(outputFileName)
...for each image
...def render_bounding_boxes
Load and run detector on target images
Read the name of all classes
remove empty lines
%% Interactive driver
%%
%%
%% File helper functions
%% Command-line driver
Hack to avoid running on already-detected images
""
"Script for evaluating a frozen graph given a COCO-style dataset, which was generated by"
the ../data_management/databases/classification/make_classification_dataset.py script.
This is useful for debugging as the accuracy reported by this script should match the
accuracy reported by the Tensorflow training.
""
Check that all files exists for easier debugging
Load frozen graph
Collect tensors for input and output
Read image
Run inference
Check if correct
""
Script for evaluating precision/recall on a two-class problem given
"a frozen graph and a COCO-style dataset, which was generated by"
the ../data_management/databases/classification/make_classification_dataset.py script.
We assume the positive class is at index 0 (with a zero-based indexing)
""
Check that all files exists for easier debugging
Load frozen graph
Collect tensors for input and output
Read image
"with open(image_path, 'rb') as fi:"
"image =  sess.run(tf.image.decode_jpeg(fi.read(), channels=3))"
image = image / 255.
Run inference
predicted_class = np.argmax(predictions)
Check if correct
if coco.imgToAnns[image_id][0]['category_id'] == predicted_class:
correct = correct + 1
"In matplotlib < 1.5, plt.fill_between does not have a 'step' argument"
""
Script for selecting testing images of a COCO-style dataset generated by the script
../data_management/databases/classification/make_classification_dataset.py in a consistent
manner and predicting the class for it.
""
Mandatory parameters
Optional parameters
Validate parameters
Derived parameters
"We assume that the dataset was generated with the make_classification_dataset.py script,"
hence the images should be located in the same folder as the json
Make seletion deterministic
Load frozen graph
Get dataset information
Get classes
...and the class list corresponding to the model outputs by assuming
that they are in order of their ids
Get images of each class
Shuffle the image list
Start the image sampling
"Set of avaiable class IDs, will be filled below"
If there are still images left for that class
"Get image for the sampled class, we already shuffled the class images before so"
we can simply pop()
Start prediction
Collect tensors for input and output
Read image
Run inference
Print output to log file
#####
""
api_apply_classifier_single_node.py
""
Takes the JSON file produced by the detection API and
classifies all boxes above a confidence threshold.
""
#####
"%% Constants, imports, environment"
Assumes that the root of the CameraTraps repo is on the PYTHONPATH
Minimum detection confidence for classifying an object
Number of top-scoring classes to show at each bounding box
Enlargment factor applied to boxes before passing them to the classifier
""
Provides more context and can lead to better results
List of detection categories for which we will run the classification
""
"Currently there are {""1"": ""animal"", ""2"": ""person"", ""4"": ""vehicle""}"
""
Should be a list of string-formatted ints.
Number of significant float digits in JSON output
%% Core detection functions
Read the name of all classes
remove empty lines
Create field with name *classification_categories*
Add classes using 0-based indexing
def add_classification_categories
Make sure we have the right json object
Get input and output tensors of classification model
For each image
Read image
"Scale pixel values to [0,1]"
For each box
Skip detections with low confidence
Skip if detection category is not in whitelist
Skip if already classified
"Get current box in relative coordinates and format [x_min, y_min, width_of_box, height_of_box]"
"Convert to [ymin, xmin, ymax, xmax] and store it as 1x4 numpy array so we can"
re-use the generic multi-box padding code
Convert normalized coordinates to pixel coordinates
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes."
""
"However, we need to make sure that it box coordinates are still within the image."
Get the first (and only) row as our bbox to classify
Get the image data for that box
Run inference
Add an empty list to the json for our predictions
Add the *num_annotated_classes* top scoring classes
...for each box
...for each image
...with tf.Session
with classification_graph
def classify_boxes
Load classification model
Load detector json
Add classes to detector_json
"Run classifier on all images, changes will be writting directly to the json"
Write output json
def load_and_run_classifier
%% Command-line driver
""
Script for selecting testing images of a COCO-style dataset generated by the script
../data_management/databases/classification/make_classification_dataset.py in a consistent
manner and predicting the class for it.
""
Assumes the cameratraps repo root is on the path
Make seletion deterministic
Mandatory parameters
Optional parameters
Validate parameters
Tranfer parameters to post-processing format
Load frozen graph
Reading image list
Reading class list
Image sampling
Start prediction
Collect tensors for input and output
Array for collecting infos for rendering the html
Read image
"Scale pixel values to [0,1]"
Run inference
Add links to all available classes
""
predict_image.py
""
"Given a pointer to a frozen detection graph, runs inference on a single image,"
printing the top classes to the console
""
%% Imports
%% Command-line processing
Check that all files exist for easier debugging
%% Inference
Load frozen graph
Load class list
Remove empty lines
Collect tensors for input and output
Read image
Run inference
Print output
""
Mostly unmodified script for freezing a model
Added for convenience and for possible future optimizations
""
Copyright 2015 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
'input_checkpoint' may be a prefix if we're using Saver V2 format
Remove all the explicit device specifications for this node. This helps to
make the graph more portable.
List of all partition variables. Because the condition is heuristic
"based, the list could include false positives."
This tensor doesn't exist in the graph (for example it's
'global_step' or a similar housekeeping element) so skip it.
`var_list` is required to be a map of variable names to Variable
tensors. Partition variables are Identity tensors that cannot be
handled by Saver.
Models that have been frozen previously do not contain Variables.
optimized_output_graph_def = optimize_for_inference_lib.optimize_for_inference(
"output_graph_def,"
"input_node_names.replace("" "", """").split("",""),"
"output_node_names.replace("" "", """").split("",""),"
tf.float32.as_datatype_enum)
Write GraphDef to file if output path has been given.
""
"Creates a graph description, which is required to create a frozen graph."
Adapted from from ./tf-slim/export_inference_graph.py
Added preprocessing to the definition for easier handling
""
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
tf.app.flags.DEFINE_integer(
"'batch_size', None,"
"'Batch size for the exported model. Defaulted to ""None"" so batch size can '"
'be specified at model runtime.')
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
#####################
Optimization Flags #
#####################
######################
Learning Rate Flags #
######################
######################
Dataset Flags #
######################
####################
Fine-Tuning Flags #
####################
"Note: when num_clones is > 1, this will actually have each clone to go"
over each epoch FLAGS.num_epochs_per_decay times. This is different
behavior from sync replicas and is expected to produce different results.
Warn the user if a checkpoint exists in the train_dir. Then we'll be
ignoring the checkpoint anyway.
TODO(sguada) variables.filter_variables()
######################
Config model_deploy #
######################
Create global_step
#####################
Select the dataset #
#####################
#####################
Select the network #
#####################
####################################
Select the preprocessing function #
####################################
#############################################################
Create a dataset provider that loads data from the dataset #
#############################################################
###################
Define the model #
###################
############################
Specify the loss function #
############################
Gather initial summaries.
"Gather update_ops from the first clone. These contain, for example,"
the updates for the batch_norm variables created by network_fn.
Add summaries for end_points.
Add summaries for losses.
Add summaries for variables.
################################
Configure the moving averages #
################################
########################################
Configure the optimization procedure. #
########################################
"If sync_replicas is enabled, the averaging will be done in the chief"
queue runner.
Update ops executed locally by trainer.
Variables to train.
and returns a train_tensor and summary_op
Add total_loss to summary.
Create gradient updates.
Add the summaries from the first clone. These contain the summaries
created by model_fn and either optimize_clones() or _gather_clone_loss().
Merge all summaries together.
##########################
Kicks off the training. #
##########################
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
#####################
Select the dataset #
#####################
###################
Select the model #
###################
#############################################################
Create a dataset provider that loads data from the dataset #
#############################################################
####################################
Select the preprocessing function #
####################################
###################
Define the model #
###################
Define the metrics:
Print the summaries to screen.
TODO(sguada) use num_epochs=1
This ensures that we make a single pass over all of the data.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Transform the image to floats.
"Randomly crop a [height, width] section of the image."
Randomly flip the image horizontally.
"Because these operations are not commutative, consider randomizing"
the order their operation.
Subtract off the mean and divide by the variance of the pixels.
Transform the image to floats.
Resize and crop if needed.
Subtract off the mean and divide by the variance of the pixels.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Use tf.slice instead of crop_to_bounding box as it accepts tensors to
define the crop size.
Compute the rank assertions.
Create a random bounding box.
""
Use tf.random_uniform and not numpy.random.rand as doing the former would
"generate random numbers at graph eval time, unlike the latter which"
generates random numbers at graph definition time.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Pass the real x only to one of the func calls.
The random_* ops do not necessarily clamp.
"Each bounding box has shape [1, num_boxes, box coords] and"
"the coordinates are ordered [ymin, xmin, ymax, xmax]."
A large fraction of image datasets contain a human-annotated bounding
box delineating the region of the image containing the object of interest.
We choose to create a new bounding box for the object which is a randomly
distorted version of the human-annotated bounding box that obeys an
"allowed range of aspect ratios, sizes and overlap with the human-annotated"
"bounding box. If no box is supplied, then we assume the bounding box is"
the entire image.
Crop the image to the specified bounding box.
"Each bounding box has shape [1, num_boxes, box coords] and"
"the coordinates are ordered [ymin, xmin, ymax, xmax]."
Restore the shape since the dynamic slice based upon the bbox_size loses
the third dimension.
This resizing operation may distort the images because the aspect
ratio is not respected. We select a resize method in a round robin
fashion based on the thread number.
Note that ResizeMethod contains 4 enumerated resizing methods.
We select only 1 case for fast_mode bilinear.
Randomly flip the image horizontally.
Randomly distort the colors. There are 1 or 4 ways to do it.
Crop the central region of the image with an area containing 87.5% of
the original image.
Resize the image to the specified height and width.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(sguada) docstring paragraph by (a) motivating the need for the file and
(b) defining clones.
TODO(sguada) describe the high-level components of model deployment.
"E.g. ""each model deployment is composed of several parts: a DeploymentConfig,"
"which captures A, B and C, an input_fn which loads data.. etc"
Set up DeploymentConfig
Create the global step on the device storing the variables.
Define the inputs
Define the optimizer.
Define the model including the loss.
Run training.
Namedtuple used to represent a clone during deployment.
"Namedtuple used to represent a DeployedModel, returned by deploy()."
Default parameters for DeploymentConfig
Create clones.
The return value.
Individual components of the loss that will need summaries.
Compute and aggregate losses on the clone device.
Add the summaries out of the clone device block.
Only use regularization_losses for the first clone
Compute the total_loss summing all the clones_losses.
Sum the gradients across clones.
Gather initial summaries.
Create Clones.
"Gather update_ops from the first clone. These contain, for example,"
the updates for the batch_norm variables created by model_fn.
Place the global step on the device storing the variables.
Compute the gradients for the clones.
Add summaries to the gradients.
Create gradient updates.
Only use regularization_losses for the first clone
Add the summaries from the first clone. These contain the summaries
created by model_fn and either optimize_clones() or _gather_clone_loss().
Add total_loss to summary.
Merge all summaries together.
Note that each grad_and_vars looks like the following:
"((grad_var0_clone0, var0), ... (grad_varN_cloneN, varN))"
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Create an easy training set:
Create an easy training set:
Create an easy training set:
clone function creates a fully_connected layer with a regularizer loss.
The model summary op should have a few summary inputs and all of them
should be on the CPU.
clone function creates a fully_connected layer with a regularizer loss.
"No optimizer here, it's an eval."
The model summary op should have a few summary inputs and all of them
should be on the CPU.
!/usr/bin/python
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Failed to find ""index"" occurrence of item."
pylint: disable=broad-except
pylint: enable=broad-except
Grab the 'index' annotation.
Some images contain bounding box annotations that
"extend outside of the supplied image. See, e.g."
n03127925/n03127925_147.xml
"Additionally, for some bounding boxes, the min > max"
or the box is entirely outside of the image.
Example: <...>/n06470073/n00141669_6790.xml
Determine if the annotation is from an ImageNet Challenge label.
Note: There is a slight bug in the bounding box annotation data.
Many of the dog labels have the human label 'Scottish_deerhound'
instead of the synset ID 'n02092002' in the bbox.label field. As a
"simple hack to overcome this issue, we only exclude bbox labels"
*which are synset ID's* that do not match original synset label for
the XML file.
Guard against improperly specified boxes.
Note bbox.filename occasionally contains '%s' in the name. This is
data set noise that is fixed by just using the basename of the XML file.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The URLs where the MNIST data can be downloaded.
The names of the classes.
"First, process the training data:"
"Next, process the testing data:"
"Finally, write the labels file:"
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The URL where the Flowers data can be downloaded.
The number of images in the validation set.
Seed for repeatability.
The number of shards per dataset split.
Initializes function that decodes RGB JPEG data.
Read the filename:
Divide into train and test:
"First, convert the training and validation sets."
"Finally, write the labels file:"
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(nsilberman): Add tfrecord file type once the script is updated.
"If set to false, will not try to set label_to_names in dataset"
by reading them from labels.txt or github.
n01440764
n01443537
n02119247    black fox
n02119359    silver fox
pylint: disable=g-line-too-long
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The URL where the CIFAR data can be downloaded.
The number of training files.
The height and width of each image.
The names of the classes.
"First, process the training data:"
"Next, process the testing data:"
"Finally, write the labels file:"
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The labels file contains a list of valid labels are held in this file.
Assumes that the file contains entries as such:
n01440764
n01443537
n01484850
where each line corresponds to a label expressed as a synset. We map
each synset contained in the file to an integer (based on the alphabetical
ordering). See below for details.
This file containing mapping from synset to human-readable label.
Assumes each line of the file looks like:
""
n02119247    black fox
n02119359    silver fox
"n02119477    red fox, Vulpes fulva"
""
where each line corresponds to a unique mapping. Note that each line is
formatted as <synset>\t<human readable label>.
This file is the output of process_bounding_box.py
Assumes each line of the file looks like:
""
"n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940"
""
where each line corresponds to one bounding box annotation associated
with an image. Each line can be parsed as:
""
"<JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>"
""
Note that there might exist mulitple bounding box annotations associated
with an image file.
pylint: disable=expression-not-assigned
pylint: enable=expression-not-assigned
Create a single Session to run all image coding calls.
Initializes function that converts PNG to JPEG data.
Initializes function that converts CMYK JPEG data to RGB JPEG data.
Initializes function that decodes RGB JPEG data.
File list from:
https://groups.google.com/forum/embed/?place=forum/torch7#!topic/torch7/fOSTXHIESSU
File list from:
https://github.com/cytsai/ilsvrc-cmyk-image-list
Read the image file.
Clean the dirty data.
1 image is a PNG.
22 JPEG images are in CMYK colorspace.
Decode the RGB JPEG.
Check that image converted to RGB
Each thread produces N shards where N = int(num_shards / num_threads).
"For instance, if num_shards = 128, and the num_threads = 2, then the first"
"thread would produce shards [0, 64)."
"Generate a sharded version of the file name, e.g. 'train-00002-of-00010'"
"Break all images into batches with a [ranges[i][0], ranges[i][1]]."
Launch a thread for each batch.
Create a mechanism for monitoring when all threads are finished.
Create a generic TensorFlow-based utility for converting all image codings.
Wait for all the threads to terminate.
Leave label index 0 empty as a background class.
Construct the list of JPEG files and labels.
Shuffle the ordering of all image files in order to guarantee
random ordering of the images with respect to label in the
saved TFRecord files. Make the randomization repeatable.
Build a map from synset to human-readable label.
Run it!
Allowing None in the signature so that dataset_factory can use the default.
!/usr/bin/python
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Read in the 50000 synsets associated with the validation data set.
Make all sub-directories in the validation data dir.
Move all of the image to the appropriate sub-directory.
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
Allowing None in the signature so that dataset_factory can use the default.
"The dataset has classes with no images (empty and human), which have ID 0 and 1,"
so we need to specify 49 here despite having only 47 classes with images
Allowing None in the signature so that dataset_factory can use the default.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
4 x Inception-A blocks
Reduction-A block
7 x Inception-B blocks
Reduction-A block
3 x Inception-C blocks
Logits and predictions
Force all Variables to reside on the device.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The endpoint shapes must be equal to the original shape even when the
separable convolution is replaced with a normal convolution.
"With the 'NCHW' data format, all endpoint activations have a transposed"
shape from the original shape with the 'NHWC' layout.
'NCWH' data format is not supported.
'NCHW' data format is not supported for separable convolution.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Final pooling and prediction
Global average pooling.
Pooling with a fixed kernel size.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Check graph construction for a number of image size/depths and batch
sizes.
Check layer depths.
Check graph construction for a number of image size/depths and batch
sizes.
Check layer depths.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
The current_stride variable keeps track of the effective stride of the
activations. This allows us to invoke atrous convolution whenever applying
the next residual unit would result in the activations having stride larger
than the target output_stride.
The atrous convolution rate parameter.
Move stride from the block's last unit to the end of the block.
"If we have reached the target output_stride, then we need to employ"
atrous convolution with stride=1 and multiply the atrous rate by the
current unit's stride for use in subsequent layers.
Collect activations at the block's end before performing subsampling.
Subsampling of the block's output activations.
"The following implies padding='SAME' for pool1, which makes feature"
alignment easier for dense prediction tasks. This is also used in
https://github.com/facebook/fb.resnet.torch. However the accompanying
code of 'Deep Residual Learning for Image Recognition' uses
padding='VALID' for pool1. You can switch to that choice by setting
"slim.arg_scope([slim.max_pool2d], padding='VALID')."
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Collect outputs for conv2d, fully_connected and max_pool2d"
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Input image.
Convolution kernel.
Input image.
Convolution kernel.
Test both odd and even input dimensions.
Dense feature extraction followed by subsampling.
Make the two networks use the same weights.
Feature extraction at the nominal network rate.
Test both odd and even input dimensions.
Subsampling at the last unit of the block.
Make the two networks use the same weights.
Subsample activations at the end of the blocks.
Make sure that the final output is the same.
Make sure that intermediate block activations in
output_end_points are subsampled versions of the corresponding
ones in expected_end_points.
"Like ResnetUtilsTest.testEndPointsV1(), but for the public API."
Dense feature extraction followed by subsampling.
Make the two networks use the same weights.
Feature extraction at the nominal network rate.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"inputs has shape [batch, 224, 224, 3]"
"inputs has shape [batch, 513, 513, 3]"
We do not include batch normalization or activation functions in
conv1 because the first ResNet unit will perform these. Cf.
Appendix of [2].
This is needed because the pre-activation variant does not have batch
normalization or activation functions in the residual unit output. See
Appendix of [2].
Convert end_points_collection into a dictionary of end_points.
Global average pooling.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
=============================================================================
"These parameters come from the online port, which don't necessarily match"
those in the paper.
TODO(nsilberman): confirm these values with Philip.
##########
Encoder #
##########
"No normalizer for the first encoder layers as per 'Image-to-Image',"
Section 5.1.1
First layer doesn't use normalizer_fn
Last layer doesn't use activation_fn nor normalizer_fn
##########
Decoder #
##########
"Dropout is used at both train and test time as per 'Image-to-Image',"
Section 2.1 (last paragraph).
The Relu comes BEFORE the upsample op:
Explicitly set the normalizer_fn to None to override any default value
"that may come from an arg_scope, such as pix2pix_arg_scope."
No normalization on the input layer.
Stride 1 on the last layer.
"1-dim logits, stride 1, no activation, no normalization."
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 Google Inc. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Most networks use 224 as their default_image_size
Most networks use 224 as their default_image_size
Most networks use 224 as their default_image_size
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(joelshor): Add a test to check generator endpoints.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Force all Variables to reside on the device.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
"Collect outputs for conv2d, fully_connected and max_pool2d."
Use conv2d instead of fully_connected layers.
Convert end_points_collection into a end_point dict.
Alias
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
pylint: disable=unused-import
pylint: enable=unused-import
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Input image.
Convolution kernel.
Input image.
Convolution kernel.
Test both odd and even input dimensions.
Dense feature extraction followed by subsampling.
Make the two networks use the same weights.
Feature extraction at the nominal network rate.
"Like ResnetUtilsTest.testEndPointsV2(), but for the public API."
Dense feature extraction followed by subsampling.
Make the two networks use the same weights.
Feature extraction at the nominal network rate.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
If we are fine tuning a checkpoint we need to start at a lower learning
rate since we are farther along on training.
We can start quantizing immediately if we are finetuning.
We need to wait for the model to train a bit before we quantize if we are
training from scratch.
Call rewriter to produce graph with fake quant ops and folded batch norms
"quant_delay delays start of quantization till quant_delay steps, allowing"
for better model accuracy.
Configure the learning rate using an exponential decay.
"When restoring from a floating point model, the min/max values for"
quantized weights and activations are not present.
We instruct slim to ignore variables that are missing during restoration
by setting ignore_missing_vars=True
"If we are restoring from a floating point model, we need to initialize"
the global step to zero for the exponential decay to result in
reasonable learning rates.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Orignaly, arg_scope = slim.arg_scope and layers = slim, now switch to more"
update-to-date tf.contrib.* API.
Decay for the moving averages.
epsilon to prevent 0s in variance.
Turns off fused batch norm.
collection containing the moving mean and moving variance.
"For inverted pyramid models, we start with gating switched off."
batch_size x 32 x 112 x 112 x 64
Separable conv is slow when used at first conv layer.
batch_size x 32 x 56 x 56 x 64
batch_size x 32 x 56 x 56 x 64
batch_size x 32 x 56 x 56 x 192
batch_size x 32 x 28 x 28 x 192
batch_size x 32 x 28 x 28 x 256
batch_size x 16 x 14 x 14 x 512
batch_size x 16 x 14 x 14 x 512
batch_size x 16 x 14 x 14 x 512
batch_size x 16 x 14 x 14 x 528
batch_size x 16 x 14 x 14 x 832
batch_size x 8 x 7 x 7 x 832
batch_size x 8 x 7 x 7 x 1024
Final pooling and prediction
Temporal average pooling.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
By default use stride=1 and SAME padding
299 x 299 x 3
149 x 149 x 32
147 x 147 x 32
147 x 147 x 64
73 x 73 x 160
71 x 71 x 192
35 x 35 x 384
4 x Inception-A blocks
35 x 35 x 384
Reduction-A block
17 x 17 x 1024
7 x Inception-B blocks
17 x 17 x 1024
Reduction-B block
8 x 8 x 1536
3 x Inception-C blocks
Auxiliary Head logits
17 x 17 x 1024
Final pooling and prediction
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which"
can be set to False to disable pooling here (as in resnet_*()).
8 x 8 x 1536
1 x 1 x 1536
1536
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN
setups need the gradient of gradient FusedBatchNormGrad.
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN
setups need the gradient of gradient FusedBatchNormGrad.
First upscaling is different because it takes the input vector.
Last layer has different normalizer and activation.
Convert to proper channels.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Use clip_by_value to simulate bandpass activation.
Use clip_by_value to simulate bandpass activation.
Use clip_by_value to simulate bandpass activation.
149 x 149 x 32
147 x 147 x 32
147 x 147 x 64
73 x 73 x 64
73 x 73 x 80
71 x 71 x 192
35 x 35 x 192
35 x 35 x 320
TODO(alemi): Register intermediate endpoints
"17 x 17 x 1088 if output_stride == 8,"
33 x 33 x 1088 if output_stride == 16
TODO(alemi): register intermediate endpoints
TODO(gpapan): Properly support output_stride for the rest of the net.
8 x 8 x 2080
TODO(alemi): register intermediate endpoints
8 x 8 x 1536
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which"
can be set to False to disable pooling here (as in resnet_*()).
Set weight_decay for weights in conv2d and fully_connected layers.
Set activation_fn and parameters for batch_norm.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Decay for the moving averages.
epsilon to prevent 0s in variance.
collection containing update_ops.
use fused batch norm if possible.
Set weight_decay for weights in Conv and FC layers.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"inputs has shape [batch, 224, 224, 3]"
"inputs has shape [batch, 513, 513, 3]"
Use clip_by_value to simulate bandpass activation.
Convert end_points_collection into a dictionary of end_points.
Global average pooling.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"end_points will collect relevant activations for external use, for example"
summaries or losses.
299 x 299 x 3
149 x 149 x 32
147 x 147 x 32
147 x 147 x 64
73 x 73 x 64
73 x 73 x 80.
71 x 71 x 192.
35 x 35 x 192.
Inception blocks
mixed: 35 x 35 x 256.
mixed_1: 35 x 35 x 288.
mixed_2: 35 x 35 x 288.
mixed_3: 17 x 17 x 768.
mixed4: 17 x 17 x 768.
mixed_5: 17 x 17 x 768.
mixed_6: 17 x 17 x 768.
mixed_7: 17 x 17 x 768.
mixed_8: 8 x 8 x 1280.
mixed_9: 8 x 8 x 2048.
mixed_10: 8 x 8 x 2048.
Auxiliary Head logits
Shape of feature map before the final layer.
Final pooling and prediction
Global average pooling.
Pooling with a fixed kernel size.
1 x 1 x 2048
2048
1000
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Reflection pad by 1 in spatial dimensions (axes 1, 2 = h, w) to make a 3x3"
'valid' convolution produce an output with the same dimension as the
input.
This corrects 1 pixel offset for images with even width and height.
conv2d is left aligned and conv2d_transpose is right aligned for even
sized images (while doing 'SAME' padding).
Note: This doesn't reflect actual model in paper.
Neither dropout nor batch norm -> dont need is_training
##########
Encoder #
##########
7x7 input stage
##################
Residual Blocks #
##################
##########
Decoder #
##########
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"end_points will collect relevant activations for external use, for example"
summaries or losses.
Used to find thinned depths for each layer.
Note that sizes in the comments below assume an input spatial size of
"224x224, however, the inputs can be of any size greater 32x32."
224 x 224 x 3
depthwise_multiplier here is different from depth_multiplier.
depthwise_multiplier determines the output channels of the initial
"depthwise conv (see docs for tf.nn.separable_conv2d), while"
depth_multiplier controls the # channels of the subsequent 1x1
convolution. Must have
in_channels * depthwise_multipler <= out_channels
so that the separable convolution is not overparameterized.
Use a normal convolution instead of a separable convolution.
112 x 112 x 64
56 x 56 x 64
56 x 56 x 64
56 x 56 x 192
28 x 28 x 192
Inception module.
28 x 28 x 256
28 x 28 x 320
14 x 14 x 576
14 x 14 x 576
14 x 14 x 576
14 x 14 x 576
14 x 14 x 576
7 x 7 x 1024
7 x 7 x 1024
Final pooling and prediction
Global average pooling.
Pooling with a fixed kernel size.
1 x 1 x 1024
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
=============================================================================
For the Conv2d_0 layer FaceNet has depth=16
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
=============================================================================
Tensorflow mandates these.
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture
Conv defines 3x3 convolution layers
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.
stride is the stride of the convolution
depth is the number of channels or filters in a layer
MOBILENETV1_CONV_DEFS specifies the MobileNet body
Used to find thinned depths for each layer.
The current_stride variable keeps track of the output stride of the
"activations, i.e., the running product of convolution strides up to the"
current network layer. This allows us to invoke atrous convolution
whenever applying the next convolution would result in the activations
having output stride larger than the target output_stride.
The atrous convolution rate parameter.
"If we have reached the target output_stride, then we need to employ"
atrous convolution with stride=1 and multiply the atrous rate by the
current unit's stride for use in subsequent layers.
By passing filters=None
separable_conv2d produces only a depthwise convolution layer
Global average pooling.
Pooling with a fixed kernel size.
1 x 1 x 1024
Set weight_decay for weights in Conv and DepthSepConv layers.
Copyright 2016 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Decay for the moving averages.
epsilon to prevent 0s in variance.
Turns off fused batch norm.
collection containing the moving mean and moving variance.
Final pooling and prediction
Temporal average pooling.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
=============================================================================
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"Orignaly, add_arg_scope = slim.add_arg_scope and layers = slim, now switch to"
more update-to-date tf.contrib.* API.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Make sure that round down does not go down by more than 10%.
"Note: want to round down, we adjust each split to match the total."
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts."
which provide numbered scopes.
b1 -> b2 * r -> b2
i -> (o * r) (bottleneck) -> o
"Note in contrast with expansion, we always have"
projection to produce the desired output size.
stride check enforces that we don't add residuals when spatial
dimensions are None
Depth matches
Don't do any splitting if we end up with less than 8 filters
on either side.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Make sure that round down does not go down by more than 10%.
Set conv defs defaults and overrides.
a) Set the tensorflow scope
b) set padding to default: note we might consider removing this
since it is also set by mobilenet_scope
c) set all defaults
d) set all extra overrides.
The current_stride variable keeps track of the output stride of the
"activations, i.e., the running product of convolution strides up to the"
current network layer. This allows us to invoke atrous convolution
whenever applying the next convolution would result in the activations
having output stride larger than the target output_stride.
The atrous convolution rate parameter.
Insert default parameters before the base scope which includes
any custom overrides set in mobilenet.
"If we have reached the target output_stride, then we need to employ"
atrous convolution with stride=1 and multiply the atrous rate by the
current unit's stride for use in subsequent layers.
Update params.
Only insert rate to params if rate > 1.
Set padding
Add all tensors that end with 'output' to
endpoints
1 x 1 x num_classes
Note: legacy scope name.
"Recover output shape, for unknown shape."
the network created will be trainble with dropout/batch norm
initialized appropriately.
Note: do not introduce parameters that would change the inference
"model here (for example whether to use bias), modify conv_def instead."
Set weight_decay for weights in Conv and FC layers.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
This is mostly a sanity test. No deep reason for these particular
constants.
""
"All but first 2 and last one have  two convolutions, and there is one"
extra conv that is not in the spec. (logits)
Check that depthwise are exposed.
"All but 3 op has 3 conv operatore, the remainign 3 have one"
and there is one unaccounted.
Verifies that depth_multiplier arg scope actually works
if no default min_depth is provided.
Verifies that depth_multiplier arg scope actually works
if no default min_depth is provided.
"All convolutions will be 8->48, except for the last one."
Verifies that mobilenet_base returns pre-pooling layer.
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
pyformat: disable
Architecture: https://arxiv.org/abs/1801.04381
Note: these parameters of batch norm affect the architecture
that's why they are here and not in training_scope.
pyformat: enable
NB: do not set depth_args unless they are provided to avoid overriding
whatever default depth_multiplier might have thanks to arg_scope.
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that
"'finegrain_classification_mode' is set to True, which means the embedding"
layer will not be shrinked when given a depth-multiplier < 1.0.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Notes for training NASNet Cifar Model
-------------------------------------
batch_size: 32
learning rate: 0.025
cosine (single period) learning rate decay
auxiliary head loss weighting: 0.4
clip global norm of all gradients by 5
600 epochs with a batch size of 32
This is used for the drop path probabilities since it needs to increase
the drop out probability over the course of training.
Notes for training large NASNet model on ImageNet
-------------------------------------
batch size (per replica): 16
learning rate: 0.015 * 100
learning rate decay factor: 0.97
num epochs per decay: 2.4
sync sgd with 100 replicas
auxiliary head loss weighting: 0.4
label smoothing: 0.1
clip global norm of all gradients by 10
Notes for training the mobile NASNet ImageNet model
-------------------------------------
batch size (per replica): 32
learning rate: 0.04 * 50
learning rate scaling factor: 0.97
num epochs per decay: 2.4
sync sgd with 50 replicas
auxiliary head weighting: 0.4
label smoothing: 0.1
clip global norm of all gradients by 10
Decay for the moving averages.
epsilon to prevent 0s in variance.
Decay for the moving averages.
epsilon to prevent 0s in variance.
Decay for the moving averages.
epsilon to prevent 0s in variance.
Shape of feature map before the final layer.
149 x 149 x 32
Run the reduction cells
Calculate the total number of cells in the network
Add 2 for the reduction cells
Calculate the total number of cells in the network
Add 2 for the reduction cells
"If ImageNet, then add an additional two for the stem cells"
Calculate the total number of cells in the network
Add 2 for the reduction cells
"If ImageNet, then add an additional two for the stem cells"
Find where to place the reduction cells or stride normal cells
Setup for building in the auxiliary head.
Run the cells
true_cell_num accounts for the stem cells
Final softmax layer
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Find where to place the reduction cells or stride normal cells
pylint: disable=protected-access
pylint: enable=protected-access
Setup for building in the auxiliary head.
Run the cells
true_cell_num accounts for the stem cells
pylint: disable=protected-access
pylint: enable=protected-access
Final softmax layer
pylint: disable=protected-access
pylint: enable=protected-access
Calculate the total number of cells in the network.
There is no distinction between reduction and normal cells in PNAS so the
total number of cells is equal to the number normal cells plus the number
of stem cells (two by default).
pylint: disable=protected-access
pylint: enable=protected-access
Calculate the total number of cells in the network.
There is no distinction between reduction and normal cells in PNAS so the
total number of cells is equal to the number normal cells plus the number
of stem cells (two by default).
Configuration for the PNASNet-5 model.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Copyright 2018 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Logits and predictions
Logits and predictions
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
Logits and predictions
Logits and predictions
Logits and predictions
Force all Variables to reside on the device.
Copyright 2017 The TensorFlow Authors. All Rights Reserved.
""
"Licensed under the Apache License, Version 2.0 (the ""License"");"
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
""
http://www.apache.org/licenses/LICENSE-2.0
""
"Unless required by applicable law or agreed to in writing, software"
"distributed under the License is distributed on an ""AS IS"" BASIS,"
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
"The cap for tf.clip_by_value, it's hinted from the activation distribution"
"that the majority of activation values are in the range [-6, 6]."
Skip path 1
Skip path 2
"First pad with 0's on the right and bottom, then shift the filter to"
include those 0's that were added.
"If odd number of filters, add an additional one to the second path."
Concat and apply BN
Set the prev layer to the current layer if it is none
Check to be sure prev layer stuff is setup correctly
num_or_size_splits=1
Apply conv operations
Combine hidden states using 'add'.
Add hiddenstate to the list of hiddenstates we can choose from
Dont stride if this is not one of the original hiddenstates
"Check if a stride is needed, then use a strided 1x1 here"
Determine if a reduction should be applied to make the number of
filters match.
Return the concat of all the states
Scale keep prob by layer number
The added 2 is for the reduction cells
Decrease the keep probability over time
""
Script for generating a two-class dataset in COCO format for training an obscured image classifier
""
Requires Python >= 3.6 because of the glob ** expression
""
Collect images and labels
"Labels: clean = 0, obscured = 1"
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils)
from data_management.megadb.schema import sequences_schema_check
resize is for displaying them more quickly
dataset and seq_id are required fields
sort the images in the sequence
pool = ThreadPool()
"print('len of rendering_info', len(rendering_info))"
"tqdm(pool.imap_unordered(render_image_info_partial, rendering_info), total=len(rendering_info))"
options = write_html_image_list()
options['headerHtml']
print('Checking that the MegaDB entries conform to the schema...')
sequences_schema_check.sequences_schema_check(sequences)
#######
""
visualize_db.py
""
Outputs an HTML page visualizing annotations (class labels and/or bounding boxes)
on a sample of images in a database in the COCO Camera Traps format
""
#######
%% Imports
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils)
Assumes the cameratraps repo root is on the path
%% Settings
Set to None to visualize all images
Target size for rendering; set either dimension to -1 to preserve aspect ratio
These are mutually exclusive
We sometimes flatten image directories by replacing a path separator with
another character.  Leave blank for the typical case where this isn't necessary.
Control rendering parallelization
%% Helper functions
"Translate the file name in an image entry in the json database to a path, possibly doing"
some manipulation of path separators
%% Core functions
"Optionally remove all images without bounding boxes, *before* sampling"
"Optionally include/remove images with specific labels, *before* sampling"
Put the annotations in a dataframe so we can select all annotations for a given image
Construct label map
Take a sample of images
Set of dicts representing inputs to render_db_bounding_boxes:
""
"bboxes, boxClasses, image_path"
iImage = 0
All the class labels we've seen for this image (with out without bboxes)
Iterate over annotations for this image
iAnn = 0; anno = annos_i.iloc[iAnn]
"We're adding html for an image before we render it, so it's possible this image will"
fail to render.  For applications where this script is being used to debua a database
"(the common case?), this is useful behavior, for other applications, this is annoying."
""
TODO: optionally write html only for images where rendering succeeded
...for each image
...def render_image_info
def process_images(...)
%% Command-line driver
"Copy all fields from a Namespace (i.e., the output from parse_args) to an object."
""
Skips fields starting with _.  Does not check existence in the target object.
Convert to an options object
%% Interactive driver(s)
%%
os.startfile(htmlOutputFile)
%% Constants and imports
%% Functions
PIL.Image.convert() returns a converted copy of this image
Null operation
Aspect ratio as width over height
ar = w / h
h = w / ar
ar = w / h
w = ar * h
The following three functions are modified versions of those at:
https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py
"To avoid duplicate colors with detection-only visualization, offset"
the classification class index by the number of detection classes
...if we have detection results
...if the confidence of this detection is above threshold
...for each detection
"print('Input must be of size [N, 4], but is ' + str(boxes_shape))"
If the total height of the display strings added to the top of the bounding
"box exceeds the top of the image, stack the strings below the bounding box"
instead of above.
Each display_str has a top and bottom margin of 0.05x.
Reverse list and print from bottom to top.
plt.grid(False)
"To fit the legend in, shrink current axis by 20%"
Put a legend to the right of the current axis
####
""
visualize_detector_output.py
""
"Render images with bounding boxes annotated on them to a folder, based on a detector output result"
file (json). The original images can be local or in Azure Blob Storage.
""
####
%% Imports
%% Constants
convert category ID from int to str
%% Options class
## Required inputs
# Options
%% Helper functions and constants
%% Main function
%% Load detector output
"%% Load images, annotate them and save"
max_conf = entry['max_detection_conf']
resize is for displaying them more quickly
%% Command-line driver
####
""
visualize_incoming_annotations.py
""
Spot-check the annotations received from iMerit by visualizing annotated bounding
boxes on a sample of images and display them in HTML.
""
####
%% Imports
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils)
%% Settings - change everything in this section to match your task
'/datadrive/SS_annotated/imerit_batch7_snapshotserengeti_2018_10_26/images'
'/home/yasiyu/mnt/wildlifeblobssc/rspb/gola/gola_camtrapr_data'
'/datadrive/IDFG/IDFG_20190104_images_to_annotate'
'/datadrive/emammal'
functions for translating from image_id in the annotation files to path to images in images_dir
the dash between seq and frame is different among the batches
specify which of the above functions to use for your dataset
%% Read in the annotations
put the annotations in a dataframes so we can select all annotations for a given image
%% Get a numerical to English label map; note that both the numerical key and the name are str
%% Visualize the bboxes on a sample of images
if len(annos_i) < 20:
continue
if len(images_html) > 400:  # cap on maximum to save
break
only save images with a particular class
"classes = list(annos_i.loc[:, 'category_id'])"
classes = [str(i) for i in classes]
if '3' not in classes:  # only save images with the 'group' class
continue
%% Write to HTML
""
prepare_api_output_for_timelapse.py
""
Takes output from the batch API and does some conversions to prepare
it for use in Timelapse.
""
Specifically:
""
* Removes the class field from each bounding box
* Optionally does query-based subsetting of rows
* Optionally does a search and replace on filenames
* Replaces backslashes with forward slashes
"* Renames ""detections"" to ""predicted_boxes"""
""
"Note that ""relative"" paths as interpreted by Timelapse aren't strictly relative as"
of 6/5/2019.  If your project is in:
""
c:\myproject
""
...and your .tdb file is:
""
c:\myproject\blah.tdb
""
...and you have an image at:
""
c:\myproject\imagefolder1\img.jpg
""
The .csv that Timelapse sees should refer to this as:
""
myproject/imagefolder1/img.jpg
""
...*not* as:
""
imagefolder1/img.jpg
""
Hence all the search/replace functionality in this script.  It's very straightforward
"once you get this and doesn't take time, but it's easy to forget to do this.  This will"
be fixed in an upcoming release.
""
%% Constants and imports
Python standard
pip-installable
"AI4E repos, expected to be available on the path"
%% Helper classes
Only process rows matching this query (if not None); this is processed
after applying os.normpath to filenames.
"If not none, replace the query token with this"
"If not none, prepend matching filenames with this"
%% Helper functions
"If there's no query, we're just pre-pending"
%% Main function
Create a temporary column we'll use to mark the rows we want to keep
This is the main loop over rows
Trim to matching rows
Timelapse legacy issue; we used to call this column 'predicted_boxes'
Write output
"write_api_results(detectionResults,outputFilename)"
%% Interactive driver
%%
%% Command-line driver (** outdated **)
"Copy all fields from a Namespace (i.e., the output from parse_args) to an object."
""
Skips fields starting with _.  Does not check existence in the target object.
"print('Setting {} to {}'.format(n,v))"
Convert to an options object
--------some stuff needed to get AJAX to work with bottle?--------#
-------------------------------------------------------------------------------- #
PREPARE TO QUEUE IMAGES FOR LABELING
-------------------------------------------------------------------------------- #
# Connect as USER to database DB_NAME through peewee and initialize database proxy
# Load embedding model
---------------------------------------------------------------------- #
CREATE QUEUE OF IMAGES TO LABEL
---------------------------------------------------------------------- #
Use classifier to generate predictions
# Update model predicted class in PostgreSQL database
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
-------------------------------------------------------------------------------- #
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI
-------------------------------------------------------------------------------- #
"# static routes (to serve CSS, etc.)"
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps"
"return bottle.static_file(filename, root='../../../../../../../../../../../.')"
# dynamic routes
if data['display_grayscale']:
indices_to_exclude.update(set(color_indices))
elif not data['display_grayscale']:
indices_to_exclude.update(set(grayscale_indices))
data['display_images'] = {}
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices]
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices]
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices]
data['display_images']['detection_categories'] = []
for i in indices:
if str(dataset.samples[i][1]) == 'None':
data['display_images']['detection_categories'].append('None')
else:
existing_category_entries = {cat.id: cat.name for cat in Category.select()}
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()"
data['display_images']['detection_categories'].append(cat_name)
Use image ids in images_to_label to get the corresponding dataset indices
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
print(set(dataset.set_indices[4]).update(set(indices_to_label)))
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Use image id images_to_label to get the corresponding dataset index
Update records in dataset dataloader but not in the PostgreSQL database
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?
Get the category id for the assigned label
Update entries in the PostgreSQL database
# get Detection table entries corresponding to the images being labeled
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database"
Update records in dataset dataloader
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])
Train on samples that have been labeled so far
dataset.set_kind(DetectionKind.UserDetection.value)
print(y_train)
Predict on the samples that have not been labeled
print(y_pred)
Update model predicted class in PostgreSQL database
timer = time.time()
for pos in range(len(y_pred)):
idx = dataset.current_set[pos]
det_id = dataset.samples[idx][0]
matching_detection_entries = (Detection
".select(Detection.id, Detection.category_id)"
.where((Detection.id == det_id)))
mde = matching_detection_entries.get()
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id)
command.execute()
print('Updating the database took %0.2f seconds'%(time.time() - timer))
Alternative: batch update PostgreSQL database
timer = time.time()
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))]
y_pred = [int(y) for y in y_pred]
"det_id_pred_pairs = list(zip(det_ids, y_pred))"
"case_statement = Case(Detection.id, det_id_pred_pairs)"
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))
command.execute()
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer))
Update dataset dataloader
"once the classifier has been trained the first time, switch to AL sampling"
""
make_oneclass_json.py
""
"Takes a coco-camera-traps .json database and collapses species classes to binary,"
optionally removing labels from empty images (to be detector-friendly) (depending on
"""experiment_type"")."
""
"Assumes that empty images are labeled as ""empty""."
""
%% Imports and environment
%% Core conversion function
"We're removing empty images from the annotation list, but not from"
"the ""images"" list; they'll still get used in detector training."
print('Ignoring empty annotation')
%% Interactive driver
%%
Load annotations
Convert from multi-class to one-class
Write out the one-class data
%% Command-line driver
""
plot_bounding_boxes.py
""
Takes a .json database containing bounding boxes and renders those boxes on the
source images.
""
"This assumes annotations in coco-camera-traps format, with absolute bbox"
coordinates.
""
%% Imports and environment
How many images should we process?  Set to -1 to process all images.
Should we randomize the image order?
%%  Read database and build up convenience mappings
Category ID to category name
Image ID to image info
Image ID to image path
Image ID to all annotations referring to this image
"%% Iterate over images, draw bounding boxes, write to file"
For each image
image = images[0]
Build up a list of bounding boxes to draw on this image
Load the image
Create figure and axes
Display the image
ann = imageAnnotations[0]
"For each annotation associated with this image, render bounding box and label"
"In the Rectangle() function, the first argument (""location"") is the bottom-left"
of the rectangle.
""
Origin is the upper-left of the image.
Add the patch to the Axes
Add a class label
This is magic goop that removes whitespace around image plots (sort of)
Write the output image
...for each image
""
plot_imerit_annotations.py
""
Takes a .json file full of bounding box annotations and renders those boxes on the
source images.
""
"This assumes annotations in the format we receive them, specifically:"
""
1) Relative bbox coordinates
"2) A list of .json objects, not a well-formatted .json file"
""
"I.e., don't use this on a COCO-style .json file.  See plot_bounding_boxes.py"
for the same operation performed on a proper COCO-camera-traps database.
""
%% Imports and environment
"os.makedirs(outputBase, exist_ok=True)"
%%  Read all source images and build up a hash table from image name to full path
"This spans training and validation directories, so it's not the same as"
just joining the image name to a base path
"%% Iterate over annotations, draw bounding boxes, write to file"
annData has keys:
""
"annotations, categories, images"
""
Each of these are lists of dictionaries
%% Render all annotations on each image in the sequence
%% Pull out image metadata
Build up a list of bounding boxes to draw on this image
Pull out just the image name from the filename
""
File names look like:
""
seq6efffac2-5567-11e8-b3fe-dca9047ef277.frame1.img59a94e52-23d2-11e8-a6a3-ec086b02610b.jpg
"m = re.findall(r'img(.*\.jpg)$', imgFileName, re.M|re.I)"
print(m)
assert(len(m) == 1)
queryFileName = m[0]
Map this image back to the original directory
"%% Loop over annotations, find annotations that match this image"
%%
"x,y,w,h"
""
"x,y is the bottom-left of the rectangle"
""
"x,y origin is the upper-left"
...for each annotation
%% Render with PIL (scrap)
%% Render with Matplotlib
Create figure and axes
Display the image
Location is the bottom-left of the rect
""
Origin is the upper-left
Add the patch to the Axes
This is magic goop that removes whitespace around image plots (sort of)
os.startfile(outputFileName)
%% Showing figures on-screen during debugging
plt.show()
Various (mostly unsuccessful) approaches to getting the plot window to show up
"in the foreground, which is a backend-specific operation..."
""
fig.canvas.manager.window.activateWindow()
fig.canvas.manager.window.raise_()
fm = plt.get_current_fig_manager()
"fm.window.attributes('-topmost', 1)"
"fm.window.attributes('-topmost', 0)"
""
# This is the one that I found to be most robust... at like 80% robust.
plt.get_current_fig_manager().window.raise_()
%%
...for each image
...for each file
""
convert_imerit_json_to_coco_json.py
""
"Takes a .json file with bounding boxes but no class labels, and a .json file containing the"
"class labels for those images, and creates a new json file with class labels and bounding"
boxes.
""
"The bounding box .json file is in the format returned by our annotators, which is not"
actually a fully-formed .json file; rather it's a series of .json objects
""
"Leaves filenames intact.  Stores imerit ""category IDs"" (empty/human/group/animal) in a new"
"field called ""annotation_type""."
""
%% Imports and constants
%% Configure files and paths
"For Snapshot Serengeti, we stored image IDs in our annotation files as:"
""
S1_B06_R1_PICT0008
""
...but the corresponding ID in the master database is actually:
""
S1\B06\R1\S1_B06_R1_PICT0008
""
"If this is ""True"", we'll expand the former to the latter"
Handling a one-off issue in which .'s were mysteriously replaced with -'s
"in our annotations.  This will be set dynamically, but I keep it here as"
a constant to remind me to remove this code when we clean this issue up.
Used in the (rare) case where a bounding box was added to an image that was originally
annotated as empty
Used in the (rare) case where we added bounding boxes to an image with multiple species
"Should we include ambiguous bounding boxes (with the ""ambiguous"" category label)"
in the output file?  Ambiguous boxes are boxes drawn on images with multiple species.
"%%  Read metadata from the master database, bounding boxes from the annotations file"
"The bounding box .json file is in the format returned by our annotators, which is not"
actually a fully-formed .json file; rather it's a series of .json objects
"Each element of annData is a dictionary corresponding to a single sequence, with keys:"
""
"annotations, categories, images"
sequence = annData[0]
%% Build convenience mappings
Image ID to images
Category ID to categories (referring to the database categories)
"Image ID to categories (i.e., species labels)"
Utility function we'll use to create annotations for images in empty
sequences (empty images in non-empty sequences already have annotations)
"%% Reformat annotations, grabbing category IDs from the master database (prep)"
iSequence = 0; sequence = annData[0]
"%% Reformat annotations, grabbing category IDs from the master database (loop)"
Make a copy here; we're going to manipulate the sequence annotations
when we need to add synthetic annotations for empty images
im = sequenceImages[0]
Are there any annotations in this sequence?
Which images in this sequence have annotations?
For each image in this sequence...
imeritImageID = im['id']
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG
Confirm that the file exists
Hande a one-off issue with our annotations
datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG
""
...had become:
""
datasetsnapshotserengeti.seqASG000001a-frame0.imgS1_B06_R1_PICT0008.JPG
Does it look like we encountered this issue?
Convert:
""
S1_B06_R1_PICT0008
""
...to:
""
S1/B06/B06_R1/S1_B06_R1_PICT0008
Make sure we only see each image once
Create empty annotations for empty images
""
Here we use the *unmodified* file name
Create an empty annotation for this image
Annotations still use the annotation filename (not database ID) at this point;
these will get converted to database IDs below when we process the
whole sequence.
Sanity-check image size
"print('Warning: img {} was listed in DB as {}x{}, annotated as {}x{}, actual size{}x{}'.format("
"old_id,new_im['width'],new_im['height'],im['width'],im['height'],imgObj.width,imgObj.height))"
...for each image in this sequence
For each annotation in this sequence...
ann = sequenceAnnotations[0]
Prepare an annotation using the category ID from the database and
the bounding box from the annotations file
Maintain iMerit's annotation category
Generate an (arbitrary) ID for this annotation; the COCO format has a concept
"of annotation ID, but our annotation files don't"
This was a one-off quirk with our file naming
We'll do special handling of images with multiple categories later
Store the annotation type (group/human/animal/empty)
This annotation has no bounding box but the image wasn't originally
annotated as empty
This annotation has a bounding box but the image was originally
annotated as empty
unnormalize the bbox
... for each annotation in this sequence
... for each sequence
%% Post-processing
Count empty images
...for each file
%% Sanity-check empty images
""
make_ss_annotation_image_folder.py
""
Take a directory full of images with the very long filenames we give annotators:
""
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension
""
"...along with a COCO-camera-traps database referring to those files, and:"
""
1) Creates a new COCO-camera-traps database with the original filenames in them
(copying the annotations)
""
2) Optionally creates a new directory with those images named according to the
"Snapshot Serengeti naming convention, including complete relative paths."
""
See convert_imerit_json_to_coco_json to see how we get from the original annotation
.json to a COCO-camera-traps database.
""
%% Constants and imports
%% Configure files/paths
%% Read the annotations (referring to the old filenames)
"%% Update filenames, optionally copying files"
im = data['images'][0]
For each image...
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG
"Find the image name, e.g. S1_B06_R1_PICT0008"
Convert:
""
S1_B06_R1_PICT0008.JPG
""
...to:
""
S1/B06/B06_R1/S1_B06_R1_PICT0008.JPG
...for each image
%% Write the revised database
""
get_annotation_tool_link.py
""
"Takes a COCO-camera-traps-style .json file with URLs already embedded, and prepares"
a link to the visipedia annotation tool that reviews a subset of those images.
""
""
create_new_annotation_json.py
""
"Creates a subset of a larger .json database, in this case specifically to pick some images"
from Snapshot Serengeti.
""
from utils import get_db_dicts
for seq in already_annotated:
seq_to_ims.pop(seq)
remove already annotated images
add lion images
lion_seqs = cat_to_seqs[cat_to_id['lionMale']] + cat_to_seqs[cat_to_id['lionFemale']]
#print(len(lion_seqs))
lion_seqs = [seq for seq in lion_seqs if seq not in already_annotated]
#print(len(lion_seqs))
"lion_seqs_to_annotate.extend(random.sample(lion_seqs, min(len(lion_seqs),num_lions)))"
print(len(lion_seqs_to_annotate))
num_elephants = 1000
elephant_seqs = cat_to_seqs[cat_to_id['elephant']]
#print(len(lion_seqs))
elephant_seqs = [seq for seq in elephant_seqs if seq not in already_annotated]
#print(len(lion_seqs))
"elephant_seqs_to_annotate = random.sample(elephant_seqs, num_elephants)"
num_empty = 10
empty_seqs_to_annotate = []
for loc in loc_to_seqs:
empty_seqs = cats_per_location[loc][cat_to_id['empty']]
empty_seqs = [seq for seq in empty_seqs if seq not in already_annotated]
empty_seqs = [seq for seq in empty_seqs if seq_to_season[seq] in seasons_to_keep]
"empty_seqs_to_annotate.extend(random.sample(empty_seqs, min(len(empty_seqs),num_empty)))"
ims_to_annotate.extend(empty_ims_to_annotate)
""
filter_database.py
""
"Look through a COCO-ct database and find images matching some crtieria, writing"
a subset of images and annotations to a new file.
""
%% Constants and imports
%% Configuration
%%  Read database and build up convenience mappings
Category ID to category name
Image ID to image info
Image ID to image path
%% Filter
ann = annotations[0]
Is this a tiny box or a group annotation?
"x,y,w,h"
All empty annotations should be classified as either empty or ambiguous
""
"The ambiguous cases are basically minor misses on the annotators' part,"
where two different small animals were present somewhere.
%% Write output file
""
find_images_for_classes.py
""
"Given a .json database, find images that are associated with one or more"
classes.
""
%% Constants and imports
%% Configuration
%%  Read database and build up convenience mappings
Category ID to category name
Image ID to image info
Image ID to image path
%% Look for target-class annotations
ann = annotations[0]
""
create_tfrecords_format.py
""
This script creates a tfrecords file from a dataset in VOTT format.
%% Imports and environment
set up the filenames and annotations
This loop reads the bboxes and corresponding labels and assigns them
the correct image. Kind of slow at the moment...
If needed: merging all classes
bbox_labels = ['Animal' for _ in bbox_labels]
BBox coords are stored in the format
"x_min (of width axis) y_min (of height axis), x_max, y_max"
Coordinate system starts in top left corner
"In this framework, we need ('ymin', 'xmin', 'ymax', 'xmax') format"
print out some stats
To make sure we loaded the bboxes correctly:
self.validate_bboxes()
For each image in the data set...
Make sure all are greater equal 0
%% Main tfrecord generation function
Propagate optional metadata to tfrecords
endfor each annotation for the current image
endfor each image
"Calculate number of shards to get the desired number of images per record,"
ensure it is evenly divisible by the number of threads
%% Command-line driver
%% Driver
""
make_tfrecords_cis_trans.py
""
Given a .json file that contains a three-element list (train/val/test) of image IDs and a .json database that contains
"those image IDs, generates tfrecords whose filenames include ""train""/""val""/""test"""
""
"dataset = json.load(open('/ai4efs/databases/snapshotserengeti/oneclass/SnapshotSerengeti_Seasons_1_to_4_tfrecord_format_valid_ims.json','r'))"
print('Creating trans_val tfrecords')
dataset = [im_id_to_im[idx] for idx in trans_val]
""
create_classification_tfrecords_from_json.py
""
Called from make_tfrecords_cis_trans.py if you're running a classification experiment.
""
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/'
datafile = 'eccv_18_annotation_files_oneclass/CaltechCameraTrapsECCV18'
image_file_root = datafolder+'eccv_18_all_images/'
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/'
datafolder = '/data/iwildcam/'
datafile = 'combined_iwildcam_annotations_oneclass/eccv_train_and_imerit_2'
need consecutive category ids
old_cat_id_to_new_cat_id = {categories[idx]['id']:idx+1 for idx in range(len(categories))}
print(old_cat_id_to_new_cat_id)
remove multiclass images
print(images[0])
print(vis_data[0])
""
make_tfrecords_from_json.py
""
"Given a coco-camera-traps .json file, creates tfrecords"
""
Thin wrapper for create_tfrecords_from_json.
""
%% Constants and imports
%% Constants and imports (interactive)
%%
%% Main tfrecord generation function
"check whether the input file has already been converted to the tfrecords format,"
"if not, convert"
"Calculate number of shards to get the desired number of images per record,"
ensure it is evenly divisible by the number of threads
%% Interactive driver
%%
%% Command-line driver
eMammal_make_tfrecords_train_val_test.py
""
"From the tfrecords_format json version of the database, creates three splits"
of tf_records according to a previously decided split of full image IDs.
configurations and paths
a tfrecord_format json
these are number of images
do not include empty images in the train set; note that some images from non-empty sequences
"end up being empty (no bbox can be labeled), so these will be included in train set anyways"
eMammal_make_splits.py
""
"Based on a tfrecords_format json file of the database, creates 3 splits according to"
the specified fractions based on location (images from the same location should be in
one split) or based on images.
""
"If a previous split is provided (append_to_previous_split is True), the entries in"
"each split will be preserved, and new entries will be appended, so that new models"
can warm start with a model trained on the original splits.
configurations and paths
approximate fraction for the new entries
read in the previous splits of image ID or location ID if available
"find new locations and assign them to a split, without reassigning any previous locations"
"find out which images are new, shuffle and split them"
do NOT sort the IDs to keep the shuffled order
export PYTHONPATH=$PYTHONPATH:tfmodels/research
add empty category
"add all images that don't have annotations, with cat empty"
print(detection_results.keys())
group the detections and gts by image id:
print(image_id)
print(len(scores))
print(len(labels))
recall_thresh = 0.9
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall])
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])"
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall])
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])"
exp_name = 'eccv_train'
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json'
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(len(detection_results['images']))
print(len(seqs))
"print(len(seqs[0]),len(seqs["
print(detection_results.keys())
group the detections by image id:
group the ground truth annotations by image id:
print(seq)
"for image_id, dets in per_image_detections.iteritems():"
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
print(num_gts)
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
if sum(detected_class_labels)>0:
print('detected animal box')
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])"
"print(scores, tp_fp_labels)"
num_total_gts += num_gts
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
print('valid box')
"print(best_im, best_score)"
exp_name = 'eccv_train'
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json'
export PYTHONPATH=$PYTHONPATH:tfmodels/research
need to loop over confidence values
"for each value, check if any detections on the image are > conf"
"If so, that image gets class ""animal"""
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0"
"for image_id, dets in per_image_detections.iteritems():"
"calc prec, rec for this confidence thresh"
group the ground truth annotations by image id:
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(detection_results.keys())
group the detections and gts by image id:
print(image_id)
print(len(scores))
print(len(labels))
recall_thresh = 0.9
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall])
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])"
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall])
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])"
print(sorted_ap)
plt.bar(sorted_ap)
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(detection_results.keys())
group the detections by image id:
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
print(gts)
print(num_gts)
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[:2])"
"print(scores, tp_fp_labels)"
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
export PYTHONPATH=$PYTHONPATH:tfmodels/research
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
print('detected animal box')
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])"
"print(scores, tp_fp_labels)"
num_total_gts += num_gts
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
print('valid box')
best_im = np.argmax(max_im_scores)
"print(best_im, best_score)"
"temp_labels = np.zeros(len(im_detection_labels),  dtype=np.int32)"
"temp_scores = np.zeros(len(im_detection_scores), dtype=np.float32)"
"for j in range(min(im_num_gts, len(im_detection_labels))):"
temp_labels[j] = True #TODO: this currently only works for oneclass?
temp_scores[j] = best_score
im_detection_labels = temp_labels
im_detection_scores = temp_scores
num_total_gts+=im_num_gts
"print(len(detection_scores), len(detection_scores[0]), len(detection_scores[1]))"
exp_name = 'small_balanced_cct'
export PYTHONPATH=$PYTHONPATH:tfmodels/research
print(seq)
"for image_id, dets in per_image_detections.iteritems():"
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
seq_num_gts.append(num_gts)
print(num_gts)
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
num_total_gts += 1
print('valid box')
"print(best_im, best_score)"
print('no valid box')
if sum(seq_num_gts)>0:
export PYTHONPATH=$PYTHONPATH:tfmodels/research
need to loop over confidence values
"for each value, check if any detections on the image are > conf"
"If so, that image gets class ""animal"""
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0"
"calc prec, rec for this confidence thresh"
export PYTHONPATH=$PYTHONPATH:tfmodels/research
group the detections by image id:
group the ground truth annotations by image id:
"[ymin, xmin, ymax, xmax] in absolute image coordinates."
detection scores for the boxes
0-indexed detection classes for the boxes
"[ymin, xmin, ymax, xmax] in absolute image coordinates"
0-indexed groundtruth classes for the boxes
print('detected animal box')
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])"
"print(scores, tp_fp_labels)"
num_total_gts += num_gts
if (tp_fp_labels[0].shape[0] != num_detections):
print('Incorrect label length')
if scores[0].shape[0] != num_detections:
print('Incorrect score length')
if tp_fp_labels[0].sum() > num_gts:
print('Too many correct detections')
print('valid box')
"print(best_im, best_score)"
""
evaluate_detections.py
""
Adapted from analyze_detection.py which is now archived.
""
%% Imports and constants
%% Functions
"labels input to compute_object_detection_metrics() needs to start at 0, not 1"
num_detections = len(dets['boxes'])
to prevent 'Invalid dimensions for box data.' error
this box will not match any detections
compute one-class precision/recall/average precision (if every box is just of an object class)
%% Command-line driver
""
detection_eval_utils.py
""
Utility functions used in evaluate_detections.py
""
group the ground truth annotations by image id
@task
def check_model_version(self):
"self.client.get('model_version', headers=headers, name='model_version')"
"part is a BodyPart object with b'Content-Type', and b'Content-Disposition', the later includes 'name' and 'filename' info"
images[part.headers['filename']] = part.content
upper limit on total content length (all images and parameters)
Camera trap images are usually 4:3 width to height
The config of the model in use (model/pipeline.config) has min_dimension
"600 and max_dimension 1024 for the keep_aspect_ratio_resizer, which first resize an image so"
"that the smaller edge is 600 pixels; if the longer edge is now more than 1024, it resizes such"
that the longer edge is 1024 pixels
(https://github.com/tensorflow/models/issues/1794)
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"# /ai4e_api_tools has been added to the PYTHONPATH, so we can reference those"
libraries directly.
Use the AI4EAppInsights library to send log messages. NOT REQURIED
"Use the AI4EService to execute your functions within a logging trace, which supports long-running/async"
"functions, handles SIGTERM signals from AKS, etc., and handles concurrent requests."
function for processing the request data to the /detect endpoint. It loads data or files into a
dictionary for access in the API function. It is passed as a parameter to the API setup.
check that the content uploaded is not too big
request.content_length is the length of the total payload
validate detection confidence value
check that the number of images is acceptable for this synchronous API
read input images and parameters
file of type SpooledTemporaryFile has attributes content_type and a read() method
"if the number of requests exceed this limit, a 503 is returned to the caller."
consolidate the images into batches and perform detection on them
detections is an array of dicts
filter the detections by the confidence threshold
"each result is [ymin, xmin, ymax, xmax, confidence, category]"
return results; optionally render the detections on the images and send the annotated images back
PIL.Image.convert() returns a converted copy of this image
performs inference
number of images should be small - all are loaded at once and a copy of resized version exists at one point
resize the images since the client side renders them as small images too
group the images into batches; image_batches is a list of lists
get the operators to go in the fetch list
the following two functions are from https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py
"print('Input must be of size [N, 4], but is ' + str(boxes_shape))"
If the total height of the display strings added to the top of the bounding
"box exceeds the top of the image, stack the strings below the bounding box"
instead of above.
Each display_str has a top and bottom margin of 0.05x.
Reverse list and print from bottom to top.
%% Constants and imports
Assumes ai4eutils is on the python path
https://github.com/Microsoft/ai4eutils
Assumes the cameratraps repo root is on the path
%% Options
"Make sure there is no overlap between the two sets, because this will cause"
issues in the code
## Required inputs
## Options
Can be a folder or a SAS URL
These apply only when we're doing ground-truth comparisons
"A list of output sets that we should count, but not render images for."
""
"Typically used to preview sets with lots of empties, where you don't want to"
"subset but also don't want to render 100,000 empty images."
""
"detections, non_detections"
"detections_animal, detections_person, detections_vehicle"
Used for summary statistics only
"Number of images to sample, -1 for ""all images"""
"Random seed for sampling, or None"
Optionally separate detections into categories (animal/vehicle/human)
Optionally replace one or more strings in filenames with other strings;
this is useful for taking a set of results generated for one folder structure
and applying them to a slightly different folder structure.
Allow bypassing API output loading when operating on previously-loaded results
Should we also split out a separate report about the detections that were
just below our main confidence threshold?
""
Currently only supported when ground truth is unavailable
Control rendering parallelization
Determines whether missing images force an error
...PostProcessingOptions
#%% Helper classes and functions
Flags used to mark images as positive or negative for P/R analysis (according
to ground truth and/or detector output)
This image is a negative
This image is a positive
Anything greater than this isn't clearly positive or negative
This image has annotations suggesting both negative and positive
"This image is not annotated or is annotated with 'unknown', 'unlabeled', ETC."
This image has not yet been assigned a state
"In some analyses, we add an additional class that lets us look at detections just below"
our main confidence threshold
Counter for the corresponding fields of class (actually enum) DetectionStatus
Check whether this image has unassigned-type labels
"assert image_has_unknown_labels is False, '{} has unknown labels'.format(annotations)"
Check whether this image has negative-type labels
"Check whether this image has positive labels, i.e. whether it has labels that are neither"
negative nor unknown
"If there are no image annotations, treat this as unknown"
n_negative += 1
im['_detection_status'] = DetectionStatus.DS_NEGATIVE
"If the image has more than one type of labels, it's ambiguous"
"note: booleans get automatically converted to 0/1, hence we can use the sum"
"After the check above, we can be sure it's only one of positive, negative, or unknown"
""
Important: do not merge the following 'unknown' branch with the first 'unknown' branch
"above, where we were testing 'if len(image_categories) == 0'"
""
If the image has only unknown labels
If the image has only negative labels
If the images has only positive labels
"Annotate the category, if it is unambiguous"
...for each image
...mark_detection_status()
"Leaving code in place for reading from blob storage, may support this"
in the future.
isfile() is slow when mounting remote directories; much faster to just try/except
on the image open.
Render images to a flat folder... we can use os.sep here because we've
already normalized paths
"errno.ENAMETOOLONG doesn't get thrown properly on Windows, so"
we awkwardly check against a hard-coded limit
Use slashes regardless of os
...render_bounding_boxes
Count items in each category
Optionally sort by filename before writing to html
Write the individual HTML files
...prepare_html_subpages()
%% Main function
#%% Expand some options for convenience
#%% Prepare output dir
#%% Load ground truth if available
Mark images in the ground truth as positive or negative
#%% Load detection (and possibly classification) results
Convert keys and values to lowercase
""
"In practice, keys are string integers, but I'm angry at variable casing"
so I'm converting those to lowercase too just to pound my fist.
"Add a column (pred_detection_label) to indicate predicted detection status, not separating out the classes"
"#%% If we have ground truth, remove images we can't match to ground truth"
fn = detector_files[0]; print(fn)
"assert fn in ground_truth_indexed_db.filename_to_id, 'Could not find ground truth for row {} ({})'.format(i_fn,fn)"
#%% Sample images for visualization
#%% Fork here depending on whether or not ground truth is available
"If we have ground truth, we'll compute precision/recall and sample tp/fp/tn/fn."
""
Otherwise we'll just visualize detections/non-detections.
#%% Detection evaluation: compute precision/recall
numpy array of detection probabilities
"numpy array of bools (0.0/1.0), and -1 as null value"
Don't include ambiguous/unknown ground truth in precision/recall analysis
"For completeness, include the result at a confidence threshold of 1.0"
Compute and print summary statistics
Thresholds go up throughout precisions/recalls/thresholds; find the last
value where recall is at or above target.  That's our precision @ target recall.
Flatten the confusion matrix
"#%% Collect classification results, if they exist"
Mapping of classnames to idx for the confusion matrix.
""
"The lambda is actually kind of a hack, because we use assume that"
the following code does not reassign classname_to_idx
Confusion matrix as defaultdict of defaultdict
""
"Rows / first index is ground truth, columns / second index is predicted category"
iDetection = 0; fn = detector_files[iDetection]; print(fn)
"If this image has classification predictions, and an unambiguous class"
"annotated, and is a positive image..."
"The unambiguous category, we make this a set for easier handling afterward"
"Compute the accuracy as intersection of union,"
i.e. (# of categories in both prediciton and GT)
divided by (# of categories in either prediction or GT
""
"In case of only one GT category, the result will be 1.0, if"
prediction is one category and this category matches GT
""
"It is 1.0/(# of predicted top-1 categories), if the GT is"
one of the predicted top-1 categories.
""
"It is 0.0, if none of the predicted categories is correct"
Distribute this accuracy across all predicted categories in the
confusion matrix
...for each file in the detection results
If we have classification results
Build confusion matrix as array from classifier_cm
Print some statistics
Prepare confusion matrix output
Get confusion matrix as string
Get fixed-size classname for each idx
Prepend class name on each line and add to the top
Print formatted confusion matrix
Plot confusion matrix
To manually add more space at bottom: plt.rcParams['figure.subplot.bottom'] = 0.1
""
"Add 0.5 to figsize for every class. For two classes, this will result in"
"fig = plt.figure(figsize=[4,4])"
...if we have classification results
#%% Render output
Write p/r table to .csv file in output directory
Write precision/recall plot to .png file in output directory
plt.show(block=False)
#%% Sampling
Sample true/false positives/negatives with correct/incorrect top-1
classification and render to html
Accumulate html image structs (in the format expected by write_html_image_lists)
"for each category, e.g. 'tp', 'fp', ..., 'class_bird', ..."
Add default entries by accessing them for the first time
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]"
"Each element will be a three-tuple with elements file,max_conf,detections"
"Assemble the information we need for rendering, so we can parallelize without"
dealing with Pandas
i_row = 0; row = images_to_visualize.iloc[0]
Filenames should already have been normalized to either '/' or '\'
This should already have been normalized to either '/' or '\'
...def render_image_with_gt(file_info)
file_info = files_to_render[0]
Map all the rendering results in the list rendering_results into the
dictionary images_html
Prepare the individual html image files
Write index.html
Show links to each GT class
""
We could do this without classification results; currently we don't.
Add links to all available classes
Close body and html tags
...for each image
"#%% Otherwise, if we don't have ground truth..."
#%% Sample detections/non-detections
Accumulate html image structs (in the format expected by write_html_image_list)
for each category
Add default entries by accessing them for the first time
"Maps detection categories - e.g. ""human"" - to result set names, e.g."
"""detections_human"""
Add a set of results for each category and combination of categories
Create output directories
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]"
"Each element will be a three-tuple with elements [file,max_conf,detections]"
"Assemble the information we need for rendering, so we can parallelize without"
dealing with Pandas
i_row = 0; row = images_to_visualize.iloc[0]
Filenames should already have been normalized to either '/' or '\'
Get unique categories above the threshold for this image
Local function for parallelization
...def render_image_no_gt(file_info):
Map all the rendering results in the list rendering_results into the
dictionary images_html
Prepare the individual html image files
Write index.HTML
Add links to all available classes
os.startfile(output_html_file)
...if we do/don't have ground truth
...process_batch_results
%% Interactive driver(s)
%%
os.start(ppresults.output_html_file)
%% Command-line driver
""
subset_json_detector_output.py
""
"Creates one or more subsets of a detector API output file (.json), doing either"
"or both of the following (if both are requested, they happen in this order):"
""
"1) Retrieve all elements where filenames contain a specified query string,"
"optionally replacing that query with a replacement token. If the query is blank,"
can also be used to prepend content to all filenames.
""
"2) Create separate .jsons for each unique path, optionally making the filenames"
"in those .json's relative paths.  In this case, you specify an output directory,"
rather than an output path.  All images in the folder blah\foo\bar will end up
in a .json file called blah_foo_bar.json.
""
##
""
Sample invocations (splitting into multiple json's):
""
"Read from ""1800_idfg_statewide_wolf_detections_w_classifications.json"", split up into"
"individual .jsons in 'd:\temp\idfg\output', making filenames relative to their individual"
folders:
""
"python subset_json_detector_output.py ""d:\temp\idfg\1800_idfg_statewide_wolf_detections_w_classifications.json"" ""d:\temp\idfg\output"" --split_folders --make_folder_relative"
""
"Now do the same thing, but instead of writing .json's to d:\temp\idfg\output, write them to *subfolders*"
corresponding to the subfolders for each .json file.
""
"python subset_json_detector_output.py ""d:\temp\idfg\1800_detections_S2.json"" ""d:\temp\idfg\output_to_folders"" --split_folders --make_folder_relative --copy_jsons_to_folders"
""
##
""
Sample invocations (creating a single subset matching a query):
""
"Read from ""1800_detections.json"", write to ""1800_detections_2017.json"""
""
"Include only images matching ""2017"", and change ""2017"" to ""blah"""
""
"python subset_json_detector_output.py ""d:\temp\1800_detections.json"" ""d:\temp\1800_detections_2017_blah.json"" --query 2017 --replacement blah"
""
"Include all images, prepend with ""prefix/"""
""
"python subset_json_detector_output.py ""d:\temp\1800_detections.json"" ""d:\temp\1800_detections_prefix.json"" --replacement ""prefix/"""
""
##
""
"To subset a COCO Camera Traps .json database, see subset_json_db.py"
""
%% Constants and imports
%% Helper classes
Only process files containing the token 'query'
"Replace 'query' with 'replacement' if 'replacement' is not None.  If 'query' is None,"
prepend 'replacement'
Should we split output into individual .json files for each folder?
"Folder level to use for splitting ['bottom','top','n_from_bottom','dict']"
""
'dict' requires 'split_folder_param' to be a dictionary mapping each filename
to a token.
"When using the 'n_from_bottom' parameter to define folder splitting, this"
defines the number of directories from the bottom.  'n_from_bottom' with
a parameter of zero is the same as 'bottom'.
""
"When 'split_folder_mode' is 'dict', this should be a dictionary mapping each filename"
to a token.
Only meaningful if split_folders is True: should we convert pathnames to be relative
the folder for each .json file?
"Only meaningful if split_folders and make_folder_relative are True: if not None,"
"will copy .json files to their corresponding output directories, relative to"
output_filename
Should we over-write .json files?
"If copy_jsons_to_folders is true, do we require that directories already exist?"
Threshold on confidence
%% Main function
Format spec:
""
https://github.com/microsoft/CameraTraps/tree/master/api/batch_processing
iImage = 0; im = images_in[0]
Find all detections above threshold for this image
"If there are no detections above threshold, set the max probability"
"to -1, unless it already had a negative probability."
Otherwise find the max confidence
Did this thresholding result in a max-confidence change?
"We should only be *lowering* max confidence values (i.e., making them negative)"
...for each image
i_image = 0; im = images_in[0]
Only take images that match the query
...for each image
"Path('/blah').parts is ('/','blah')"
Handle paths like:
""
"/, \, /stuff, c:, c:\stuff"
Input validation
data = add_missing_detection_results_fields(data)
Map images to unique folders
im = data['images'][0]
Optionally make paths relative
dirname = list(folders_to_images.keys())[0]
im = folders_to_images[dirname][0]
dirname = list(folders_to_images.keys())[0]
"Recycle the 'data' struct, replacing 'images' every time... medium-hacky, but"
forward-compatible in that I don't take dependencies on the other fields
...for each directory
...if we're splitting folders
%% Interactive driver
%%
%% Subset a file without splitting
"%% Subset and split, but don't copy to individual folders"
"input_filename = r""D:\temp\idfg\1800_detections_S2.json"""
"%% Subset and split, copying to individual folders"
%% Just do a filename replacement
"python subset_json_detector_output.py ""D:\temp\idfg\detections_idfg_20190625_refiltered.json"" ""D:\temp\idfg\detections_idfg_20190625_refiltered_renamed.json"" --query ""20190625-hddrop/"" --replacement """""
"python subset_json_detector_output.py ""D:\temp\idfg\detections_idfg_20190625_refiltered_renamed.json"" ""D:\temp\idfg\output"" --split_folders --make_folder_relative --copy_jsons_to_folders"
%% Command-line driver
Convert to an options object
###
""
combine_api_outputs.py
""
"Merges two or more .json files in batch API output format, optionally"
writing the results to another .json file.
"- Concatenates image lists, erroring if images are not unique."
- Errors if class lists are conflicting; errors on unrecognized fields.
"- Checks compatibility in info structs, within reason."
""
File format:
""
https://github.com/microsoft/CameraTraps/tree/master/api/batch_processing#batch-processing-api-output-format
""
Command-line use:
""
combine_api_outputs input1.json input2.json ... inputN.json output.json
""
Also see combine_api_shard_files (not exposed via the command line yet) to combine the
intermediate files created by the API.
""
###
%% Constants and imports
%% Merge functions
"Map image filenames to detections, we'll convert to a list later"
Check compatibility of detection categories
Check compatibility of classification categories
"Merge image lists, checking uniqueness"
"print('Warning, duplicate results for image: {}'.format(im['file']))"
"Merge info dicts, within reason"
Don't check completion time fields
...for each dictionary
Convert merged image dictionaries to a sorted list
detection_list = input_lists[0]
d = detection_list[0]
%% Driver
""
load_api_results.py
""
Loads the output of the batch processing API (json).
Also functions to group entries by seq_id.
""
Includes the deprecated functions that worked with the old CSV API output format.
""
%% Constants and imports
%% Functions for grouping by sequence_id
example
input 'file': 'SER/S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG'
output 'id': 'S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG'
%% Functions for loading the result as a Pandas DataFrame
Sanity-check that this is really a detector output file
Fields in the API output json other than 'images'
Normalize paths to simplify comparisons later
Pack the json output into a Pandas DataFrame
Optionally replace some path tokens to match local paths to the original blob structure
string_to_replace = list(options.detector_output_filename_replacements.keys())[0]
"TODO: hit some silly issues with vectorized str() and escaped characters, vectorize"
this later.
""
"detection_results['image_path'].str.replace(string_to_replace,replacement_string)"
i_row = 0
TODO: read double_precision from a config elsewhere
Sanity-check that this is really a detector output file
Normalize paths to simplify comparisons later
De-serialize detections
Optionally replace some path tokens to match local paths to the original blob structure
string_to_replace = list(options.detector_output_filename_replacements.keys())[0]
"TODO: hit some silly issues with vectorized str() and escaped characters, vectorize"
this later.
""
"detection_results['image_path'].str.replace(string_to_replace,replacement_string)"
iRow = 0
#######
""
convert_output_format.py
""
Converts between file formats output by our batch processing API.  Currently
"supports json <--> csv conversion, but this should be the landing place for any"
conversion - including between future .json versions - that we support in the
future.
""
#######
%% Imports
%% Conversion functions
"We add an output column for each class other than 'empty',"
containing the maximum probability of  that class for each image
Skip sub-threshold detections
Our .json format is xmin/ymin/w/h
""
Our .csv format was ymin/xmin/ymax/xmax
"Category 0 is empty, for which we don't have a column, so the max"
confidence for category N goes in column N-1
...for each detection
...for each image
Format spec:
""
https://github.com/microsoft/CameraTraps/tree/master/api/batch_processing
iFile = 0; row = df.iloc[iFile]
Our .csv format was ymin/xmin/ymax/xmax
""
Our .json format is xmin/ymin/w/h
...for each detection
...for each image
%% Interactive driver
%%
%%
%% Command-line driver
""
separate_detections_into_folders.py
""
"Given a .json file with batch processing results, separate the files in that"
"set of results into folders that contain animals/people/vehicles/nothing,"
according to per-class thresholds.
""
Places images that are above threshold for multiple classes into 'multiple'
folder.
""
"Image files are copied, not moved."
""
Preserves relative paths within each of those folders; cannot be used with .json
files that have absolute paths in them.
""
"For example, if your .json file has these images:"
""
a/b/c/1.jpg
a/b/d/2.jpg
a/b/e/3.jpg
a/b/f/4.jpg
""
And let's say:
""
"* The results say that the first three images are empty/person/vehicle, respectively"
"* The fourth image is above threshold for ""animal"" and ""person"""
* You specify an output base folder of c:\out
""
You will get the following files:
""
c:\out\empty\a\b\c\1.jpg
c:\out\people\a\b\d\2.jpg
c:\out\vehicles\a\b\e\3.jpg
c:\out\multiple\a\b\f\4.jpg
""
Hard-coded to work with MDv3 and MDv4 output files.  Not currently future-proofed
"past the classes in MegaDetector v4, not currently ready for species-level classification."
""
%% Constants and imports
Occasionally we have near-zero confidence detections associated with COCO classes that
didn't quite get squeezed out of the model in training.  As long as they're near zero
"confidence, we just ignore them."
%% Options class
Inputs
Dictionary mapping categories (plus 'multiple' and 'empty') to output folders
%% Support functions
Find the maximum confidence for each category
""
det = detections[0]
"For zero-confidence detections, we occasionally have leftover goop"
from COCO classes
assert det['conf'] < invalid_category_epsilon
Count the number of thresholds exceeded
Do we have a custom threshold for this category?
If this is above multiple thresholds
...def process_detection()
%% Main function
Create output folder if necessary
Load detection results
Map class names to output folders
i_image = 7600; d = detections[i_image]; print(d)
%% Interactive driver
%%
%%
%% Find a particular file
%% Command-line driver
"python api\batch_processing\postprocessing\separate_detections_into_folders.py ""d:\temp\rspb_mini.json"" ""d:\temp\demo_images\rspb_2018_2019_mini"" ""d:\temp\separation_test"" --nthreads 2"
Convert to an options object
#######
""
remove_repeat_detections.py
""
"Used after running find_repeat_detections, then manually filtering the results,"
to create a final filtered output file.
""
"If you want to use this script, we recommend that you read the user's guide:"
""
https://github.com/microsoft/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms
""
#######
%% Constants and imports
%% Main function
%% Interactive driver
%%
"python remove_repeat_detections.py ""F:\wpz\6714_detections_wpz_all_20191015233705.SUCP_subset.json"" ""F:\wpz\6714_detections_wpz_all_20191015233705.SUCP_subset_filtered.json"" ""F:\wpz\rde\filtering_2019.10.24.16.52.54"""
%% Command-line driver
#######
""
repeat_detections_core.py
""
Core utilities shared by find_repeat_detections and remove_repeat_detections.
""
#######
%% Imports and environment
"from ai4eutils; this is assumed to be on the path, as per repo convention"
Imports I'm not using but use when I tinker with parallelization
""
from multiprocessing import Pool
from multiprocessing.pool import ThreadPool
import multiprocessing
import joblib
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings"
"Metadata Warning, tag 256 had too many entries: 42, expected 1"
%% Constants
%% Classes
inputFlename = r'D:\temp\tigers_20190308_all_output.csv'
Relevant for rendering HTML or filtering folder of images
""
"imageBase can also be a SAS URL, in which case some error-checking is"
disabled.
Don't consider detections with confidence lower than this as suspicious
Don't consider detections with confidence higher than this as suspicious
What's the IOU threshold for considering two boxes the same?
How many occurrences of a single location (as defined by the IOU threshold)
are required before we declare it suspicious?
"Ignore ""suspicious"" detections larger than some size; these are often animals"
taking up the whole image.  This is expressed as a fraction of the image size.
A list of classes we don't want to treat as suspicious. Each element is an int.
Set to zero to disable parallelism
Load detections from a filter file rather than finding them from the detector output
".json file containing detections, should be called detectionIndex.json in the filtering_* folder"
produced in the first pass
(optional) List of filenames remaining after deletion of identified
repeated detections that are actually animals.  This should be a flat
"text file, one relative filename per line.  See enumerate_images()."
Turn on/off optional outputs
State variables
"Replace filename tokens after reading, useful when the directory structure"
has changed relative to the structure the detector saw
How many folders up from the leaf nodes should we be going to aggregate images?
"The data table (Pandas DataFrame), as loaded from the input json file via"
load_api_results()
"The other fields in the input json file, loaded via load_api_results()"
The data table after modification
dict mapping folder names to whole rows from the data table
dict mapping filenames to rows in the master table
"An array of length nDirs, where each element is a list of DetectionLocation"
objects for that directory that have been flagged as suspicious
#%% Helper functions
#%% Look for matches (one directory) (function)
List of DetectionLocations
iDirectoryRow = 0; row = rows.iloc[iDirectoryRow]
Don't bother checking images with no detections above threshold
"Array of dict, where each element is"
{
"'category': '1',  # str value, category ID"
"'conf': 0.926,  # confidence of this detections"
"'bbox': [x_min, y_min, width_of_box, height_of_box]  # (x_min, y_min) is upper-left,"
all in relative coordinates and length
}
For each detection in this image
Optionally exclude some classes from consideration as suspicious
Is this detection too big to be suspicious?
These are relative coordinates
print('Ignoring very large detection with area {}'.format(area))
For each detection in our candidate list
Is this a match?
"If so, add this example to the list for this detection"
We *don't* break here; we allow this instance to possibly
match multiple candidates.  There isn't an obvious right or
wrong here.
...for each detection on our candidate list
"If we found no matches, add this to the candidate list"
...for each detection
...for each row
...def find_matches_in_directory(dirName)
#%% Render problematic locations to html (function)
suspiciousDetectionsThisDir is a list of DetectionLocation objects
For each problematic detection in this directory
""
iDetection = 0; detection = suspiciousDetectionsThisDir[iDetection];
iDetection = 0; detection = suspiciousDetectionsThisDir[0]
_ = pretty_print_object(detection)
Render images
iInstance = 0; instance = detection.instances[iInstance]
...for each instance
Write html for this detection
Use the first image from this detection (arbitrary) as the canonical example
that we'll render for the directory-level page.
...for each detection
Write the html file for this directory
...def render_images_for_directory(iDir)
"#%% Update the detection table based on suspicious results, write .csv output"
"An array of length nDirs, where each element is a list of DetectionLocation"
objects for that directory that have been flagged as suspicious
For each directory
For each suspicious detection group in this directory
For each instance of this suspicious detection
This should match the bbox for the detection event
The bbox for this instance should be almost the same as the bbox
"for this detection group, where ""almost"" is defined by the IOU"
threshold.
if iou < options.iouThreshold:
"print('IOU warning: {},{}'.format(iou,options.iouThreshold))"
Make sure the bounding box matches
"Make the probability negative, if it hasn't been switched by"
another bounding box
...for each instance
...for each detection
...for each directory
Update maximum probabilities
For each row...
We should only be making detections *less* likely
row['max_confidence'] = str(maxP)
"Negative probabilities should be the only reason maxP changed, so"
we should have found at least one negative value
...if there was a meaningful change to the max probability for this row
...for each row
If we're also writing output...
"...def update_detection_table(RepeatDetectionResults,options)"
#%% Main function
#%% Input handling
Check early to avoid problems with the output folder
Load file
"Before doing any real work, make sure we can *probably* access images"
"This is just a cursory check on the first image, but it heads off most"
"problems related to incorrect mount points, etc.  Better to do this before"
spending 20 minutes finding repeat detections.
#%% Separate files into directories
This will be a map from a directory name to smaller data frames
This is a mapping back into the rows of the original table
"TODO: in the case where we're loading an existing set of FPs after manual filtering,"
"we should load these data frames too, rather than re-building them from the input."
iRow = 0; row = detectionResults.iloc[0]
Create a new DataFrame with just this row
rowsByDirectory[dirName] = pd.DataFrame(row)
Convert lists of rows to proper DataFrames
#% Look for matches (or load them from file)
length-nDirs list of lists of DetectionLocation objects
"Are we actually looking for matches, or just loading from a file?"
We're actually looking for matches...
iDir = 0; dirName = dirsToSearch[iDir]
#%% Find suspicious locations based on match results
For each directory
""
iDir = 51
A list of DetectionLocation objects
A list of DetectionLocation objects
occurrenceList is a list of file/detection pairs
"Find the images corresponding to this bounding box, render boxes"
Load the filtering file
"We're skipping detection-finding, but to see which images are actually legit false"
"positives, we may be looking for physical files or loading from a text file."
For each directory
iDir = 0; detections = suspiciousDetections[0]
""
"suspiciousDetections is an array of DetectionLocation objects,"
one per directory.
For each detection that was present before filtering
iDetection = 0; detection = detections[iDetection]
Are we checking the directory to see whether detections were actually false
"positives, or reading from a list?"
Is the image still there?
"If not, remove this from the list of suspicious detections"
...for each detection
...for each directory
...if we are/aren't finding detections (vs. loading from file)
Render problematic locations with html (loop)
options.pbar = tqdm(total=nDirs)
For each directory
iDir = 51
Add this directory to the master list of html files
...for each directory
Write master html file
Remove unicode characters before formatting
...if we're rendering html
Create filtering directory
iDir = 0; suspiciousDetectionsThisDir = suspiciousDetections[iDir]
suspiciousDetectionsThisDir is a list of DetectionLocation objects
iDetection = 0; detection = suspiciousDetectionsThisDir[0]
Write out the detection index
...if we're writing filtering info
...find_repeat_detections()
#######
""
find_repeat_detections.py
""
"If you want to use this script, we recommend that you read the user's guide:"
""
https://github.com/microsoft/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms
""
"Really, don't try to run this script without reading the user's guide, you'll think"
it's more magical than it is.
""
"This script looks through a sequence of detections in the API output json file, and finds"
"candidates that might be ""repeated false positives"", i.e. that random branch that the"
detector thinks is an animal/person/vehicle.
""
"Typically after running this script, you would do a manual step to remove"
"true positives, then run remove_repeat_detections to produce a final output file."
""
There's no way that statement was self-explanatory; see the user's guide.
""
#######
%% Constants and imports
%% Interactive driver
%%
"inputFilename = os.path.join(baseDir, '5570_blah_detections.json')"
"outputFilename = mpt.insert_before_extension(inputFilename,"
'filtered')
%% Command-line driver
With HTML (debug)
"python find_repeat_detections.py ""D:\temp\tigers_20190308_all_output.json"" ""D:\temp\tigers_20190308_all_output.filtered.json"" --renderHtml --debugMaxDir 100 --imageBase ""d:\wildlife_data\tigerblobs"" --outputBase ""d:\temp\repeatDetections"""
Without HTML (debug)
"python find_repeat_detections.py ""D:\temp\tigers_20190308_all_output.json"" ""D:\temp\tigers_20190308_all_output.filtered.json"" --debugMaxDir 100 --imageBase ""d:\wildlife_data\tigerblobs"" --outputBase ""d:\temp\repeatDetections"""
With HTML (for real)
"python find_repeat_detections.py ""D:\temp\tigers_20190308_all_output.json"" ""D:\temp\tigers_20190308_all_output.filtered.json"" --renderHtml --imageBase ""d:\wildlife_data\tigerblobs"" --outputBase ""d:\temp\repeatDetections"""
Convert to an options object
""
If a request has been sent to AML for batch scoring but the monitoring thread of the API was
"interrupted (uncaught exception or having to re-start the API container), we could manually"
"aggregate results from each shard using this script, assuming all jobs submitted to AML have finished."
""
Need to have set environment variables STORAGE_ACCOUNT_NAME and STORAGE_ACCOUNT_KEY to those of the
"storage account backing the API. Also need to adjust the INTERNAL_CONTAINER, AML_CONTAINER and"
AML_CONFIG fields in api_core/orchestrator_api/api_config.py to match the instance of the API that this
request was submitted to.
""
May need to change the import statement in api_core/orchestrator_api/orchestrator.py
"""from sas_blob_utils import SasBlob"" to"
"""from .sas_blob_utils import SasBlob"" to not confuse with the module in AI4Eutils;"
"and change ""import api_config"" to"
"""from api.batch_processing.api_core.orchestrator_api import api_config"""
Execute this script from the root of the repository. You may need to add the repository to PYTHONPATH.
"list_jobs_submitted cannot be serialized (""can't pickle _thread.RLock objects ""), but"
do not need it for aggregating results
name of the container in the internal storage account to store user facing files:
"image list, detection results and failed images list."
name of the container in the internal storage account to store outputs of each AML job
"if this number of times the thread wakes up to check is exceeded, stop the monitoring thread"
number of retries in the monitoring thread for getting job status and aggregating results (each counted separately)
lower case; must be tuple for endswith to take as arg
max number of images in a container to accept for processing
how many images are processed by each call to the scoring API
update API task manager after submitting x jobs to AML Compute
AML Compute
service principle for authenticating to AML
version of the detector model in use
max number of blobs to list in the output blob container
URLs to the 3 output files expires after this many days
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
Get the entire path with all slashes after the container
TODO - fix this for testing out only the query string
exhaustively listed all blobs in the container
list_blobs will have been returned by one of the two stopping conditions
Service principle authentication for AML
%% Utility functions
return the current UTC time in string format '2019-05-19 08:57:43'
"return current UTC time in succinct format as a string, e.g. '20190519085759'"
"image_paths will have length at least 1, otherwise would have ended before this step"
%% AML Compute
default values are required and need to be literal values or data references as JSON
setting the overwrite flag to True overwrites any datastore that was created previously with that name
"internal_datastore stores all user-facing files: list of images, detection results, list of failed images"
and it so happens that each job also needs the list of images as an input
"output_datastore stores the output from score.py in each job, which is another container"
in the same storage account as interl_datastore
various attempts at getting the child_run's ID
child_run_id = None
"print('pipeline_run:', pipeline_run)"
for child_run in pipeline_run.get_children():
child_run_id = child_run.id  # we can do this because there's only one step in the pipeline - not working
""
print('=' * 20)
"exp = Experiment(self.ws, job_id)"
"run = Run(exp, pipeline_run.id)"
"print('run:', run)"
for c in run.get_children():
print('found run.get_children:')
print(c.id)
child_run_id = c.id
print('=' * 20)
list_jobs_active[job_id]['step_run_id']  = child_run_id  # this is the ID we can identify the output folder with
%% AML Monitor
list of images do not have request_name and timestamp in the file name so score.py can locate it easily
The more efficient method is to know the run_id which is the folder name that the result is written to.
"Since we can't reliably get the run_id after submitting the run, resort to listing all blobs in the output"
container and match by the request_id
listing all (up to a large limit) because don't want to worry about generator next_marker
blob_path is azureml/run_id/output_requestID/out_file_name.json
"""request"" is part of the AML job_id"
order the json output keys
upload aggregated results to output_store
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
"ai4e_api_tools has been added to the PYTHONPATH, so we can reference those"
libraries directly.
"Log requests, traces and exceptions to the Application Insights service"
Use the AI4EAppInsights library to send log messages.
Use the internal-container AI for Earth Task Manager (not for production use!).
Use the AI4EWrapper to executes your functions within a logging trace.
"Also, helps support long-running/async functions."
Instantiate blob storage service to the internal container to put intermediate results and files
required params
"if use_url, then images_requested_json_sas is required"
check model_version and request_name params are valid
check request_name has only allowed characters
TODO check that the expiry date of input_container_sas is at least a month into the future
TODO check images_requested_json_sas is a blob not a container
wrap_async_endpoint executes the function in a new thread and wraps it within a logging trace
request_name and request_submission_timestamp are for appending to output file names
"image_paths can be a list of strings (paths on Azure blobs or public URLs), or a list of lists,"
"each of length 2 and is the [image_id, metadata] pair"
case 1 - listing all images in the container
list all images to process
case 2 - user supplied a list of images to process; can include metadata
apply the first_n and sample_n filters
we sample by shuffling the image paths and take the first sample_n images
finalized image_paths is uploaded to internal_container; all sharding and scoring use the uploaded list
the list of images json does not have request_name or timestamp in the file name so that score.py can locate it
set up connection to AML Compute and data stores
do this for each request since pipeline step is associated with the data stores
start another thread to monitor the jobs and consolidate the results when they finish
time.sleep() blocks the current thread only
"check the status of the jobs, with retries"
need to periodically check the enumerations are what AML returns - not the same as in their doc
"if all jobs finished, aggregate the results and return the URLs to the output files"
"retrieve and join the output CSVs from each job, with retries"
output_file_urls_str = json.dumps(output_file_urls)
"not all jobs are finished, update the status with number of shards finished"
"not all jobs are finished but the maximum number of checking cycle is reached, stop this thread"
wrap_sync_endpoint wraps your function within a logging trace.
wrap_sync_endpoint wraps your function within a logging trace.
Number of decimal places to round to for confidence and bbox coordinates
PIL.Image.convert() returns a converted copy of this image
"change from [y1, x1, y2, x2] to [x1, y1, width_box, height_box]"
convert numpy floats to Python floats
performs inference
number of images should be small - all are loaded at once and a copy of resized version exists at one point
2000 images are okay on a NC6s_v3
group the images into batches; image_batches is a list of lists
we keep track of the image_ids (and image_metas when available) to be able to output the list of failed images
start the TF session to process all images
get the operators
apply the confidence threshold
determine if there is metadata attached to each image_id
im_to_open will be a tempfile with a generated name
open is lazy; load() loads the image so we know it can be read successfully
self.image_ids does not include any failed images; self.image_ids is overwritten here
API's internal container where the list of image paths is stored
a json file containing a list of image paths this job should process
bool argument parsing is tricky - bool(any string) is True
get model from model registry
exclude the end_index; default is to process all images in this request
"items in this array can be strings or [image_id, metadata]"
""
prepare_api_submission.py
""
"This module is somewhere between ""documentation"" and ""code"".  It is intended to"
capture the steps the precede running a job via the AI for Earth Camera Trap
"Image Processing API, and it automates a couple of those steps.  We hope to"
gradually automate all of these.
""
Here's the stuff we usually do before submitting a job:
""
"1) Upload data to Azure... we do this with azcopy, not addressed in this script"
""
2) List the files you want the API to process... this module supports that via
enumerate_blobs_to_file.
""
3) Divide that list into chunks that will become individual API submissions...
this module supports that via divide_files_into_tasks.
"3) Put each .json file in a blob container, and generate a read-only SAS"
URL for it.  Not automated right now.
""
4) Generate the API query(ies) you'll submit to the API... this module supports that
via generate_api_queries.
""
5) Submit the API query... I currently do this with Postman.
""
6) Monitor task status
""
7) Combine multiple API outputs
""
"8) We're now into what we really call ""postprocessing"", rather than ""data_preparation"","
"but... possibly do some amount of partner-specific renaming, folder manipulation, etc."
"This is very partner-specific, but generally done via:"
""
find_repeat_detections.py
subset_json_detector_output.py.
postprocess_batch_results.py
""
%% Imports and constants
assumes ai4eutils is on the path
%% File enumeration
print('Finished writing list {}'.format(output_file))
%% Dividing files into multiple tasks
https://www.geeksforgeeks.org/break-list-chunks-size-n-python/
i_chunk = 0; chunk = chunks[0]
keep only whitelisted chars
i_url = 0; file_list_sas_url = file_list_sas_urls[0]
%% Tools for working with API output
"I suspect this whole section will move to a separate file at some point,"
so leaving these imports and constants here for now.
Download all three urls to temporary files
""
"detections, failed_images, images"
Load all three files
Diff submitted and processed images
Confirm that the failed images are a subset of the missing images
%% Interactive driver
%%
%%
%%
%%
""
manage_api_submission.py
""
Semi-automated process for submitting and managing camera trap
API jobs.
""
%% Imports
%% Constants I set per job
These point to the same container; the read-only token is used for
accessing images; the write-enabled token is used for writing file lists
"Supported model_versions: '4', '3', '4_prelim'"
""
Also available at the /supported_model_versions and /default_model_version endpoints
""
"additional_job_args = {""model_version"":""4_prelim""}"
"%% Derived variables, path setup"
Turn warnings into errors if more than this many images are missing
import clipboard; clipboard.copy(read_only_sas_url)
configure mount point with rclone config
rclone mount mountname: z:
Not yet automated:
""
Mounting the image source (see comment above)
""
"Submitting the jobs (code written below, but it doesn't really work)"
""
Handling failed jobs/shards/images (though most of the code exists in generate_resubmission_list)
""
Pushing the final results to shared storage and generating a SAS URL to share with the collaborator
""
Pushing the previews to shared storage
%% Support functions
https://gist.github.com/zed/c2168b9c52b032b5fb7d
"scheme, netloc, path, query, fragment"
%% Enumerate blobs to files
folder_name = folder_names[0]
"If this is intended to be a folder, it needs to end in '/', otherwise files that start"
with the same string will match too
%% Divide images into chunks for each folder
This will be a list of lists
list_file = list_files[0]
%% Copy image lists to blob storage for each job
Maps  job name to a remote path
chunked_folder_files = folder_chunks[0]; chunk_file = chunked_folder_files[0]
periods not allowed in job names
...for each task within this task group
...for each folder
%% Generate API calls for each job
job_name = list(job_name_to_list_url.keys())[0]
%% Estimate total time
"%% Run the jobs (still in progress, doesn't actually work yet)"
"Not working yet, something is wrong with my post call"
import requests
task_group_request_strings = request_strings_by_task_group[0]; request_string = task_group_request_strings[0]
"response = requests.post(submission_endpoint_url,json=request_string)"
print(response.json())
"List of task IDs, grouped by logical job"
%% Manually define task groups if we ran the jobs manually
%% Status check
"%% Look for failed shards or missing images, start new jobs if necessary"
i_task_group = 0; task_group = task_groups[i_task_group]; task_id = task_group[0]
assert n_failed_shards == 0
Each task group corresponds to one of our folders
...for each task
...for each task group
"%% Resubmit jobs for failed shards, add to appropriate task groups"
%%
%% Pull results
i_task_group = 0; task_group = task_groups[i_task_group]; task_id = task_group[0]
n_failed_shards = int(response['status']['message']['num_failed_shards'])
assert n_failed_shards == 0
Each task group corresponds to one of our folders
...for each task
...for each task group
%% Combine results from task groups into final output files
i_folder = 0; folder_name = folder_names[i_folder]
task_id = task_group[0]
Check that we have (almost) all the images
Something has gone bonkers if there are images in the results that
aren't in the request
...for each folder
%% Post-processing (no ground truth)
i_folder = 0; folder_name_raw = folder_names[i_folder]
%% Manual processing follows
""
"Everything after this should be considered mostly manual, and no longer includes"
looping over folders.
""
"%% Repeat detection elimination, phase 1"
%% Manual RDE step
# DELETE THE ANIMALS ##
%% Re-filtering
%% Post-processing (post-RDE)
i_folder = 0; folder_name_raw = folder_names[i_folder]
api_output_file = folder_name_to_combined_output_file[folder_name]
%% Subsetting
i_folder = 0; folder_name = folders[i_folder]
img_file = BytesIO(urlopen.urlopen(url).read())
image = Image.open(img_file).convert('RGB')
image = mpimg.imread(url)
Actual detection
calculate the size
img_file = BytesIO(urlopen.urlopen(inputFileName).read())
image = Image.open(img_file).convert('RGB')
Add the patch to the Axes
"plt.savefig(outputFileName, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)"
model configuration
"if(path == ""/""):"
"path = ""index"""
#####
""
run_tf_detector.py
""
"Functions to load a TensorFlow detection model, run inference,"
and render bounding boxes on images.
""
"See the ""test driver"" cell for example invocation."
""
#####
"%% Constants, imports, environment"
%% Core detection functions
image = mpimg.imread(url)
Actual detection
Read the image file
image = mpimg.imread(inputFileName)
Display the image
"top, left, bottom, right"
""
"x,y origin is the upper-left"
Location is the bottom-left of the rect
""
Origin is the upper-left
Add the patch to the Axes
This is magic goop that removes whitespace around image plots (sort of)
"plt.savefig(outputFileName, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)"
%% Test driver
import os
MODEL_FILE = r'/Users/ranjanbalappa/backup/camera-trap/checkpoint/frozen_inference_graph.pb'
TARGET_IMAGES = os.listdir('static/gallery')
TARGET_IMAGES = ['static/gallery/' + f for f in TARGET_IMAGES]
# # Load and run detector on target images
detection_graph = load_model(MODEL_FILE)
startTime = time.time()
"boxes,scores,classes,images = generate_detections(detection_graph,TARGET_IMAGES)"
elapsed = time.time() - startTime
"print(""Done running detector on {} files in {}"".format(len(images),humanfriendly.format_timespan(elapsed)))"
assert len(boxes) == len(TARGET_IMAGES)
inputFileNames = TARGET_IMAGES
outputFileNames=[]
confidenceThreshold=0.9
plt.ioff()
"render_bounding_boxes(boxes, scores, classes, TARGET_IMAGES)"
print(TARGET_IMAGES[0])
output_img = {}
for img_file in TARGET_IMAGES:
"box, score, clss = generate_image_detections(detection_graph, img_file)"
"name, ext = os.path.splitext(img_file.split('/')[-1])"
"num_objects, bboxes = draw_image_detections(box, score, clss, img_file, 'static/results/' + name )"
output_img[img_file.split('/')[-1]] = {
"'num_objects': num_objects,"
"'image_name': img_file.split('/')[-1],"
"'result': 'Animal Detected' if num_objects > 0 else 'No Animal Detected',"
'bboxes': bboxes
}
import json
"with open('static/gallery_results/results.json', 'w') as res:"
"json.dump(output_img, res)"
from . import model
from . import aadConfig as aad
api_url = apiconfig.api['base_url'] + '/camera-trap/detect?confidence={1}&render={1}'
routes for cameratrapassets as these are being loaded
from the cameratrapassets directory instead of the static directory
"def track_images(file, name):"
print(str(e))
resize_images(images)
"bbox points, confidence"
print(img_result)
redirect to home if no images to display
"gallery_images = random.sample(gallery_images, 12)"
from . import aadConfig as aad
api url
Dropzone settings
app.config['AUTHORITY_URL'] =  aad.AUTHORITY_HOST_URL + '/' + aad.TENANT
app.config['DROPZONE_IN_FORM'] = True
app.config['DROPZONE_UPLOAD_ON_CLICK'] = True
app.config['DROPZONE_UPLOAD_BTN_ID'] =  'submit'
app.config[' DROPZONE_UPLOAD_ACTION'] = 'processimages'
Uploads settings
model configuration
# sourceMappingURL=popper.min.js.map
Noty.overrideDefaults({
"layout   : 'topRight',"
"theme    : 'mint',"
"closeWith: ['click', 'button'],"
"timeout: 1500,"
animation: {
"open : 'animated fadeInRight',"
close: 'animated fadeOutRight'
}
});
Initialize
var bLazy = new Blazy({
container: '.scroll-class'
});
timeout: 2500
Defining this global in .eslintrc.json would create a danger of using the global
"unguarded in another place, it seems safer to define global only for this module"
Define a local copy of jQuery
The jQuery object is actually just the init constructor 'enhanced'
Need init if jQuery is called (just allow error to be thrown if not included)
Support: Android <=4.0 only
Make sure we trim BOM and NBSP
The current version of jQuery being used
The default length of a jQuery object is 0
Get the Nth element in the matched element set OR
Get the whole matched element set as a clean array
Return all the elements in a clean array
Return just the one element from the set
Take an array of elements and push it onto the stack
(returning the new matched element set)
Build a new jQuery matched element set
Add the old object onto the stack (as a reference)
Return the newly-formed element set
Execute a callback for every element in the matched set.
For internal use only.
"Behaves like an Array's method, not like a jQuery method."
Handle a deep copy situation
Skip the boolean and the target
Handle case when target is a string or something (possible in deep copy)
Extend jQuery itself if only one argument is passed
Only deal with non-null/undefined values
Extend the base object
Prevent Object.prototype pollution
Prevent never-ending loop
Recurse if we're merging plain objects or arrays
Ensure proper type for the source value
"Never move original objects, clone them"
Don't bring in undefined values
Return the modified object
Unique for each copy of jQuery on the page
Assume jQuery is ready without the ready module
Detect obvious negatives
Use toString instead of jQuery.type to catch host objects
"Objects with no prototype (e.g., `Object.create( null )`) are plain"
Objects with prototype are plain iff they were constructed by a global Object function
Evaluates a script in a global context
Support: Android <=4.0 only
results is for internal usage only
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
"Go through the array, only saving the items"
that pass the validator function
arg is for internal usage only
"Go through the array, translating each of the items to their new values"
"Go through every key on the object,"
Flatten any nested arrays
A global GUID counter for objects
jQuery.support is not used in Core but other projects attach their
properties to it so it needs to exist.
Populate the class2type map
Support: real iOS 8.2 only (not reproducible in simulator)
`in` check used to prevent JIT error (gh-2145)
hasOwn isn't used here due to false negatives
regarding Nodelist length in IE
For CommonJS and CommonJS-like environments where a proper `window`
"is present, execute the factory and get jQuery."
For environments that do not have a `window` with a `document`
"(such as Node.js), expose a factory as module.exports."
This accentuates the need for the creation of a real `window`.
"e.g. var jQuery = require(""jquery"")(window);"
See ticket #14549 for more info.
Pass this if window is not defined yet
"Edge <= 12 - 13+, Firefox <=18 - 45+, IE 10 - 11, Safari 5.1 - 9+, iOS 6 - 9.1"
"throw exceptions when non-strict code (e.g., ASP.NET 4.5) accesses strict mode"
"arguments.callee.caller (trac-13335). But as of jQuery 3.0 (2016), strict mode should be common"
enough that all such attempts are guarded in a try block.
"Support: Chrome <=57, Firefox <=52"
"In some browsers, typeof returns ""function"" for HTML <object> elements"
"(i.e., `typeof document.createElement( ""object"" ) === ""function""`)."
We don't want to classify *any* DOM node as a function.
"Support: Firefox 64+, Edge 18+"
"Some browsers don't support the ""nonce"" property on scripts."
"On the other hand, just using `getAttribute` is not enough as"
the `nonce` attribute is reset to an empty string whenever it
becomes browsing-context connected.
See https://github.com/whatwg/html/issues/2369
See https://html.spec.whatwg.org/#nonce-attributes
The `node.getAttribute` check was added for the sake of
`jQuery.globalEval` so that it can fake a nonce-containing node
via an object.
Support: Android <=2.3 only (functionish RegExp)
Defining this global in .eslintrc.json would create a danger of using the global
"unguarded in another place, it seems safer to define global only for this module"
Define a local copy of jQuery
The jQuery object is actually just the init constructor 'enhanced'
Need init if jQuery is called (just allow error to be thrown if not included)
Support: Android <=4.0 only
Make sure we trim BOM and NBSP
The current version of jQuery being used
The default length of a jQuery object is 0
Get the Nth element in the matched element set OR
Get the whole matched element set as a clean array
Return all the elements in a clean array
Return just the one element from the set
Take an array of elements and push it onto the stack
(returning the new matched element set)
Build a new jQuery matched element set
Add the old object onto the stack (as a reference)
Return the newly-formed element set
Execute a callback for every element in the matched set.
For internal use only.
"Behaves like an Array's method, not like a jQuery method."
Handle a deep copy situation
Skip the boolean and the target
Handle case when target is a string or something (possible in deep copy)
Extend jQuery itself if only one argument is passed
Only deal with non-null/undefined values
Extend the base object
Prevent Object.prototype pollution
Prevent never-ending loop
Recurse if we're merging plain objects or arrays
Ensure proper type for the source value
"Never move original objects, clone them"
Don't bring in undefined values
Return the modified object
Unique for each copy of jQuery on the page
Assume jQuery is ready without the ready module
Detect obvious negatives
Use toString instead of jQuery.type to catch host objects
"Objects with no prototype (e.g., `Object.create( null )`) are plain"
Objects with prototype are plain iff they were constructed by a global Object function
Evaluates a script in a global context
Support: Android <=4.0 only
results is for internal usage only
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
"Go through the array, only saving the items"
that pass the validator function
arg is for internal usage only
"Go through the array, translating each of the items to their new values"
"Go through every key on the object,"
Flatten any nested arrays
A global GUID counter for objects
jQuery.support is not used in Core but other projects attach their
properties to it so it needs to exist.
Populate the class2type map
Support: real iOS 8.2 only (not reproducible in simulator)
`in` check used to prevent JIT error (gh-2145)
hasOwn isn't used here due to false negatives
regarding Nodelist length in IE
Local document vars
Instance-specific data
Instance methods
Use a stripped-down indexOf as it's faster than native
https://jsperf.com/thor-indexof-vs-for/5
Regular expressions
http://www.w3.org/TR/css3-selectors/#whitespace
http://www.w3.org/TR/CSS21/syndata.html#value-def-identifier
Attribute selectors: http://www.w3.org/TR/selectors/#attribute-selectors
Operator (capture 2)
"""Attribute values must be CSS identifiers [capture 5] or strings [capture 3 or capture 4]"""
"To reduce the number of selectors needing tokenize in the preFilter, prefer arguments:"
1. quoted (capture 3; capture 4 or capture 5)
2. simple (capture 6)
3. anything else (capture 2)
"Leading and non-escaped trailing whitespace, capturing some non-whitespace characters preceding the latter"
For use in libraries implementing .is()
We use this for POS matching in `select`
Easily-parseable/retrievable ID or TAG or CLASS selectors
CSS escapes
http://www.w3.org/TR/CSS21/syndata.html#escaped-characters
NaN means non-codepoint
Support: Firefox<24
"Workaround erroneous numeric interpretation of +""0x"""
BMP codepoint
Supplemental Plane codepoint (surrogate pair)
CSS string/identifier serialization
https://drafts.csswg.org/cssom/#common-serializing-idioms
U+0000 NULL becomes U+FFFD REPLACEMENT CHARACTER
Control characters and (dependent upon position) numbers get escaped as code points
Other potentially-special ASCII characters get backslash-escaped
Used for iframes
See setDocument()
"Removing the function wrapper causes a ""Permission Denied"""
error in IE
"Optimize for push.apply( _, NodeList )"
Support: Android<4.0
Detect silently failing push.apply
Leverage slice if possible
Support: IE<9
Otherwise append directly
Can't trust NodeList.length
"nodeType defaults to 9, since context defaults to document"
Return early from calls with invalid selector or context
Try to shortcut find operations (as opposed to filters) in HTML documents
"If the selector is sufficiently simple, try using a ""get*By*"" DOM method"
"(excepting DocumentFragment context, where the methods don't exist)"
ID selector
Document context
"Support: IE, Opera, Webkit"
TODO: identify versions
getElementById can match elements by name instead of ID
Element context
"Support: IE, Opera, Webkit"
TODO: identify versions
getElementById can match elements by name instead of ID
Type selector
Class selector
Take advantage of querySelectorAll
Support: IE 8 only
Exclude object elements
qSA considers elements outside a scoping root when evaluating child or
"descendant combinators, which is not what we want."
"In such cases, we work around the behavior by prefixing every selector in the"
list with an ID selector referencing the scope context.
Thanks to Andrew Dupont for this technique.
"Capture the context ID, setting it first if necessary"
Prefix every selector in the list
Expand context for sibling selectors
All others
"Use (key + "" "") to avoid collision with native prototype properties (see Issue #157)"
Only keep the most recent entries
Remove from its parent by default
release memory in IE
Use IE sourceIndex if available on both nodes
Check if b follows a
Known :disabled false positives: fieldset[disabled] > legend:nth-of-type(n+2) :can-disable
Only certain elements can match :enabled or :disabled
https://html.spec.whatwg.org/multipage/scripting.html#selector-enabled
https://html.spec.whatwg.org/multipage/scripting.html#selector-disabled
Check for inherited disabledness on relevant non-disabled elements:
* listed form-associated elements in a disabled fieldset
https://html.spec.whatwg.org/multipage/forms.html#category-listed
https://html.spec.whatwg.org/multipage/forms.html#concept-fe-disabled
* option elements in a disabled optgroup
https://html.spec.whatwg.org/multipage/forms.html#concept-option-disabled
"All such elements have a ""form"" property."
Option elements defer to a parent optgroup if present
Support: IE 6 - 11
Use the isDisabled shortcut property to check for disabled fieldset ancestors
"Where there is no isDisabled, check manually"
Try to winnow out elements that can't be disabled before trusting the disabled property.
"Some victims get caught in our net (label, legend, menu, track), but it shouldn't"
"even exist on them, let alone have a boolean value."
Remaining elements are neither :enabled nor :disabled
Match elements found at the specified indexes
Expose support vars for convenience
Support: IE <=8
"Assume HTML when documentElement doesn't yet exist, such as inside loading iframes"
https://bugs.jquery.com/ticket/4833
Return early if doc is invalid or already selected
Update global variables
"Support: IE 9-11, Edge"
"Accessing iframe documents after unload throws ""permission denied"" errors (jQuery #13936)"
"Support: IE 11, Edge"
Support: IE 9 - 10 only
Support: IE<8
Verify that getAttribute really returns attributes and not properties
(excepting IE8 booleans)
"Check if getElementsByTagName(""*"") returns only elements"
Support: IE<9
Support: IE<10
Check if getElementById returns elements by name
"The broken getElementById methods don't pick up programmatically-set names,"
so use a roundabout getElementsByName test
ID filter and find
Support: IE 6 - 7 only
getElementById is not reliable as a find shortcut
Verify the id attribute
Fall back on getElementsByName
Tag
DocumentFragment nodes don't have gEBTN
"By happy coincidence, a (broken) gEBTN appears on DocumentFragment nodes too"
Filter out possible comments
Class
QSA and matchesSelector support
matchesSelector(:active) reports false when true (IE9/Opera 11.5)
qSa(:focus) reports false when true (Chrome 21)
We allow this because of a bug in IE8/9 that throws an error
whenever `document.activeElement` is accessed on an iframe
"So, we allow :focus to pass through QSA all the time to avoid the IE error"
See https://bugs.jquery.com/ticket/13378
Build QSA regex
Regex strategy adopted from Diego Perini
Select is set to empty string on purpose
This is to test IE's treatment of not explicitly
"setting a boolean content attribute,"
since its presence should be enough
https://bugs.jquery.com/ticket/12359
"Support: IE8, Opera 11-12.16"
Nothing should be selected when empty strings follow ^= or $= or *=
"The test attribute must be unknown in Opera but ""safe"" for WinRT"
https://msdn.microsoft.com/en-us/library/ie/hh465388.aspx#attribute_section
Support: IE8
"Boolean attributes and ""value"" are not treated correctly"
"Support: Chrome<29, Android<4.4, Safari<7.0+, iOS<7.0+, PhantomJS<1.9.8+"
Webkit/Opera - :checked should return selected option elements
http://www.w3.org/TR/2011/REC-css3-selectors-20110929/#checked
IE8 throws error here and will not see later tests
"Support: Safari 8+, iOS 8+"
https://bugs.webkit.org/show_bug.cgi?id=136851
In-page `selector#id sibling-combinator selector` fails
Support: Windows 8 Native Apps
The type and name attributes are restricted during .innerHTML assignment
Support: IE8
Enforce case-sensitivity of name attribute
FF 3.5 - :enabled/:disabled and hidden elements (hidden elements are still enabled)
IE8 throws error here and will not see later tests
Support: IE9-11+
IE's :disabled selector does not pick up the children of disabled fieldsets
Opera 10-11 does not throw on post-comma invalid pseudos
Check to see if it's possible to do matchesSelector
on a disconnected node (IE 9)
This should fail with an exception
"Gecko does not error, returns false instead"
Element contains another
Purposefully self-exclusive
"As in, an element does not contain itself"
Document order sorting
Flag for duplicate removal
Sort on method existence if only one input has compareDocumentPosition
Calculate position if both inputs belong to the same document
Otherwise we know they are disconnected
Disconnected nodes
Choose the first element that is related to our preferred document
Maintain original order
Exit early if the nodes are identical
Parentless nodes are either documents or disconnected
"If the nodes are siblings, we can do a quick check"
Otherwise we need full lists of their ancestors for comparison
Walk down the tree looking for a discrepancy
Do a sibling check if the nodes have a common ancestor
Otherwise nodes in our document sort first
Set document vars if needed
IE 9's matchesSelector returns false on disconnected nodes
"As well, disconnected nodes are said to be in a document"
fragment in IE 9
Set document vars if needed
Set document vars if needed
Don't get fooled by Object.prototype properties (jQuery #13807)
"Unless we *know* we can detect duplicates, assume their presence"
Clear input after sorting to release objects
See https://github.com/jquery/sizzle/pull/225
"If no nodeType, this is expected to be an array"
Do not traverse comment nodes
Use textContent for elements
innerText usage removed for consistency of new lines (jQuery #11153)
Traverse its children
Do not include comment or processing instruction nodes
Can be adjusted by the user
Move the given value to match[3] whether quoted or unquoted
nth-* requires argument
numeric x and y parameters for Expr.filter.CHILD
remember that false/true cast respectively to 0/1
other types prohibit arguments
Accept quoted arguments as-is
Strip excess characters from unquoted arguments
Get excess from tokenize (recursively)
advance to the next closing parenthesis
excess is a negative index
Return only captures needed by the pseudo filter method (type and argument)
Shortcut for :nth-*(n)
:(first|last|only)-(child|of-type)
Reverse direction for :only-* (if we haven't yet done so)
non-xml :nth-child(...) stores cache data on `parent`
Seek `elem` from a previously-cached index
...in a gzip-friendly way
Support: IE <9 only
Defend against cloned attroperties (jQuery gh-1709)
Fallback to seeking `elem` from the start
"When found, cache indexes on `parent` and break"
Use previously-cached element index if available
...in a gzip-friendly way
Support: IE <9 only
Defend against cloned attroperties (jQuery gh-1709)
xml :nth-child(...)
or :nth-last-child(...) or :nth(-last)?-of-type(...)
Use the same loop as above to seek `elem` from the start
Cache the index of each encountered element
Support: IE <9 only
Defend against cloned attroperties (jQuery gh-1709)
"Incorporate the offset, then check against cycle size"
pseudo-class names are case-insensitive
http://www.w3.org/TR/selectors/#pseudo-classes
Prioritize by case sensitivity in case custom pseudos are added with uppercase letters
Remember that setFilters inherits from pseudos
The user may use createPseudo to indicate that
arguments are needed to create the filter function
just as Sizzle does
But maintain support for old signatures
Potentially complex pseudos
Trim the selector passed to compile
to avoid treating leading and trailing
spaces as combinators
Match elements unmatched by `matcher`
Don't keep the element (issue #299)
"""Whether an element is represented by a :lang() selector"
is based solely on the element's language value
"being equal to the identifier C,"
"or beginning with the identifier C immediately followed by ""-""."
The matching of C against the element's language value is performed case-insensitively.
"The identifier C does not have to be a valid language name."""
http://www.w3.org/TR/selectors/#lang-pseudo
lang value must be a valid identifier
Miscellaneous
Boolean properties
"In CSS3, :checked should return both checked and selected elements"
http://www.w3.org/TR/2011/REC-css3-selectors-20110929/#checked
Accessing this property makes selected-by-default
options in Safari work properly
Contents
http://www.w3.org/TR/selectors/#empty-pseudo
":empty is negated by element (1) or content nodes (text: 3; cdata: 4; entity ref: 5),"
but not by others (comment: 8; processing instruction: 7; etc.)
nodeType < 6 works because attributes (2) do not appear as children
Element/input types
Support: IE<8
"New HTML5 attribute values (e.g., ""search"") appear with elem.type === ""text"""
Position-in-collection
Add button/input type pseudos
Easy API for creating new setFilters
Comma and first run
Don't consume trailing commas as valid
Combinators
Cast descendant combinators to space
Filters
Return the length of the invalid excess
if we're just parsing
"Otherwise, throw an error or return tokens"
Cache the tokens
Check against closest ancestor/preceding element
Check against all ancestor/preceding elements
"We can't set arbitrary data on XML nodes, so they don't benefit from combinator caching"
Support: IE <9 only
Defend against cloned attroperties (jQuery gh-1709)
Assign to newCache so results back-propagate to previous elements
Reuse newcache so results back-propagate to previous elements
A match means we're done; a fail means we have to keep checking
Get initial elements from seed or context
"Prefilter to get matcher input, preserving a map for seed-results synchronization"
"If we have a postFinder, or filtered seed, or non-seed postFilter or preexisting results,"
...intermediate processing is necessary
...otherwise use results directly
Find primary matches
Apply postFilter
Un-match failing elements by moving them back to matcherIn
Get the final matcherOut by condensing this intermediate into postFinder contexts
Restore matcherIn since elem is not yet a final match
Move matched elements from seed to results to keep them synchronized
"Add elements to results, through postFinder if defined"
The foundational matcher ensures that elements are reachable from top-level context(s)
Avoid hanging onto element (issue #299)
Return special upon seeing a positional matcher
Find the next relative operator (if any) for proper handling
"If the preceding token was a descendant combinator, insert an implicit any-element `*`"
We must always have either seed elements or outermost context
Use integer dirruns iff this is the outermost matcher
Add elements passing elementMatchers directly to results
"Support: IE<9, Safari"
"Tolerate NodeList properties (IE: ""length""; Safari: <number>) matching elements by id"
Track unmatched elements for set filters
They will have gone through all possible matchers
"Lengthen the array for every element, matched or not"
"`i` is now the count of elements visited above, and adding it to `matchedCount`"
makes the latter nonnegative.
Apply set filters to unmatched elements
"NOTE: This can be skipped if there are no unmatched elements (i.e., `matchedCount`"
"equals `i`), unless we didn't visit _any_ elements in the above loop because we have"
no element matchers and no seed.
"Incrementing an initially-string ""0"" `i` allows `i` to remain a string only in that"
"case, which will result in a ""00"" `matchedCount` that differs from `i` but is also"
numerically zero.
Reintegrate element matches to eliminate the need for sorting
Discard index placeholder values to get only actual matches
Add matches to results
Seedless set matches succeeding multiple successful matchers stipulate sorting
Override manipulation of globals by nested matchers
Generate a function of recursive functions that can be used to check each element
Cache the compiled function
Save selector and tokenization
Try to minimize operations if there is only one selector in the list and no seed
(the latter of which guarantees us context)
Reduce context if the leading compound selector is an ID
"Precompiled matchers will still verify ancestry, so step up a level"
Fetch a seed set for right-to-left matching
Abort if we hit a combinator
"Search, expanding context for leading sibling combinators"
"If seed is empty or no tokens remain, we can return early"
Compile and execute a filtering function if one is not provided
Provide `match` to avoid retokenization if we modified the selector above
One-time assignments
Sort stability
Support: Chrome 14-35+
Always assume duplicates if they aren't passed to the comparison function
Initialize against the default document
Support: Webkit<537.32 - Safari 6.0.3/Chrome 25 (fixed in Chrome 27)
Detached nodes confoundingly follow *each other*
"Should return 1, but returns 4 (following)"
Support: IE<8
"Prevent attribute/property ""interpolation"""
https://msdn.microsoft.com/en-us/library/ms536429%28VS.85%29.aspx
Support: IE<9
"Use defaultValue in place of getAttribute(""value"")"
Support: IE<9
Use getAttributeNode to fetch booleans when getAttribute lies
Deprecated
Implement the identical functionality for filter and not
Single element
"Arraylike of elements (jQuery, arguments, Array)"
Filtered directly for both simple and complex selectors
"If this is a positional/relative selector, check membership in the returned set"
"so $(""p:first"").is(""p:last"") won't return true for a doc with two ""p""."
Initialize a jQuery object
A central reference to the root jQuery(document)
A simple way to check for HTML strings
Prioritize #id over <tag> to avoid XSS via location.hash (#9521)
Strict HTML recognition (#11290: must start with <)
Shortcut simple #id case for speed
"HANDLE: $(""""), $(null), $(undefined), $(false)"
Method init() accepts an alternate rootjQuery
so migrate can support jQuery.sub (gh-2101)
Handle HTML strings
Assume that strings that start and end with <> are HTML and skip the regex check
Match html or make sure no context is specified for #id
HANDLE: $(html) -> $(array)
Option to run scripts is true for back-compat
Intentionally let the error be thrown if parseHTML is not present
"HANDLE: $(html, props)"
Properties of context are called as methods if possible
...and otherwise set as attributes
HANDLE: $(#id)
Inject the element directly into the jQuery object
"HANDLE: $(expr, $(...))"
"HANDLE: $(expr, context)"
(which is just equivalent to: $(context).find(expr)
HANDLE: $(DOMElement)
HANDLE: $(function)
Shortcut for document ready
Execute immediately if ready is not present
Give the init function the jQuery prototype for later instantiation
Initialize central reference
Methods guaranteed to produce a unique set when starting from a unique set
"Positional selectors never match, since there's no _selection_ context"
Always skip document fragments
Don't pass non-elements to Sizzle
Determine the position of an element within the set
"No argument, return index in parent"
Index in selector
Locate the position of the desired element
"If it receives a jQuery object, the first element is used"
"Support: IE 9 - 11 only, iOS 7 only, Android Browser <=4.3 only"
Treat the template element as a regular one in browsers that
don't support it.
Remove duplicates
Reverse order for parents* and prev-derivatives
Convert String-formatted options into Object-formatted ones
Convert options from String-formatted to Object-formatted if needed
(we check in cache first)
Last fire value for non-forgettable lists
Flag to know if list was already fired
Flag to prevent firing
Actual callback list
Queue of execution data for repeatable lists
Index of currently firing callback (modified by add/remove as needed)
Fire callbacks
Enforce single-firing
"Execute callbacks for all pending executions,"
respecting firingIndex overrides and runtime changes
Run callback and check for early termination
Jump to end and forget the data so .add doesn't re-fire
Forget the data if we're done with it
Clean up if we're done firing for good
Keep an empty list if we have data for future add calls
"Otherwise, this object is spent"
Actual Callbacks object
Add a callback or a collection of callbacks to the list
"If we have memory from a past run, we should fire after adding"
Inspect recursively
Remove a callback from the list
Handle firing indexes
Check if a given callback is in the list.
"If no argument is given, return whether or not list has callbacks attached."
Remove all callbacks from the list
Disable .fire and .add
Abort any current/pending executions
Clear all callbacks and values
Disable .fire
Also disable .add unless we have memory (since it would have no effect)
Abort any pending executions
Call all callbacks with the given context and arguments
Call all the callbacks with the given arguments
To know if the callbacks have already been called at least once
Check for promise aspect first to privilege synchronous behavior
Other thenables
Other non-thenables
Control `resolve` arguments by letting Array#slice cast boolean `noValue` to integer:
* false: [ value ].slice( 0 ) => resolve( value )
* true: [ value ].slice( 1 ) => resolve()
"For Promises/A+, convert exceptions into rejections"
"Since jQuery.when doesn't unwrap thenables, we can skip the extra checks appearing in"
Deferred#then to conditionally suppress rejection.
Support: Android 4.0 only
Strict mode functions invoked without .call/.apply get global-object context
"action, add listener, callbacks,"
"... .then handlers, argument index, [final state]"
Keep pipe for back-compat
"Map tuples (progress, done, fail) to arguments (done, fail, progress)"
deferred.progress(function() { bind to newDefer or newDefer.notify })
deferred.done(function() { bind to newDefer or newDefer.resolve })
deferred.fail(function() { bind to newDefer or newDefer.reject })
Support: Promises/A+ section 2.3.3.3.3
https://promisesaplus.com/#point-59
Ignore double-resolution attempts
Support: Promises/A+ section 2.3.1
https://promisesaplus.com/#point-48
"Support: Promises/A+ sections 2.3.3.1, 3.5"
https://promisesaplus.com/#point-54
https://promisesaplus.com/#point-75
Retrieve `then` only once
Support: Promises/A+ section 2.3.4
https://promisesaplus.com/#point-64
Only check objects and functions for thenability
Handle a returned thenable
Special processors (notify) just wait for resolution
Normal processors (resolve) also hook into progress
...and disregard older resolution values
Handle all other returned values
Only substitute handlers pass on context
and multiple values (non-spec behavior)
Process the value(s)
Default process is resolve
Only normal processors (resolve) catch and reject exceptions
Support: Promises/A+ section 2.3.3.3.4.1
https://promisesaplus.com/#point-61
Ignore post-resolution exceptions
Only substitute handlers pass on context
and multiple values (non-spec behavior)
Support: Promises/A+ section 2.3.3.3.1
https://promisesaplus.com/#point-57
Re-resolve promises immediately to dodge false rejection from
subsequent errors
"Call an optional hook to record the stack, in case of exception"
since it's otherwise lost when execution goes async
progress_handlers.add( ... )
fulfilled_handlers.add( ... )
rejected_handlers.add( ... )
Get a promise for this deferred
"If obj is provided, the promise aspect is added to the object"
Add list-specific methods
promise.progress = list.add
promise.done = list.add
promise.fail = list.add
Handle state
"state = ""resolved"" (i.e., fulfilled)"
"state = ""rejected"""
rejected_callbacks.disable
fulfilled_callbacks.disable
rejected_handlers.disable
fulfilled_handlers.disable
progress_callbacks.lock
progress_handlers.lock
progress_handlers.fire
fulfilled_handlers.fire
rejected_handlers.fire
deferred.notify = function() { deferred.notifyWith(...) }
deferred.resolve = function() { deferred.resolveWith(...) }
deferred.reject = function() { deferred.rejectWith(...) }
deferred.notifyWith = list.fireWith
deferred.resolveWith = list.fireWith
deferred.rejectWith = list.fireWith
Make the deferred a promise
Call given func if any
All done!
Deferred helper
count of uncompleted subordinates
count of unprocessed arguments
subordinate fulfillment data
the master Deferred
subordinate callback factory
Single- and empty arguments are adopted like Promise.resolve
Use .then() to unwrap secondary thenables (cf. gh-3000)
Multiple arguments are aggregated like Promise.all array elements
"These usually indicate a programmer mistake during development,"
warn about them ASAP rather than swallowing them by default.
Support: IE 8 - 9 only
"Console exists when dev tools are open, which can happen at any time"
The deferred used on DOM ready
Wrap jQuery.readyException in a function so that the lookup
happens at the time of error handling instead of callback
registration.
Is the DOM ready to be used? Set to true once it occurs.
A counter to track how many items to wait for before
the ready event fires. See #6781
Handle when the DOM is ready
Abort if there are pending holds or we're already ready
Remember that the DOM is ready
"If a normal DOM Ready event fired, decrement, and wait if need be"
"If there are functions bound, to execute"
The ready event handler and self cleanup method
Catch cases where $(document).ready() is called
after the browser event has already occurred.
Support: IE <=9 - 10 only
"Older IE sometimes signals ""interactive"" too soon"
Handle it asynchronously to allow scripts the opportunity to delay ready
Use the handy event callback
"A fallback to window.onload, that will always work"
Multifunctional method to get and set values of a collection
The value/s can optionally be executed if it's a function
Sets many values
Sets one value
Bulk operations run against the entire set
...except when executing function values
Gets
Matches dashed string for camelizing
Used by camelCase as callback to replace()
Convert dashed to camelCase; used by the css and data modules
"Support: IE <=9 - 11, Edge 12 - 15"
Microsoft forgot to hump their vendor prefix (#9572)
Accepts only:
- Node
- Node.ELEMENT_NODE
- Node.DOCUMENT_NODE
- Object
- Any
Check if the owner object already has a cache
"If not, create one"
"We can accept data for non-element nodes in modern browsers,"
"but we should not, see #8335."
Always return an empty object.
If it is a node unlikely to be stringify-ed or looped over
use plain assignment
Otherwise secure it in a non-enumerable property
configurable must be true to allow the property to be
deleted when data is removed
"Handle: [ owner, key, value ] args"
Always use camelCase key (gh-2257)
"Handle: [ owner, { properties } ] args"
Copy the properties one-by-one to the cache object
Always use camelCase key (gh-2257)
In cases where either:
""
1. No key was specified
"2. A string key was specified, but no value provided"
""
"Take the ""read"" path and allow the get method to determine"
"which value to return, respectively either:"
""
1. The entire cache object
2. The data stored at the key
""
"When the key is not a string, or both a key and value"
"are specified, set or extend (existing objects) with either:"
""
1. An object of properties
2. A key and value
""
"Since the ""set"" path can have two possible entry points"
return the expected data based on which path was taken[*]
Support array or space separated string of keys
If key is an array of keys...
"We always set camelCase keys, so remove that."
"If a key with the spaces exists, use it."
"Otherwise, create an array by matching non-whitespace"
Remove the expando if there's no more data
Support: Chrome <=35 - 45
Webkit & Blink performance suffers when deleting properties
"from DOM nodes, so set to undefined instead"
https://bugs.chromium.org/p/chromium/issues/detail?id=378607 (bug restricted)
Implementation Summary
""
1. Enforce API surface and semantic compatibility with 1.9.x branch
2. Improve the module's maintainability by reducing the storage
paths to a single mechanism.
"3. Use the same single mechanism to support ""private"" and ""user"" data."
"4. _Never_ expose ""private"" data to user code (TODO: Drop _data, _removeData)"
5. Avoid exposing implementation details on user objects (eg. expando properties)
6. Provide a clear path for implementation upgrade to WeakMap in 2014
Only convert to a number if it doesn't change the string
"If nothing was found internally, try to fetch any"
data from the HTML5 data-* attribute
Make sure we set the data so it isn't changed later
TODO: Now that all calls to _data and _removeData have been replaced
"with direct calls to dataPriv methods, these can be deprecated."
Gets all values
Support: IE 11 only
The attrs elements can be null (#14894)
Sets multiple values
The calling jQuery object (element matches) is not empty
(and therefore has an element appears at this[ 0 ]) and the
`value` parameter was not undefined. An empty jQuery object
will result in `undefined` for elem = this[ 0 ] which will
throw an exception if an attempt to read a data cache is made.
Attempt to get data from the cache
The key will always be camelCased in Data
"Attempt to ""discover"" the data in"
HTML5 custom data-* attrs
"We tried really hard, but the data doesn't exist."
Set the data...
We always store the camelCased key
Speed up dequeue by getting out quickly if this is just a lookup
"If the fx queue is dequeued, always remove the progress sentinel"
Add a progress sentinel to prevent the fx queue from being
automatically dequeued
Clear up the last queue stop function
"Not public - generate a queueHooks object, or return the current one"
Ensure a hooks for this queue
Get a promise resolved when queues of a certain type
are emptied (fx is the type by default)
Check attachment across shadow DOM boundaries when possible (gh-3504)
isHiddenWithinTree might be called from jQuery#filter function;
"in that case, element will be second argument"
Inline style trumps all
"Otherwise, check computed style"
Support: Firefox <=43 - 45
"Disconnected elements can have computed display: none, so first confirm that elem is"
in the document.
"Remember the old values, and insert the new ones"
Revert the old values
Starting value computation is required for potential unit mismatches
Support: Firefox <=54
Halve the iteration target value to prevent interference from CSS upper bounds (gh-2144)
Trust units reported by jQuery.css
Iteratively approximate from a nonzero starting point
Evaluate and update our best guess (doubling guesses that zero out).
Finish if the scale equals or crosses 1 (making the old*new product non-positive).
Make sure we update the tween properties later on
Apply relative offset (+=/-=) if specified
Determine new display value for elements that need to change
"Since we force visibility upon cascade-hidden elements, an immediate (and slow)"
check is required in this first loop unless we have a nonempty display value (either
inline or about-to-be-restored)
Remember what we're overwriting
Set the display of the elements in a second loop to avoid constant reflow
We have to close these tags to support XHTML (#13200)
Support: IE <=9 only
XHTML parsers do not magically insert elements in the
same way that tag soup parsers do. So we cannot shorten
this by omitting <tbody> or other required elements.
Support: IE <=9 only
Support: IE <=9 - 11 only
Use typeof to avoid zero-argument method invocation on host objects (#15151)
Mark scripts as having already been evaluated
Add nodes directly
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
Convert non-html into a text node
Convert html into DOM nodes
Deserialize a standard representation
Descend through wrappers to the right content
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
Remember the top-level container
Ensure the created nodes are orphaned (#12392)
Remove wrapper from fragment
Skip elements already in the context collection (trac-4087)
Append to fragment
Preserve script evaluation history
Capture executables
Support: Android 4.0 - 4.3 only
Check state lost if the name is set (#11217)
Support: Windows Web Apps (WWA)
`name` and `type` must use .setAttribute for WWA (#14901)
Support: Android <=4.1 only
Older WebKit doesn't clone checked state correctly in fragments
Support: IE <=11 only
Make sure textarea (and checkbox) defaultValue is properly cloned
Support: IE <=9 - 11+
"focus() and blur() are asynchronous, except when they are no-op."
"So expect focus to be synchronous when the element is already active,"
and blur to be synchronous when the element is not already active.
"(focus and blur are always synchronous in other supported browsers,"
this just defines when we can count on it).
Support: IE <=9 only
Accessing document.activeElement can throw unexpectedly
https://bugs.jquery.com/ticket/13393
Types can be a map of types/handlers
"( types-Object, selector, data )"
"( types-Object, data )"
"( types, fn )"
"( types, selector, fn )"
"( types, data, fn )"
"Can use an empty set, since event contains the info"
Use same guid so caller can remove using origFn
Don't attach events to noData or text/comment nodes (but allow plain objects)
Caller can pass in an object of custom data in lieu of the handler
Ensure that invalid selectors throw exceptions at attach time
"Evaluate against documentElement in case elem is a non-element node (e.g., document)"
"Make sure that the handler has a unique ID, used to find/remove it later"
"Init the element's event structure and main handler, if this is the first"
Discard the second event of a jQuery.event.trigger() and
when an event is called after a page has unloaded
Handle multiple events separated by a space
"There *must* be a type, no attaching namespace-only handlers"
"If event changes its type, use the special event handlers for the changed type"
"If selector defined, determine special event api type, otherwise given type"
Update special based on newly reset type
handleObj is passed to all event handlers
Init the event handler queue if we're the first
Only use addEventListener if the special events handler returns false
"Add to the element's handler list, delegates in front"
"Keep track of which events have ever been used, for event optimization"
Detach an event or set of events from an element
Once for each type.namespace in types; type may be omitted
"Unbind all events (on this namespace, if provided) for the element"
Remove matching events
Remove generic event handler if we removed something and no more handlers exist
(avoids potential for endless recursion during removal of special event handlers)
Remove data and the expando if it's no longer used
Make a writable jQuery.Event from the native event object
Use the fix-ed jQuery.Event rather than the (read-only) native event
"Call the preDispatch hook for the mapped type, and let it bail if desired"
Determine handlers
Run delegates first; they may want to stop propagation beneath us
"If the event is namespaced, then each handler is only invoked if it is"
specially universal or its namespaces are a superset of the event's.
Call the postDispatch hook for the mapped type
Find delegate handlers
Support: IE <=9
Black-hole SVG <use> instance trees (trac-13180)
Support: Firefox <=42
Suppress spec-violating clicks indicating a non-primary pointer button (trac-3861)
https://www.w3.org/TR/DOM-Level-3-Events/#event-type-click
Support: IE 11 only
"...but not arrow key ""clicks"" of radio inputs, which can have `button` -1 (gh-2343)"
Don't check non-elements (#13208)
"Don't process clicks on disabled elements (#6911, #8165, #11382, #11764)"
Don't conflict with Object.prototype properties (#13203)
Add the remaining (directly-bound) handlers
Prevent triggered image.load events from bubbling to window.load
Utilize native event to ensure correct state for checkable inputs
"For mutual compressibility with _default, replace `this` access with a local var."
`|| data` is dead code meant only to preserve the variable through minification.
Claim the first handler
"dataPriv.set( el, ""click"", ... )"
Return false to allow normal processing in the caller
"For mutual compressibility with _default, replace `this` access with a local var."
`|| data` is dead code meant only to preserve the variable through minification.
Force setup before triggering a click
Return non-false to allow normal event-path propagation
"For cross-browser consistency, suppress native .click() on links"
Also prevent it if we're currently inside a leveraged native-event stack
Support: Firefox 20+
Firefox doesn't alert if the returnValue field is not set.
Ensure the presence of an event listener that handles manually-triggered
synthetic events by interrupting progress until reinvoked in response to
"*native* events that it fires directly, ensuring that state changes have"
already occurred before other listeners are invoked.
"Missing expectSync indicates a trigger call, which must force setup through jQuery.event.add"
Register the controller as a special universal handler for all event namespaces
Interrupt processing of the outer synthetic .trigger()ed event
Store arguments for use when handling the inner native event
Trigger the native event and capture its result
Support: IE <=9 - 11+
focus() and blur() are asynchronous
Cancel the outer synthetic event
If this is an inner synthetic event for an event with a bubbling surrogate
"(focus or blur), assume that the surrogate already propagated from triggering the"
native event and prevent that from happening again here.
This technically gets the ordering wrong w.r.t. to `.trigger()` (in which the
"bubbling surrogate propagates *after* the non-bubbling base), but that seems"
less bad than duplication.
"If this is a native event triggered above, everything is now in order"
Fire an inner synthetic event with the original arguments
...and capture the result
Support: IE <=9 - 11+
Extend with the prototype to reset the above stopImmediatePropagation()
Abort handling of the native event
"This ""if"" is needed for plain objects"
Allow instantiation without the 'new' keyword
Event object
Events bubbling up the document may have been marked as prevented
by a handler lower down the tree; reflect the correct value.
Support: Android <=2.3 only
Create target properties
Support: Safari <=6 - 7 only
"Target should not be a text node (#504, #13143)"
Event type
Put explicitly provided properties onto the event object
Create a timestamp if incoming event doesn't have one
Mark it as fixed
jQuery.Event is based on DOM3 Events as specified by the ECMAScript Language Binding
https://www.w3.org/TR/2003/WD-DOM-Level-3-Events-20030331/ecma-script-binding.html
Includes all common event props including KeyEvent and MouseEvent specific props
Add which for key events
Add which for click: 1 === left; 2 === middle; 3 === right
Utilize native event if possible so blur/focus sequence is correct
Claim the first handler
"dataPriv.set( this, ""focus"", ... )"
"dataPriv.set( this, ""blur"", ... )"
Return false to allow normal processing in the caller
Force setup before trigger
Return non-false to allow normal event-path propagation
Create mouseenter/leave events using mouseover/out and event-time checks
so that event delegation works in jQuery.
Do the same for pointerenter/pointerleave and pointerover/pointerout
""
Support: Safari 7 only
Safari sends mouseenter too often; see:
https://bugs.chromium.org/p/chromium/issues/detail?id=470258
for the description of the bug (it existed in older Chrome versions as well).
For mouseenter/leave call the handler if related is outside the target.
NB: No relatedTarget if the mouse left/entered the browser window
( event )  dispatched jQuery.Event
"( types-object [, selector] )"
"( types [, fn] )"
See https://github.com/eslint/eslint/issues/3229
"Support: IE <=10 - 11, Edge 12 - 13 only"
In IE/Edge using regex groups here causes severe slowdowns.
See https://connect.microsoft.com/IE/feedback/details/1736512/
"checked=""checked"" or checked"
Prefer a tbody over its parent table for containing new rows
Replace/restore the type attribute of script elements for safe DOM manipulation
"1. Copy private data: events, handlers, etc."
2. Copy user data
"Fix IE bugs, see support tests"
Fails to persist the checked state of a cloned checkbox or radio button.
Fails to return the selected option to the default selected state when cloning options
Flatten any nested arrays
"We can't cloneNode fragments that contain checked, in WebKit"
Require either new content or an interest in ignored elements to invoke the callback
Use the original fragment for the last item
instead of the first because it can end up
being emptied incorrectly in certain situations (#8070).
Keep references to cloned scripts for later restoration
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
Reenable scripts
Evaluate executable scripts on first document insertion
"Optional AJAX dependency, but won't run scripts if not present"
Fix IE cloning issues
We eschew Sizzle here for performance reasons: https://jsperf.com/getall-vs-sizzle/2
Copy the events from the original to the clone
Preserve script evaluation history
Return the cloned set
This is a shortcut to avoid jQuery.event.remove's overhead
Support: Chrome <=35 - 45+
"Assign undefined instead of using delete, see Data#remove"
Support: Chrome <=35 - 45+
"Assign undefined instead of using delete, see Data#remove"
Prevent memory leaks
Remove any remaining nodes
See if we can take a shortcut and just use innerHTML
Remove element nodes and prevent memory leaks
"If using innerHTML throws an exception, use the fallback method"
"Make the changes, replacing each non-ignored context element with the new content"
Force callback invocation
"Support: Android <=4.0 only, PhantomJS 1 only"
".get() because push.apply(_, arraylike) throws on ancient WebKit"
"Support: IE <=11 only, Firefox <=30 (#15098, #14150)"
IE throws on elements created in popups
"FF meanwhile throws on frame elements through ""defaultView.getComputedStyle"""
Executing both pixelPosition & boxSizingReliable tests require only one layout
so they're executed at the same time to save the second computation.
"This is a singleton, we need to execute it only once"
"Support: Android 4.0 - 4.3 only, Firefox <=3 - 44"
"Support: Android 4.0 - 4.3 only, Safari <=9.1 - 10.1, iOS <=7.0 - 9.3"
"Some styles come back with percentage values, even though they shouldn't"
Support: IE 9 - 11 only
Detect misreporting of content dimensions for box-sizing:border-box elements
Support: IE 9 only
Detect overflow:scroll screwiness (gh-3699)
Support: Chrome <=64
Don't get tricked when zoom affects offsetWidth (gh-4029)
Nullify the div so it wouldn't be stored in the memory and
it will also be a sign that checks already performed
Finish early in limited (non-browser) environments
Support: IE <=9 - 11 only
Style of cloned element affects source element cloned (#8908)
Support: Firefox 51+
Retrieving style before computed somehow
fixes an issue with getting wrong values
on detached elements
getPropertyValue is needed for:
".css('filter') (IE 9 only, #12537)"
.css('--customProperty) (#3144)
"A tribute to the ""awesome hack by Dean Edwards"""
"Android Browser returns percentage for some values,"
but width seems to be reliably pixels.
This is against the CSSOM draft spec:
https://drafts.csswg.org/cssom/#resolved-values
Remember the original values
Put in the new values to get a computed value out
Revert the changed values
Support: IE <=9 - 11 only
IE returns zIndex value as an integer.
"Define the hook, we'll check on the first run if it's really needed."
Hook not needed (or it's not possible to use it due
"to missing dependency), remove it."
Hook needed; redefine it so that the support test is not executed again.
Return a vendor-prefixed property or undefined
Check for vendor prefixed names
Return a potentially-mapped jQuery.cssProps or vendor prefixed property
Swappable if display is none or starts with table
"except ""table"", ""table-cell"", or ""table-caption"""
See here for display values: https://developer.mozilla.org/en-US/docs/CSS/display
Any relative (+/-) values have already been
normalized at this point
"Guard against undefined ""subtract"", e.g., when used as in cssHooks"
Adjustment may not be necessary
Both box models exclude margin
"If we get here with a content-box, we're seeking ""padding"" or ""border"" or ""margin"""
Add padding
"For ""border"" or ""margin"", add border"
But still keep track of it otherwise
"If we get here with a border-box (content + padding + border), we're seeking ""content"" or"
"""padding"" or ""margin"""
"For ""content"", subtract padding"
"For ""content"" or ""padding"", subtract border"
Account for positive content-box scroll gutter when requested by providing computedVal
"offsetWidth/offsetHeight is a rounded sum of content, padding, scroll gutter, and border"
"Assuming integer scroll gutter, subtract the rest and round down"
"If offsetWidth/offsetHeight is unknown, then we can't determine content-box scroll gutter"
Use an explicit zero to avoid NaN (gh-3964)
Start with computed style
"To avoid forcing a reflow, only fetch boxSizing if we need it (gh-4322)."
Fake content-box until we know it's needed to know the true value.
Support: Firefox <=54
"Return a confounding non-pixel value or feign ignorance, as appropriate."
"Fall back to offsetWidth/offsetHeight when value is ""auto"""
This happens for inline elements with no explicit setting (gh-3571)
Support: Android <=4.1 - 4.3 only
Also use offsetWidth/offsetHeight for misreported inline dimensions (gh-3602)
Support: IE 9-11 only
Also use offsetWidth/offsetHeight for when box sizing is unreliable
We use getClientRects() to check for hidden/disconnected.
"In those cases, the computed value can be trusted to be border-box"
"Where available, offsetWidth/offsetHeight approximate border box dimensions."
"Where not available (e.g., SVG), assume unreliable box-sizing and interpret the"
retrieved value as a content box dimension.
"Normalize """" and auto"
Adjust for the element's box model
Provide the current computed size to request scroll gutter calculation (gh-3589)
Add in style property hooks for overriding the default
behavior of getting and setting a style property
We should always get a number back from opacity
"Don't automatically add ""px"" to these possibly-unitless properties"
Add in properties whose names you wish to fix before
setting or getting the value
Get and set the style property on a DOM Node
Don't set styles on text and comment nodes
Make sure that we're working with the right name
Make sure that we're working with the right name. We don't
want to query the value if it is a CSS custom property
since they are user-defined.
"Gets hook for the prefixed version, then unprefixed version"
Check if we're setting a value
"Convert ""+="" or ""-="" to relative numbers (#7345)"
Fixes bug #9237
Make sure that null and NaN values aren't set (#7116)
"If a number was passed in, add the unit (except for certain CSS properties)"
The isCustomProp check can be removed in jQuery 4.0 when we only auto-append
"""px"" to a few hardcoded values."
background-* props affect original clone's values
"If a hook was provided, use that value, otherwise just set the specified value"
If a hook was provided get the non-computed value from there
Otherwise just get the value from the style object
Make sure that we're working with the right name. We don't
want to modify the value if it is a CSS custom property
since they are user-defined.
Try prefixed name followed by the unprefixed name
If a hook was provided get the computed value from there
"Otherwise, if a way to get the computed value exists, use that"
"Convert ""normal"" to computed value"
Make numeric if forced or a qualifier was provided and val looks numeric
Certain elements can have dimension info if we invisibly show them
but it must have a current display style that would benefit
Support: Safari 8+
Table columns in Safari have non-zero offsetWidth & zero
getBoundingClientRect().width unless display is changed.
Support: IE <=11 only
Running getBoundingClientRect on a disconnected node
in IE throws an error.
Only read styles.position if the test has a chance to fail
to avoid forcing a reflow.
"To avoid forcing a reflow, only fetch boxSizing if we need it (gh-3991)"
Account for unreliable border-box dimensions by comparing offset* to computed and
faking a content-box to get border and padding (gh-3699)
Convert to pixels if value adjustment is needed
These hooks are used by animate to expand properties
Assumes a single number if not a string
"Use a property on the element directly when it is not a DOM element,"
or when there is no matching style property that exists.
Passing an empty string as a 3rd parameter to .css will automatically
attempt a parseFloat and fallback to a string if the parse fails.
"Simple values such as ""10px"" are parsed to Float;"
"complex values such as ""rotate(1rad)"" are returned as-is."
"Empty strings, null, undefined and ""auto"" are converted to 0."
Use step hook for back compat.
Use cssHook if its there.
Use .style if available and use plain properties where available.
Support: IE <=9 only
Panic based approach to setting things on disconnected nodes
Back compat <1.8 extension point
Animations created synchronously will run synchronously
Generate parameters to create a standard animation
"If we include width, step value is 1 to do all cssExpand values,"
otherwise step value is 2 to skip over Left and Right
We're done with this property
Queue-skipping animations hijack the fx hooks
Ensure the complete handler is called before this completes
Detect show/hide animations
"Pretend to be hidden if this is a ""show"" and"
there is still data from a stopped show/hide
Ignore all other no-op show/hide data
Bail out if this is a no-op like .hide().hide()
"Restrict ""overflow"" and ""display"" styles during box animations"
"Support: IE <=9 - 11, Edge 12 - 15"
Record all 3 overflow attributes because IE does not infer the shorthand
from identically-valued overflowX and overflowY and Edge just mirrors
the overflowX value there.
"Identify a display type, preferring old show/hide data over the CSS cascade"
Get nonempty value(s) by temporarily forcing visibility
Animate inline elements as inline-block
Restore the original display value at the end of pure show/hide animations
Implement show/hide animations
General show/hide setup for this element animation
"Store hidden/visible for toggle so `.stop().toggle()` ""reverses"""
Show elements before animating them
"The final step of a ""hide"" animation is actually hiding the element"
Per-property setup
"camelCase, specialEasing and expand cssHook pass"
"Not quite $.extend, this won't overwrite existing keys."
"Reusing 'index' because we have the correct ""name"""
Don't match elem in the :animated selector
Support: Android 2.3 only
Archaic crash bug won't allow us to use `1 - ( 0.5 || 0 )` (#12497)
"If there's more to do, yield"
"If this was an empty animation, synthesize a final progress notification"
Resolve the animation and report its conclusion
"If we are going to the end, we want to run all the tweens"
otherwise we skip this part
"Resolve when we played the last frame; otherwise, reject"
Attach callbacks from options
Go to the end state if fx are off
"Normalize opt.queue - true/undefined/null -> ""fx"""
Queueing
Show any hidden elements after setting opacity to 0
Animate to the value specified
Operate on a copy of prop so per-property easing won't be lost
"Empty animations, or finishing resolves immediately"
Start the next in the queue if the last step wasn't forced.
"Timers currently will call their complete callbacks, which"
will dequeue but only if they were gotoEnd.
Enable finishing flag on private data
Empty the queue first
"Look for any active animations, and finish them"
Look for any animations in the old queue and finish them
Turn off finishing flag
Generate shortcuts for custom animations
Run the timer and safely remove it when done (allowing for external removal)
Default speed
"Based off of the plugin by Clint Helfers, with permission."
https://web.archive.org/web/20100324014747/http://blindsignals.com/index.php/2009/07/jquery-delay/
Support: Android <=4.3 only
"Default value for a checkbox should be ""on"""
Support: IE <=11 only
Must access selectedIndex to make default options select
Support: IE <=11 only
An input loses its value after becoming a radio
"Don't get/set attributes on text, comment and attribute nodes"
Fallback to prop when attributes are not supported
Attribute hooks are determined by the lowercase version
Grab necessary hook if one is defined
"Non-existent attributes return null, we normalize to undefined"
Attribute names can contain non-HTML whitespace characters
https://html.spec.whatwg.org/multipage/syntax.html#attributes-2
Hooks for boolean attributes
Remove boolean attributes when set to false
Avoid an infinite loop by temporarily removing this function from the getter
"Don't get/set properties on text, comment and attribute nodes"
Fix name and attach hooks
Support: IE <=9 - 11 only
elem.tabIndex doesn't always return the
correct value when it hasn't been explicitly set
https://web.archive.org/web/20141116233347/http://fluidproject.org/blog/2008/01/09/getting-setting-and-removing-tabindex-values-with-javascript/
Use proper attribute retrieval(#12072)
Support: IE <=11 only
Accessing the selectedIndex property
forces the browser to respect setting selected
on the option
The getter ensures a default option is selected
when in an optgroup
"eslint rule ""no-unused-expressions"" is disabled for this code"
since it considers such accessions noop
Strip and collapse whitespace according to HTML spec
https://infra.spec.whatwg.org/#strip-and-collapse-ascii-whitespace
Only assign if different to avoid unneeded rendering.
This expression is here for better compressibility (see addClass)
Remove *all* instances
Only assign if different to avoid unneeded rendering.
Toggle individual class names
"Check each className given, space separated list"
Toggle whole class name
Store className if set
"If the element has a class name or if we're passed `false`,"
"then remove the whole classname (if there was one, the above saved it)."
"Otherwise bring back whatever was previously saved (if anything),"
falling back to the empty string if nothing was stored.
Handle most common string cases
Handle cases where value is null/undef or number
"Treat null/undefined as """"; convert numbers to string"
"If set returns undefined, fall back to normal setting"
Support: IE <=10 - 11 only
"option.text throws exceptions (#14686, #14858)"
Strip and collapse whitespace
https://html.spec.whatwg.org/#strip-and-collapse-whitespace
Loop through all the selected options
Support: IE <=9 only
IE8-9 doesn't update selected after form reset (#2551)
Don't return options that are disabled or in a disabled optgroup
Get the specific value for the option
We don't need an array for one selects
Multi-Selects return an array
Force browsers to behave consistently when non-matching value is set
Radios and checkboxes getter/setter
Return jQuery for attributes-only inclusion
Don't do events on text and comment nodes
focus/blur morphs to focusin/out; ensure we're not firing them right now
Namespaced trigger; create a regexp to match event type in handle()
"Caller can pass in a jQuery.Event object, Object, or just an event type string"
Trigger bitmask: & 1 for native handlers; & 2 for jQuery (always true)
Clean up the event in case it is being reused
"Clone any incoming data and prepend the event, creating the handler arg list"
Allow special events to draw outside the lines
"Determine event propagation path in advance, per W3C events spec (#9951)"
"Bubble up to document, then to window; watch for a global ownerDocument var (#9724)"
"Only add window if we got to document (e.g., not plain obj or detached DOM)"
Fire handlers on the event path
jQuery handler
Native handler
"If nobody prevented the default action, do it now"
Call a native DOM method on the target with the same name as the event.
"Don't do default actions on window, that's where global variables be (#6170)"
Don't re-trigger an onFOO event when we call its FOO() method
"Prevent re-triggering of the same event, since we already bubbled it above"
Piggyback on a donor event to simulate a different one
Used only for `focus(in | out)` events
Support: Firefox <=44
Firefox doesn't have focus(in | out) events
Related ticket - https://bugzilla.mozilla.org/show_bug.cgi?id=687787
""
"Support: Chrome <=48 - 49, Safari <=9.0 - 9.1"
"focus(in | out) events fire after focus & blur events,"
which is spec violation - http://www.w3.org/TR/DOM-Level-3-Events/#events-focusevent-event-order
Related ticket - https://bugs.chromium.org/p/chromium/issues/detail?id=449857
Attach a single capturing handler on the document while someone wants focusin/focusout
Cross-browser xml parsing
Support: IE 9 - 11 only
IE throws on parseFromString with invalid input.
Serialize array item.
Treat each array item as a scalar.
"Item is non-scalar (array or object), encode its numeric index."
Serialize object item.
Serialize scalar item.
Serialize an array of form elements or a set of
key/values into a query string
"If value is a function, invoke it and use its return value"
"If an array was passed in, assume that it is an array of form elements."
Serialize the form elements
"If traditional, encode the ""old"" way (the way 1.3.2 or older"
"did it), otherwise encode params recursively."
Return the resulting serialization
"Can add propHook for ""elements"" to filter or add form elements"
"Use .is( "":disabled"" ) so that fieldset[disabled] works"
"#7653, #8125, #8152: local protocol detection"
Avoid comment-prolog char sequence (#10098); must appease lint and evade compression
Anchor tag for parsing the document origin
"Base ""constructor"" for jQuery.ajaxPrefilter and jQuery.ajaxTransport"
"dataTypeExpression is optional and defaults to ""*"""
For each dataType in the dataTypeExpression
Prepend if requested
Otherwise append
Base inspection function for prefilters and transports
A special extend for ajax options
"that takes ""flat"" options (not to be deep extended)"
Fixes #9887
Remove auto dataType and get content-type in the process
Check if we're dealing with a known content-type
Check to see if we have a response for the expected dataType
Try convertible dataTypes
Or just use first one
If we found a dataType
We add the dataType to the list if needed
and return the corresponding response
Work with a copy of dataTypes in case we need to modify it for conversion
Create converters map with lowercased keys
Convert to each sequential dataType
Apply the dataFilter if provided
There's only work to do if current dataType is non-auto
Convert response if prev dataType is non-auto and differs from current
Seek a direct converter
"If none found, seek a pair"
If conv2 outputs current
If prev can be converted to accepted input
Condense equivalence converters
"Otherwise, insert the intermediate dataType"
Apply converter (if not an equivalence)
"Unless errors are allowed to bubble, catch and return them"
Counter for holding the number of active queries
Last-Modified header cache for next request
Data converters
"Keys separate source (or catchall ""*"") and destination types with a single space"
Convert anything to text
Text to html (true = no transformation)
Evaluate text as a json expression
Parse text as xml
For options that shouldn't be deep extended:
you can add your own custom options here if
and when you create one that shouldn't be
deep extended (see ajaxExtend)
Creates a full fledged settings object into target
with both ajaxSettings and settings fields.
"If target is omitted, writes into ajaxSettings."
Building a settings object
Extending ajaxSettings
Main method
"If url is an object, simulate pre-1.5 signature"
Force options to be an object
URL without anti-cache param
Response headers
timeout handle
Url cleanup var
Request state (becomes false upon send and true upon completion)
To know if global events are to be dispatched
Loop variable
uncached part of the url
Create the final options object
Callbacks context
Context for global events is callbackContext if it is a DOM node or jQuery collection
Deferreds
Status-dependent callbacks
Headers (they are sent all at once)
Default abort message
Fake xhr
Builds headers hashtable if needed
Raw string
Caches the header
Overrides response content-type header
Status-dependent callbacks
Execute the appropriate callbacks
Lazy-add the new callbacks in a way that preserves old ones
Cancel the request
Attach deferreds
Add protocol if not provided (prefilters might expect it)
Handle falsy url in the settings object (#10093: consistency with old signature)
We also use the url parameter if available
Alias method option to type as per ticket #12004
Extract dataTypes list
A cross-domain request is in order when the origin doesn't match the current origin.
"Support: IE <=8 - 11, Edge 12 - 15"
"IE throws exception on accessing the href property if url is malformed,"
e.g. http://example.com:80x/
Support: IE <=8 - 11 only
Anchor's host property isn't correctly set when s.url is relative
"If there is an error parsing the URL, assume it is crossDomain,"
it can be rejected by the transport if it is invalid
Convert data if not already a string
Apply prefilters
"If request was aborted inside a prefilter, stop there"
We can fire global events as of now if asked to
Don't fire events if jQuery.event is undefined in an AMD-usage scenario (#15118)
Watch for a new set of requests
Uppercase the type
Determine if request has content
Save the URL in case we're toying with the If-Modified-Since
and/or If-None-Match header later on
Remove hash to simplify url manipulation
More options handling for requests with no content
Remember the hash so we can put it back
"If data is available and should be processed, append data to url"
#9682: remove data so that it's not used in an eventual retry
Add or update anti-cache param if needed
Put hash and anti-cache on the URL that will be requested (gh-1732)
Change '%20' to '+' if this is encoded form body content (gh-2658)
"Set the If-Modified-Since and/or If-None-Match header, if in ifModified mode."
"Set the correct header, if data is being sent"
"Set the Accepts header for the server, depending on the dataType"
Check for headers option
Allow custom headers/mimetypes and early abort
Abort if not done already and return
Aborting is no longer a cancellation
Install callbacks on deferreds
Get transport
"If no transport, we auto-abort"
Send global event
"If request was aborted inside ajaxSend, stop there"
Timeout
Rethrow post-completion exceptions
Propagate others as results
Callback for when everything is done
Ignore repeat invocations
Clear timeout if it exists
Dereference transport for early garbage collection
(no matter how long the jqXHR object will be used)
Cache response headers
Set readyState
Determine if successful
Get response data
Convert no matter what (that way responseXXX fields are always set)
"If successful, handle type chaining"
"Set the If-Modified-Since and/or If-None-Match header, if in ifModified mode."
if no content
if not modified
"If we have data, let's convert it"
Extract error from statusText and normalize for non-aborts
Set data for the fake xhr object
Success/Error
Status-dependent callbacks
Complete
Handle the global AJAX counter
Shift arguments if data argument was omitted
The url can be an options object (which then must have .url)
"Make this explicit, since user can override this through ajaxSetup (#11264)"
Only evaluate the response if it is successful (gh-4126)
"dataFilter is not invoked for failure responses, so using it instead"
of the default converter is kludgy but it works.
The elements to wrap the target around
"File protocol always yields status code 0, assume 200"
Support: IE <=9 only
#1450: sometimes IE returns 1223 when it should be 204
Cross domain only allowed if supported through XMLHttpRequest
Apply custom fields if provided
Override mime type if needed
X-Requested-With header
"For cross-domain requests, seeing as conditions for a preflight are"
"akin to a jigsaw puzzle, we simply never set it to be sure."
(it can always be set on a per-request basis or even using ajaxSetup)
"For same-domain requests, won't change header if already provided."
Set headers
Callback
Support: IE <=9 only
"On a manual native abort, IE9 throws"
errors on any property access that is not readyState
"File: protocol always yields status 0; see #8605, #14207"
Support: IE <=9 only
IE9 has no XHR2 but throws on binary (trac-11426)
"For XHR2 non-text, let the caller handle it (gh-2498)"
Listen to events
Support: IE 9 only
Use onreadystatechange to replace onabort
to handle uncaught aborts
Check readyState before timeout as it changes
"Allow onerror to be called first,"
but that will not handle a native abort
"Also, save errorCallback to a variable"
as xhr.onerror cannot be accessed
Create the abort callback
Do send the request (this may raise an exception)
#14683: Only rethrow if this hasn't been notified as an error yet
Prevent auto-execution of scripts when no explicit dataType was provided (See gh-2432)
Install script dataType
Handle cache's special case and crossDomain
Bind script tag hack transport
This transport only deals with cross domain or forced-by-attrs requests
Use native DOM manipulation to avoid our domManip AJAX trickery
Default jsonp settings
"Detect, normalize options and install callbacks for jsonp requests"
"Handle iff the expected data type is ""jsonp"" or we have a parameter to set"
"Get callback name, remembering preexisting value associated with it"
Insert callback into url or form data
Use data converter to retrieve json after script execution
Force json dataType
Install callback
Clean-up function (fires after converters)
If previous value didn't exist - remove it
Otherwise restore preexisting value
Save back as free
Make sure that re-using the options doesn't screw things around
Save the callback name for future use
Call if it was a function and we have a response
Delegate to script
Support: Safari 8 only
In Safari 8 documents created via document.implementation.createHTMLDocument
collapse sibling forms: the second one becomes a child of the first one.
"Because of that, this security measure has to be disabled in Safari 8."
https://bugs.webkit.org/show_bug.cgi?id=137337
"Argument ""data"" should be string of html"
"context (optional): If specified, the fragment will be created in this context,"
defaults to document
"keepScripts (optional): If true, will include scripts passed in the html string"
Stop scripts or inline event handlers from being executed immediately
by using document.implementation
Set the base href for the created document
so any parsed elements with URLs
are based on the document's URL (gh-2965)
Single tag
If it's a function
We assume that it's the callback
"Otherwise, build a param string"
"If we have elements to modify, make the request"
"If ""type"" variable is undefined, then ""GET"" method will be used."
Make value of this field explicit since
user can override it through ajaxSetup method
Save response for use in complete callback
"If a selector was specified, locate the right elements in a dummy div"
Exclude scripts to avoid IE 'Permission Denied' errors
Otherwise use the full result
"If the request succeeds, this function gets ""data"", ""status"", ""jqXHR"""
but they are ignored because response was set above.
"If it fails, this function gets ""jqXHR"", ""status"", ""error"""
Attach a bunch of functions for handling common AJAX events
"Set position first, in-case top/left are set even on static elem"
Need to be able to calculate position if either
top or left is auto and position is either absolute or fixed
Use jQuery.extend here to allow modification of coordinates argument (gh-1848)
offset() relates an element's border box to the document origin
Preserve chaining for setter
Return zeros for disconnected and hidden (display: none) elements (gh-2310)
Support: IE <=11 only
Running getBoundingClientRect on a
disconnected node in IE throws an error
Get document-relative position by adding viewport scroll to viewport-relative gBCR
position() relates an element's margin box to its offset parent's padding box
This corresponds to the behavior of CSS absolute positioning
"position:fixed elements are offset from the viewport, which itself always has zero offset"
Assume position:fixed implies availability of getBoundingClientRect
"Account for the *real* offset parent, which can be the document or its root element"
when a statically positioned element is identified
"Incorporate borders into its offset, since they are outside its content origin"
Subtract parent offsets and element margins
This method will return documentElement in the following cases:
"1) For the element inside the iframe without offsetParent, this method will return"
documentElement of the parent window
2) For the hidden or detached element
"3) For body or html element, i.e. in case of the html node - it will return itself"
""
but those exceptions were never presented as a real life use-cases
and might be considered as more preferable results.
""
"This logic, however, is not guaranteed and can change at any point in the future"
Create scrollLeft and scrollTop methods
Coalesce documents and windows
"Support: Safari <=7 - 9.1, Chrome <=37 - 49"
Add the top/left cssHooks using jQuery.fn.position
Webkit bug: https://bugs.webkit.org/show_bug.cgi?id=29084
Blink bug: https://bugs.chromium.org/p/chromium/issues/detail?id=589347
getComputedStyle returns percent when specified for top/left/bottom/right;
"rather than make the css module depend on the offset module, just check for it here"
"If curCSS returns percentage, fallback to offset"
"Create innerHeight, innerWidth, height, width, outerHeight and outerWidth methods"
"Margin is only for outerHeight, outerWidth"
$( window ).outerWidth/Height return w/h including scrollbars (gh-1729)
Get document width or height
"Either scroll[Width/Height] or offset[Width/Height] or client[Width/Height],"
whichever is greatest
"Get width or height on the element, requesting but not forcing parseFloat"
Set width or height on the element
Handle event binding
"( namespace ) or ( selector, types [, fn] )"
"Bind a function to a context, optionally partially applying any"
arguments.
jQuery.proxy is deprecated to promote standards (specifically Function#bind)
"However, it is not slated for removal any time soon"
"Quick check to determine if target is callable, in the spec"
"this throws a TypeError, but we will just return undefined."
Simulated bind
"Set the guid of unique handler to the same of original handler, so it can be removed"
"As of jQuery 3.0, isNumeric is limited to"
strings and numbers (primitives or objects)
that can be coerced to finite numbers (gh-2662)
"parseFloat NaNs numeric-cast false positives ("""")"
"...but misinterprets leading-number strings, particularly hex literals (""0x..."")"
subtraction forces infinities to NaN
"Register as a named AMD module, since jQuery can be concatenated with other"
"files that may use define, but not via a proper concatenation script that"
understands anonymous AMD modules. A named AMD is safest and most robust
way to register. Lowercase jquery is used because AMD module names are
"derived from file names, and jQuery is normally delivered in a lowercase"
file name. Do this after creating the global so that if an AMD module wants
"to call noConflict to hide this version of jQuery, it will work."
"Note that for maximum portability, libraries that are not jQuery should"
"declare themselves as anonymous modules, and avoid setting a global if an"
"AMD loader is present. jQuery is a special case. For more information, see"
https://github.com/jrburke/requirejs/wiki/Updating-existing-libraries#wiki-anon
Map over jQuery in case of overwrite
Map over the $ in case of overwrite
"Expose jQuery and $ identifiers, even in AMD"
"(#7102#comment:10, https://github.com/jquery/jquery/pull/557)"
and CommonJS for browser emulators (#13566)
For CommonJS and CommonJS-like environments where a proper `window`
"is present, execute the factory and get jQuery."
For environments that do not have a `window` with a `document`
"(such as Node.js), expose a factory as module.exports."
This accentuates the need for the creation of a real `window`.
"e.g. var jQuery = require(""jquery"")(window);"
See ticket #14549 for more info.
Pass this if window is not defined yet
"Edge <= 12 - 13+, Firefox <=18 - 45+, IE 10 - 11, Safari 5.1 - 9+, iOS 6 - 9.1"
"throw exceptions when non-strict code (e.g., ASP.NET 4.5) accesses strict mode"
"arguments.callee.caller (trac-13335). But as of jQuery 3.0 (2016), strict mode should be common"
enough that all such attempts are guarded in a try block.
"Support: Chrome <=57, Firefox <=52"
"In some browsers, typeof returns ""function"" for HTML <object> elements"
"(i.e., `typeof document.createElement( ""object"" ) === ""function""`)."
We don't want to classify *any* DOM node as a function.
"Support: Firefox 64+, Edge 18+"
"Some browsers don't support the ""nonce"" property on scripts."
"On the other hand, just using `getAttribute` is not enough as"
the `nonce` attribute is reset to an empty string whenever it
becomes browsing-context connected.
See https://github.com/whatwg/html/issues/2369
See https://html.spec.whatwg.org/#nonce-attributes
The `node.getAttribute` check was added for the sake of
`jQuery.globalEval` so that it can fake a nonce-containing node
via an object.
Support: Android <=2.3 only (functionish RegExp)
Defining this global in .eslintrc.json would create a danger of using the global
"unguarded in another place, it seems safer to define global only for this module"
Define a local copy of jQuery
The jQuery object is actually just the init constructor 'enhanced'
Need init if jQuery is called (just allow error to be thrown if not included)
Support: Android <=4.0 only
Make sure we trim BOM and NBSP
The current version of jQuery being used
The default length of a jQuery object is 0
Get the Nth element in the matched element set OR
Get the whole matched element set as a clean array
Return all the elements in a clean array
Return just the one element from the set
Take an array of elements and push it onto the stack
(returning the new matched element set)
Build a new jQuery matched element set
Add the old object onto the stack (as a reference)
Return the newly-formed element set
Execute a callback for every element in the matched set.
For internal use only.
"Behaves like an Array's method, not like a jQuery method."
Handle a deep copy situation
Skip the boolean and the target
Handle case when target is a string or something (possible in deep copy)
Extend jQuery itself if only one argument is passed
Only deal with non-null/undefined values
Extend the base object
Prevent Object.prototype pollution
Prevent never-ending loop
Recurse if we're merging plain objects or arrays
Ensure proper type for the source value
"Never move original objects, clone them"
Don't bring in undefined values
Return the modified object
Unique for each copy of jQuery on the page
Assume jQuery is ready without the ready module
Detect obvious negatives
Use toString instead of jQuery.type to catch host objects
"Objects with no prototype (e.g., `Object.create( null )`) are plain"
Objects with prototype are plain iff they were constructed by a global Object function
Evaluates a script in a global context
Support: Android <=4.0 only
results is for internal usage only
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
"Go through the array, only saving the items"
that pass the validator function
arg is for internal usage only
"Go through the array, translating each of the items to their new values"
"Go through every key on the object,"
Flatten any nested arrays
A global GUID counter for objects
jQuery.support is not used in Core but other projects attach their
properties to it so it needs to exist.
Populate the class2type map
Support: real iOS 8.2 only (not reproducible in simulator)
`in` check used to prevent JIT error (gh-2145)
hasOwn isn't used here due to false negatives
regarding Nodelist length in IE
Local document vars
Instance-specific data
Instance methods
Use a stripped-down indexOf as it's faster than native
https://jsperf.com/thor-indexof-vs-for/5
Regular expressions
http://www.w3.org/TR/css3-selectors/#whitespace
http://www.w3.org/TR/CSS21/syndata.html#value-def-identifier
Attribute selectors: http://www.w3.org/TR/selectors/#attribute-selectors
Operator (capture 2)
"""Attribute values must be CSS identifiers [capture 5] or strings [capture 3 or capture 4]"""
"To reduce the number of selectors needing tokenize in the preFilter, prefer arguments:"
1. quoted (capture 3; capture 4 or capture 5)
2. simple (capture 6)
3. anything else (capture 2)
"Leading and non-escaped trailing whitespace, capturing some non-whitespace characters preceding the latter"
For use in libraries implementing .is()
We use this for POS matching in `select`
Easily-parseable/retrievable ID or TAG or CLASS selectors
CSS escapes
http://www.w3.org/TR/CSS21/syndata.html#escaped-characters
NaN means non-codepoint
Support: Firefox<24
"Workaround erroneous numeric interpretation of +""0x"""
BMP codepoint
Supplemental Plane codepoint (surrogate pair)
CSS string/identifier serialization
https://drafts.csswg.org/cssom/#common-serializing-idioms
U+0000 NULL becomes U+FFFD REPLACEMENT CHARACTER
Control characters and (dependent upon position) numbers get escaped as code points
Other potentially-special ASCII characters get backslash-escaped
Used for iframes
See setDocument()
"Removing the function wrapper causes a ""Permission Denied"""
error in IE
"Optimize for push.apply( _, NodeList )"
Support: Android<4.0
Detect silently failing push.apply
Leverage slice if possible
Support: IE<9
Otherwise append directly
Can't trust NodeList.length
"nodeType defaults to 9, since context defaults to document"
Return early from calls with invalid selector or context
Try to shortcut find operations (as opposed to filters) in HTML documents
"If the selector is sufficiently simple, try using a ""get*By*"" DOM method"
"(excepting DocumentFragment context, where the methods don't exist)"
ID selector
Document context
"Support: IE, Opera, Webkit"
TODO: identify versions
getElementById can match elements by name instead of ID
Element context
"Support: IE, Opera, Webkit"
TODO: identify versions
getElementById can match elements by name instead of ID
Type selector
Class selector
Take advantage of querySelectorAll
Support: IE 8 only
Exclude object elements
qSA considers elements outside a scoping root when evaluating child or
"descendant combinators, which is not what we want."
"In such cases, we work around the behavior by prefixing every selector in the"
list with an ID selector referencing the scope context.
Thanks to Andrew Dupont for this technique.
"Capture the context ID, setting it first if necessary"
Prefix every selector in the list
Expand context for sibling selectors
All others
"Use (key + "" "") to avoid collision with native prototype properties (see Issue #157)"
Only keep the most recent entries
Remove from its parent by default
release memory in IE
Use IE sourceIndex if available on both nodes
Check if b follows a
Known :disabled false positives: fieldset[disabled] > legend:nth-of-type(n+2) :can-disable
Only certain elements can match :enabled or :disabled
https://html.spec.whatwg.org/multipage/scripting.html#selector-enabled
https://html.spec.whatwg.org/multipage/scripting.html#selector-disabled
Check for inherited disabledness on relevant non-disabled elements:
* listed form-associated elements in a disabled fieldset
https://html.spec.whatwg.org/multipage/forms.html#category-listed
https://html.spec.whatwg.org/multipage/forms.html#concept-fe-disabled
* option elements in a disabled optgroup
https://html.spec.whatwg.org/multipage/forms.html#concept-option-disabled
"All such elements have a ""form"" property."
Option elements defer to a parent optgroup if present
Support: IE 6 - 11
Use the isDisabled shortcut property to check for disabled fieldset ancestors
"Where there is no isDisabled, check manually"
Try to winnow out elements that can't be disabled before trusting the disabled property.
"Some victims get caught in our net (label, legend, menu, track), but it shouldn't"
"even exist on them, let alone have a boolean value."
Remaining elements are neither :enabled nor :disabled
Match elements found at the specified indexes
Expose support vars for convenience
Support: IE <=8
"Assume HTML when documentElement doesn't yet exist, such as inside loading iframes"
https://bugs.jquery.com/ticket/4833
Return early if doc is invalid or already selected
Update global variables
"Support: IE 9-11, Edge"
"Accessing iframe documents after unload throws ""permission denied"" errors (jQuery #13936)"
"Support: IE 11, Edge"
Support: IE 9 - 10 only
Support: IE<8
Verify that getAttribute really returns attributes and not properties
(excepting IE8 booleans)
"Check if getElementsByTagName(""*"") returns only elements"
Support: IE<9
Support: IE<10
Check if getElementById returns elements by name
"The broken getElementById methods don't pick up programmatically-set names,"
so use a roundabout getElementsByName test
ID filter and find
Support: IE 6 - 7 only
getElementById is not reliable as a find shortcut
Verify the id attribute
Fall back on getElementsByName
Tag
DocumentFragment nodes don't have gEBTN
"By happy coincidence, a (broken) gEBTN appears on DocumentFragment nodes too"
Filter out possible comments
Class
QSA and matchesSelector support
matchesSelector(:active) reports false when true (IE9/Opera 11.5)
qSa(:focus) reports false when true (Chrome 21)
We allow this because of a bug in IE8/9 that throws an error
whenever `document.activeElement` is accessed on an iframe
"So, we allow :focus to pass through QSA all the time to avoid the IE error"
See https://bugs.jquery.com/ticket/13378
Build QSA regex
Regex strategy adopted from Diego Perini
Select is set to empty string on purpose
This is to test IE's treatment of not explicitly
"setting a boolean content attribute,"
since its presence should be enough
https://bugs.jquery.com/ticket/12359
"Support: IE8, Opera 11-12.16"
Nothing should be selected when empty strings follow ^= or $= or *=
"The test attribute must be unknown in Opera but ""safe"" for WinRT"
https://msdn.microsoft.com/en-us/library/ie/hh465388.aspx#attribute_section
Support: IE8
"Boolean attributes and ""value"" are not treated correctly"
"Support: Chrome<29, Android<4.4, Safari<7.0+, iOS<7.0+, PhantomJS<1.9.8+"
Webkit/Opera - :checked should return selected option elements
http://www.w3.org/TR/2011/REC-css3-selectors-20110929/#checked
IE8 throws error here and will not see later tests
"Support: Safari 8+, iOS 8+"
https://bugs.webkit.org/show_bug.cgi?id=136851
In-page `selector#id sibling-combinator selector` fails
Support: Windows 8 Native Apps
The type and name attributes are restricted during .innerHTML assignment
Support: IE8
Enforce case-sensitivity of name attribute
FF 3.5 - :enabled/:disabled and hidden elements (hidden elements are still enabled)
IE8 throws error here and will not see later tests
Support: IE9-11+
IE's :disabled selector does not pick up the children of disabled fieldsets
Opera 10-11 does not throw on post-comma invalid pseudos
Check to see if it's possible to do matchesSelector
on a disconnected node (IE 9)
This should fail with an exception
"Gecko does not error, returns false instead"
Element contains another
Purposefully self-exclusive
"As in, an element does not contain itself"
Document order sorting
Flag for duplicate removal
Sort on method existence if only one input has compareDocumentPosition
Calculate position if both inputs belong to the same document
Otherwise we know they are disconnected
Disconnected nodes
Choose the first element that is related to our preferred document
Maintain original order
Exit early if the nodes are identical
Parentless nodes are either documents or disconnected
"If the nodes are siblings, we can do a quick check"
Otherwise we need full lists of their ancestors for comparison
Walk down the tree looking for a discrepancy
Do a sibling check if the nodes have a common ancestor
Otherwise nodes in our document sort first
Set document vars if needed
IE 9's matchesSelector returns false on disconnected nodes
"As well, disconnected nodes are said to be in a document"
fragment in IE 9
Set document vars if needed
Set document vars if needed
Don't get fooled by Object.prototype properties (jQuery #13807)
"Unless we *know* we can detect duplicates, assume their presence"
Clear input after sorting to release objects
See https://github.com/jquery/sizzle/pull/225
"If no nodeType, this is expected to be an array"
Do not traverse comment nodes
Use textContent for elements
innerText usage removed for consistency of new lines (jQuery #11153)
Traverse its children
Do not include comment or processing instruction nodes
Can be adjusted by the user
Move the given value to match[3] whether quoted or unquoted
nth-* requires argument
numeric x and y parameters for Expr.filter.CHILD
remember that false/true cast respectively to 0/1
other types prohibit arguments
Accept quoted arguments as-is
Strip excess characters from unquoted arguments
Get excess from tokenize (recursively)
advance to the next closing parenthesis
excess is a negative index
Return only captures needed by the pseudo filter method (type and argument)
Shortcut for :nth-*(n)
:(first|last|only)-(child|of-type)
Reverse direction for :only-* (if we haven't yet done so)
non-xml :nth-child(...) stores cache data on `parent`
Seek `elem` from a previously-cached index
...in a gzip-friendly way
Support: IE <9 only
Defend against cloned attroperties (jQuery gh-1709)
Fallback to seeking `elem` from the start
"When found, cache indexes on `parent` and break"
Use previously-cached element index if available
...in a gzip-friendly way
Support: IE <9 only
Defend against cloned attroperties (jQuery gh-1709)
xml :nth-child(...)
or :nth-last-child(...) or :nth(-last)?-of-type(...)
Use the same loop as above to seek `elem` from the start
Cache the index of each encountered element
Support: IE <9 only
Defend against cloned attroperties (jQuery gh-1709)
"Incorporate the offset, then check against cycle size"
pseudo-class names are case-insensitive
http://www.w3.org/TR/selectors/#pseudo-classes
Prioritize by case sensitivity in case custom pseudos are added with uppercase letters
Remember that setFilters inherits from pseudos
The user may use createPseudo to indicate that
arguments are needed to create the filter function
just as Sizzle does
But maintain support for old signatures
Potentially complex pseudos
Trim the selector passed to compile
to avoid treating leading and trailing
spaces as combinators
Match elements unmatched by `matcher`
Don't keep the element (issue #299)
"""Whether an element is represented by a :lang() selector"
is based solely on the element's language value
"being equal to the identifier C,"
"or beginning with the identifier C immediately followed by ""-""."
The matching of C against the element's language value is performed case-insensitively.
"The identifier C does not have to be a valid language name."""
http://www.w3.org/TR/selectors/#lang-pseudo
lang value must be a valid identifier
Miscellaneous
Boolean properties
"In CSS3, :checked should return both checked and selected elements"
http://www.w3.org/TR/2011/REC-css3-selectors-20110929/#checked
Accessing this property makes selected-by-default
options in Safari work properly
Contents
http://www.w3.org/TR/selectors/#empty-pseudo
":empty is negated by element (1) or content nodes (text: 3; cdata: 4; entity ref: 5),"
but not by others (comment: 8; processing instruction: 7; etc.)
nodeType < 6 works because attributes (2) do not appear as children
Element/input types
Support: IE<8
"New HTML5 attribute values (e.g., ""search"") appear with elem.type === ""text"""
Position-in-collection
Add button/input type pseudos
Easy API for creating new setFilters
Comma and first run
Don't consume trailing commas as valid
Combinators
Cast descendant combinators to space
Filters
Return the length of the invalid excess
if we're just parsing
"Otherwise, throw an error or return tokens"
Cache the tokens
Check against closest ancestor/preceding element
Check against all ancestor/preceding elements
"We can't set arbitrary data on XML nodes, so they don't benefit from combinator caching"
Support: IE <9 only
Defend against cloned attroperties (jQuery gh-1709)
Assign to newCache so results back-propagate to previous elements
Reuse newcache so results back-propagate to previous elements
A match means we're done; a fail means we have to keep checking
Get initial elements from seed or context
"Prefilter to get matcher input, preserving a map for seed-results synchronization"
"If we have a postFinder, or filtered seed, or non-seed postFilter or preexisting results,"
...intermediate processing is necessary
...otherwise use results directly
Find primary matches
Apply postFilter
Un-match failing elements by moving them back to matcherIn
Get the final matcherOut by condensing this intermediate into postFinder contexts
Restore matcherIn since elem is not yet a final match
Move matched elements from seed to results to keep them synchronized
"Add elements to results, through postFinder if defined"
The foundational matcher ensures that elements are reachable from top-level context(s)
Avoid hanging onto element (issue #299)
Return special upon seeing a positional matcher
Find the next relative operator (if any) for proper handling
"If the preceding token was a descendant combinator, insert an implicit any-element `*`"
We must always have either seed elements or outermost context
Use integer dirruns iff this is the outermost matcher
Add elements passing elementMatchers directly to results
"Support: IE<9, Safari"
"Tolerate NodeList properties (IE: ""length""; Safari: <number>) matching elements by id"
Track unmatched elements for set filters
They will have gone through all possible matchers
"Lengthen the array for every element, matched or not"
"`i` is now the count of elements visited above, and adding it to `matchedCount`"
makes the latter nonnegative.
Apply set filters to unmatched elements
"NOTE: This can be skipped if there are no unmatched elements (i.e., `matchedCount`"
"equals `i`), unless we didn't visit _any_ elements in the above loop because we have"
no element matchers and no seed.
"Incrementing an initially-string ""0"" `i` allows `i` to remain a string only in that"
"case, which will result in a ""00"" `matchedCount` that differs from `i` but is also"
numerically zero.
Reintegrate element matches to eliminate the need for sorting
Discard index placeholder values to get only actual matches
Add matches to results
Seedless set matches succeeding multiple successful matchers stipulate sorting
Override manipulation of globals by nested matchers
Generate a function of recursive functions that can be used to check each element
Cache the compiled function
Save selector and tokenization
Try to minimize operations if there is only one selector in the list and no seed
(the latter of which guarantees us context)
Reduce context if the leading compound selector is an ID
"Precompiled matchers will still verify ancestry, so step up a level"
Fetch a seed set for right-to-left matching
Abort if we hit a combinator
"Search, expanding context for leading sibling combinators"
"If seed is empty or no tokens remain, we can return early"
Compile and execute a filtering function if one is not provided
Provide `match` to avoid retokenization if we modified the selector above
One-time assignments
Sort stability
Support: Chrome 14-35+
Always assume duplicates if they aren't passed to the comparison function
Initialize against the default document
Support: Webkit<537.32 - Safari 6.0.3/Chrome 25 (fixed in Chrome 27)
Detached nodes confoundingly follow *each other*
"Should return 1, but returns 4 (following)"
Support: IE<8
"Prevent attribute/property ""interpolation"""
https://msdn.microsoft.com/en-us/library/ms536429%28VS.85%29.aspx
Support: IE<9
"Use defaultValue in place of getAttribute(""value"")"
Support: IE<9
Use getAttributeNode to fetch booleans when getAttribute lies
Deprecated
Implement the identical functionality for filter and not
Single element
"Arraylike of elements (jQuery, arguments, Array)"
Filtered directly for both simple and complex selectors
"If this is a positional/relative selector, check membership in the returned set"
"so $(""p:first"").is(""p:last"") won't return true for a doc with two ""p""."
Initialize a jQuery object
A central reference to the root jQuery(document)
A simple way to check for HTML strings
Prioritize #id over <tag> to avoid XSS via location.hash (#9521)
Strict HTML recognition (#11290: must start with <)
Shortcut simple #id case for speed
"HANDLE: $(""""), $(null), $(undefined), $(false)"
Method init() accepts an alternate rootjQuery
so migrate can support jQuery.sub (gh-2101)
Handle HTML strings
Assume that strings that start and end with <> are HTML and skip the regex check
Match html or make sure no context is specified for #id
HANDLE: $(html) -> $(array)
Option to run scripts is true for back-compat
Intentionally let the error be thrown if parseHTML is not present
"HANDLE: $(html, props)"
Properties of context are called as methods if possible
...and otherwise set as attributes
HANDLE: $(#id)
Inject the element directly into the jQuery object
"HANDLE: $(expr, $(...))"
"HANDLE: $(expr, context)"
(which is just equivalent to: $(context).find(expr)
HANDLE: $(DOMElement)
HANDLE: $(function)
Shortcut for document ready
Execute immediately if ready is not present
Give the init function the jQuery prototype for later instantiation
Initialize central reference
Methods guaranteed to produce a unique set when starting from a unique set
"Positional selectors never match, since there's no _selection_ context"
Always skip document fragments
Don't pass non-elements to Sizzle
Determine the position of an element within the set
"No argument, return index in parent"
Index in selector
Locate the position of the desired element
"If it receives a jQuery object, the first element is used"
"Support: IE 9 - 11 only, iOS 7 only, Android Browser <=4.3 only"
Treat the template element as a regular one in browsers that
don't support it.
Remove duplicates
Reverse order for parents* and prev-derivatives
Convert String-formatted options into Object-formatted ones
Convert options from String-formatted to Object-formatted if needed
(we check in cache first)
Last fire value for non-forgettable lists
Flag to know if list was already fired
Flag to prevent firing
Actual callback list
Queue of execution data for repeatable lists
Index of currently firing callback (modified by add/remove as needed)
Fire callbacks
Enforce single-firing
"Execute callbacks for all pending executions,"
respecting firingIndex overrides and runtime changes
Run callback and check for early termination
Jump to end and forget the data so .add doesn't re-fire
Forget the data if we're done with it
Clean up if we're done firing for good
Keep an empty list if we have data for future add calls
"Otherwise, this object is spent"
Actual Callbacks object
Add a callback or a collection of callbacks to the list
"If we have memory from a past run, we should fire after adding"
Inspect recursively
Remove a callback from the list
Handle firing indexes
Check if a given callback is in the list.
"If no argument is given, return whether or not list has callbacks attached."
Remove all callbacks from the list
Disable .fire and .add
Abort any current/pending executions
Clear all callbacks and values
Disable .fire
Also disable .add unless we have memory (since it would have no effect)
Abort any pending executions
Call all callbacks with the given context and arguments
Call all the callbacks with the given arguments
To know if the callbacks have already been called at least once
Check for promise aspect first to privilege synchronous behavior
Other thenables
Other non-thenables
Control `resolve` arguments by letting Array#slice cast boolean `noValue` to integer:
* false: [ value ].slice( 0 ) => resolve( value )
* true: [ value ].slice( 1 ) => resolve()
"For Promises/A+, convert exceptions into rejections"
"Since jQuery.when doesn't unwrap thenables, we can skip the extra checks appearing in"
Deferred#then to conditionally suppress rejection.
Support: Android 4.0 only
Strict mode functions invoked without .call/.apply get global-object context
"action, add listener, callbacks,"
"... .then handlers, argument index, [final state]"
Keep pipe for back-compat
"Map tuples (progress, done, fail) to arguments (done, fail, progress)"
deferred.progress(function() { bind to newDefer or newDefer.notify })
deferred.done(function() { bind to newDefer or newDefer.resolve })
deferred.fail(function() { bind to newDefer or newDefer.reject })
Support: Promises/A+ section 2.3.3.3.3
https://promisesaplus.com/#point-59
Ignore double-resolution attempts
Support: Promises/A+ section 2.3.1
https://promisesaplus.com/#point-48
"Support: Promises/A+ sections 2.3.3.1, 3.5"
https://promisesaplus.com/#point-54
https://promisesaplus.com/#point-75
Retrieve `then` only once
Support: Promises/A+ section 2.3.4
https://promisesaplus.com/#point-64
Only check objects and functions for thenability
Handle a returned thenable
Special processors (notify) just wait for resolution
Normal processors (resolve) also hook into progress
...and disregard older resolution values
Handle all other returned values
Only substitute handlers pass on context
and multiple values (non-spec behavior)
Process the value(s)
Default process is resolve
Only normal processors (resolve) catch and reject exceptions
Support: Promises/A+ section 2.3.3.3.4.1
https://promisesaplus.com/#point-61
Ignore post-resolution exceptions
Only substitute handlers pass on context
and multiple values (non-spec behavior)
Support: Promises/A+ section 2.3.3.3.1
https://promisesaplus.com/#point-57
Re-resolve promises immediately to dodge false rejection from
subsequent errors
"Call an optional hook to record the stack, in case of exception"
since it's otherwise lost when execution goes async
progress_handlers.add( ... )
fulfilled_handlers.add( ... )
rejected_handlers.add( ... )
Get a promise for this deferred
"If obj is provided, the promise aspect is added to the object"
Add list-specific methods
promise.progress = list.add
promise.done = list.add
promise.fail = list.add
Handle state
"state = ""resolved"" (i.e., fulfilled)"
"state = ""rejected"""
rejected_callbacks.disable
fulfilled_callbacks.disable
rejected_handlers.disable
fulfilled_handlers.disable
progress_callbacks.lock
progress_handlers.lock
progress_handlers.fire
fulfilled_handlers.fire
rejected_handlers.fire
deferred.notify = function() { deferred.notifyWith(...) }
deferred.resolve = function() { deferred.resolveWith(...) }
deferred.reject = function() { deferred.rejectWith(...) }
deferred.notifyWith = list.fireWith
deferred.resolveWith = list.fireWith
deferred.rejectWith = list.fireWith
Make the deferred a promise
Call given func if any
All done!
Deferred helper
count of uncompleted subordinates
count of unprocessed arguments
subordinate fulfillment data
the master Deferred
subordinate callback factory
Single- and empty arguments are adopted like Promise.resolve
Use .then() to unwrap secondary thenables (cf. gh-3000)
Multiple arguments are aggregated like Promise.all array elements
"These usually indicate a programmer mistake during development,"
warn about them ASAP rather than swallowing them by default.
Support: IE 8 - 9 only
"Console exists when dev tools are open, which can happen at any time"
The deferred used on DOM ready
Wrap jQuery.readyException in a function so that the lookup
happens at the time of error handling instead of callback
registration.
Is the DOM ready to be used? Set to true once it occurs.
A counter to track how many items to wait for before
the ready event fires. See #6781
Handle when the DOM is ready
Abort if there are pending holds or we're already ready
Remember that the DOM is ready
"If a normal DOM Ready event fired, decrement, and wait if need be"
"If there are functions bound, to execute"
The ready event handler and self cleanup method
Catch cases where $(document).ready() is called
after the browser event has already occurred.
Support: IE <=9 - 10 only
"Older IE sometimes signals ""interactive"" too soon"
Handle it asynchronously to allow scripts the opportunity to delay ready
Use the handy event callback
"A fallback to window.onload, that will always work"
Multifunctional method to get and set values of a collection
The value/s can optionally be executed if it's a function
Sets many values
Sets one value
Bulk operations run against the entire set
...except when executing function values
Gets
Matches dashed string for camelizing
Used by camelCase as callback to replace()
Convert dashed to camelCase; used by the css and data modules
"Support: IE <=9 - 11, Edge 12 - 15"
Microsoft forgot to hump their vendor prefix (#9572)
Accepts only:
- Node
- Node.ELEMENT_NODE
- Node.DOCUMENT_NODE
- Object
- Any
Check if the owner object already has a cache
"If not, create one"
"We can accept data for non-element nodes in modern browsers,"
"but we should not, see #8335."
Always return an empty object.
If it is a node unlikely to be stringify-ed or looped over
use plain assignment
Otherwise secure it in a non-enumerable property
configurable must be true to allow the property to be
deleted when data is removed
"Handle: [ owner, key, value ] args"
Always use camelCase key (gh-2257)
"Handle: [ owner, { properties } ] args"
Copy the properties one-by-one to the cache object
Always use camelCase key (gh-2257)
In cases where either:
""
1. No key was specified
"2. A string key was specified, but no value provided"
""
"Take the ""read"" path and allow the get method to determine"
"which value to return, respectively either:"
""
1. The entire cache object
2. The data stored at the key
""
"When the key is not a string, or both a key and value"
"are specified, set or extend (existing objects) with either:"
""
1. An object of properties
2. A key and value
""
"Since the ""set"" path can have two possible entry points"
return the expected data based on which path was taken[*]
Support array or space separated string of keys
If key is an array of keys...
"We always set camelCase keys, so remove that."
"If a key with the spaces exists, use it."
"Otherwise, create an array by matching non-whitespace"
Remove the expando if there's no more data
Support: Chrome <=35 - 45
Webkit & Blink performance suffers when deleting properties
"from DOM nodes, so set to undefined instead"
https://bugs.chromium.org/p/chromium/issues/detail?id=378607 (bug restricted)
Implementation Summary
""
1. Enforce API surface and semantic compatibility with 1.9.x branch
2. Improve the module's maintainability by reducing the storage
paths to a single mechanism.
"3. Use the same single mechanism to support ""private"" and ""user"" data."
"4. _Never_ expose ""private"" data to user code (TODO: Drop _data, _removeData)"
5. Avoid exposing implementation details on user objects (eg. expando properties)
6. Provide a clear path for implementation upgrade to WeakMap in 2014
Only convert to a number if it doesn't change the string
"If nothing was found internally, try to fetch any"
data from the HTML5 data-* attribute
Make sure we set the data so it isn't changed later
TODO: Now that all calls to _data and _removeData have been replaced
"with direct calls to dataPriv methods, these can be deprecated."
Gets all values
Support: IE 11 only
The attrs elements can be null (#14894)
Sets multiple values
The calling jQuery object (element matches) is not empty
(and therefore has an element appears at this[ 0 ]) and the
`value` parameter was not undefined. An empty jQuery object
will result in `undefined` for elem = this[ 0 ] which will
throw an exception if an attempt to read a data cache is made.
Attempt to get data from the cache
The key will always be camelCased in Data
"Attempt to ""discover"" the data in"
HTML5 custom data-* attrs
"We tried really hard, but the data doesn't exist."
Set the data...
We always store the camelCased key
Speed up dequeue by getting out quickly if this is just a lookup
"If the fx queue is dequeued, always remove the progress sentinel"
Add a progress sentinel to prevent the fx queue from being
automatically dequeued
Clear up the last queue stop function
"Not public - generate a queueHooks object, or return the current one"
Ensure a hooks for this queue
Get a promise resolved when queues of a certain type
are emptied (fx is the type by default)
Check attachment across shadow DOM boundaries when possible (gh-3504)
isHiddenWithinTree might be called from jQuery#filter function;
"in that case, element will be second argument"
Inline style trumps all
"Otherwise, check computed style"
Support: Firefox <=43 - 45
"Disconnected elements can have computed display: none, so first confirm that elem is"
in the document.
"Remember the old values, and insert the new ones"
Revert the old values
Starting value computation is required for potential unit mismatches
Support: Firefox <=54
Halve the iteration target value to prevent interference from CSS upper bounds (gh-2144)
Trust units reported by jQuery.css
Iteratively approximate from a nonzero starting point
Evaluate and update our best guess (doubling guesses that zero out).
Finish if the scale equals or crosses 1 (making the old*new product non-positive).
Make sure we update the tween properties later on
Apply relative offset (+=/-=) if specified
Determine new display value for elements that need to change
"Since we force visibility upon cascade-hidden elements, an immediate (and slow)"
check is required in this first loop unless we have a nonempty display value (either
inline or about-to-be-restored)
Remember what we're overwriting
Set the display of the elements in a second loop to avoid constant reflow
We have to close these tags to support XHTML (#13200)
Support: IE <=9 only
XHTML parsers do not magically insert elements in the
same way that tag soup parsers do. So we cannot shorten
this by omitting <tbody> or other required elements.
Support: IE <=9 only
Support: IE <=9 - 11 only
Use typeof to avoid zero-argument method invocation on host objects (#15151)
Mark scripts as having already been evaluated
Add nodes directly
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
Convert non-html into a text node
Convert html into DOM nodes
Deserialize a standard representation
Descend through wrappers to the right content
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
Remember the top-level container
Ensure the created nodes are orphaned (#12392)
Remove wrapper from fragment
Skip elements already in the context collection (trac-4087)
Append to fragment
Preserve script evaluation history
Capture executables
Support: Android 4.0 - 4.3 only
Check state lost if the name is set (#11217)
Support: Windows Web Apps (WWA)
`name` and `type` must use .setAttribute for WWA (#14901)
Support: Android <=4.1 only
Older WebKit doesn't clone checked state correctly in fragments
Support: IE <=11 only
Make sure textarea (and checkbox) defaultValue is properly cloned
Support: IE <=9 - 11+
"focus() and blur() are asynchronous, except when they are no-op."
"So expect focus to be synchronous when the element is already active,"
and blur to be synchronous when the element is not already active.
"(focus and blur are always synchronous in other supported browsers,"
this just defines when we can count on it).
Support: IE <=9 only
Accessing document.activeElement can throw unexpectedly
https://bugs.jquery.com/ticket/13393
Types can be a map of types/handlers
"( types-Object, selector, data )"
"( types-Object, data )"
"( types, fn )"
"( types, selector, fn )"
"( types, data, fn )"
"Can use an empty set, since event contains the info"
Use same guid so caller can remove using origFn
Don't attach events to noData or text/comment nodes (but allow plain objects)
Caller can pass in an object of custom data in lieu of the handler
Ensure that invalid selectors throw exceptions at attach time
"Evaluate against documentElement in case elem is a non-element node (e.g., document)"
"Make sure that the handler has a unique ID, used to find/remove it later"
"Init the element's event structure and main handler, if this is the first"
Discard the second event of a jQuery.event.trigger() and
when an event is called after a page has unloaded
Handle multiple events separated by a space
"There *must* be a type, no attaching namespace-only handlers"
"If event changes its type, use the special event handlers for the changed type"
"If selector defined, determine special event api type, otherwise given type"
Update special based on newly reset type
handleObj is passed to all event handlers
Init the event handler queue if we're the first
Only use addEventListener if the special events handler returns false
"Add to the element's handler list, delegates in front"
"Keep track of which events have ever been used, for event optimization"
Detach an event or set of events from an element
Once for each type.namespace in types; type may be omitted
"Unbind all events (on this namespace, if provided) for the element"
Remove matching events
Remove generic event handler if we removed something and no more handlers exist
(avoids potential for endless recursion during removal of special event handlers)
Remove data and the expando if it's no longer used
Make a writable jQuery.Event from the native event object
Use the fix-ed jQuery.Event rather than the (read-only) native event
"Call the preDispatch hook for the mapped type, and let it bail if desired"
Determine handlers
Run delegates first; they may want to stop propagation beneath us
"If the event is namespaced, then each handler is only invoked if it is"
specially universal or its namespaces are a superset of the event's.
Call the postDispatch hook for the mapped type
Find delegate handlers
Support: IE <=9
Black-hole SVG <use> instance trees (trac-13180)
Support: Firefox <=42
Suppress spec-violating clicks indicating a non-primary pointer button (trac-3861)
https://www.w3.org/TR/DOM-Level-3-Events/#event-type-click
Support: IE 11 only
"...but not arrow key ""clicks"" of radio inputs, which can have `button` -1 (gh-2343)"
Don't check non-elements (#13208)
"Don't process clicks on disabled elements (#6911, #8165, #11382, #11764)"
Don't conflict with Object.prototype properties (#13203)
Add the remaining (directly-bound) handlers
Prevent triggered image.load events from bubbling to window.load
Utilize native event to ensure correct state for checkable inputs
"For mutual compressibility with _default, replace `this` access with a local var."
`|| data` is dead code meant only to preserve the variable through minification.
Claim the first handler
"dataPriv.set( el, ""click"", ... )"
Return false to allow normal processing in the caller
"For mutual compressibility with _default, replace `this` access with a local var."
`|| data` is dead code meant only to preserve the variable through minification.
Force setup before triggering a click
Return non-false to allow normal event-path propagation
"For cross-browser consistency, suppress native .click() on links"
Also prevent it if we're currently inside a leveraged native-event stack
Support: Firefox 20+
Firefox doesn't alert if the returnValue field is not set.
Ensure the presence of an event listener that handles manually-triggered
synthetic events by interrupting progress until reinvoked in response to
"*native* events that it fires directly, ensuring that state changes have"
already occurred before other listeners are invoked.
"Missing expectSync indicates a trigger call, which must force setup through jQuery.event.add"
Register the controller as a special universal handler for all event namespaces
Interrupt processing of the outer synthetic .trigger()ed event
Store arguments for use when handling the inner native event
Trigger the native event and capture its result
Support: IE <=9 - 11+
focus() and blur() are asynchronous
Cancel the outer synthetic event
If this is an inner synthetic event for an event with a bubbling surrogate
"(focus or blur), assume that the surrogate already propagated from triggering the"
native event and prevent that from happening again here.
This technically gets the ordering wrong w.r.t. to `.trigger()` (in which the
"bubbling surrogate propagates *after* the non-bubbling base), but that seems"
less bad than duplication.
"If this is a native event triggered above, everything is now in order"
Fire an inner synthetic event with the original arguments
...and capture the result
Support: IE <=9 - 11+
Extend with the prototype to reset the above stopImmediatePropagation()
Abort handling of the native event
"This ""if"" is needed for plain objects"
Allow instantiation without the 'new' keyword
Event object
Events bubbling up the document may have been marked as prevented
by a handler lower down the tree; reflect the correct value.
Support: Android <=2.3 only
Create target properties
Support: Safari <=6 - 7 only
"Target should not be a text node (#504, #13143)"
Event type
Put explicitly provided properties onto the event object
Create a timestamp if incoming event doesn't have one
Mark it as fixed
jQuery.Event is based on DOM3 Events as specified by the ECMAScript Language Binding
https://www.w3.org/TR/2003/WD-DOM-Level-3-Events-20030331/ecma-script-binding.html
Includes all common event props including KeyEvent and MouseEvent specific props
Add which for key events
Add which for click: 1 === left; 2 === middle; 3 === right
Utilize native event if possible so blur/focus sequence is correct
Claim the first handler
"dataPriv.set( this, ""focus"", ... )"
"dataPriv.set( this, ""blur"", ... )"
Return false to allow normal processing in the caller
Force setup before trigger
Return non-false to allow normal event-path propagation
Create mouseenter/leave events using mouseover/out and event-time checks
so that event delegation works in jQuery.
Do the same for pointerenter/pointerleave and pointerover/pointerout
""
Support: Safari 7 only
Safari sends mouseenter too often; see:
https://bugs.chromium.org/p/chromium/issues/detail?id=470258
for the description of the bug (it existed in older Chrome versions as well).
For mouseenter/leave call the handler if related is outside the target.
NB: No relatedTarget if the mouse left/entered the browser window
( event )  dispatched jQuery.Event
"( types-object [, selector] )"
"( types [, fn] )"
See https://github.com/eslint/eslint/issues/3229
"Support: IE <=10 - 11, Edge 12 - 13 only"
In IE/Edge using regex groups here causes severe slowdowns.
See https://connect.microsoft.com/IE/feedback/details/1736512/
"checked=""checked"" or checked"
Prefer a tbody over its parent table for containing new rows
Replace/restore the type attribute of script elements for safe DOM manipulation
"1. Copy private data: events, handlers, etc."
2. Copy user data
"Fix IE bugs, see support tests"
Fails to persist the checked state of a cloned checkbox or radio button.
Fails to return the selected option to the default selected state when cloning options
Flatten any nested arrays
"We can't cloneNode fragments that contain checked, in WebKit"
Require either new content or an interest in ignored elements to invoke the callback
Use the original fragment for the last item
instead of the first because it can end up
being emptied incorrectly in certain situations (#8070).
Keep references to cloned scripts for later restoration
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
Reenable scripts
Evaluate executable scripts on first document insertion
"Optional AJAX dependency, but won't run scripts if not present"
Fix IE cloning issues
We eschew Sizzle here for performance reasons: https://jsperf.com/getall-vs-sizzle/2
Copy the events from the original to the clone
Preserve script evaluation history
Return the cloned set
This is a shortcut to avoid jQuery.event.remove's overhead
Support: Chrome <=35 - 45+
"Assign undefined instead of using delete, see Data#remove"
Support: Chrome <=35 - 45+
"Assign undefined instead of using delete, see Data#remove"
Prevent memory leaks
Remove any remaining nodes
See if we can take a shortcut and just use innerHTML
Remove element nodes and prevent memory leaks
"If using innerHTML throws an exception, use the fallback method"
"Make the changes, replacing each non-ignored context element with the new content"
Force callback invocation
"Support: Android <=4.0 only, PhantomJS 1 only"
".get() because push.apply(_, arraylike) throws on ancient WebKit"
"Support: IE <=11 only, Firefox <=30 (#15098, #14150)"
IE throws on elements created in popups
"FF meanwhile throws on frame elements through ""defaultView.getComputedStyle"""
Executing both pixelPosition & boxSizingReliable tests require only one layout
so they're executed at the same time to save the second computation.
"This is a singleton, we need to execute it only once"
"Support: Android 4.0 - 4.3 only, Firefox <=3 - 44"
"Support: Android 4.0 - 4.3 only, Safari <=9.1 - 10.1, iOS <=7.0 - 9.3"
"Some styles come back with percentage values, even though they shouldn't"
Support: IE 9 - 11 only
Detect misreporting of content dimensions for box-sizing:border-box elements
Support: IE 9 only
Detect overflow:scroll screwiness (gh-3699)
Support: Chrome <=64
Don't get tricked when zoom affects offsetWidth (gh-4029)
Nullify the div so it wouldn't be stored in the memory and
it will also be a sign that checks already performed
Finish early in limited (non-browser) environments
Support: IE <=9 - 11 only
Style of cloned element affects source element cloned (#8908)
Support: Firefox 51+
Retrieving style before computed somehow
fixes an issue with getting wrong values
on detached elements
getPropertyValue is needed for:
".css('filter') (IE 9 only, #12537)"
.css('--customProperty) (#3144)
"A tribute to the ""awesome hack by Dean Edwards"""
"Android Browser returns percentage for some values,"
but width seems to be reliably pixels.
This is against the CSSOM draft spec:
https://drafts.csswg.org/cssom/#resolved-values
Remember the original values
Put in the new values to get a computed value out
Revert the changed values
Support: IE <=9 - 11 only
IE returns zIndex value as an integer.
"Define the hook, we'll check on the first run if it's really needed."
Hook not needed (or it's not possible to use it due
"to missing dependency), remove it."
Hook needed; redefine it so that the support test is not executed again.
Return a vendor-prefixed property or undefined
Check for vendor prefixed names
Return a potentially-mapped jQuery.cssProps or vendor prefixed property
Swappable if display is none or starts with table
"except ""table"", ""table-cell"", or ""table-caption"""
See here for display values: https://developer.mozilla.org/en-US/docs/CSS/display
Any relative (+/-) values have already been
normalized at this point
"Guard against undefined ""subtract"", e.g., when used as in cssHooks"
Adjustment may not be necessary
Both box models exclude margin
"If we get here with a content-box, we're seeking ""padding"" or ""border"" or ""margin"""
Add padding
"For ""border"" or ""margin"", add border"
But still keep track of it otherwise
"If we get here with a border-box (content + padding + border), we're seeking ""content"" or"
"""padding"" or ""margin"""
"For ""content"", subtract padding"
"For ""content"" or ""padding"", subtract border"
Account for positive content-box scroll gutter when requested by providing computedVal
"offsetWidth/offsetHeight is a rounded sum of content, padding, scroll gutter, and border"
"Assuming integer scroll gutter, subtract the rest and round down"
"If offsetWidth/offsetHeight is unknown, then we can't determine content-box scroll gutter"
Use an explicit zero to avoid NaN (gh-3964)
Start with computed style
"To avoid forcing a reflow, only fetch boxSizing if we need it (gh-4322)."
Fake content-box until we know it's needed to know the true value.
Support: Firefox <=54
"Return a confounding non-pixel value or feign ignorance, as appropriate."
"Fall back to offsetWidth/offsetHeight when value is ""auto"""
This happens for inline elements with no explicit setting (gh-3571)
Support: Android <=4.1 - 4.3 only
Also use offsetWidth/offsetHeight for misreported inline dimensions (gh-3602)
Support: IE 9-11 only
Also use offsetWidth/offsetHeight for when box sizing is unreliable
We use getClientRects() to check for hidden/disconnected.
"In those cases, the computed value can be trusted to be border-box"
"Where available, offsetWidth/offsetHeight approximate border box dimensions."
"Where not available (e.g., SVG), assume unreliable box-sizing and interpret the"
retrieved value as a content box dimension.
"Normalize """" and auto"
Adjust for the element's box model
Provide the current computed size to request scroll gutter calculation (gh-3589)
Add in style property hooks for overriding the default
behavior of getting and setting a style property
We should always get a number back from opacity
"Don't automatically add ""px"" to these possibly-unitless properties"
Add in properties whose names you wish to fix before
setting or getting the value
Get and set the style property on a DOM Node
Don't set styles on text and comment nodes
Make sure that we're working with the right name
Make sure that we're working with the right name. We don't
want to query the value if it is a CSS custom property
since they are user-defined.
"Gets hook for the prefixed version, then unprefixed version"
Check if we're setting a value
"Convert ""+="" or ""-="" to relative numbers (#7345)"
Fixes bug #9237
Make sure that null and NaN values aren't set (#7116)
"If a number was passed in, add the unit (except for certain CSS properties)"
The isCustomProp check can be removed in jQuery 4.0 when we only auto-append
"""px"" to a few hardcoded values."
background-* props affect original clone's values
"If a hook was provided, use that value, otherwise just set the specified value"
If a hook was provided get the non-computed value from there
Otherwise just get the value from the style object
Make sure that we're working with the right name. We don't
want to modify the value if it is a CSS custom property
since they are user-defined.
Try prefixed name followed by the unprefixed name
If a hook was provided get the computed value from there
"Otherwise, if a way to get the computed value exists, use that"
"Convert ""normal"" to computed value"
Make numeric if forced or a qualifier was provided and val looks numeric
Certain elements can have dimension info if we invisibly show them
but it must have a current display style that would benefit
Support: Safari 8+
Table columns in Safari have non-zero offsetWidth & zero
getBoundingClientRect().width unless display is changed.
Support: IE <=11 only
Running getBoundingClientRect on a disconnected node
in IE throws an error.
Only read styles.position if the test has a chance to fail
to avoid forcing a reflow.
"To avoid forcing a reflow, only fetch boxSizing if we need it (gh-3991)"
Account for unreliable border-box dimensions by comparing offset* to computed and
faking a content-box to get border and padding (gh-3699)
Convert to pixels if value adjustment is needed
These hooks are used by animate to expand properties
Assumes a single number if not a string
"Based off of the plugin by Clint Helfers, with permission."
https://web.archive.org/web/20100324014747/http://blindsignals.com/index.php/2009/07/jquery-delay/
Support: Android <=4.3 only
"Default value for a checkbox should be ""on"""
Support: IE <=11 only
Must access selectedIndex to make default options select
Support: IE <=11 only
An input loses its value after becoming a radio
"Don't get/set attributes on text, comment and attribute nodes"
Fallback to prop when attributes are not supported
Attribute hooks are determined by the lowercase version
Grab necessary hook if one is defined
"Non-existent attributes return null, we normalize to undefined"
Attribute names can contain non-HTML whitespace characters
https://html.spec.whatwg.org/multipage/syntax.html#attributes-2
Hooks for boolean attributes
Remove boolean attributes when set to false
Avoid an infinite loop by temporarily removing this function from the getter
"Don't get/set properties on text, comment and attribute nodes"
Fix name and attach hooks
Support: IE <=9 - 11 only
elem.tabIndex doesn't always return the
correct value when it hasn't been explicitly set
https://web.archive.org/web/20141116233347/http://fluidproject.org/blog/2008/01/09/getting-setting-and-removing-tabindex-values-with-javascript/
Use proper attribute retrieval(#12072)
Support: IE <=11 only
Accessing the selectedIndex property
forces the browser to respect setting selected
on the option
The getter ensures a default option is selected
when in an optgroup
"eslint rule ""no-unused-expressions"" is disabled for this code"
since it considers such accessions noop
Strip and collapse whitespace according to HTML spec
https://infra.spec.whatwg.org/#strip-and-collapse-ascii-whitespace
Only assign if different to avoid unneeded rendering.
This expression is here for better compressibility (see addClass)
Remove *all* instances
Only assign if different to avoid unneeded rendering.
Toggle individual class names
"Check each className given, space separated list"
Toggle whole class name
Store className if set
"If the element has a class name or if we're passed `false`,"
"then remove the whole classname (if there was one, the above saved it)."
"Otherwise bring back whatever was previously saved (if anything),"
falling back to the empty string if nothing was stored.
Handle most common string cases
Handle cases where value is null/undef or number
"Treat null/undefined as """"; convert numbers to string"
"If set returns undefined, fall back to normal setting"
Support: IE <=10 - 11 only
"option.text throws exceptions (#14686, #14858)"
Strip and collapse whitespace
https://html.spec.whatwg.org/#strip-and-collapse-whitespace
Loop through all the selected options
Support: IE <=9 only
IE8-9 doesn't update selected after form reset (#2551)
Don't return options that are disabled or in a disabled optgroup
Get the specific value for the option
We don't need an array for one selects
Multi-Selects return an array
Force browsers to behave consistently when non-matching value is set
Radios and checkboxes getter/setter
Return jQuery for attributes-only inclusion
Don't do events on text and comment nodes
focus/blur morphs to focusin/out; ensure we're not firing them right now
Namespaced trigger; create a regexp to match event type in handle()
"Caller can pass in a jQuery.Event object, Object, or just an event type string"
Trigger bitmask: & 1 for native handlers; & 2 for jQuery (always true)
Clean up the event in case it is being reused
"Clone any incoming data and prepend the event, creating the handler arg list"
Allow special events to draw outside the lines
"Determine event propagation path in advance, per W3C events spec (#9951)"
"Bubble up to document, then to window; watch for a global ownerDocument var (#9724)"
"Only add window if we got to document (e.g., not plain obj or detached DOM)"
Fire handlers on the event path
jQuery handler
Native handler
"If nobody prevented the default action, do it now"
Call a native DOM method on the target with the same name as the event.
"Don't do default actions on window, that's where global variables be (#6170)"
Don't re-trigger an onFOO event when we call its FOO() method
"Prevent re-triggering of the same event, since we already bubbled it above"
Piggyback on a donor event to simulate a different one
Used only for `focus(in | out)` events
Support: Firefox <=44
Firefox doesn't have focus(in | out) events
Related ticket - https://bugzilla.mozilla.org/show_bug.cgi?id=687787
""
"Support: Chrome <=48 - 49, Safari <=9.0 - 9.1"
"focus(in | out) events fire after focus & blur events,"
which is spec violation - http://www.w3.org/TR/DOM-Level-3-Events/#events-focusevent-event-order
Related ticket - https://bugs.chromium.org/p/chromium/issues/detail?id=449857
Attach a single capturing handler on the document while someone wants focusin/focusout
Serialize array item.
Treat each array item as a scalar.
"Item is non-scalar (array or object), encode its numeric index."
Serialize object item.
Serialize scalar item.
Serialize an array of form elements or a set of
key/values into a query string
"If value is a function, invoke it and use its return value"
"If an array was passed in, assume that it is an array of form elements."
Serialize the form elements
"If traditional, encode the ""old"" way (the way 1.3.2 or older"
"did it), otherwise encode params recursively."
Return the resulting serialization
"Can add propHook for ""elements"" to filter or add form elements"
"Use .is( "":disabled"" ) so that fieldset[disabled] works"
The elements to wrap the target around
Support: Safari 8 only
In Safari 8 documents created via document.implementation.createHTMLDocument
collapse sibling forms: the second one becomes a child of the first one.
"Because of that, this security measure has to be disabled in Safari 8."
https://bugs.webkit.org/show_bug.cgi?id=137337
"Argument ""data"" should be string of html"
"context (optional): If specified, the fragment will be created in this context,"
defaults to document
"keepScripts (optional): If true, will include scripts passed in the html string"
Stop scripts or inline event handlers from being executed immediately
by using document.implementation
Set the base href for the created document
so any parsed elements with URLs
are based on the document's URL (gh-2965)
Single tag
"Set position first, in-case top/left are set even on static elem"
Need to be able to calculate position if either
top or left is auto and position is either absolute or fixed
Use jQuery.extend here to allow modification of coordinates argument (gh-1848)
offset() relates an element's border box to the document origin
Preserve chaining for setter
Return zeros for disconnected and hidden (display: none) elements (gh-2310)
Support: IE <=11 only
Running getBoundingClientRect on a
disconnected node in IE throws an error
Get document-relative position by adding viewport scroll to viewport-relative gBCR
position() relates an element's margin box to its offset parent's padding box
This corresponds to the behavior of CSS absolute positioning
"position:fixed elements are offset from the viewport, which itself always has zero offset"
Assume position:fixed implies availability of getBoundingClientRect
"Account for the *real* offset parent, which can be the document or its root element"
when a statically positioned element is identified
"Incorporate borders into its offset, since they are outside its content origin"
Subtract parent offsets and element margins
This method will return documentElement in the following cases:
"1) For the element inside the iframe without offsetParent, this method will return"
documentElement of the parent window
2) For the hidden or detached element
"3) For body or html element, i.e. in case of the html node - it will return itself"
""
but those exceptions were never presented as a real life use-cases
and might be considered as more preferable results.
""
"This logic, however, is not guaranteed and can change at any point in the future"
Create scrollLeft and scrollTop methods
Coalesce documents and windows
"Support: Safari <=7 - 9.1, Chrome <=37 - 49"
Add the top/left cssHooks using jQuery.fn.position
Webkit bug: https://bugs.webkit.org/show_bug.cgi?id=29084
Blink bug: https://bugs.chromium.org/p/chromium/issues/detail?id=589347
getComputedStyle returns percent when specified for top/left/bottom/right;
"rather than make the css module depend on the offset module, just check for it here"
"If curCSS returns percentage, fallback to offset"
"Create innerHeight, innerWidth, height, width, outerHeight and outerWidth methods"
"Margin is only for outerHeight, outerWidth"
$( window ).outerWidth/Height return w/h including scrollbars (gh-1729)
Get document width or height
"Either scroll[Width/Height] or offset[Width/Height] or client[Width/Height],"
whichever is greatest
"Get width or height on the element, requesting but not forcing parseFloat"
Set width or height on the element
Handle event binding
"( namespace ) or ( selector, types [, fn] )"
"Bind a function to a context, optionally partially applying any"
arguments.
jQuery.proxy is deprecated to promote standards (specifically Function#bind)
"However, it is not slated for removal any time soon"
"Quick check to determine if target is callable, in the spec"
"this throws a TypeError, but we will just return undefined."
Simulated bind
"Set the guid of unique handler to the same of original handler, so it can be removed"
"As of jQuery 3.0, isNumeric is limited to"
strings and numbers (primitives or objects)
that can be coerced to finite numbers (gh-2662)
"parseFloat NaNs numeric-cast false positives ("""")"
"...but misinterprets leading-number strings, particularly hex literals (""0x..."")"
subtraction forces infinities to NaN
"Register as a named AMD module, since jQuery can be concatenated with other"
"files that may use define, but not via a proper concatenation script that"
understands anonymous AMD modules. A named AMD is safest and most robust
way to register. Lowercase jquery is used because AMD module names are
"derived from file names, and jQuery is normally delivered in a lowercase"
file name. Do this after creating the global so that if an AMD module wants
"to call noConflict to hide this version of jQuery, it will work."
"Note that for maximum portability, libraries that are not jQuery should"
"declare themselves as anonymous modules, and avoid setting a global if an"
"AMD loader is present. jQuery is a special case. For more information, see"
https://github.com/jrburke/requirejs/wiki/Updating-existing-libraries#wiki-anon
Map over jQuery in case of overwrite
Map over the $ in case of overwrite
"Expose jQuery and $ identifiers, even in AMD"
"(#7102#comment:10, https://github.com/jquery/jquery/pull/557)"
and CommonJS for browser emulators (#13566)
Implementation Summary
""
1. Enforce API surface and semantic compatibility with 1.9.x branch
2. Improve the module's maintainability by reducing the storage
paths to a single mechanism.
"3. Use the same single mechanism to support ""private"" and ""user"" data."
"4. _Never_ expose ""private"" data to user code (TODO: Drop _data, _removeData)"
5. Avoid exposing implementation details on user objects (eg. expando properties)
6. Provide a clear path for implementation upgrade to WeakMap in 2014
Only convert to a number if it doesn't change the string
"If nothing was found internally, try to fetch any"
data from the HTML5 data-* attribute
Make sure we set the data so it isn't changed later
TODO: Now that all calls to _data and _removeData have been replaced
"with direct calls to dataPriv methods, these can be deprecated."
Gets all values
Support: IE 11 only
The attrs elements can be null (#14894)
Sets multiple values
The calling jQuery object (element matches) is not empty
(and therefore has an element appears at this[ 0 ]) and the
`value` parameter was not undefined. An empty jQuery object
will result in `undefined` for elem = this[ 0 ] which will
throw an exception if an attempt to read a data cache is made.
Attempt to get data from the cache
The key will always be camelCased in Data
"Attempt to ""discover"" the data in"
HTML5 custom data-* attrs
"We tried really hard, but the data doesn't exist."
Set the data...
We always store the camelCased key
Convert String-formatted options into Object-formatted ones
Convert options from String-formatted to Object-formatted if needed
(we check in cache first)
Last fire value for non-forgettable lists
Flag to know if list was already fired
Flag to prevent firing
Actual callback list
Queue of execution data for repeatable lists
Index of currently firing callback (modified by add/remove as needed)
Fire callbacks
Enforce single-firing
"Execute callbacks for all pending executions,"
respecting firingIndex overrides and runtime changes
Run callback and check for early termination
Jump to end and forget the data so .add doesn't re-fire
Forget the data if we're done with it
Clean up if we're done firing for good
Keep an empty list if we have data for future add calls
"Otherwise, this object is spent"
Actual Callbacks object
Add a callback or a collection of callbacks to the list
"If we have memory from a past run, we should fire after adding"
Inspect recursively
Remove a callback from the list
Handle firing indexes
Check if a given callback is in the list.
"If no argument is given, return whether or not list has callbacks attached."
Remove all callbacks from the list
Disable .fire and .add
Abort any current/pending executions
Clear all callbacks and values
Disable .fire
Also disable .add unless we have memory (since it would have no effect)
Abort any pending executions
Call all callbacks with the given context and arguments
Call all the callbacks with the given arguments
To know if the callbacks have already been called at least once
Support: IE <=9 - 11+
"focus() and blur() are asynchronous, except when they are no-op."
"So expect focus to be synchronous when the element is already active,"
and blur to be synchronous when the element is not already active.
"(focus and blur are always synchronous in other supported browsers,"
this just defines when we can count on it).
Support: IE <=9 only
Accessing document.activeElement can throw unexpectedly
https://bugs.jquery.com/ticket/13393
Types can be a map of types/handlers
"( types-Object, selector, data )"
"( types-Object, data )"
"( types, fn )"
"( types, selector, fn )"
"( types, data, fn )"
"Can use an empty set, since event contains the info"
Use same guid so caller can remove using origFn
Don't attach events to noData or text/comment nodes (but allow plain objects)
Caller can pass in an object of custom data in lieu of the handler
Ensure that invalid selectors throw exceptions at attach time
"Evaluate against documentElement in case elem is a non-element node (e.g., document)"
"Make sure that the handler has a unique ID, used to find/remove it later"
"Init the element's event structure and main handler, if this is the first"
Discard the second event of a jQuery.event.trigger() and
when an event is called after a page has unloaded
Handle multiple events separated by a space
"There *must* be a type, no attaching namespace-only handlers"
"If event changes its type, use the special event handlers for the changed type"
"If selector defined, determine special event api type, otherwise given type"
Update special based on newly reset type
handleObj is passed to all event handlers
Init the event handler queue if we're the first
Only use addEventListener if the special events handler returns false
"Add to the element's handler list, delegates in front"
"Keep track of which events have ever been used, for event optimization"
Detach an event or set of events from an element
Once for each type.namespace in types; type may be omitted
"Unbind all events (on this namespace, if provided) for the element"
Remove matching events
Remove generic event handler if we removed something and no more handlers exist
(avoids potential for endless recursion during removal of special event handlers)
Remove data and the expando if it's no longer used
Make a writable jQuery.Event from the native event object
Use the fix-ed jQuery.Event rather than the (read-only) native event
"Call the preDispatch hook for the mapped type, and let it bail if desired"
Determine handlers
Run delegates first; they may want to stop propagation beneath us
"If the event is namespaced, then each handler is only invoked if it is"
specially universal or its namespaces are a superset of the event's.
Call the postDispatch hook for the mapped type
Find delegate handlers
Support: IE <=9
Black-hole SVG <use> instance trees (trac-13180)
Support: Firefox <=42
Suppress spec-violating clicks indicating a non-primary pointer button (trac-3861)
https://www.w3.org/TR/DOM-Level-3-Events/#event-type-click
Support: IE 11 only
"...but not arrow key ""clicks"" of radio inputs, which can have `button` -1 (gh-2343)"
Don't check non-elements (#13208)
"Don't process clicks on disabled elements (#6911, #8165, #11382, #11764)"
Don't conflict with Object.prototype properties (#13203)
Add the remaining (directly-bound) handlers
Prevent triggered image.load events from bubbling to window.load
Utilize native event to ensure correct state for checkable inputs
"For mutual compressibility with _default, replace `this` access with a local var."
`|| data` is dead code meant only to preserve the variable through minification.
Claim the first handler
"dataPriv.set( el, ""click"", ... )"
Return false to allow normal processing in the caller
"For mutual compressibility with _default, replace `this` access with a local var."
`|| data` is dead code meant only to preserve the variable through minification.
Force setup before triggering a click
Return non-false to allow normal event-path propagation
"For cross-browser consistency, suppress native .click() on links"
Also prevent it if we're currently inside a leveraged native-event stack
Support: Firefox 20+
Firefox doesn't alert if the returnValue field is not set.
Ensure the presence of an event listener that handles manually-triggered
synthetic events by interrupting progress until reinvoked in response to
"*native* events that it fires directly, ensuring that state changes have"
already occurred before other listeners are invoked.
"Missing expectSync indicates a trigger call, which must force setup through jQuery.event.add"
Register the controller as a special universal handler for all event namespaces
Interrupt processing of the outer synthetic .trigger()ed event
Store arguments for use when handling the inner native event
Trigger the native event and capture its result
Support: IE <=9 - 11+
focus() and blur() are asynchronous
Cancel the outer synthetic event
If this is an inner synthetic event for an event with a bubbling surrogate
"(focus or blur), assume that the surrogate already propagated from triggering the"
native event and prevent that from happening again here.
This technically gets the ordering wrong w.r.t. to `.trigger()` (in which the
"bubbling surrogate propagates *after* the non-bubbling base), but that seems"
less bad than duplication.
"If this is a native event triggered above, everything is now in order"
Fire an inner synthetic event with the original arguments
...and capture the result
Support: IE <=9 - 11+
Extend with the prototype to reset the above stopImmediatePropagation()
Abort handling of the native event
"This ""if"" is needed for plain objects"
Allow instantiation without the 'new' keyword
Event object
Events bubbling up the document may have been marked as prevented
by a handler lower down the tree; reflect the correct value.
Support: Android <=2.3 only
Create target properties
Support: Safari <=6 - 7 only
"Target should not be a text node (#504, #13143)"
Event type
Put explicitly provided properties onto the event object
Create a timestamp if incoming event doesn't have one
Mark it as fixed
jQuery.Event is based on DOM3 Events as specified by the ECMAScript Language Binding
https://www.w3.org/TR/2003/WD-DOM-Level-3-Events-20030331/ecma-script-binding.html
Includes all common event props including KeyEvent and MouseEvent specific props
Add which for key events
Add which for click: 1 === left; 2 === middle; 3 === right
Utilize native event if possible so blur/focus sequence is correct
Claim the first handler
"dataPriv.set( this, ""focus"", ... )"
"dataPriv.set( this, ""blur"", ... )"
Return false to allow normal processing in the caller
Force setup before trigger
Return non-false to allow normal event-path propagation
Create mouseenter/leave events using mouseover/out and event-time checks
so that event delegation works in jQuery.
Do the same for pointerenter/pointerleave and pointerover/pointerout
""
Support: Safari 7 only
Safari sends mouseenter too often; see:
https://bugs.chromium.org/p/chromium/issues/detail?id=470258
for the description of the bug (it existed in older Chrome versions as well).
For mouseenter/leave call the handler if related is outside the target.
NB: No relatedTarget if the mouse left/entered the browser window
( event )  dispatched jQuery.Event
"( types-object [, selector] )"
"( types [, fn] )"
Defining this global in .eslintrc.json would create a danger of using the global
"unguarded in another place, it seems safer to define global only for this module"
Define a local copy of jQuery
The jQuery object is actually just the init constructor 'enhanced'
Need init if jQuery is called (just allow error to be thrown if not included)
Support: Android <=4.0 only
Make sure we trim BOM and NBSP
The current version of jQuery being used
The default length of a jQuery object is 0
Get the Nth element in the matched element set OR
Get the whole matched element set as a clean array
Return all the elements in a clean array
Return just the one element from the set
Take an array of elements and push it onto the stack
(returning the new matched element set)
Build a new jQuery matched element set
Add the old object onto the stack (as a reference)
Return the newly-formed element set
Execute a callback for every element in the matched set.
For internal use only.
"Behaves like an Array's method, not like a jQuery method."
Handle a deep copy situation
Skip the boolean and the target
Handle case when target is a string or something (possible in deep copy)
Extend jQuery itself if only one argument is passed
Only deal with non-null/undefined values
Extend the base object
Prevent Object.prototype pollution
Prevent never-ending loop
Recurse if we're merging plain objects or arrays
Ensure proper type for the source value
"Never move original objects, clone them"
Don't bring in undefined values
Return the modified object
Unique for each copy of jQuery on the page
Assume jQuery is ready without the ready module
Detect obvious negatives
Use toString instead of jQuery.type to catch host objects
"Objects with no prototype (e.g., `Object.create( null )`) are plain"
Objects with prototype are plain iff they were constructed by a global Object function
Evaluates a script in a global context
Support: Android <=4.0 only
results is for internal usage only
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
"Go through the array, only saving the items"
that pass the validator function
arg is for internal usage only
"Go through the array, translating each of the items to their new values"
"Go through every key on the object,"
Flatten any nested arrays
A global GUID counter for objects
jQuery.support is not used in Core but other projects attach their
properties to it so it needs to exist.
Populate the class2type map
Support: real iOS 8.2 only (not reproducible in simulator)
`in` check used to prevent JIT error (gh-2145)
hasOwn isn't used here due to false negatives
regarding Nodelist length in IE
"Create innerHeight, innerWidth, height, width, outerHeight and outerWidth methods"
"Margin is only for outerHeight, outerWidth"
$( window ).outerWidth/Height return w/h including scrollbars (gh-1729)
Get document width or height
"Either scroll[Width/Height] or offset[Width/Height] or client[Width/Height],"
whichever is greatest
"Get width or height on the element, requesting but not forcing parseFloat"
Set width or height on the element
Methods guaranteed to produce a unique set when starting from a unique set
"Positional selectors never match, since there's no _selection_ context"
Always skip document fragments
Don't pass non-elements to Sizzle
Determine the position of an element within the set
"No argument, return index in parent"
Index in selector
Locate the position of the desired element
"If it receives a jQuery object, the first element is used"
"Support: IE 9 - 11 only, iOS 7 only, Android Browser <=4.3 only"
Treat the template element as a regular one in browsers that
don't support it.
Remove duplicates
Reverse order for parents* and prev-derivatives
Return jQuery for attributes-only inclusion
Speed up dequeue by getting out quickly if this is just a lookup
"If the fx queue is dequeued, always remove the progress sentinel"
Add a progress sentinel to prevent the fx queue from being
automatically dequeued
Clear up the last queue stop function
"Not public - generate a queueHooks object, or return the current one"
Ensure a hooks for this queue
Get a promise resolved when queues of a certain type
are emptied (fx is the type by default)
The elements to wrap the target around
Serialize array item.
Treat each array item as a scalar.
"Item is non-scalar (array or object), encode its numeric index."
Serialize object item.
Serialize scalar item.
Serialize an array of form elements or a set of
key/values into a query string
"If value is a function, invoke it and use its return value"
"If an array was passed in, assume that it is an array of form elements."
Serialize the form elements
"If traditional, encode the ""old"" way (the way 1.3.2 or older"
"did it), otherwise encode params recursively."
Return the resulting serialization
"Can add propHook for ""elements"" to filter or add form elements"
"Use .is( "":disabled"" ) so that fieldset[disabled] works"
"Set position first, in-case top/left are set even on static elem"
Need to be able to calculate position if either
top or left is auto and position is either absolute or fixed
Use jQuery.extend here to allow modification of coordinates argument (gh-1848)
offset() relates an element's border box to the document origin
Preserve chaining for setter
Return zeros for disconnected and hidden (display: none) elements (gh-2310)
Support: IE <=11 only
Running getBoundingClientRect on a
disconnected node in IE throws an error
Get document-relative position by adding viewport scroll to viewport-relative gBCR
position() relates an element's margin box to its offset parent's padding box
This corresponds to the behavior of CSS absolute positioning
"position:fixed elements are offset from the viewport, which itself always has zero offset"
Assume position:fixed implies availability of getBoundingClientRect
"Account for the *real* offset parent, which can be the document or its root element"
when a statically positioned element is identified
"Incorporate borders into its offset, since they are outside its content origin"
Subtract parent offsets and element margins
This method will return documentElement in the following cases:
"1) For the element inside the iframe without offsetParent, this method will return"
documentElement of the parent window
2) For the hidden or detached element
"3) For body or html element, i.e. in case of the html node - it will return itself"
""
but those exceptions were never presented as a real life use-cases
and might be considered as more preferable results.
""
"This logic, however, is not guaranteed and can change at any point in the future"
Create scrollLeft and scrollTop methods
Coalesce documents and windows
"Support: Safari <=7 - 9.1, Chrome <=37 - 49"
Add the top/left cssHooks using jQuery.fn.position
Webkit bug: https://bugs.webkit.org/show_bug.cgi?id=29084
Blink bug: https://bugs.chromium.org/p/chromium/issues/detail?id=589347
getComputedStyle returns percent when specified for top/left/bottom/right;
"rather than make the css module depend on the offset module, just check for it here"
"If curCSS returns percentage, fallback to offset"
"#7653, #8125, #8152: local protocol detection"
Avoid comment-prolog char sequence (#10098); must appease lint and evade compression
Anchor tag for parsing the document origin
"Base ""constructor"" for jQuery.ajaxPrefilter and jQuery.ajaxTransport"
"dataTypeExpression is optional and defaults to ""*"""
For each dataType in the dataTypeExpression
Prepend if requested
Otherwise append
Base inspection function for prefilters and transports
A special extend for ajax options
"that takes ""flat"" options (not to be deep extended)"
Fixes #9887
Remove auto dataType and get content-type in the process
Check if we're dealing with a known content-type
Check to see if we have a response for the expected dataType
Try convertible dataTypes
Or just use first one
If we found a dataType
We add the dataType to the list if needed
and return the corresponding response
Work with a copy of dataTypes in case we need to modify it for conversion
Create converters map with lowercased keys
Convert to each sequential dataType
Apply the dataFilter if provided
There's only work to do if current dataType is non-auto
Convert response if prev dataType is non-auto and differs from current
Seek a direct converter
"If none found, seek a pair"
If conv2 outputs current
If prev can be converted to accepted input
Condense equivalence converters
"Otherwise, insert the intermediate dataType"
Apply converter (if not an equivalence)
"Unless errors are allowed to bubble, catch and return them"
Counter for holding the number of active queries
Last-Modified header cache for next request
Data converters
"Keys separate source (or catchall ""*"") and destination types with a single space"
Convert anything to text
Text to html (true = no transformation)
Evaluate text as a json expression
Parse text as xml
For options that shouldn't be deep extended:
you can add your own custom options here if
and when you create one that shouldn't be
deep extended (see ajaxExtend)
Creates a full fledged settings object into target
with both ajaxSettings and settings fields.
"If target is omitted, writes into ajaxSettings."
Building a settings object
Extending ajaxSettings
Main method
"If url is an object, simulate pre-1.5 signature"
Force options to be an object
URL without anti-cache param
Response headers
timeout handle
Url cleanup var
Request state (becomes false upon send and true upon completion)
To know if global events are to be dispatched
Loop variable
uncached part of the url
Create the final options object
Callbacks context
Context for global events is callbackContext if it is a DOM node or jQuery collection
Deferreds
Status-dependent callbacks
Headers (they are sent all at once)
Default abort message
Fake xhr
Builds headers hashtable if needed
Raw string
Caches the header
Overrides response content-type header
Status-dependent callbacks
Execute the appropriate callbacks
Lazy-add the new callbacks in a way that preserves old ones
Cancel the request
Attach deferreds
Add protocol if not provided (prefilters might expect it)
Handle falsy url in the settings object (#10093: consistency with old signature)
We also use the url parameter if available
Alias method option to type as per ticket #12004
Extract dataTypes list
A cross-domain request is in order when the origin doesn't match the current origin.
"Support: IE <=8 - 11, Edge 12 - 15"
"IE throws exception on accessing the href property if url is malformed,"
e.g. http://example.com:80x/
Support: IE <=8 - 11 only
Anchor's host property isn't correctly set when s.url is relative
"If there is an error parsing the URL, assume it is crossDomain,"
it can be rejected by the transport if it is invalid
Convert data if not already a string
Apply prefilters
"If request was aborted inside a prefilter, stop there"
We can fire global events as of now if asked to
Don't fire events if jQuery.event is undefined in an AMD-usage scenario (#15118)
Watch for a new set of requests
Uppercase the type
Determine if request has content
Save the URL in case we're toying with the If-Modified-Since
and/or If-None-Match header later on
Remove hash to simplify url manipulation
More options handling for requests with no content
Remember the hash so we can put it back
"If data is available and should be processed, append data to url"
#9682: remove data so that it's not used in an eventual retry
Add or update anti-cache param if needed
Put hash and anti-cache on the URL that will be requested (gh-1732)
Change '%20' to '+' if this is encoded form body content (gh-2658)
"Set the If-Modified-Since and/or If-None-Match header, if in ifModified mode."
"Set the correct header, if data is being sent"
"Set the Accepts header for the server, depending on the dataType"
Check for headers option
Allow custom headers/mimetypes and early abort
Abort if not done already and return
Aborting is no longer a cancellation
Install callbacks on deferreds
Get transport
"If no transport, we auto-abort"
Send global event
"If request was aborted inside ajaxSend, stop there"
Timeout
Rethrow post-completion exceptions
Propagate others as results
Callback for when everything is done
Ignore repeat invocations
Clear timeout if it exists
Dereference transport for early garbage collection
(no matter how long the jqXHR object will be used)
Cache response headers
Set readyState
Determine if successful
Get response data
Convert no matter what (that way responseXXX fields are always set)
"If successful, handle type chaining"
"Set the If-Modified-Since and/or If-None-Match header, if in ifModified mode."
if no content
if not modified
"If we have data, let's convert it"
Extract error from statusText and normalize for non-aborts
Set data for the fake xhr object
Success/Error
Status-dependent callbacks
Complete
Handle the global AJAX counter
Shift arguments if data argument was omitted
The url can be an options object (which then must have .url)
"( namespace ) or ( selector, types [, fn] )"
"Bind a function to a context, optionally partially applying any"
arguments.
jQuery.proxy is deprecated to promote standards (specifically Function#bind)
"However, it is not slated for removal any time soon"
"Quick check to determine if target is callable, in the spec"
"this throws a TypeError, but we will just return undefined."
Simulated bind
"Set the guid of unique handler to the same of original handler, so it can be removed"
"As of jQuery 3.0, isNumeric is limited to"
strings and numbers (primitives or objects)
that can be coerced to finite numbers (gh-2662)
"parseFloat NaNs numeric-cast false positives ("""")"
"...but misinterprets leading-number strings, particularly hex literals (""0x..."")"
subtraction forces infinities to NaN
CSS string/identifier serialization
https://drafts.csswg.org/cssom/#common-serializing-idioms
U+0000 NULL becomes U+FFFD REPLACEMENT CHARACTER
Control characters and (dependent upon position) numbers get escaped as code points
Other potentially-special ASCII characters get backslash-escaped
Flag for duplicate removal
Sort on method existence if only one input has compareDocumentPosition
Calculate position if both inputs belong to the same document
Otherwise we know they are disconnected
Disconnected nodes
Choose the first element that is related to our preferred document
Maintain original order
Clear input after sorting to release objects
See https://github.com/jquery/sizzle/pull/225
Same basic safeguard as Sizzle
Early return if context is not an element or document
"If no nodeType, this is expected to be an array"
Do not traverse comment nodes
Use textContent for elements
Do not include comment or processing instruction nodes
documentElement is verified for cases where it doesn't yet exist
(such as loading iframes in IE - #4833)
Don't get fooled by Object.prototype properties (jQuery #13807)
Deprecated
Animations created synchronously will run synchronously
Generate parameters to create a standard animation
"If we include width, step value is 1 to do all cssExpand values,"
otherwise step value is 2 to skip over Left and Right
We're done with this property
Queue-skipping animations hijack the fx hooks
Ensure the complete handler is called before this completes
Detect show/hide animations
"Pretend to be hidden if this is a ""show"" and"
there is still data from a stopped show/hide
Ignore all other no-op show/hide data
Bail out if this is a no-op like .hide().hide()
"Restrict ""overflow"" and ""display"" styles during box animations"
"Support: IE <=9 - 11, Edge 12 - 15"
Record all 3 overflow attributes because IE does not infer the shorthand
from identically-valued overflowX and overflowY and Edge just mirrors
the overflowX value there.
"Identify a display type, preferring old show/hide data over the CSS cascade"
Get nonempty value(s) by temporarily forcing visibility
Animate inline elements as inline-block
Restore the original display value at the end of pure show/hide animations
Implement show/hide animations
General show/hide setup for this element animation
"Store hidden/visible for toggle so `.stop().toggle()` ""reverses"""
Show elements before animating them
"The final step of a ""hide"" animation is actually hiding the element"
Per-property setup
"camelCase, specialEasing and expand cssHook pass"
"Not quite $.extend, this won't overwrite existing keys."
"Reusing 'index' because we have the correct ""name"""
Don't match elem in the :animated selector
Support: Android 2.3 only
Archaic crash bug won't allow us to use `1 - ( 0.5 || 0 )` (#12497)
"If there's more to do, yield"
"If this was an empty animation, synthesize a final progress notification"
Resolve the animation and report its conclusion
"If we are going to the end, we want to run all the tweens"
otherwise we skip this part
"Resolve when we played the last frame; otherwise, reject"
Attach callbacks from options
Go to the end state if fx are off
"Normalize opt.queue - true/undefined/null -> ""fx"""
Queueing
Show any hidden elements after setting opacity to 0
Animate to the value specified
Operate on a copy of prop so per-property easing won't be lost
"Empty animations, or finishing resolves immediately"
Start the next in the queue if the last step wasn't forced.
"Timers currently will call their complete callbacks, which"
will dequeue but only if they were gotoEnd.
Enable finishing flag on private data
Empty the queue first
"Look for any active animations, and finish them"
Look for any animations in the old queue and finish them
Turn off finishing flag
Generate shortcuts for custom animations
Run the timer and safely remove it when done (allowing for external removal)
Default speed
Swappable if display is none or starts with table
"except ""table"", ""table-cell"", or ""table-caption"""
See here for display values: https://developer.mozilla.org/en-US/docs/CSS/display
Any relative (+/-) values have already been
normalized at this point
"Guard against undefined ""subtract"", e.g., when used as in cssHooks"
Adjustment may not be necessary
Both box models exclude margin
"If we get here with a content-box, we're seeking ""padding"" or ""border"" or ""margin"""
Add padding
"For ""border"" or ""margin"", add border"
But still keep track of it otherwise
"If we get here with a border-box (content + padding + border), we're seeking ""content"" or"
"""padding"" or ""margin"""
"For ""content"", subtract padding"
"For ""content"" or ""padding"", subtract border"
Account for positive content-box scroll gutter when requested by providing computedVal
"offsetWidth/offsetHeight is a rounded sum of content, padding, scroll gutter, and border"
"Assuming integer scroll gutter, subtract the rest and round down"
"If offsetWidth/offsetHeight is unknown, then we can't determine content-box scroll gutter"
Use an explicit zero to avoid NaN (gh-3964)
Start with computed style
"To avoid forcing a reflow, only fetch boxSizing if we need it (gh-4322)."
Fake content-box until we know it's needed to know the true value.
Support: Firefox <=54
"Return a confounding non-pixel value or feign ignorance, as appropriate."
"Fall back to offsetWidth/offsetHeight when value is ""auto"""
This happens for inline elements with no explicit setting (gh-3571)
Support: Android <=4.1 - 4.3 only
Also use offsetWidth/offsetHeight for misreported inline dimensions (gh-3602)
Support: IE 9-11 only
Also use offsetWidth/offsetHeight for when box sizing is unreliable
We use getClientRects() to check for hidden/disconnected.
"In those cases, the computed value can be trusted to be border-box"
"Where available, offsetWidth/offsetHeight approximate border box dimensions."
"Where not available (e.g., SVG), assume unreliable box-sizing and interpret the"
retrieved value as a content box dimension.
"Normalize """" and auto"
Adjust for the element's box model
Provide the current computed size to request scroll gutter calculation (gh-3589)
Add in style property hooks for overriding the default
behavior of getting and setting a style property
We should always get a number back from opacity
"Don't automatically add ""px"" to these possibly-unitless properties"
Add in properties whose names you wish to fix before
setting or getting the value
Get and set the style property on a DOM Node
Don't set styles on text and comment nodes
Make sure that we're working with the right name
Make sure that we're working with the right name. We don't
want to query the value if it is a CSS custom property
since they are user-defined.
"Gets hook for the prefixed version, then unprefixed version"
Check if we're setting a value
"Convert ""+="" or ""-="" to relative numbers (#7345)"
Fixes bug #9237
Make sure that null and NaN values aren't set (#7116)
"If a number was passed in, add the unit (except for certain CSS properties)"
The isCustomProp check can be removed in jQuery 4.0 when we only auto-append
"""px"" to a few hardcoded values."
background-* props affect original clone's values
"If a hook was provided, use that value, otherwise just set the specified value"
If a hook was provided get the non-computed value from there
Otherwise just get the value from the style object
Make sure that we're working with the right name. We don't
want to modify the value if it is a CSS custom property
since they are user-defined.
Try prefixed name followed by the unprefixed name
If a hook was provided get the computed value from there
"Otherwise, if a way to get the computed value exists, use that"
"Convert ""normal"" to computed value"
Make numeric if forced or a qualifier was provided and val looks numeric
Certain elements can have dimension info if we invisibly show them
but it must have a current display style that would benefit
Support: Safari 8+
Table columns in Safari have non-zero offsetWidth & zero
getBoundingClientRect().width unless display is changed.
Support: IE <=11 only
Running getBoundingClientRect on a disconnected node
in IE throws an error.
Only read styles.position if the test has a chance to fail
to avoid forcing a reflow.
"To avoid forcing a reflow, only fetch boxSizing if we need it (gh-3991)"
Account for unreliable border-box dimensions by comparing offset* to computed and
faking a content-box to get border and padding (gh-3699)
Convert to pixels if value adjustment is needed
These hooks are used by animate to expand properties
Assumes a single number if not a string
Check for promise aspect first to privilege synchronous behavior
Other thenables
Other non-thenables
Control `resolve` arguments by letting Array#slice cast boolean `noValue` to integer:
* false: [ value ].slice( 0 ) => resolve( value )
* true: [ value ].slice( 1 ) => resolve()
"For Promises/A+, convert exceptions into rejections"
"Since jQuery.when doesn't unwrap thenables, we can skip the extra checks appearing in"
Deferred#then to conditionally suppress rejection.
Support: Android 4.0 only
Strict mode functions invoked without .call/.apply get global-object context
"action, add listener, callbacks,"
"... .then handlers, argument index, [final state]"
Keep pipe for back-compat
"Map tuples (progress, done, fail) to arguments (done, fail, progress)"
deferred.progress(function() { bind to newDefer or newDefer.notify })
deferred.done(function() { bind to newDefer or newDefer.resolve })
deferred.fail(function() { bind to newDefer or newDefer.reject })
Support: Promises/A+ section 2.3.3.3.3
https://promisesaplus.com/#point-59
Ignore double-resolution attempts
Support: Promises/A+ section 2.3.1
https://promisesaplus.com/#point-48
"Support: Promises/A+ sections 2.3.3.1, 3.5"
https://promisesaplus.com/#point-54
https://promisesaplus.com/#point-75
Retrieve `then` only once
Support: Promises/A+ section 2.3.4
https://promisesaplus.com/#point-64
Only check objects and functions for thenability
Handle a returned thenable
Special processors (notify) just wait for resolution
Normal processors (resolve) also hook into progress
...and disregard older resolution values
Handle all other returned values
Only substitute handlers pass on context
and multiple values (non-spec behavior)
Process the value(s)
Default process is resolve
Only normal processors (resolve) catch and reject exceptions
Support: Promises/A+ section 2.3.3.3.4.1
https://promisesaplus.com/#point-61
Ignore post-resolution exceptions
Only substitute handlers pass on context
and multiple values (non-spec behavior)
Support: Promises/A+ section 2.3.3.3.1
https://promisesaplus.com/#point-57
Re-resolve promises immediately to dodge false rejection from
subsequent errors
"Call an optional hook to record the stack, in case of exception"
since it's otherwise lost when execution goes async
progress_handlers.add( ... )
fulfilled_handlers.add( ... )
rejected_handlers.add( ... )
Get a promise for this deferred
"If obj is provided, the promise aspect is added to the object"
Add list-specific methods
promise.progress = list.add
promise.done = list.add
promise.fail = list.add
Handle state
"state = ""resolved"" (i.e., fulfilled)"
"state = ""rejected"""
rejected_callbacks.disable
fulfilled_callbacks.disable
rejected_handlers.disable
fulfilled_handlers.disable
progress_callbacks.lock
progress_handlers.lock
progress_handlers.fire
fulfilled_handlers.fire
rejected_handlers.fire
deferred.notify = function() { deferred.notifyWith(...) }
deferred.resolve = function() { deferred.resolveWith(...) }
deferred.reject = function() { deferred.rejectWith(...) }
deferred.notifyWith = list.fireWith
deferred.resolveWith = list.fireWith
deferred.rejectWith = list.fireWith
Make the deferred a promise
Call given func if any
All done!
Deferred helper
count of uncompleted subordinates
count of unprocessed arguments
subordinate fulfillment data
the master Deferred
subordinate callback factory
Single- and empty arguments are adopted like Promise.resolve
Use .then() to unwrap secondary thenables (cf. gh-3000)
Multiple arguments are aggregated like Promise.all array elements
See https://github.com/eslint/eslint/issues/3229
"Support: IE <=10 - 11, Edge 12 - 13 only"
In IE/Edge using regex groups here causes severe slowdowns.
See https://connect.microsoft.com/IE/feedback/details/1736512/
"checked=""checked"" or checked"
Prefer a tbody over its parent table for containing new rows
Replace/restore the type attribute of script elements for safe DOM manipulation
"1. Copy private data: events, handlers, etc."
2. Copy user data
"Fix IE bugs, see support tests"
Fails to persist the checked state of a cloned checkbox or radio button.
Fails to return the selected option to the default selected state when cloning options
Flatten any nested arrays
"We can't cloneNode fragments that contain checked, in WebKit"
Require either new content or an interest in ignored elements to invoke the callback
Use the original fragment for the last item
instead of the first because it can end up
being emptied incorrectly in certain situations (#8070).
Keep references to cloned scripts for later restoration
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
Reenable scripts
Evaluate executable scripts on first document insertion
"Optional AJAX dependency, but won't run scripts if not present"
Fix IE cloning issues
We eschew Sizzle here for performance reasons: https://jsperf.com/getall-vs-sizzle/2
Copy the events from the original to the clone
Preserve script evaluation history
Return the cloned set
This is a shortcut to avoid jQuery.event.remove's overhead
Support: Chrome <=35 - 45+
"Assign undefined instead of using delete, see Data#remove"
Support: Chrome <=35 - 45+
"Assign undefined instead of using delete, see Data#remove"
Prevent memory leaks
Remove any remaining nodes
See if we can take a shortcut and just use innerHTML
Remove element nodes and prevent memory leaks
"If using innerHTML throws an exception, use the fallback method"
"Make the changes, replacing each non-ignored context element with the new content"
Force callback invocation
"Support: Android <=4.0 only, PhantomJS 1 only"
".get() because push.apply(_, arraylike) throws on ancient WebKit"
"Define the hook, we'll check on the first run if it's really needed."
Hook not needed (or it's not possible to use it due
"to missing dependency), remove it."
Hook needed; redefine it so that the support test is not executed again.
Executing both pixelPosition & boxSizingReliable tests require only one layout
so they're executed at the same time to save the second computation.
"This is a singleton, we need to execute it only once"
"Support: Android 4.0 - 4.3 only, Firefox <=3 - 44"
"Support: Android 4.0 - 4.3 only, Safari <=9.1 - 10.1, iOS <=7.0 - 9.3"
"Some styles come back with percentage values, even though they shouldn't"
Support: IE 9 - 11 only
Detect misreporting of content dimensions for box-sizing:border-box elements
Support: IE 9 only
Detect overflow:scroll screwiness (gh-3699)
Support: Chrome <=64
Don't get tricked when zoom affects offsetWidth (gh-4029)
Nullify the div so it wouldn't be stored in the memory and
it will also be a sign that checks already performed
Finish early in limited (non-browser) environments
Support: IE <=9 - 11 only
Style of cloned element affects source element cloned (#8908)
Support: Firefox 51+
Retrieving style before computed somehow
fixes an issue with getting wrong values
on detached elements
getPropertyValue is needed for:
".css('filter') (IE 9 only, #12537)"
.css('--customProperty) (#3144)
"A tribute to the ""awesome hack by Dean Edwards"""
"Android Browser returns percentage for some values,"
but width seems to be reliably pixels.
This is against the CSSOM draft spec:
https://drafts.csswg.org/cssom/#resolved-values
Remember the original values
Put in the new values to get a computed value out
Revert the changed values
Support: IE <=9 - 11 only
IE returns zIndex value as an integer.
Determine new display value for elements that need to change
"Since we force visibility upon cascade-hidden elements, an immediate (and slow)"
check is required in this first loop unless we have a nonempty display value (either
inline or about-to-be-restored)
Remember what we're overwriting
Set the display of the elements in a second loop to avoid constant reflow
Starting value computation is required for potential unit mismatches
Support: Firefox <=54
Halve the iteration target value to prevent interference from CSS upper bounds (gh-2144)
Trust units reported by jQuery.css
Iteratively approximate from a nonzero starting point
Evaluate and update our best guess (doubling guesses that zero out).
Finish if the scale equals or crosses 1 (making the old*new product non-positive).
Make sure we update the tween properties later on
Apply relative offset (+=/-=) if specified
"Support: IE <=11 only, Firefox <=30 (#15098, #14150)"
IE throws on elements created in popups
"FF meanwhile throws on frame elements through ""defaultView.getComputedStyle"""
css is assumed
"isHiddenWithinTree reports if an element has a non-""none"" display style (inline and/or"
"through the CSS cascade), which is useful in deciding whether or not to make it visible."
It differs from the :hidden selector (jQuery.expr.pseudos.hidden) in two important ways:
* A hidden ancestor does not force an element to be classified as hidden.
* Being disconnected from the document does not force an element to be classified as hidden.
These differences improve the behavior of .toggle() et al. when applied to elements that are
"detached or contained within hidden ancestors (gh-2404, gh-2863)."
isHiddenWithinTree might be called from jQuery#filter function;
"in that case, element will be second argument"
Inline style trumps all
"Otherwise, check computed style"
Support: Firefox <=43 - 45
"Disconnected elements can have computed display: none, so first confirm that elem is"
in the document.
A method for quickly swapping in/out CSS properties to get correct calculations.
"Remember the old values, and insert the new ones"
Revert the old values
Don't do events on text and comment nodes
focus/blur morphs to focusin/out; ensure we're not firing them right now
Namespaced trigger; create a regexp to match event type in handle()
"Caller can pass in a jQuery.Event object, Object, or just an event type string"
Trigger bitmask: & 1 for native handlers; & 2 for jQuery (always true)
Clean up the event in case it is being reused
"Clone any incoming data and prepend the event, creating the handler arg list"
Allow special events to draw outside the lines
"Determine event propagation path in advance, per W3C events spec (#9951)"
"Bubble up to document, then to window; watch for a global ownerDocument var (#9724)"
"Only add window if we got to document (e.g., not plain obj or detached DOM)"
Fire handlers on the event path
jQuery handler
Native handler
"If nobody prevented the default action, do it now"
Call a native DOM method on the target with the same name as the event.
"Don't do default actions on window, that's where global variables be (#6170)"
Don't re-trigger an onFOO event when we call its FOO() method
"Prevent re-triggering of the same event, since we already bubbled it above"
Piggyback on a donor event to simulate a different one
Used only for `focus(in | out)` events
Attach a bunch of functions for handling common AJAX events
Support: Firefox <=44
Firefox doesn't have focus(in | out) events
Related ticket - https://bugzilla.mozilla.org/show_bug.cgi?id=687787
""
"Support: Chrome <=48 - 49, Safari <=9.0 - 9.1"
"focus(in | out) events fire after focus & blur events,"
which is spec violation - http://www.w3.org/TR/DOM-Level-3-Events/#events-focusevent-event-order
Related ticket - https://bugs.chromium.org/p/chromium/issues/detail?id=449857
Attach a single capturing handler on the document while someone wants focusin/focusout
Handle event binding
"Based off of the plugin by Clint Helfers, with permission."
https://web.archive.org/web/20100324014747/http://blindsignals.com/index.php/2009/07/jquery-delay/
Support: IE <=9 - 11 only
Use typeof to avoid zero-argument method invocation on host objects (#15151)
Mark scripts as having already been evaluated
Support: Android 4.0 - 4.3 only
Check state lost if the name is set (#11217)
Support: Windows Web Apps (WWA)
`name` and `type` must use .setAttribute for WWA (#14901)
Support: Android <=4.1 only
Older WebKit doesn't clone checked state correctly in fragments
Support: IE <=11 only
Make sure textarea (and checkbox) defaultValue is properly cloned
Add nodes directly
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
Convert non-html into a text node
Convert html into DOM nodes
Deserialize a standard representation
Descend through wrappers to the right content
"Support: Android <=4.0 only, PhantomJS 1 only"
"push.apply(_, arraylike) throws on ancient WebKit"
Remember the top-level container
Ensure the created nodes are orphaned (#12392)
Remove wrapper from fragment
Skip elements already in the context collection (trac-4087)
Append to fragment
Preserve script evaluation history
Capture executables
We have to close these tags to support XHTML (#13200)
Support: IE <=9 only
XHTML parsers do not magically insert elements in the
same way that tag soup parsers do. So we cannot shorten
this by omitting <tbody> or other required elements.
Support: IE <=9 only
"Make this explicit, since user can override this through ajaxSetup (#11264)"
Only evaluate the response if it is successful (gh-4126)
"dataFilter is not invoked for failure responses, so using it instead"
of the default converter is kludgy but it works.
rtagName captures the name from the first start tag in a string of HTML
https://html.spec.whatwg.org/multipage/syntax.html#tag-open-state
https://html.spec.whatwg.org/multipage/syntax.html#tag-name-state
"These usually indicate a programmer mistake during development,"
warn about them ASAP rather than swallowing them by default.
Support: IE 8 - 9 only
"Console exists when dev tools are open, which can happen at any time"
Check if the owner object already has a cache
"If not, create one"
"We can accept data for non-element nodes in modern browsers,"
"but we should not, see #8335."
Always return an empty object.
If it is a node unlikely to be stringify-ed or looped over
use plain assignment
Otherwise secure it in a non-enumerable property
configurable must be true to allow the property to be
deleted when data is removed
"Handle: [ owner, key, value ] args"
Always use camelCase key (gh-2257)
"Handle: [ owner, { properties } ] args"
Copy the properties one-by-one to the cache object
Always use camelCase key (gh-2257)
In cases where either:
""
1. No key was specified
"2. A string key was specified, but no value provided"
""
"Take the ""read"" path and allow the get method to determine"
"which value to return, respectively either:"
""
1. The entire cache object
2. The data stored at the key
""
"When the key is not a string, or both a key and value"
"are specified, set or extend (existing objects) with either:"
""
1. An object of properties
2. A key and value
""
"Since the ""set"" path can have two possible entry points"
return the expected data based on which path was taken[*]
Support array or space separated string of keys
If key is an array of keys...
"We always set camelCase keys, so remove that."
"If a key with the spaces exists, use it."
"Otherwise, create an array by matching non-whitespace"
Remove the expando if there's no more data
Support: Chrome <=35 - 45
Webkit & Blink performance suffers when deleting properties
"from DOM nodes, so set to undefined instead"
https://bugs.chromium.org/p/chromium/issues/detail?id=378607 (bug restricted)
Accepts only:
- Node
- Node.ELEMENT_NODE
- Node.DOCUMENT_NODE
- Object
- Any
Only count HTML whitespace
Other whitespace should count in values
https://infra.spec.whatwg.org/#ascii-whitespace
[[Class]] -> type pairs
All support tests are defined in their respective modules.
"Support: Chrome <=57, Firefox <=52"
"In some browsers, typeof returns ""function"" for HTML <object> elements"
"(i.e., `typeof document.createElement( ""object"" ) === ""function""`)."
We don't want to classify *any* DOM node as a function.
"Support: Firefox 64+, Edge 18+"
"Some browsers don't support the ""nonce"" property on scripts."
"On the other hand, just using `getAttribute` is not enough as"
the `nonce` attribute is reset to an empty string whenever it
becomes browsing-context connected.
See https://github.com/whatwg/html/issues/2369
See https://html.spec.whatwg.org/#nonce-attributes
The `node.getAttribute` check was added for the sake of
`jQuery.globalEval` so that it can fake a nonce-containing node
via an object.
Support: Safari 8 only
In Safari 8 documents created via document.implementation.createHTMLDocument
collapse sibling forms: the second one becomes a child of the first one.
"Because of that, this security measure has to be disabled in Safari 8."
https://bugs.webkit.org/show_bug.cgi?id=137337
This is the only module that needs core/support
"Argument ""data"" should be string of html"
"context (optional): If specified, the fragment will be created in this context,"
defaults to document
"keepScripts (optional): If true, will include scripts passed in the html string"
Stop scripts or inline event handlers from being executed immediately
by using document.implementation
Set the base href for the created document
so any parsed elements with URLs
are based on the document's URL (gh-2965)
Single tag
Matches dashed string for camelizing
Used by camelCase as callback to replace()
Convert dashed to camelCase; used by the css and data modules
"Support: IE <=9 - 11, Edge 12 - 15"
Microsoft forgot to hump their vendor prefix (#9572)
Multifunctional method to get and set values of a collection
The value/s can optionally be executed if it's a function
Sets many values
Sets one value
Bulk operations run against the entire set
...except when executing function values
Gets
Strip and collapse whitespace according to HTML spec
https://infra.spec.whatwg.org/#strip-and-collapse-ascii-whitespace
Initialize a jQuery object
A central reference to the root jQuery(document)
A simple way to check for HTML strings
Prioritize #id over <tag> to avoid XSS via location.hash (#9521)
Strict HTML recognition (#11290: must start with <)
Shortcut simple #id case for speed
"HANDLE: $(""""), $(null), $(undefined), $(false)"
Method init() accepts an alternate rootjQuery
so migrate can support jQuery.sub (gh-2101)
Handle HTML strings
Assume that strings that start and end with <> are HTML and skip the regex check
Match html or make sure no context is specified for #id
HANDLE: $(html) -> $(array)
Option to run scripts is true for back-compat
Intentionally let the error be thrown if parseHTML is not present
"HANDLE: $(html, props)"
Properties of context are called as methods if possible
...and otherwise set as attributes
HANDLE: $(#id)
Inject the element directly into the jQuery object
"HANDLE: $(expr, $(...))"
"HANDLE: $(expr, context)"
(which is just equivalent to: $(context).find(expr)
HANDLE: $(DOMElement)
HANDLE: $(function)
Shortcut for document ready
Execute immediately if ready is not present
Give the init function the jQuery prototype for later instantiation
Initialize central reference
Support: Android <=2.3 only (functionish RegExp)
Prevent errors from freezing future callback execution (gh-1823)
Not backwards-compatible as this does not execute sync
Is the DOM ready to be used? Set to true once it occurs.
A counter to track how many items to wait for before
the ready event fires. See #6781
Abort if there are pending holds or we're already ready
Remember that the DOM is ready
"If a normal DOM Ready event fired, decrement, and wait if need be"
Make jQuery.ready Promise consumable (gh-1778)
Catch cases where $(document).ready() is called
after the browser event has already occurred.
Support: IE9-10 only
"Older IE sometimes signals ""interactive"" too soon"
Handle it asynchronously to allow scripts the opportunity to delay ready
Use the handy event callback
"A fallback to window.onload, that will always work"
The deferred used on DOM ready
Wrap jQuery.readyException in a function so that the lookup
happens at the time of error handling instead of callback
registration.
Is the DOM ready to be used? Set to true once it occurs.
A counter to track how many items to wait for before
the ready event fires. See #6781
Handle when the DOM is ready
Abort if there are pending holds or we're already ready
Remember that the DOM is ready
"If a normal DOM Ready event fired, decrement, and wait if need be"
"If there are functions bound, to execute"
The ready event handler and self cleanup method
Catch cases where $(document).ready() is called
after the browser event has already occurred.
Support: IE <=9 - 10 only
"Older IE sometimes signals ""interactive"" too soon"
Handle it asynchronously to allow scripts the opportunity to delay ready
Use the handy event callback
"A fallback to window.onload, that will always work"
rsingleTag matches a string consisting of a single HTML element with no attributes
and captures the element's name
"Register as a named AMD module, since jQuery can be concatenated with other"
"files that may use define, but not via a proper concatenation script that"
understands anonymous AMD modules. A named AMD is safest and most robust
way to register. Lowercase jquery is used because AMD module names are
"derived from file names, and jQuery is normally delivered in a lowercase"
file name. Do this after creating the global so that if an AMD module wants
"to call noConflict to hide this version of jQuery, it will work."
"Note that for maximum portability, libraries that are not jQuery should"
"declare themselves as anonymous modules, and avoid setting a global if an"
"AMD loader is present. jQuery is a special case. For more information, see"
https://github.com/jrburke/requirejs/wiki/Updating-existing-libraries#wiki-anon
Map over jQuery in case of overwrite
Map over the $ in case of overwrite
"Expose jQuery and $ identifiers, even in AMD"
"(#7102#comment:10, https://github.com/jquery/jquery/pull/557)"
and CommonJS for browser emulators (#13566)
"Use a property on the element directly when it is not a DOM element,"
or when there is no matching style property that exists.
Passing an empty string as a 3rd parameter to .css will automatically
attempt a parseFloat and fallback to a string if the parse fails.
"Simple values such as ""10px"" are parsed to Float;"
"complex values such as ""rotate(1rad)"" are returned as-is."
"Empty strings, null, undefined and ""auto"" are converted to 0."
Use step hook for back compat.
Use cssHook if its there.
Use .style if available and use plain properties where available.
Support: IE <=9 only
Panic based approach to setting things on disconnected nodes
Back compat <1.8 extension point
Handle most common string cases
Handle cases where value is null/undef or number
"Treat null/undefined as """"; convert numbers to string"
"If set returns undefined, fall back to normal setting"
Support: IE <=10 - 11 only
"option.text throws exceptions (#14686, #14858)"
Strip and collapse whitespace
https://html.spec.whatwg.org/#strip-and-collapse-whitespace
Loop through all the selected options
Support: IE <=9 only
IE8-9 doesn't update selected after form reset (#2551)
Don't return options that are disabled or in a disabled optgroup
Get the specific value for the option
We don't need an array for one selects
Multi-Selects return an array
Force browsers to behave consistently when non-matching value is set
Radios and checkboxes getter/setter
"Don't get/set attributes on text, comment and attribute nodes"
Fallback to prop when attributes are not supported
Attribute hooks are determined by the lowercase version
Grab necessary hook if one is defined
"Non-existent attributes return null, we normalize to undefined"
Attribute names can contain non-HTML whitespace characters
https://html.spec.whatwg.org/multipage/syntax.html#attributes-2
Hooks for boolean attributes
Remove boolean attributes when set to false
Avoid an infinite loop by temporarily removing this function from the getter
Support: Android <=4.3 only
"Default value for a checkbox should be ""on"""
Support: IE <=11 only
Must access selectedIndex to make default options select
Support: IE <=11 only
An input loses its value after becoming a radio
Only assign if different to avoid unneeded rendering.
This expression is here for better compressibility (see addClass)
Remove *all* instances
Only assign if different to avoid unneeded rendering.
Toggle individual class names
"Check each className given, space separated list"
Toggle whole class name
Store className if set
"If the element has a class name or if we're passed `false`,"
"then remove the whole classname (if there was one, the above saved it)."
"Otherwise bring back whatever was previously saved (if anything),"
falling back to the empty string if nothing was stored.
"Don't get/set properties on text, comment and attribute nodes"
Fix name and attach hooks
Support: IE <=9 - 11 only
elem.tabIndex doesn't always return the
correct value when it hasn't been explicitly set
https://web.archive.org/web/20141116233347/http://fluidproject.org/blog/2008/01/09/getting-setting-and-removing-tabindex-values-with-javascript/
Use proper attribute retrieval(#12072)
Support: IE <=11 only
Accessing the selectedIndex property
forces the browser to respect setting selected
on the option
The getter ensures a default option is selected
when in an optgroup
"eslint rule ""no-unused-expressions"" is disabled for this code"
since it considers such accessions noop
If it's a function
We assume that it's the callback
"Otherwise, build a param string"
"If we have elements to modify, make the request"
"If ""type"" variable is undefined, then ""GET"" method will be used."
Make value of this field explicit since
user can override it through ajaxSetup method
Save response for use in complete callback
"If a selector was specified, locate the right elements in a dummy div"
Exclude scripts to avoid IE 'Permission Denied' errors
Otherwise use the full result
"If the request succeeds, this function gets ""data"", ""status"", ""jqXHR"""
but they are ignored because response was set above.
"If it fails, this function gets ""jqXHR"", ""status"", ""error"""
Cross-browser xml parsing
Support: IE 9 - 11 only
IE throws on parseFromString with invalid input.
Prevent auto-execution of scripts when no explicit dataType was provided (See gh-2432)
Install script dataType
Handle cache's special case and crossDomain
Bind script tag hack transport
This transport only deals with cross domain or forced-by-attrs requests
Use native DOM manipulation to avoid our domManip AJAX trickery
Default jsonp settings
"Detect, normalize options and install callbacks for jsonp requests"
"Handle iff the expected data type is ""jsonp"" or we have a parameter to set"
"Get callback name, remembering preexisting value associated with it"
Insert callback into url or form data
Use data converter to retrieve json after script execution
Force json dataType
Install callback
Clean-up function (fires after converters)
If previous value didn't exist - remove it
Otherwise restore preexisting value
Save back as free
Make sure that re-using the options doesn't screw things around
Save the callback name for future use
Call if it was a function and we have a response
Delegate to script
"File protocol always yields status code 0, assume 200"
Support: IE <=9 only
#1450: sometimes IE returns 1223 when it should be 204
Cross domain only allowed if supported through XMLHttpRequest
Apply custom fields if provided
Override mime type if needed
X-Requested-With header
"For cross-domain requests, seeing as conditions for a preflight are"
"akin to a jigsaw puzzle, we simply never set it to be sure."
(it can always be set on a per-request basis or even using ajaxSetup)
"For same-domain requests, won't change header if already provided."
Set headers
Callback
Support: IE <=9 only
"On a manual native abort, IE9 throws"
errors on any property access that is not readyState
"File: protocol always yields status 0; see #8605, #14207"
Support: IE <=9 only
IE9 has no XHR2 but throws on binary (trac-11426)
"For XHR2 non-text, let the caller handle it (gh-2498)"
Listen to events
Support: IE 9 only
Use onreadystatechange to replace onabort
to handle uncaught aborts
Check readyState before timeout as it changes
"Allow onerror to be called first,"
but that will not handle a native abort
"Also, save errorCallback to a variable"
as xhr.onerror cannot be accessed
Create the abort callback
Do send the request (this may raise an exception)
#14683: Only rethrow if this hasn't been notified as an error yet
Implement the identical functionality for filter and not
Single element
"Arraylike of elements (jQuery, arguments, Array)"
Filtered directly for both simple and complex selectors
"If this is a positional/relative selector, check membership in the returned set"
"so $(""p:first"").is(""p:last"") won't return true for a doc with two ""p""."
# sourceMappingURL=sizzle.min.map
Local document vars
Instance-specific data
Instance methods
Use a stripped-down indexOf as it's faster than native
https://jsperf.com/thor-indexof-vs-for/5
Regular expressions
http://www.w3.org/TR/css3-selectors/#whitespace
http://www.w3.org/TR/CSS21/syndata.html#value-def-identifier
Attribute selectors: http://www.w3.org/TR/selectors/#attribute-selectors
Operator (capture 2)
"""Attribute values must be CSS identifiers [capture 5] or strings [capture 3 or capture 4]"""
"To reduce the number of selectors needing tokenize in the preFilter, prefer arguments:"
1. quoted (capture 3; capture 4 or capture 5)
2. simple (capture 6)
3. anything else (capture 2)
"Leading and non-escaped trailing whitespace, capturing some non-whitespace characters preceding the latter"
For use in libraries implementing .is()
We use this for POS matching in `select`
Easily-parseable/retrievable ID or TAG or CLASS selectors
CSS escapes
http://www.w3.org/TR/CSS21/syndata.html#escaped-characters
NaN means non-codepoint
Support: Firefox<24
"Workaround erroneous numeric interpretation of +""0x"""
BMP codepoint
Supplemental Plane codepoint (surrogate pair)
CSS string/identifier serialization
https://drafts.csswg.org/cssom/#common-serializing-idioms
U+0000 NULL becomes U+FFFD REPLACEMENT CHARACTER
Control characters and (dependent upon position) numbers get escaped as code points
Other potentially-special ASCII characters get backslash-escaped
Used for iframes
See setDocument()
"Removing the function wrapper causes a ""Permission Denied"""
error in IE
"Optimize for push.apply( _, NodeList )"
Support: Android<4.0
Detect silently failing push.apply
Leverage slice if possible
Support: IE<9
Otherwise append directly
Can't trust NodeList.length
"nodeType defaults to 9, since context defaults to document"
Return early from calls with invalid selector or context
Try to shortcut find operations (as opposed to filters) in HTML documents
"If the selector is sufficiently simple, try using a ""get*By*"" DOM method"
"(excepting DocumentFragment context, where the methods don't exist)"
ID selector
Document context
"Support: IE, Opera, Webkit"
TODO: identify versions
getElementById can match elements by name instead of ID
Element context
"Support: IE, Opera, Webkit"
TODO: identify versions
getElementById can match elements by name instead of ID
Type selector
Class selector
Take advantage of querySelectorAll
Support: IE 8 only
Exclude object elements
qSA considers elements outside a scoping root when evaluating child or
"descendant combinators, which is not what we want."
"In such cases, we work around the behavior by prefixing every selector in the"
list with an ID selector referencing the scope context.
Thanks to Andrew Dupont for this technique.
"Capture the context ID, setting it first if necessary"
Prefix every selector in the list
Expand context for sibling selectors
All others
"Use (key + "" "") to avoid collision with native prototype properties (see Issue #157)"
Only keep the most recent entries
Remove from its parent by default
release memory in IE
Use IE sourceIndex if available on both nodes
Check if b follows a
Known :disabled false positives: fieldset[disabled] > legend:nth-of-type(n+2) :can-disable
Only certain elements can match :enabled or :disabled
https://html.spec.whatwg.org/multipage/scripting.html#selector-enabled
https://html.spec.whatwg.org/multipage/scripting.html#selector-disabled
Check for inherited disabledness on relevant non-disabled elements:
* listed form-associated elements in a disabled fieldset
https://html.spec.whatwg.org/multipage/forms.html#category-listed
https://html.spec.whatwg.org/multipage/forms.html#concept-fe-disabled
* option elements in a disabled optgroup
https://html.spec.whatwg.org/multipage/forms.html#concept-option-disabled
"All such elements have a ""form"" property."
Option elements defer to a parent optgroup if present
Support: IE 6 - 11
Use the isDisabled shortcut property to check for disabled fieldset ancestors
"Where there is no isDisabled, check manually"
Try to winnow out elements that can't be disabled before trusting the disabled property.
"Some victims get caught in our net (label, legend, menu, track), but it shouldn't"
"even exist on them, let alone have a boolean value."
Remaining elements are neither :enabled nor :disabled
Match elements found at the specified indexes
Expose support vars for convenience
Support: IE <=8
"Assume HTML when documentElement doesn't yet exist, such as inside loading iframes"
https://bugs.jquery.com/ticket/4833
Return early if doc is invalid or already selected
Update global variables
"Support: IE 9-11, Edge"
"Accessing iframe documents after unload throws ""permission denied"" errors (jQuery #13936)"
"Support: IE 11, Edge"
Support: IE 9 - 10 only
Support: IE<8
Verify that getAttribute really returns attributes and not properties
(excepting IE8 booleans)
"Check if getElementsByTagName(""*"") returns only elements"
Support: IE<9
Support: IE<10
Check if getElementById returns elements by name
"The broken getElementById methods don't pick up programmatically-set names,"
so use a roundabout getElementsByName test
ID filter and find
Support: IE 6 - 7 only
getElementById is not reliable as a find shortcut
Verify the id attribute
Fall back on getElementsByName
Tag
DocumentFragment nodes don't have gEBTN
"By happy coincidence, a (broken) gEBTN appears on DocumentFragment nodes too"
Filter out possible comments
Class
QSA and matchesSelector support
matchesSelector(:active) reports false when true (IE9/Opera 11.5)
qSa(:focus) reports false when true (Chrome 21)
We allow this because of a bug in IE8/9 that throws an error
whenever `document.activeElement` is accessed on an iframe
"So, we allow :focus to pass through QSA all the time to avoid the IE error"
See https://bugs.jquery.com/ticket/13378
Build QSA regex
Regex strategy adopted from Diego Perini
Select is set to empty string on purpose
This is to test IE's treatment of not explicitly
"setting a boolean content attribute,"
since its presence should be enough
https://bugs.jquery.com/ticket/12359
"Support: IE8, Opera 11-12.16"
Nothing should be selected when empty strings follow ^= or $= or *=
"The test attribute must be unknown in Opera but ""safe"" for WinRT"
https://msdn.microsoft.com/en-us/library/ie/hh465388.aspx#attribute_section
Support: IE8
"Boolean attributes and ""value"" are not treated correctly"
"Support: Chrome<29, Android<4.4, Safari<7.0+, iOS<7.0+, PhantomJS<1.9.8+"
Webkit/Opera - :checked should return selected option elements
http://www.w3.org/TR/2011/REC-css3-selectors-20110929/#checked
IE8 throws error here and will not see later tests
"Support: Safari 8+, iOS 8+"
https://bugs.webkit.org/show_bug.cgi?id=136851
In-page `selector#id sibling-combinator selector` fails
Support: Windows 8 Native Apps
The type and name attributes are restricted during .innerHTML assignment
Support: IE8
Enforce case-sensitivity of name attribute
FF 3.5 - :enabled/:disabled and hidden elements (hidden elements are still enabled)
IE8 throws error here and will not see later tests
Support: IE9-11+
IE's :disabled selector does not pick up the children of disabled fieldsets
Opera 10-11 does not throw on post-comma invalid pseudos
Check to see if it's possible to do matchesSelector
on a disconnected node (IE 9)
This should fail with an exception
"Gecko does not error, returns false instead"
Element contains another
Purposefully self-exclusive
"As in, an element does not contain itself"
Document order sorting
Flag for duplicate removal
Sort on method existence if only one input has compareDocumentPosition
Calculate position if both inputs belong to the same document
Otherwise we know they are disconnected
Disconnected nodes
Choose the first element that is related to our preferred document
Maintain original order
Exit early if the nodes are identical
Parentless nodes are either documents or disconnected
"If the nodes are siblings, we can do a quick check"
Otherwise we need full lists of their ancestors for comparison
Walk down the tree looking for a discrepancy
Do a sibling check if the nodes have a common ancestor
Otherwise nodes in our document sort first
Set document vars if needed
IE 9's matchesSelector returns false on disconnected nodes
"As well, disconnected nodes are said to be in a document"
fragment in IE 9
Set document vars if needed
Set document vars if needed
Don't get fooled by Object.prototype properties (jQuery #13807)
"Unless we *know* we can detect duplicates, assume their presence"
Clear input after sorting to release objects
See https://github.com/jquery/sizzle/pull/225
"If no nodeType, this is expected to be an array"
Do not traverse comment nodes
Use textContent for elements
innerText usage removed for consistency of new lines (jQuery #11153)
Traverse its children
Do not include comment or processing instruction nodes
Can be adjusted by the user
Move the given value to match[3] whether quoted or unquoted
nth-* requires argument
numeric x and y parameters for Expr.filter.CHILD
remember that false/true cast respectively to 0/1
other types prohibit arguments
Accept quoted arguments as-is
Strip excess characters from unquoted arguments
Get excess from tokenize (recursively)
advance to the next closing parenthesis
excess is a negative index
Return only captures needed by the pseudo filter method (type and argument)
Shortcut for :nth-*(n)
:(first|last|only)-(child|of-type)
Reverse direction for :only-* (if we haven't yet done so)
non-xml :nth-child(...) stores cache data on `parent`
Seek `elem` from a previously-cached index
...in a gzip-friendly way
Support: IE <9 only
Defend against cloned attroperties (jQuery gh-1709)
Fallback to seeking `elem` from the start
"When found, cache indexes on `parent` and break"
Use previously-cached element index if available
...in a gzip-friendly way
Support: IE <9 only
Defend against cloned attroperties (jQuery gh-1709)
xml :nth-child(...)
or :nth-last-child(...) or :nth(-last)?-of-type(...)
Use the same loop as above to seek `elem` from the start
Cache the index of each encountered element
Support: IE <9 only
Defend against cloned attroperties (jQuery gh-1709)
"Incorporate the offset, then check against cycle size"
pseudo-class names are case-insensitive
http://www.w3.org/TR/selectors/#pseudo-classes
Prioritize by case sensitivity in case custom pseudos are added with uppercase letters
Remember that setFilters inherits from pseudos
The user may use createPseudo to indicate that
arguments are needed to create the filter function
just as Sizzle does
But maintain support for old signatures
Potentially complex pseudos
Trim the selector passed to compile
to avoid treating leading and trailing
spaces as combinators
Match elements unmatched by `matcher`
Don't keep the element (issue #299)
"""Whether an element is represented by a :lang() selector"
is based solely on the element's language value
"being equal to the identifier C,"
"or beginning with the identifier C immediately followed by ""-""."
The matching of C against the element's language value is performed case-insensitively.
"The identifier C does not have to be a valid language name."""
http://www.w3.org/TR/selectors/#lang-pseudo
lang value must be a valid identifier
Miscellaneous
Boolean properties
"In CSS3, :checked should return both checked and selected elements"
http://www.w3.org/TR/2011/REC-css3-selectors-20110929/#checked
Accessing this property makes selected-by-default
options in Safari work properly
Contents
http://www.w3.org/TR/selectors/#empty-pseudo
":empty is negated by element (1) or content nodes (text: 3; cdata: 4; entity ref: 5),"
but not by others (comment: 8; processing instruction: 7; etc.)
nodeType < 6 works because attributes (2) do not appear as children
Element/input types
Support: IE<8
"New HTML5 attribute values (e.g., ""search"") appear with elem.type === ""text"""
Position-in-collection
Add button/input type pseudos
Easy API for creating new setFilters
Comma and first run
Don't consume trailing commas as valid
Combinators
Cast descendant combinators to space
Filters
Return the length of the invalid excess
if we're just parsing
"Otherwise, throw an error or return tokens"
Cache the tokens
Check against closest ancestor/preceding element
Check against all ancestor/preceding elements
"We can't set arbitrary data on XML nodes, so they don't benefit from combinator caching"
Support: IE <9 only
Defend against cloned attroperties (jQuery gh-1709)
Assign to newCache so results back-propagate to previous elements
Reuse newcache so results back-propagate to previous elements
A match means we're done; a fail means we have to keep checking
Get initial elements from seed or context
"Prefilter to get matcher input, preserving a map for seed-results synchronization"
"If we have a postFinder, or filtered seed, or non-seed postFilter or preexisting results,"
...intermediate processing is necessary
...otherwise use results directly
Find primary matches
Apply postFilter
Un-match failing elements by moving them back to matcherIn
Get the final matcherOut by condensing this intermediate into postFinder contexts
Restore matcherIn since elem is not yet a final match
Move matched elements from seed to results to keep them synchronized
"Add elements to results, through postFinder if defined"
The foundational matcher ensures that elements are reachable from top-level context(s)
Avoid hanging onto element (issue #299)
Return special upon seeing a positional matcher
Find the next relative operator (if any) for proper handling
"If the preceding token was a descendant combinator, insert an implicit any-element `*`"
We must always have either seed elements or outermost context
Use integer dirruns iff this is the outermost matcher
Add elements passing elementMatchers directly to results
"Support: IE<9, Safari"
"Tolerate NodeList properties (IE: ""length""; Safari: <number>) matching elements by id"
Track unmatched elements for set filters
They will have gone through all possible matchers
"Lengthen the array for every element, matched or not"
"`i` is now the count of elements visited above, and adding it to `matchedCount`"
makes the latter nonnegative.
Apply set filters to unmatched elements
"NOTE: This can be skipped if there are no unmatched elements (i.e., `matchedCount`"
"equals `i`), unless we didn't visit _any_ elements in the above loop because we have"
no element matchers and no seed.
"Incrementing an initially-string ""0"" `i` allows `i` to remain a string only in that"
"case, which will result in a ""00"" `matchedCount` that differs from `i` but is also"
numerically zero.
Reintegrate element matches to eliminate the need for sorting
Discard index placeholder values to get only actual matches
Add matches to results
Seedless set matches succeeding multiple successful matchers stipulate sorting
Override manipulation of globals by nested matchers
Generate a function of recursive functions that can be used to check each element
Cache the compiled function
Save selector and tokenization
Try to minimize operations if there is only one selector in the list and no seed
(the latter of which guarantees us context)
Reduce context if the leading compound selector is an ID
"Precompiled matchers will still verify ancestry, so step up a level"
Fetch a seed set for right-to-left matching
Abort if we hit a combinator
"Search, expanding context for leading sibling combinators"
"If seed is empty or no tokens remain, we can return early"
Compile and execute a filtering function if one is not provided
Provide `match` to avoid retokenization if we modified the selector above
One-time assignments
Sort stability
Support: Chrome 14-35+
Always assume duplicates if they aren't passed to the comparison function
Initialize against the default document
Support: Webkit<537.32 - Safari 6.0.3/Chrome 25 (fixed in Chrome 27)
Detached nodes confoundingly follow *each other*
"Should return 1, but returns 4 (following)"
Support: IE<8
"Prevent attribute/property ""interpolation"""
https://msdn.microsoft.com/en-us/library/ms536429%28VS.85%29.aspx
Support: IE<9
"Use defaultValue in place of getAttribute(""value"")"
Support: IE<9
Use getAttributeNode to fetch booleans when getAttribute lies
EXPOSE
Sizzle requires that there be a global window in Common-JS like environments
EXPOSE
# sourceMappingURL=noty.min.js.map
Trim the opening space.
Replace the class name.
Trim the opening and closing spaces.
Opera 12.10 and Firefox 18 and later support
fix for Chrome < 45
"If len is 2, that means that we need to schedule an async flush."
"If additional callbacks are queued before the queue is flushed, they"
will be processed by this flush that we are scheduling.
test for web worker but not in IE10
node
node version 0.10.x displays a deprecation warning when nextTick is used recursively
see https://github.com/cujojs/when/issues/410 for details
vertx
web worker
Store setTimeout reference so es6-promise will be unaffected by
other code modifying setTimeout (like sinon.useFakeTimers())
Decide what async method to use to triggering processing of queued callbacks:
value === 1
value === 1
noop
"The array here would be [ 1, 2, 3 ];"
Code here never runs because there are rejected promises!
"error.message === ""2"""
result === 'promise 2' because it was resolved before promise1
was resolved.
Code here never runs
reason.message === 'promise 2' because promise 2 became rejected before
promise 1 became fulfilled
Code here doesn't run because the promise is rejected!
reason.message === 'WHOOPS'
Code here doesn't run because the promise is rejected!
reason.message === 'WHOOPS'
on success
on failure
on fulfillment
on rejection
on fulfillment
on rejection
user is available
"user is unavailable, and you are given the reason why"
"If `findUser` fulfilled, `userName` will be the user's name, otherwise it"
will be `'default name'`
never reached
"if `findUser` fulfilled, `reason` will be 'Found user, but still unhappy'."
"If `findUser` rejected, `reason` will be '`findUser` rejected and we're unhappy'."
never reached
never reached
The `PedgagocialException` is propagated all the way down to here
The user's comments are now available
"If `findCommentsByAuthor` fulfills, we'll have the value here"
"If `findCommentsByAuthor` rejects, we'll have the reason here"
success
failure
failure
success
success
failure
success
failure
failure
success
found books
something went wrong
synchronous
something went wrong
async with promises
something went wrong
silently ignored
Strange compat..
# sourceMappingURL=es6-promise.map
removed by extract-text-webpack-plugin
bind button events if any
ugly fix for progressbar display bug
it's in the queue
API functions
Document visibility change controller
shim for using process in browser
cached from whatever global is present so that test runners that stub it
don't break things.  But we need to wrap it in a try catch in case it is
wrapped in strict mode code which doesn't define any globals.  It's inside a
function because try/catches deoptimize in certain engines.
normal enviroments in sane situations
if setTimeout wasn't available but was latter defined
when when somebody has screwed with setTimeout but no I.E. maddness
When we are in I.E. but the script has been evaled so I.E. doesn't trust the global object when called normally
"same as above but when it's a version of I.E. that must have the global object for 'this', hopfully our context correct otherwise it will throw a global error"
normal enviroments in sane situations
if clearTimeout wasn't available but was latter defined
when when somebody has screwed with setTimeout but no I.E. maddness
When we are in I.E. but the script has been evaled so I.E. doesn't  trust the global object when called normally
"same as above but when it's a version of I.E. that must have the global object for 'this', hopfully our context correct otherwise it will throw a global error."
Some versions of I.E. have different rules for clearTimeout vs setTimeout
v8 likes predictible objects
This works in non-strict mode
This works if eval is allowed (see CSP)
This works if the window reference is available
"g can still be undefined, but nothing to do about it..."
"We return undefined, instead of nothing here, so it's"
easier to handle this case. if(!global) { ...}
# sourceMappingURL=noty.js.map
Trim the opening space.
Replace the class name.
Trim the opening and closing spaces.
Opera 12.10 and Firefox 18 and later support
fix for Chrome < 45
bind button events if any
ugly fix for progressbar display bug
it's in the queue
API functions
Document visibility change controller
# sourceMappingURL=bootstrap.bundle.min.js.map
# sourceMappingURL=bootstrap.min.js.map
eslint-disable-next-line no-bitwise
TODO: Remove in v5
Public
Public
Public
Don't call next when the page isn't visible
or the carousel or its parent isn't visible
"If it's a touch-enabled device, mouseenter/leave are fired as"
part of the mouse compatibility events on first tap - the carousel
would stop cycling until user tapped out of it;
"here, we listen for touchend, explicitly pause the carousel"
"(as if it's the second time we tap on it, mouseenter compat event"
is NOT fired) and after a timeout (to allow for mouse compatibility
events to fire) we explicitly restart cycling
"Some weirdness is happening, so we bail"
Public
preventDefault only for <a> elements (which change the URL) not inside the collapsible element
NOTE: 1 DOM access here
"Return body, `getScroll` will take care to get the correct `scrollTop` from it"
Firefox want us to check `-x` and `-y` variations as well
NOTE: 1 DOM access here
Skip hidden elements which don't have an offsetParent
.offsetParent will return the closest TD or TABLE in case
"no offsetParent is present, I hate this job..."
This check is needed to avoid errors in case one of the elements isn't defined for any reason
"Here we make sure to give as ""start"" the element that comes first in the DOM"
Get common ancestor container
Both nodes are inside #document
"one of the nodes is inside shadowDOM, find which one"
"IE10 10 FIX: Please, don't ask, the element isn't"
considered in DOM in some circumstances...
This isn't reproducible in IE10 compatibility mode of IE11
subtract scrollbar size from sizes
"if an hypothetical scrollbar is detected, we must be sure it's not a `border`"
we make this check conditional for performance reasons
"In cases where the parent is fixed, we must ignore negative scroll in offset calc"
Subtract margins of documentElement in case it's being used as parent
we do this only on HTML because it's the only element that behaves
differently when margins are applied to it. The margins are included in
"the box of the documentElement, in the other cases not."
Attach marginTop and marginLeft because in some circumstances we may need them
This check is needed to avoid errors in case one of the elements isn't defined for any reason
NOTE: 1 DOM access here
Handle viewport case
Handle other cases based on DOM element used as boundaries
"In case of HTML, we need a different computation"
"for all the other DOM elements, this one is good"
Add paddings
Get popper node sizes
"Add position, width and height to our offsets object"
depending by the popper placement we have to compute its offsets slightly differently
use native find if supported
use `filter` to obtain the same behavior of `find`
use native findIndex if supported
use `find` + `indexOf` if `findIndex` isn't supported
eslint-disable-line dot-notation
Add properties to offsets to make them a complete clientRect object
we do this before each modifier to make sure the previous one doesn't
mess with these values
"if popper is destroyed, don't perform any further update"
compute reference element offsets
"compute auto placement, store placement inside the data object,"
modifiers will be able to edit `placement` if needed
and refer to originalPlacement to know the original value
store the computed placement inside `originalPlacement`
compute the popper offsets
run the modifiers
the first `update` will call `onCreate` callback
the other ones will call `onUpdate` callback
touch DOM only if `applyStyle` modifier is enabled
remove the popper if user explicity asked for the deletion on destroy
do not use `remove` because IE11 doesn't support it
Resize event listener on window
Scroll event listener on scroll parents
Remove resize event listener on window
Remove scroll event listener on scroll parents
Reset state
add unit if the value is numeric and is one of the following
"any property present in `data.styles` will be applied to the popper,"
in this way we can make the 3rd party modifiers add custom styles to it
"Be aware, modifiers could override the properties defined in the previous"
lines of this modifier!
"any property present in `data.attributes` will be applied to the popper,"
they will be set as HTML attributes of the element
if arrowElement is defined and arrowStyles has some properties
compute reference element offsets
"compute auto placement, store placement inside the data object,"
modifiers will be able to edit `placement` if needed
and refer to originalPlacement to know the original value
Apply `position` to popper before anything else because
without the position applied we can't guarantee correct computations
Remove this legacy support in Popper.js v2
Styles
Avoid blurry text by using full pixel integers.
"For pixel-perfect positioning, top/bottom prefers rounded"
"values, while left/right prefers floored values."
"if gpuAcceleration is set to `true` and transform is supported,"
we use `translate3d` to apply the position to the popper we
automatically use the supported prefixed version if needed
"now, let's make a step back and look at this code closely (wtf?)"
"If the content of the popper grows once it's been positioned, it"
may happen that the popper gets misplaced because of the new content
overflowing its reference element
"To avoid this problem, we provide two options (x and y), which allow"
the consumer to define the offset origin.
"If we position a popper on top of a reference element, we can set"
`x` to `top` to make the popper grow towards its top instead of
its bottom.
"othwerise, we use the standard `top`, `left`, `bottom` and `right` properties"
Attributes
"Update `data` attributes, styles and arrowStyles"
arrow depends on keepTogether in order to work
"if arrowElement is a string, suppose it's a CSS selector"
"if arrowElement is not found, don't run the modifier"
if the arrowElement isn't a query selector we must check that the
provided DOM node is child of its popper node
""
extends keepTogether behavior making sure the popper and its
reference have enough pixels in conjuction
""
top/left side
bottom/right side
compute center of the popper
Compute the sideValue using the updated popper offsets
take popper margin in account because we don't have this info available
prevent arrowElement from being placed not contiguously to its popper
Get rid of `auto` `auto-start` and `auto-end`
"if `inner` modifier is enabled, we can't use the `flip` modifier"
"seems like flip is trying to loop, probably there's not enough space on any of the flippable sides"
using floor because the reference offsets may contain decimals we are not going to consider here
flip the variation if required
this boolean to detect any flip loop
"this object contains `position`, we want to preserve it along with"
any additional property we may add in the future
separate value from unit
"If it's not a number it's an operator, I guess"
"if is a vh or vw, we calculate the size based on the viewport"
"if is an explicit pixel unit, we get rid of the unit and keep the value"
"if is an implicit unit, it's px, and we return just the value"
Use height if placement is left or right and index is 0 otherwise use width
in this way the first offset will use an axis and the second one
will use the other one
Split the offset string to obtain a list of values and operands
"The regex addresses values with the plus or minus sign in front (+10, -20, etc)"
Detect if the offset string contains a pair of values or a single one
they could be separated by comma or space
"If divider is found, we divide the list of values and operands to divide"
them by ofset X and Y.
Convert the values with units to absolute pixels to allow our computations
Most of the units rely on the orientation of the popper
This aggregates any `+` or `-` sign that aren't considered operators
"e.g.: 10 + +5 => [10, +, +5]"
Here we convert the string values into number values (in px)
Loop trough the offsets arrays and execute the operations
"If offsetParent is the reference element, we really want to"
go one step up and use the next offsetParent as reference to
avoid to make this modifier completely useless and look like broken
NOTE: DOM access here
resets the popper's position so that the document size can be calculated excluding
the size of the popper element itself
NOTE: DOM access here
restores the original style properties after the offsets have been computed
"if shift shiftvariation is specified, run the modifier"
Avoid unnecessary DOM access if visibility hasn't changed
Avoid unnecessary DOM access if visibility hasn't changed
Utils
Methods
"make update() debounced, so that it only runs at most once-per-tick"
with {} we create a new object with the options inside it
init state
get reference and popper elements (allow jQuery wrappers)
Deep merge modifiers options
Refactoring modifiers' list (Object => Array)
sort the modifiers by order
modifiers have the ability to execute arbitrary code when Popper.js get inited
such code is executed in the same order of its modifier
they could add new properties to their options configuration
BE AWARE: don't add options to `options.modifiers.name` but to `modifierOptions`!
fire the first update to position the popper in the right place
"setup event listeners, they will take care of update the position in specific situations"
We can't use class properties because they don't get listed in the
class prototype and break stuff like Sinon stubs
Public
"to allow the menu to ""escape"" the scroll parent's boundaries"
https://github.com/twbs/bootstrap/issues/24251
empty mouseover listeners to the body's immediate children;
only needed because of broken event delegation on iOS
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html
empty mouseover listeners we added for iOS support
If not input/textarea:
- And not a key in REGEXP_KEYDOWN => not a dropdown command
If input/textarea:
- If space key => not a dropdown command
- If key is other than escape
- If key is not up or down => not a dropdown command
- If trigger inside the menu => not a dropdown command
Up
Down
Public
Don't move modal's DOM position
the following methods are used to handle overflowing modals
todo (fat): these should probably be refactored out of modal.js
----------------------------------------------------------------------
Note: DOMNode.style.paddingRight returns the actual value or '' if not set
while $(DOMNode).css('padding-right') returns the calculated value or 0 if not set
Restore fixed content padding
thx d.walsh
Only register focus restorer if modal will actually get shown
Public
empty mouseover listeners to the body's immediate children;
only needed because of broken event delegation on iOS
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html
empty mouseover listeners we added for iOS support
Content is a DOM node or a jQuery
Overrides
Getters
Public
TODO (fat): remove sketch reliance on jQuery position/offset
Set triggered link as active
With both <ul> and <nav> markup a parent is the previous sibling of any nav ancestor
Public
# sourceMappingURL=bootstrap.bundle.js.map
eslint-disable-next-line no-bitwise
TODO: Remove in v5
Public
Public
Public
Don't call next when the page isn't visible
or the carousel or its parent isn't visible
"If it's a touch-enabled device, mouseenter/leave are fired as"
part of the mouse compatibility events on first tap - the carousel
would stop cycling until user tapped out of it;
"here, we listen for touchend, explicitly pause the carousel"
"(as if it's the second time we tap on it, mouseenter compat event"
is NOT fired) and after a timeout (to allow for mouse compatibility
events to fire) we explicitly restart cycling
"Some weirdness is happening, so we bail"
Public
preventDefault only for <a> elements (which change the URL) not inside the collapsible element
Public
"to allow the menu to ""escape"" the scroll parent's boundaries"
https://github.com/twbs/bootstrap/issues/24251
empty mouseover listeners to the body's immediate children;
only needed because of broken event delegation on iOS
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html
empty mouseover listeners we added for iOS support
If not input/textarea:
- And not a key in REGEXP_KEYDOWN => not a dropdown command
If input/textarea:
- If space key => not a dropdown command
- If key is other than escape
- If key is not up or down => not a dropdown command
- If trigger inside the menu => not a dropdown command
Up
Down
Public
Don't move modal's DOM position
the following methods are used to handle overflowing modals
todo (fat): these should probably be refactored out of modal.js
----------------------------------------------------------------------
Note: DOMNode.style.paddingRight returns the actual value or '' if not set
while $(DOMNode).css('padding-right') returns the calculated value or 0 if not set
Restore fixed content padding
thx d.walsh
Only register focus restorer if modal will actually get shown
Public
empty mouseover listeners to the body's immediate children;
only needed because of broken event delegation on iOS
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html
empty mouseover listeners we added for iOS support
Content is a DOM node or a jQuery
Overrides
Getters
Public
TODO (fat): remove sketch reliance on jQuery position/offset
Set triggered link as active
With both <ul> and <nav> markup a parent is the previous sibling of any nav ancestor
Public
# sourceMappingURL=bootstrap.js.map
Public
empty mouseover listeners to the body's immediate children;
only needed because of broken event delegation on iOS
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html
empty mouseover listeners we added for iOS support
Content is a DOM node or a jQuery
# sourceMappingURL=tooltip.js.map
Public
preventDefault only for <a> elements (which change the URL) not inside the collapsible element
# sourceMappingURL=collapse.js.map
Public
Don't move modal's DOM position
the following methods are used to handle overflowing modals
todo (fat): these should probably be refactored out of modal.js
----------------------------------------------------------------------
Note: DOMNode.style.paddingRight returns the actual value or '' if not set
while $(DOMNode).css('padding-right') returns the calculated value or 0 if not set
Restore fixed content padding
thx d.walsh
Only register focus restorer if modal will actually get shown
# sourceMappingURL=modal.js.map
Overrides
Getters
# sourceMappingURL=popover.js.map
Public
# sourceMappingURL=tab.js.map
Public
"to allow the menu to ""escape"" the scroll parent's boundaries"
https://github.com/twbs/bootstrap/issues/24251
empty mouseover listeners to the body's immediate children;
only needed because of broken event delegation on iOS
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html
empty mouseover listeners we added for iOS support
If not input/textarea:
- And not a key in REGEXP_KEYDOWN => not a dropdown command
If input/textarea:
- If space key => not a dropdown command
- If key is other than escape
- If key is not up or down => not a dropdown command
- If trigger inside the menu => not a dropdown command
Up
Down
# sourceMappingURL=dropdown.js.map
Public
Don't call next when the page isn't visible
or the carousel or its parent isn't visible
"If it's a touch-enabled device, mouseenter/leave are fired as"
part of the mouse compatibility events on first tap - the carousel
would stop cycling until user tapped out of it;
"here, we listen for touchend, explicitly pause the carousel"
"(as if it's the second time we tap on it, mouseenter compat event"
is NOT fired) and after a timeout (to allow for mouse compatibility
events to fire) we explicitly restart cycling
"Some weirdness is happening, so we bail"
# sourceMappingURL=carousel.js.map
Public
# sourceMappingURL=alert.js.map
# sourceMappingURL=index.js.map
Public
# sourceMappingURL=button.js.map
Public
TODO (fat): remove sketch reliance on jQuery position/offset
Set triggered link as active
With both <ul> and <nav> markup a parent is the previous sibling of any nav ancestor
# sourceMappingURL=scrollspy.js.map
eslint-disable-next-line no-bitwise
TODO: Remove in v5
# sourceMappingURL=util.js.map
private
Protected
Getters
Public
If this is a touch-enabled device we add extra
empty mouseover listeners to the body's immediate children;
only needed because of broken event delegation on iOS
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html
If this is a touch-enabled device we remove the extra
empty mouseover listeners we added for iOS support
Protected
Content is a DOM node or a jQuery
Private
Static
Getters
Public
Private
It's a jQuery object
Static
preventDefault only for <a> elements (which change the URL) not inside the collapsible element
Getters
Public
Private
Don't move modal's DOM position
----------------------------------------------------------------------
the following methods are used to handle overflowing modals
todo (fat): these should probably be refactored out of modal.js
----------------------------------------------------------------------
Note: DOMNode.style.paddingRight returns the actual value or '' if not set
while $(DOMNode).css('padding-right') returns the calculated value or 0 if not set
Adjust fixed content padding
Adjust sticky content margin
Adjust body padding
Restore fixed content padding
Restore sticky content
Restore body padding
Static
Only register focus restorer if modal will actually get shown
Getters
Overrides
We use append for html objects to maintain js events
Private
Static
Getters
Public
Private
Static
Getters
Public
Disable totally Popper.js for Dropdown in Navbar
Check if it's jQuery element
"If boundary is not `scrollParent`, then set position to `static`"
"to allow the menu to ""escape"" the scroll parent's boundaries"
https://github.com/twbs/bootstrap/issues/24251
If this is a touch-enabled device we add extra
empty mouseover listeners to the body's immediate children;
only needed because of broken event delegation on iOS
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html
Private
Handle dropup
Disable Popper.js if we have a static display
Static
If this is a touch-enabled device we remove the extra
empty mouseover listeners we added for iOS support
eslint-disable-next-line complexity
If not input/textarea:
- And not a key in REGEXP_KEYDOWN => not a dropdown command
If input/textarea:
- If space key => not a dropdown command
- If key is other than escape
- If key is not up or down => not a dropdown command
- If trigger inside the menu => not a dropdown command
Getters
Public
Don't call next when the page isn't visible
or the carousel or its parent isn't visible
Private
"If it's a touch-enabled device, mouseenter/leave are fired as"
part of the mouse compatibility events on first tap - the carousel
would stop cycling until user tapped out of it;
"here, we listen for touchend, explicitly pause the carousel"
"(as if it's the second time we tap on it, mouseenter compat event"
is NOT fired) and after a timeout (to allow for mouse compatibility
events to fire) we explicitly restart cycling
"Some weirdness is happening, so we bail"
Static
Getters
Public
Private
Static
Getters
Public
Static
Getters
Public
TODO (fat): remove sketch reliance on jQuery position/offset
Private
eslint-disable-next-line arrow-body-style
Set triggered link as active
Set triggered links parents as active
With both <ul> and <nav> markup a parent is the previous sibling of any nav ancestor
Handle special case when .nav-link is inside .nav-item
Static
Shoutout AngusCroll (https://goo.gl/pxwQGp)
eslint-disable-next-line no-bitwise
Get transition-duration of the element
Return 0 if element or transition duration is not found
"If multiple durations are defined, take the first"
TODO: Remove in v5
