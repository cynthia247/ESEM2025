Commit Message
mmap seems to be much more memory efficient
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
Extract all the contents of zip file in current directory
use model name as subfolder
Lazy import
output information
Extract all the contents of zip file in current directory
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
load_big_file is a workaround byhttps://github.com/highway11git
to load models on some Mac/Windows setups
see https://github.com/zalandoresearch/flair/issues/351
first determine the distribution of classes in the dataset
weight for each sample
Create blocks
shuffle the blocks
concatenate the shuffled blocks
Create blocks
shuffle the blocks
concatenate the shuffled blocks
Build the regular expression pattern dynamically based on the provided symbols
This will match any character from the symbols list that doesn't have spaces around it
"Add space before and after symbols, where necessary"
Ensure that we are adding a space only if there isn't one already
increment for last token in sentence if not followed by whitespace
this is the default init size of a lmdb database for embeddings
get db filename from embedding name
"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
SequenceTagger
TextClassifier
get db filename from embedding name
if embedding database already exists
"otherwise, push embedding to database"
if embedding database already exists
open the database in read mode
we need to set self.k
create and load the database in write mode
"no idea why, but we need to close and reopen the environment to avoid"
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
when opening new transaction !
init dictionaries
"in order to deal with unknown tokens, add <unk>"
set 'add_unk' if the dictionary was created with a version of Flair older than 0.9
set 'add_unk' depending on whether <unk> is a key
"if one embedding name, directly return it"
"if multiple embedding names, concatenate them"
First we remove any existing labels for this PartOfSentence in self.sentence
labels also need to be deleted at Sentence object
delete labels at object itself
The Token is a special _PartOfSentence in that it may be initialized without a Sentence.
"therefore, labels get added only to the Sentence if it exists"
The Token is a special _PartOfSentence in that it may be initialized without a Sentence.
"Therefore, labels get set only to the Sentence if it exists"
"check if the span already exists. If so, return it"
else make a new span
"check if the relation already exists. If so, return it"
else make a new relation
private field for all known spans
the tokenizer used for this sentence
some sentences represent a document boundary (but most do not)
internal variables to denote position inside dataset
"if text is passed, instantiate sentence with tokens (words)"
determine token positions and whitespace_after flag
the last token has no whitespace after
log a warning if the dataset is empty
data with zero-width characters cannot be handled
set token idx and sentence
append token to sentence
register token annotations on sentence
move sentence embeddings to device
also move token embeddings to device
clear token embeddings
infer whitespace after field
"if sentence has no tokens, return empty string"
"otherwise, return concatenation of tokens with the correct offsets"
The sentence's start position is not propagated to its tokens.
"Therefore, we need to add the sentence's start position to its last token's end position, including whitespaces."
No character at the corresponding code point: remove it
"if no label if specified, return all labels"
"if the label type exists in the Sentence, return it"
return empty list if none of the above
labels also need to be deleted at all tokens
labels also need to be deleted at all known spans
remove spans without labels
delete labels at object itself
set name
abort if no data is provided
sample test data from train if none is provided
sample dev data from train if none is provided
set train dev and test data
find out empty sentence indices
create subset of non-empty sentence indices
find out empty sentence indices
create subset of non-empty sentence indices
"first, determine the datapoint type by going through dataset until first label is found"
count all label types per sentence
go through all labels of label_type and count values
special handling for Token-level annotations. Add all untagged as 'O' label
"if an unk threshold is set, UNK all label values below this threshold"
sample randomly from a label distribution according to the probabilities defined by the noise transition matrix
replace the old label with the new one
keep track of the old (clean) label using another label type category
keep track of how many labels in total are flipped
sample randomly from a label distribution according to the probabilities defined by the desired noise share
replace the old label with the new one
keep track of the old (clean) label using another label type category
keep track of how many labels in total are flipped
"add a dummy ""O"" to close final prediction"
return complex list
internal variables
non-set tags are OUT tags
anything that is not OUT is IN
does this prediction start a new span?
B- and S- always start new spans
"if the predicted class changes, I- starts a new span"
"if the predicted class changes and S- was previous tag, start a new span"
if an existing span is ended (either by reaching O or starting a new span)
determine score and value
append to result list
reset for-loop variables for new span
remember previous tag
global variable: cache_root
Get the device from the environment variable
global variable: device
"No need for correctness checks, torch is doing it"
global variable: version
global variable: arrow symbol
Attach optimizer
"convert `metrics` to float, in case it's a zero-dim Tensor"
if storage mode option 'none' delete everything
"if dynamic embedding keys not passed, identify them automatically"
always delete dynamic embeddings
"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)"
add tokens before the entity
add new entity tokens
add any remaining tokens to a new chunk
optional metric space decoder if prototypes have different length than embedding
create initial prototypes for all classes (all initial prototypes are a vector of all 1s)
"if set, create initial prototypes from normal distribution"
"if set, use a radius"
all parameters will be pushed internally to the specified device
decode embeddings into prototype space
"if unlabeled distance is set, mask out loss to unlabeled class prototype"
all parameters will be pushed internally to the specified device
Reset prototype updates
verbalize BIOES labels
"if label is not BIOES, use label itself"
Always include the name of the Model class for which the state dict holds
"this seems to just return model name, not a model with that name"
"write out a ""model card"" if one is set"
save model
"if this class is abstract, go through all inheriting classes and try to fetch and load the model"
get all non-abstract subclasses
"try to fetch the model for each subclass. if fetching is possible, load model and return it"
"skip any invalid loadings, e.g. not found on HuggingFace hub"
"if the model cannot be fetched, load as a file"
try to get model class from state
"older (flair 11.3 and below) models do not contain cls information. In this case, try all subclasses"
"skip any invalid loadings, e.g. not found on HuggingFace hub"
"if this class is not abstract, fetch the model and load it"
"make sure <unk> is contained in gold_label_dictionary, if given"
"read Dataset into data loader, if list of sentences passed, make Dataset first"
loss calculation
variables for printing
variables for computing scores
remove any previously predicted labels
predict for batch
get the gold labels
add to all_predicted_values
make printout lines
convert true and predicted values to two span-aligned lists
delete excluded labels if exclude_labels is given
"if after excluding labels, no label is left, ignore the datapoint"
write all_predicted_values to out_file if set
make the evaluation dictionary
check if this is a multi-label problem
compute numbers by formatting true and predicted such that Scikit-Learn can use them
multi-label problems require a multi-hot vector for each true and predicted label
single-label problems can do with a single index for each true and predicted label
"now, calculate evaluation numbers"
there is at least one gold label or one prediction (default)
compute accuracy separately as it is not always in classification_report (e.g. when micro avg exists)
"if there is only one label, then ""micro avg"" = ""macro avg"""
"The ""micro avg"" appears only in the classification report if no prediction is possible."
"Otherwise, it is identical to the ""macro avg"". In this case, we add it to the report."
"Create and populate score object for logging with all evaluation values, plus the loss"
issue error and default all evaluation numbers to 0.
check if there is a label mismatch
print info
set the embeddings
initialize the label dictionary
initialize the decoder
set up multi-label logic
init dropouts
loss weights and loss function
Initialize the weight tensor
set up gradient reversal if so specified
embed sentences
get a tensor of data points
do dropout
make a forward pass to produce embedded data points and labels
get the data points for which to predict labels
get their gold labels as a tensor
pass data points through network to get encoded data point tensor
"decode, passing label tensor if needed, such as for prototype updates"
an optional masking step (no masking in most cases)
calculate the loss
filter empty sentences
reverse sort all sequences by their length
progress bar for verbosity
filter data points in batch
stop if all sentences are empty
pass data points through network and decode
if anything could possibly be predicted
remove previously predicted labels of this type
filter data points that have labels outside of dictionary
add DefaultClassifier arguments
add variables of DefaultClassifier
Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23
Get projected 1st dimension
Compute bilinear form
Arcosh
Project the input data to n+1 dimensions
"The first dimension, is recomputed in the distance module"
header for 'weights.txt'
"determine the column index of loss, f-score and accuracy for"
"train, dev and test split"
then get all relevant values from the tsv
then get all relevant values from the tsv
plot i
save plots
save plots
plt.show()
save plot
auto-spawn on GPU if available
progress bar for verbosity
stop if all sentences are empty
clearing token embeddings to save memory
"read Dataset into data loader, if list of sentences passed, make Dataset first"
TODO: not saving lines yet
TODO: This closely shadows the RelationExtractor name. Maybe we need a better name here.
- MaskedRelationClassifier ?
This depends if this relation classification architecture should replace or offer as an alternative.
Set label type and prepare label dictionary
Initialize super default classifier
Add the special tokens from the encoding strategy
"Auto-spawn on GPU, if available"
Only use entities labelled with the specified labels for each label type
Only use entities above the specified threshold
Use a dictionary to find gold relation annotations for a given entity pair
Yield head and tail entity pairs from the cross product of all entities
Remove identity relation entity pairs
Remove entity pairs with labels that do not match any
of the specified relations in `self.entity_pair_labels`
"Obtain gold label, if existing"
Preserve context around the entities. Always include their in-between context.
Some sanity checks
Sanity check: Do not create a labeled span if one entity contains the other
Pre-compute non-leading head and tail tokens for entity masking
We can not use the plaintext of the head/tail span in the sentence as the mask/marker
since there may be multiple occurrences of the same entity mentioned in the sentence.
"Therefore, we use the span's position in the sentence."
Filter cases in which the distance between the two entities is too large
Remove excess tokens left and right of entity pair to make encoded sentence shorter
Create masked sentence
Add gold relation annotation as sentence label
"Using the sentence label instead of annotating a separate `Relation` object is easier to manage since,"
"during prediction, the forward pass does not need any knowledge about the entities in the sentence."
"If we sample missing splits, the encoded sentences that correspond to the same original sentences"
"may get distributed into different splits. For training purposes, this is always undesired."
Ensure that all sentences are encoded properly
Deal with the case where all sentences are encoded sentences
"mypy does not infer the type of ""sentences"" restricted by the if statement"
Deal with the case where all sentences are standard (non-encoded) sentences
"For each encoded sentence, transfer its prediction onto the original relation"
check if there is a label mismatch
"if the gold label is O and is correctly predicted as no label, do not print out as this clutters"
the output file with trivial predictions
auto-spawn on GPU if available
pad strings with whitespaces to longest sentence
cut up the input into chunks of max charlength = chunk_size
push each chunk through the RNN language model
concatenate all chunks to make final output
initial hidden state
get predicted weights
divide by temperature
"to prevent overflow problem with small temperature values, substract largest value from all"
this makes a vector in which the largest value is 0
compute word weights with exponential function
try sampling multinomial distribution for next character
print(word_idx)
input ids
push list of character IDs through model
the target is always the next character
use cross entropy loss to compare output of forward pass with targets
exponentiate cross-entropy loss to calculate perplexity
"""document_delimiter"" property may be missing in some older pre-trained models"
serialize the language models and the constructor arguments (but nothing else)
special handling for deserializing language models
re-initialize language model with constructor arguments
copy over state dictionary to self
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
"in their ""self.train()"" method)"
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
Transform input data into TARS format
"if there are no labels, return a random sample as negatives"
"otherwise, go through all labels"
make sure the probabilities always sum up to 1
get and embed all labels by making a Sentence object that contains only the label text
get each label embedding and scale between 0 and 1
compute similarity matrix
"the higher the similarity, the greater the chance that a label is"
sampled as negative example
make label dictionary if no Dictionary object is passed
prepare dictionary of tags (without B- I- prefixes and without UNK)
check if candidate_label_set is empty
make list if only one candidate label is passed
create label dictionary
note current task
create a temporary task
make zero shot predictions
switch to the pre-existing task
prepare TARS dictionary
initialize a bare-bones sequence tagger
transformer separator
Store task specific labels since TARS can handle multiple tasks
make a tars sentence where all labels are O by default
init new TARS classifier
set all task information
progress bar for verbosity
stop if all sentences are empty
always remove tags first
go through each sentence in the batch
always remove tags first
get the span and its label
determine whether tokens in this span already have a label
only add if all tokens have no label
make and add a corresponding predicted span
set indices so that no token can be tagged twice
clearing token embeddings to save memory
"all labels default to ""O"""
set gold token-level
set predicted token-level
now print labels in CoNLL format
prepare TARS dictionary
initialize a bare-bones sequence tagger
transformer separator
Store task specific labels since TARS can handle multiple tasks
get the serialized embeddings
remap state dict for models serialized with Flair <= 0.11.3
init new TARS classifier
set all task information
with torch.no_grad():
progress bar for verbosity
stop if all sentences are empty
always remove tags first
go through each sentence in the batch
always remove tags first
add all labels that according to TARS match the text and are above threshold
do not add labels below confidence threshold
only use label with the highest confidence if enforcing single-label predictions
add the label with the highest score even if below the threshold if force label is activated.
remove previously added labels and only add the best label
clearing token embeddings to save memory
set separator to concatenate three sentences
auto-spawn on GPU if available
set separator to concatenate two sentences
auto-spawn on GPU if available
"If the concatenated version of the text pair does not exist yet, create it"
pooling operation to get embeddings for entites
set embeddings
set relation and entity label types
"whether to use gold entity pairs, and whether to filter entity pairs by type"
filter entity pairs according to their tags if set
whether to encode characters and whether to use attention (attention can only be used if chars are encoded)
character dictionary for decoding and encoding
make sure <unk> is in dictionary for handling of unknown characters
add special symbols to dictionary if necessary and save respective indices
---- ENCODER ----
encoder character embeddings
encoder pre-trained embeddings
encoder RNN
additional encoder linear layer if bidirectional encoding
---- DECODER ----
decoder: linear layers to transform vectors to and from alphabet_size
when using attention we concatenate attention outcome and decoder hidden states
decoder RNN
loss and softmax
self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction
add additional columns for special symbols if necessary
initialize with dummy symbols
encode inputs
get labels (we assume each token has a lemma label)
get char indices for labels of sentence
"(batch_size, max_sequence_length) batch_size = #words in sentence,"
max_sequence_length = length of longest label of sentence + 1
get char embeddings
"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size"
take decoder input and initial hidden and pass through RNN
"if all encoder outputs are provided, use attention"
take convex combinations of encoder hidden states as new output using the computed attention coefficients
"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)"
get all tokens
encode input characters by sending them through RNN
get one-hots for characters and add special symbols / padding
determine length of each token
embed sentences
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
variable to store initial hidden states for decoder
encode input characters by sending them through RNN
test packing and padding
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
concatenate the final hidden states of the encoder. These will be projected to hidden_size of
decoder later with self.emb_to_hidden
mask out vectors that correspond to a dummy symbol (TODO: check attention masking)
use token embedding as initial hidden state for decoder
concatenate everything together and project to appropriate size for decoder
variable to store initial hidden states for decoder
encode input characters by sending them through RNN
note that we do not need to fill up with dummy symbols since we process each token seperately
embed character one-hots
send through encoder RNN (produces initial hidden for decoder)
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
project 2*hidden_size to hidden_size
concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder
later with self.emb_to_hidden
use token embedding as initial hidden state for decoder
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
concatenate everything together and project to appropriate size for decoder
"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)"
"create target vector (batch_size, max_label_seq_length + 1)"
filter empty sentences
max length of the predicted sequences
for printing
stop if all sentences are empty
remove previously predicted labels of this type
create list of tokens in batch
encode inputs
"create input for first pass (batch_size, 1, input_size), first letter is special character <S>"
sequence length is always set to one in prediction
option 1: greedy decoding
predictions
decode next character
pick top beam size many outputs with highest probabilities
option 2: beam search
out_probs = self.softmax(output_vectors).squeeze(1)
make sure no dummy symbol <> or start symbol <S> is predicted
pick top beam size many outputs with highest probabilities
"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1"
"leading_indices and probabilities have size (batch_size, beam_size)"
keep scores of beam_size many hypothesis for each token in the batch
stack all leading indices of all hypothesis and corresponding hidden states in two tensors
save sequences so far
keep track of how many hypothesis were completed for each token
"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)"
decode with log softmax
make sure no dummy symbol <> or start symbol <S> is predicted
"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside"
"if the sequence is already ended, do not record as candidate"
index of token in in list tokens_in_batch
print(token_number)
hypothesis score
TODO: remove token if number of completed hypothesis exceeds given value
set score of corresponding entry to -inf so it will not be expanded
get leading_indices for next expansion
find highest scoring hypothesis among beam_size*beam_size possible ones for each token
take beam_size many copies of scores vector and add scores of possible new extensions
"size (beam_size*batch_size, beam_size)"
print(hypothesis_scores)
"reshape to vector of size (batch_size, beam_size*beam_size),"
each row contains beam_size*beam_size scores of the new possible hypothesis
print(hypothesis_scores_per_token)
"choose beam_size best for each token - size (batch_size, beam_size)"
out of indices_per_token we now need to recompute the original indices of the hypothesis in
a list of length beam_size*batch_size
"where the first three inidices belong to the first token, the next three to the second token,"
and so on
with these indices we can compute the tensors for the next iteration
expand sequences with corresponding index
add log-probabilities to the scores
save new leading indices
save corresponding hidden states
it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations
in that case we append one of the final seuqences without end symbol to the final_candidates
get best final hypothesis for each token
get characters from index sequences and add predicted label to token
"Overwrites evaluate of parent class to remove the ""by class"" printout"
set separator to concatenate two sentences
init dropouts
auto-spawn on GPU if available
make a forward pass to produce embedded data points and labels
get their gold labels as a tensor
pass data points through network to get encoded data point tensor
decode
calculate the loss
get a tensor of data points
do dropout
"If the concatenated version of the text pair does not exist yet, create it"
add Model arguments
progress bar for verbosity
stop if all sentences are empty
clearing token embeddings to save memory
"read Dataset into data loader, if list of sentences passed, make Dataset first"
"if the classifier predicts BIO/BIOES span labels, the internal label dictionary must be computed"
fields in case this is a span-prediction problem
the label type
all parameters will be pushed internally to the specified device
special handling during training if this is a span prediction problem
internal variables
non-set tags are OUT tags
anything that is not OUT is IN
does this prediction start a new span?
B- and S- always start new spans
"if the predicted class changes, I- starts a new span"
"if the predicted class changes and S- was previous tag, start a new span"
if an existing span is ended (either by reaching O or starting a new span)
reset for-loop variables for new span
remember previous tag
"if there is a span at end of sentence, add it"
"all labels default to ""O"""
set gold token-level
set predicted token-level
now print labels in CoNLL format
print labels in CoNLL format
internal candidate lists of generator
load Zelda candidates if so passed
create candidate lists
"if lower casing is enabled, create candidate lists of lower cased versions"
create a new dictionary for lower cased mentions
go through each mention and its candidates
"check if backoff mention already seen. If so, add candidates. Else, create new entry."
set lowercased version as map
"only use span label type if there are predictions, otherwise search for output label type (training labels)"
remap state dict for models serialized with Flair <= 0.11.3
get the candidates
"during training, add the gold value as candidate"
----- Create the internal tag dictionary -----
span-labels need special encoding (BIO or BIOES)
the big question is whether the label dictionary should contain an UNK or not
"without UNK, we cannot evaluate on data that contains labels not seen in test"
"with UNK, the model learns less well if there are no UNK examples"
is this a span prediction problem?
----- Embeddings -----
----- Initial loss weights parameters -----
----- RNN specific parameters -----
----- Conditional Random Field parameters -----
"Previously trained models have been trained without an explicit CRF, thus it is required to check"
whether we are loading a model from state dict in order to skip or add START and STOP token
----- Dropout parameters -----
dropouts
remove word dropout if there is no contact over the sequence dimension.
----- Model layers -----
----- RNN layer -----
"If shared RNN provided, else create one for model"
Whether to train initial hidden state
final linear map to tag space
"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss"
"if using CRF, we also require a CRF and a Viterbi decoder"
"if there are no sentences, there is no loss"
forward pass to get scores
calculate loss given scores and labels
make a zero-padded tensor for the whole sentence
linear map to tag space
"Depending on whether we are using CRF or a linear layer, scores is either:"
"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF"
"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer"
spans need to be encoded as token-level predictions
all others are regular labels for each token
make sure it's a list
filter empty sentences
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
get features from forward propagation
remove previously predicted labels of this type
"if return_loss, get loss value"
make predictions
add predictions to Sentence
BIOES-labels need to be converted to spans
"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)"
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
core Flair models on Huggingface ModelHub
"Large NER models,"
Multilingual NER models
English POS models
Multilingual POS models
English SRL models
English chunking models
Language-specific NER models
Language-specific POS models
Historic German
English NER models
English SRL models
Danish models
German models
Arabic models
French models
Dutch models
Malayalam models
Portuguese models
Biomedical models
check if model name is a valid local file
"check if model key is remapped to HF key - if so, print out information"
get mapped name
"if not, check if model key is remapped to direct download location. If so, download model"
"for all other cases (not local file or special download location), use HF model hub"
## Demo: How to use in Flair
load tagger
make example sentence
predict NER tags
print sentence
print predicted NER spans
iterate over entities and print
Lazy import
Save model weight
Determine if model card already exists
Generate and save model card
Upload files
"all labels default to ""O"""
set gold token-level
set predicted token-level
now print labels in CoNLL format
print labels in CoNLL format
Dense + sparse retrieval
fetched from original repo to avoid download
"just in case we add: fuzzy search, Levenstein, ..."
"for now we always fall back to SapBERT,"
but we should train our own models at some point
NOTE: Avoid emtpy string if mentions are just punctutations (e.g. `-` or `(`)
NOTE: Avoid emtpy string if mentions are just punctuations (e.g. `-` or `(`)
Ab3P works on sentence-level and not on a single entity mention / name
- so we just apply the wrapped text pre-processing here (if configured)
NOTE: ensure correct similarity metric for pretrained model
empty cuda cache if device is a cuda device
"Sanity conversion: if flair.device was set as a string, convert to torch.device"
NOTE: This is a hacky workaround for the fact that
the `label_type`s in `Classifier.load('hunflair)` are
"'diseases', 'genes', 'species', 'chemical' instead of 'ner'."
We warn users once they need to update SequenceTagger model
See: https://github.com/flairNLP/flair/pull/3387
make sure sentences is a list of sentences
Make sure entity label types are represented as dict
Collect all entities based on entity type labels configuration
Preprocess entity mentions
Retrieve top-k concept / entity candidates
Add a label annotation for each candidate
load model by entity_type
check if we have a hybrid pre-trained model
the multi task model has several labels
Add metrics so they will be available to _publish_eval_result.
biomedical models
entity linker
print(match)
print(span)
auto-spawn on GPU if available
remap state dict for models serialized with Flair <= 0.11.3
English sentiment models
Communicative Functions Model
"If we sample missing splits, the encoded sentences that correspond to the same original sentences"
"may get distributed into different splits. For training purposes, this is always undesired."
Prepend the task description prompt to the sentence text
Make sure it's a list
Reconstruct all annotations from the original sentence (necessary for learning classifiers)
If all sentences are not augmented -> augment them
"mypy does not infer the type of ""sentences"" restricted by the if statement"
"mypy does not infer the type of ""sentences"" restricted by code above"
Compute prediction label type
make sure it's a list
"If all sentences are already augmented (i.e. compatible with this class), just forward the sentences"
"mypy does not infer the type of ""sentences"" restricted by the if statement"
Remove existing labels
Augment sentences - copy all annotation of the given tag type
Predict on augmented sentence and store it in an internal annotation layer / label
Append predicted labels to the original sentences
check if model name is a valid local file
check if model name is a pre-configured hf model
"scores_at_targets[range(features.shape[0]), lengths.values -1]"
Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices
"Initially, get scores from <start> tag to all other tags"
"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp"
"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep"
Create a tensor to hold accumulated sequence scores at each current tag
Create a tensor to hold back-pointers
"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag"
"Let pads be the <end> tag index, since that was the last tag in the decoded sequence"
"We add scores at current timestep to scores accumulated up to previous timestep, and"
choose the previous timestep that corresponds to the max. accumulated score for each current timestep
"If sentence is over, add transition to STOP-tag"
Decode/trace best path backwards
Sanity check
remove start-tag and backscore to stop-tag
Max + Softmax to get confidence score for predicted label and append label to each token
"Transitions are used in the following way: transitions[to, from]."
"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag"
to START-tag and from STOP-tag to any other tag to -10000.
"if necessary, make batch_steps"
break up the batch into slices of size
mini_batch_chunk_size
"if training also uses dev/train data, include in training set"
evaluation and monitoring
sampling and shuffling
evaluation and monitoring
when and what to save
logging parameters
acceleration
plugins
activate annealing plugin
call self.train_custom with all parameters (minus the ones specific to the AnnealingPlugin)
training parameters
evaluation and monitoring
sampling and shuffling
evaluation and monitoring
when and what to save
logging parameters
acceleration
plugins
annealing logic
training parameters
evaluation and monitoring
sampling and shuffling
evaluation and monitoring
when and what to save
logging parameters
acceleration
plugins
training parameters
evaluation and monitoring
sampling and shuffling
evaluation and monitoring
when and what to save
logging parameters
acceleration
plugins
Create output folder
=== START BLOCK: ACTIVATE PLUGINS === #
We first activate all optional plugins. These take care of optional functionality such as various
logging techniques and checkpointing
log file plugin
loss file plugin
plugin for writing weights
plugin for checkpointing
=== END BLOCK: ACTIVATE PLUGINS === #
derive parameters the function was called with (or defaults)
initialize model card with these parameters
Prepare training data and get dataset size
"determine what splits (train, dev, test) to evaluate"
determine how to determine best model and whether to save it
instantiate the optimizer
initialize sampler if provided
init with default values if only class is provided
set dataset to sample from
configure special behavior to use multiple GPUs
Guard against each process initializing corpus differently due to e.g. different random seeds
this field stores the names of all dynamic embeddings in the model (determined after first forward pass)
Sanity checks
"Sanity conversion: if flair.device was set as a string, convert to torch.device"
-- AmpPlugin -> wraps with AMP
-- AnnealingPlugin -> initialize schedulers (requires instantiated optimizer)
At any point you can hit Ctrl + C to break out of training early.
"- SchedulerPlugin -> load state for anneal_with_restarts, batch_growth_annealing, logic for early stopping"
- LossFilePlugin -> get the current epoch for loss file logging
"if shuffle_first_epoch==False, the first epoch is not shuffled"
log infos on training progress every `log_modulo` batches
process mini-batches
zero the gradients on the model and optimizer
forward and backward for batch
forward pass
We need to __call__ ddp_model() because this triggers hooks that sync gradients.
But that calls forward rather than forward_loss. So we patch forward to redirect
to forward_loss. Then undo the patch in case forward_loss itself calls forward.
identify dynamic embeddings (always deleted) on first sentence
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
do the optimizer step
DDP averages across processes but we want the sum
- SchedulerPlugin -> do the scheduler step if one-cycle or linear decay
- WeightExtractorPlugin -> extracts weights
- CheckpointPlugin -> executes save_model_each_k_epochs
- SchedulerPlugin -> log bad epochs
Determine if this is the best model or if we need to anneal
log results
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
use DEV split to determine if this is the best model so far
"if not using DEV score, determine best model using train loss"
- LossFilePlugin -> somehow prints all relevant metrics
- AnnealPlugin -> scheduler step
- SWAPlugin -> restores SGD weights from SWA
"if we do not use dev data for model selection, save final model"
TensorboardLogger -> closes writer
test best model if test data is present
get and return the final test score of best model
MetricHistoryPlugin -> stores the loss history in return_values
"Store return values, as they will be erased by reset_training_attributes"
get a random sample of training sentences
create a model card for this model with Flair and PyTorch version
record Transformers version if library is loaded
remember the training parameters
special rule for Path variables to make sure models can be deserialized on other OS
classes are only serialized as names
"TextDataset returns a list. valid and test are only one file,"
so return the first element
cast string to Path
error message if the validation dataset is too small
Shuffle training files randomly after serially iterating
through corpus one
"iterate through training data, starting at"
self.split (for checkpointing)
off by one for printing
go into train mode
reset variables
not really sure what this does
do the forward pass in the model
try to predict the targets
Backward
`clip_grad_norm` helps prevent the exploding gradient
problem in RNNs / LSTMs.
We detach the hidden state from how it was
previously produced.
"If we didn't, the model would try backpropagating"
all the way to start of the dataset.
explicitly remove loss to clear up memory
#########################################################
Save the model if the validation loss is the best we've
seen so far.
#########################################################
print info
#########################################################
##############################################################################
final testing
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
"no need to check for MetricName, as __add__ of other would be called in this case"
"This flag tracks, whether an event is currently being processed (otherwise it is added to the queue)"
instantiate plugin
"Reset the flag, since an exception event might be dispatched"
"If there is no **kw argument in the callback, check if any of the passed kw args is not accepted by"
the callback
go through all attributes
get attribute hook events (may raise an AttributeError)
register function as a hook
"Decorator was used with parentheses, but no args"
Decorator was used with args (strings specifiying the events)
Decorator was used without args
path to store the model
special annealing modes
determine the min learning rate
"minimize training loss if training with dev data, else maximize dev score"
instantiate the scheduler
stop training if learning rate becomes too small
reload last best model if annealing with restarts is enabled
calculate warmup steps
skip if no optimization has happened.
saves the model with full vocab as checkpoints etc were created with reduced vocab.
TODO: check if metric is in tracked metrics
prepare loss logging file and set up header
set up all metrics to collect
set up headers
name: HEADER
Add all potentially relevant metrics. If a metric is not published
"after the first epoch (when the header is written), the column is"
removed at that point.
initialize the first log line
record is a list of scalars
output log file
remove columns where no value was found on the first epoch (could be != 1 if training was resumed)
make headers on epoch 1
write header
adjust alert level
"the default model for ELMo is the 'original' model, which is very large"
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
put on Cuda if available
embed a dummy sentence to determine embedding_length
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
"multilingual forward (English, German, French, Italian, Dutch, Polish)"
"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
Dutch forward
Dutch backward
Swedish forward
Swedish backward
French forward
French backward
Czech forward
Czech backward
Portuguese forward
Portuguese backward
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
"if only one sentence is passed, convert to list of sentence"
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
"legacy pickle-like saving for image embeddings, as implementation details are not obvious"
"legacy pickle-like loading for image embeddings, as implementation details are not obvious"
"<cls> token initially set to 1/D, so it attends to all image features equally"
add positional encodings
reshape the pixels into the sequence
layer norm after convolution and positional encodings
add <cls> token
"transformer requires input in the shape [h*w+1, b, d]"
the output is an embedding of <cls> token
this parameter is fixed
optional fine-tuning on top of embedding layer
"if only one sentence is passed, convert to list of sentence"
"if only one sentence is passed, convert to list of sentence"
bidirectional RNN on top of embedding layer
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push through RNN
after-RNN dropout
extract embeddings from RNN
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
serialize the language models and the constructor arguments (but nothing else)
re-initialize language model with constructor arguments
special handling for deserializing language models
copy over state dictionary to self
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
"in their ""self.train()"" method)"
IMPORTANT: add embeddings as torch modules
iterate over sentences
"if its a forward LM, take last state"
"convert to plain strings, embedded in a list for the encode function"
CNN
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push CNN
after-CNN dropout
extract embeddings from CNN
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
"if only one sentence is passed, convert to list of sentence"
Expose base classses
Expose document embedding classes
Expose image embedding classes
Expose legacy embedding classes
Expose token embedding classes
in some cases we need to insert zero vectors for tokens without embedding.
sum embeddings for each token
calculate the mean of subtokens
Create a mask for valid tokens based on token_lengths
padding
remove special markup
check if special tokens exist to circumvent error message
iterate over subtokens and reconstruct tokens
remove special markup
check if reconstructed token is special begin token ([CLS] or similar)
some BERT tokenizers somehow omit words - in such cases skip to next token
"we cannot handle unk_tokens perfectly, so let's assume that one unk_token corresponds to one token."
if tokens are unaccounted for
check if all tokens were matched to subtokens
The layoutlm tokenizer doesn't handle ocr themselves
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
"cannot run `.encode` if ocr boxes are required, assume"
"transformers returns the ""added_tokens.json"" even if it doesn't create it"
"transformers returns the ""added_tokens.json"" even if it doesn't create it"
in case of doubt: token embedding has higher priority than document embedding
random check some tokens to save performance.
Models such as FNet do not have an attention_mask
set language IDs for XLM-style transformers
"word_ids is only supported for fast rust tokenizers. Some models like ""xlm-mlm-ende-1024"" do not have"
"a fast tokenizer implementation, hence we need to fall back to our own reconstruction of word_ids."
set context if not set already
flair specific pre-tokenization
fields to store left and right context
expand context only if context_length is set
"if context_dropout is set, randomly deactivate left context during training"
"if context_dropout is set, randomly deactivate right context during training"
"if use_context_separator is set, add a [FLERT] token"
return expanded sentence and context length information
"onnx prepares numpy arrays, no mather if it runs on gpu or cpu, the input is on cpu first."
temporary fix to disable tokenizer parallelism warning
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
do not print transformer warnings as these are confusing in this case
load tokenizer and transformer model
load tokenizer from inmemory zip-file
if model is quantized by BitsAndBytes this will fail
add adapters for finetuning
peft_config: PeftConfig
model name
embedding parameters
send mini-token through to check how many layers the model has
return length
"If we use a context separator, add a new special token"
"most models have an initial BOS token, except for XLNet, T5 and GPT2"
"when initializing, embeddings are in eval mode by default"
in case of doubt: token embedding has higher priority than document embedding
in case of doubt: token embedding has higher priority than document embedding
legacy TransformerDocumentEmbedding
legacy TransformerTokenEmbedding
legacy Flair <= 0.12
legacy Flair <= 0.7
legacy TransformerTokenEmbedding
Legacy TransformerDocumentEmbedding
legacy TransformerTokenEmbedding
legacy TransformerDocumentEmbedding
some models like the tars model somehow lost this information.
copy values from new embedding
do not switch the attention implementation upon reload.
those parameters are only from the super class and will be recreated in the constructor.
cls first pooling can be done without recreating sentence hidden states
make the tuple a tensor; makes working with it easier.
"for multimodal models like layoutlmv3, we truncate the image embeddings as they are only used via attention"
only use layers that will be outputted
this parameter is fixed
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
make compatible with serialized models
gensim version 4
gensim version 3
"if no embedding is set, the vocab and embedding length is required"
GLOVE embeddings
TURIAN embeddings
KOMNINOS embeddings
pubmed embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
twitter embeddings
two-letter language code wiki embeddings
two-letter language code wiki embeddings
two-letter language code crawl embeddings
"this is required to force the module on the cpu,"
"if a parent module is put to gpu, the _apply is called to each sub_module"
self.to(..) actually sets the device properly
this ignores the get_cached_vec method when loading older versions
it is needed for compatibility reasons
gensim version 4
gensim version 3
"when loading the old versions from pickle, the embeddings might not be added as pytorch module."
"we do this delayed, when the weights are collected (e.g. for saving), as doing this earlier might"
lead to issues while loading (trying to load weights that weren't stored as python weights and therefore
not finding them)
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
multilingual models
English models
Arabic
Bulgarian
Czech
Danish
German
Spanish
Basque
Persian
Finnish
French
Hebrew
Hindi
Croatian
Indonesian
Italian
Japanese
Malayalam
Dutch
Norwegian
Polish
Portuguese
Pubmed
Slovenian
Swedish
Tamil
Spanish clinical
CLEF HIPE Shared task
Amharic
Ukrainian
load model if in pretrained model map
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
CLEF HIPE models are lowercased
embeddings are static if we don't do finetuning
embed a dummy sentence to determine embedding_length
set to eval mode
"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout"
gradients are enable if fine-tuning is enabled
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
offset mode that extracts at whitespace after last character
offset mode that extracts at last character
make compatible with old models
use the character language model embeddings as basis
length is twice the original character LM embedding length
these fields are for the embedding memory
whether to add only capitalized words to memory (faster runtime and lower memory consumption)
we re-compute embeddings dynamically at each epoch
set the memory method
memory is wiped each time we do a training run
"if we keep a pooling, it needs to be updated continuously"
update embedding
check token.text is empty or not
set aggregation operation
add embeddings after updating
model architecture
model architecture
"""pl"","
download if necessary
load the model
"this is required to force the module on the cpu,"
"if a parent module is put to gpu, the _apply is called to each sub_module"
self.to(..) actually sets the device properly
"when loading the old versions from pickle, the embeddings might not be added as pytorch module."
"we do this delayed, when the weights are collected (e.g. for saving), as doing this earlier might"
lead to issues while loading (trying to load weights that weren't stored as python weights and therefore
not finding them)
old embeddings do not have a torch-embedding and therefore do not store the weights in the saved torch state_dict
"however they are already initialized rightfully, so we just set the state dict from our current state dict"
GLOVE embeddings
no need to recreate as NILCEmbeddings
read in test file if exists
read in dev file if exists
"find train, dev and test files if not specified"
Add tags for each annotated span
Remove leading and trailing whitespaces from annotated spans
Search start and end token index for current span
If end index is not found set to last token
Throw error if indices are not valid
Add metadatas for sentence
Currently all Jsonl Datasets are stored in Memory
get train data
read in test file if exists
read in dev file if exists
"find train, dev and test files if not specified"
special key for space after
special key for feature columns
special key for dependency head id
"store either Sentence objects in memory, or only file offsets"
"most data sets have the token text in the first column, if not, pass 'text' as column"
determine encoding of text file
identify which columns are spans and which are word-level
now load all sentences
skip first line if to selected
option 1: keep Sentence objects in memory
pointer to previous
parse next sentence
quit if last sentence reached
skip banned sentences
set previous and next sentence for context
append parsed sentence to list in memory
option 2: keep source data in memory
"read lines for next sentence, but don't parse"
quit if last sentence reached
append raw lines for each sentence
we make a distinction between word-level tags and span-level tags
read first sentence to determine which columns are span-labels
skip first line if to selected
check the first 5 sentences
go through all annotations and identify word- and span-level annotations
- if a column has at least one BIES we know it's a Span label
"- if a column has at least one tag that is not BIOES, we know it's a Token label"
- problem cases are columns for which we see only O - in this case we default to Span
skip assigned columns
the space after key is always word-levels
"if at least one token has a BIES, we know it's a span label"
"if at least one token has a label other than BIOES, we know it's a token label"
all remaining columns that are not word-level are span-level
for column in self.word_level_tag_columns:
"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")"
"if sentence ends, break"
parse comments if possible
"otherwise, this line is a token. parse and add to sentence"
check if this sentence is a document boundary
add span labels
discard tags from tokens that are not added to the sentence
parse relations if they are set
head and tail span indices are 1-indexed and end index is inclusive
parse comments such as '# id cd27886d-6895-4d02-a8df-e5fa763fa88f	domain=de-orcas'
"to set the metadata ""domain"" to ""de-orcas"""
get fields from line
get head_id if exists (only in dependency parses)
initialize token
go through all columns
'feats' and 'misc' column should be split into different fields
special handling for whitespace after
add each other feature as label-value pair
get the task name (e.g. 'ner')
get the label value
add label
remap regular tag names
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
set sentence context using partials TODO: pointer to dataset is really inefficient
use all domains
iter over all domains / sources and create target files
The conll representation of coref spans allows spans to
"overlap. If spans end or begin at the same word, they are"
"separated by a ""|""."
The span begins at this word.
The span begins and ends at this word (single word span).
"The span is starting, so we record the index of the word."
"The span for this id is ending, but didn't start at this word."
Retrieve the start index from the document state and
add the span to the clusters for this id.
strip all bracketing information to
get the actual propbank label.
Entering into a span for a particular semantic role label.
We append the label and set the current span for this annotation.
"If there's no '(' token, but the current_span_label is not None,"
then we are inside a span.
We're outside a span.
"Exiting a span, so we reset the current span label for this annotation."
The words in the sentence.
The pos tags of the words in the sentence.
the pieces of the parse tree.
The lemmatised form of the words in the sentence which
have SRL or word sense information.
The FrameNet ID of the predicate.
"The sense of the word, if available."
"The current speaker, if available."
"Cluster id -> List of (start_index, end_index) spans."
Cluster id -> List of start_indices which are open for this id.
Replace brackets in text and pos tags
with a different token for parse trees.
only keep ')' if there are nested brackets with nothing in them.
There are some bad annotations in the CONLL data.
"They contain no information, so to make this explicit,"
we just set the parse piece to be None which will result
in the overall parse tree being None.
"If this is the first word in the sentence, create"
empty lists to collect the NER and SRL BIO labels.
"We can't do this upfront, because we don't know how many"
"components we are collecting, as a sentence can have"
variable numbers of SRL frames.
Create variables representing the current label for each label
sequence we are collecting.
"If any annotation marks this word as a verb predicate,"
we need to record its index. This also has the side effect
of ordering the verbal predicates by their location in the
"sentence, automatically aligning them with the annotations."
"this would not be reached if parse_pieces contained None, hence the cast"
Non-empty line. Collect the annotation.
Collect any stragglers or files which might not
have the '#end document' format for the end of the file.
this dataset name
check if data there
column format
this dataset name
check if data there
column format
this dataset name
download data if necessary
download files if not present locally
we need to slightly modify the original files by adding some new lines after document separators
column format
this dataset name
download data if necessary
Set the base path for the dataset
Define column format
Define dataset name
Define data folder path
"Check if the train data file exists, otherwise download and prepare the dataset"
Download and prepare the dataset
Initialize the parent class with the specified parameters
"Check if the line is a change, delete or add command (like 17721c17703,17705 or 5728d5727)"
Append the previous change block to the changes list
Start a new change block
"Capture original lines (those marked with ""<"")"
"Capture new lines (those marked with "">"")"
Append the last change block to the changes list
Apply each change in reverse order (important to avoid index shift issues)
"Determine the type of the change: `c` for change, `d` for delete, `a` for add"
"Example command: 17721c17703,17705"
Example command: 5728d5727
"Example command: 1000a1001,1002"
Write the modified content to the output file
Strip whitespace to check if the line is empty
Write the first token followed by a newline if the line is not empty
Write an empty line if the line is empty
Strip the leading '[TOKEN]\t' from the annotation
Create a temporary directory
Check the contents of the temporary directory
Extract only the tokens from the original CoNLL03 files
Apply the downloaded patch files to apply our token modifications (e.g. line breaks)
Merge the updated token files with the CleanCoNLL annotations
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
Remove CoNLL-U meta information in the last column
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
entity_mapping
this dataset name
download data if necessary
data validation
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
download files if not present locallys
column format
this dataset name
default dataset folder is the cache root
# download zip
merge the files in one as the zip is containing multiples files
column format
this dataset name
download data if necessary
"unzip the downloaded repo and merge the train, dev and test datasets"
column format
this dataset name
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
check if data there
create folder
download dataset
column format
this dataset name
download and parse data if necessary
create train test dev if not exist
column format
this dataset name
If the extracted corpus file is not yet present in dir
download zip if necessary
"extracted corpus is not present , so unpacking it."
column format
this dataset name
download zip
unpacking the zip
merge the files in one as the zip is containing multiples files
column format
this dataset name
"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)"
download files if not present locally
we need to modify the original files by adding new lines after after the end of each sentence
if only one language is given
column format
this dataset name
"use all languages if explicitly set to ""all"""
download data if necessary
initialize comlumncorpus and add it to list
column format
this dataset name
column format
this dataset name
column format
this dataset name
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
this list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
get google drive id from list
download from google drive
unzip
transform data into required format
"the processed dataset has the additional ending ""_new"""
remove the unprocessed dataset
initialize comlumncorpus and add it to list
if no languages are given as argument all languages used in XTREME will be loaded
if only one language is given
column format
this dataset name
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
This list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
download from HU Server
unzip
transform data into required format
initialize comlumncorpus and add it to list
if only one language is given
column format
this dataset name
download data if necessary
initialize comlumncorpus and add it to list
download data if necessary
unpack and write out in CoNLL column-like format
column format
this dataset name
download data if necessary
data is not in IOB2 format. Thus we transform it to IOB2
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
column format
this dataset name
rename according to train - test - dev - convention
column format
this dataset name
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
Add missing newline after header
Workaround for empty tokens
"Add ""real"" document marker"
Dataset split mapping
v2.0 only adds new language and splits for AJMC dataset
Special document marker for sample splits in AJMC dataset
column format
this dataset name
download data if necessary
column format
this dataset name
"No test data is available, so do not shrink dev data for shared task preparation!"
create dataset files from index and train/test splits
news date is usually in 3rd or 4th sentence of each article
"generate NoiseBench dataset variants, given CleanCoNLL, noisy label files and index file"
"os.makedirs(os.path.join('data','noisebench'), exist_ok=True)"
copy test set
if only one language is given
column format
this dataset name
"use all languages if explicitly set to ""all"""
download data if necessary
initialize comlumncorpus and add it to list
Get original version
Add sentence boundary marker
"Only allowed classes in course setting are: PER, LOC, ORG and MISC."
"All other NEs are normalized to O, except EVENT and WOA are normalized to MISC (cf. Table 3 of paper)."
this dataset name
one name can map to multiple concepts
NOTE: EntityLinkingDictionary are lazy-loaded from a preprocessed file.
Use this class to load into memory all candidates
"if identifier == ""MESH:D013749"":"
# This MeSH ID was used by MeSH when this chemical was part of the MeSH controlled vocabulary.
continue
parse line
this dataset name
default dataset folder is the cache root
download and parse data if necessary
paths to train and test splits
init corpus
this dataset name
default dataset folder is the cache root
download and parse data if necessary
iterate over all html files
"get rid of html syntax, we only need the text"
between all documents we write a separator symbol
skip empty strings
"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)"
"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention"
sentence splitting and tokenization
iterate through all annotations and add to corresponding tokens
find sentence to which annotation belongs
position within corresponding sentence
set annotation for tokens of entity mention
write to out-file in column format
"in case something goes wrong, delete the dataset and raise error"
this dataset name
download and parse data if necessary
from qwikidata.linked_data_interface import get_entity_dict_from_api
generate qid wikiname dictionaries
merge dictionaries
ignore first line
commented and empty lines
read all Q-IDs
ignore first line
request
this dataset name
we use the wikiids in the data instead of directly utilizing the wikipedia urls.
like this we can quickly check if the corresponding page exists
if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi
delete unprocessed file
collect all wikiids
create the dictionary
request
this dataset name
names of raw text documents
open output_file
iterate through all documents
split sentences and tokenize
iterate through all annotations and add to corresponding tokens
find sentence to which annotation belongs
position within corresponding sentence
set annotation for tokens of entity mention
write to out file
annotation from one annotator or two agreeing annotators
this dataset name
download and parse data if necessary
this dataset name
download and parse data if necessary
First parse the post titles
Keep track of how many and which entity mentions does a given post title have
Check if the current post title has an entity link and parse accordingly
Post titles with entity mentions (if any) are handled via this function
Then parse the comments
"Iterate over the comments.tsv file, until the end is reached"
"Keep track of the current comment thread and its corresponding key, on which the annotations are matched."
Each comment thread is handled as one 'document'.
Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.
This if-condition is needed to handle this problem.
"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure"
"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above"
"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle."
"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,"
and not just single letters into single rows.
If there are annotated entity mentions for given post title or a comment thread
"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence"
Write the token with a corresponding tag to file
"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed"
"If a comment thread or a post title has no entity link, all tokens are assigned the O tag"
Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized
"incorrectly, in order to keep the desired format (empty line as a sentence separator)."
"Thrown when the second check above happens, but the last token of a sentence is reached."
"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below."
"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS"
Check if further annotations belong to the current post title or comment thread as well
Stop when the end of an annotation file is reached
Check if further annotations belong to the current sentence as well
"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)"
Docstart
if there is more than one word in the chunk we write each in a separate line
print(chunks)
empty line after each sentence
convert the file to CoNLL
this dataset name
"check if data there, if not, download the data"
create folder
download data
transform data into column format if necessary
if no filenames are specified we use all the data
"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled"
also we remove 'raganato_ALL' from filenames in case its in the list
generate the test file
make column file and save to data_folder
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
TODO: Adapt this following: https://github.com/flairNLP/flair/pull/3146
+1 assumes the title and abstract will be joined by a space.
"We need a unique identifier for this entity, so build it from the document id and entity id"
The user can provide a callable that returns the database name.
some entities are not linked and
some entities are linked to multiple normalized ids
passages must not overlap and spans must cover the entire document
entities
parse db ids
Some of the entities have a off-by-one error. Correct these annotations!
"passage offsets/lengths do not connect, recalculate them for this schema."
this dataset name
download data if necessary
if True:
write CoNLL-U Plus header
"Some special cases (e.g., missing spaces before entity marker)"
necessary if text should be whitespace tokenizeable
Handle case where tail may occur before the head
this dataset name
write CoNLL-U Plus header
this dataset name
TODO: change data source to original CoNLL04 -- this dataset has span formatting errors
download data if necessary
write CoNLL-U Plus header
The span has ended.
We are entering a new span; reset indices
and active tag to new span.
We're inside a span.
Last token might have been a part of a valid span.
this dataset name
write CoNLL-U Plus header
"for source_file_path, target_filename in zip(source_file_paths, target_filenames):"
"with zip_file.open(source_file_path, mode=""r"") as source_file:"
target_file_path = Path(data_folder) / target_filename
"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:"
# write CoNLL-U Plus header
"target_file.write(""# global.columns = id form ner\n"")"
for example in json.load(source_file):
token_list = self._tacred_example_to_token_list(example)
target_file.write(token_list.serialize())
check if first tag row is already occupied
"if first tag row is occupied, use second tag row"
hardcoded mapping TODO: perhaps find nicer solution
remap regular tag names
else skip to position in file where sentence begins
set sentence context using partials TODO: pointer to dataset is really inefficient
read in dev file if exists
read in test file if exists
the url is copied from https://huggingface.co/datasets/darentang/sroie/blob/main/sroie.py#L44
"find train, dev and test files if not specified"
use test_file to create test split if available
use dev_file to create test split if available
"if data point contains black-listed label, do not use"
first check if valid sentence
"if so, add to indices"
"find train, dev and test files if not specified"
variables
different handling of in_memory data than streaming data
"most data sets have the token text in the first column, if not, pass 'text' as column"
test if format is OK
test if at least one label given
make sentence from text (and filter for length)
"if a pair column is defined, make a sentence pair object"
noinspection PyDefaultArgument
dataset name includes the split size
default dataset folder is the cache root
download data if necessary
download each of the 28 splits
create dataset directory if necessary
download senteval datasets if necessary und unzip
this dataset name
download data if necessary
this dataset name
download data if necessary
download data from same source as in huggingface's implementations
read label order
"Original labels are [1, 2, 3, 4] -> ['World', 'Sports', 'Business', 'Sci/Tech']"
"Re-map to [0, 1, 2, 3]."
this dataset name
download data if necessary
handle labels file
handle data file
Create flair compatible labels
"by default, map point score to POSITIVE / NEGATIVE values"
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file from CSV
create test.txt file from CSV
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create train dev and test files in fasttext format
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
convert to FastText format
download data if necessary
"if data is not downloaded yet, download it"
get the zip file
move original .tsv files to another folder
create train and dev splits in fasttext format
create eval_dataset file with no labels
download zip archive
unpack file in datasets directory (zip archive contains a directory named SST-2)
this dataset name
default dataset folder is the cache root
download data if necessary
download datasets if necessary
create dataset directory if necessary
create correctly formated txt files
multiple labels are possible
this dataset name
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
download data if necessary
this dataset name
download data if necessary
create a separate directory for different tasks
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
check if dataset is supported
set file names
set file names
download and unzip in file structure if necessary
instantiate corpus
"find train, dev and test files if not specified"
"create DataPairDataset for train, test and dev file, if they are given"
stop if file does not exist
create a DataPair object from strings
"if in_memory is True we return a datapair, otherwise we create one from the lists of strings"
"find train, dev, and test files if not specified"
"create DataTripleDataset for train, test, and dev files, if they are given"
stop if the file does not exist
create a DataTriple object from strings
"if in_memory is True we return a DataTriple, otherwise we create one from the lists of strings"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"reorder dev datasets to have same columns as in train set: 8, 9, and 11"
dev sets include 5 different annotations but we will only keep the gold label
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get test and dev sets
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data not downloaded yet, download it"
get the zip file
"the downloaded files have json format, we transform them to tsv"
Function to transform JSON file to tsv for Recognizing Textual Entailment Data
remove json file
Uses dynamic programming approach to calculate maximum independent set in interval graph
with sum of all entity lengths as secondary key
calculate offset without current text
because we stick all passages of a document together
TODO For split entities we also annotate everything inbetween which might be a bad idea?
Try to fix incorrect annotations
print(
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
)
Ignore empty lines or relation annotations
FIX annotation of whitespaces (necessary for PDR)
Add task description for multi-task learning
One token may contain multiple entities -> deque all of them
column format
this dataset name
Create tokenization-dependent CONLL files. This is necessary to prevent
from caching issues (e.g. loading the same corpus with different sentence splitters)
column format
this dataset name
column format
this dataset name
Edge case: last token starts a new entity
Last document in file
column format
this dataset name
column format
this dataset name
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
Edge case: last token starts a new entity
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
column format
this dataset name
Read texts
Read annotations
column format
this dataset name
column format
this dataset name
column format
this dataset name
We need to apply a patch to correct the original training file
Articles title
Article abstract
Entity annotations
column format
this dataset name
Edge case: last token starts a new entity
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
Incomplete article
Invalid XML syntax
column format
this dataset name
column format
this dataset name
if len(mid) != 3:
continue
Try to fix entity offsets
column format
this dataset name
There is still one illegal annotation in the file ..
column format
this dataset name
"Abstract first, title second to prevent issues with sentence splitting"
column format
this dataset name
column format
this dataset name
column format
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
column format
this dataset name
"Filter for specific entity types, by default no entities will be filtered"
Get original HUNER splits to retrieve a list of all document ids contained in V2
train and dev split of V2 will be train in V4
test split of V2 will be dev in V4
New documents in V4 will become test documents
column format
this dataset name
column format
this dataset name
default dataset folder is the cache root
column format
build dataset name and full huggingface reference name
Download data if necessary
"Some datasets in BigBio only have train or test splits, not both"
"If only test split, assign it to train split"
"If only train split, sample other from it (sample_missing_splits=True)"
Not every dataset has a dev / validation set!
Perform type mapping if necessary
return None
TODO: Add entity type mapping for all remaining bigbio datasets not in HunFlair?
"""simple_chemical"": ""chemical"",  # BioNLP ST 2013 CG"
"""cancer"": ""disease"",  # BioNLP ST 2013 CG"
"""gene_or_gene_product"": ""gene"",  # BioNLP ST 2013 CG"
"""gene"": ""gene"",  # NLM Gene"
"""chemical"": ""chemical"",  # NLM Chem"
"""cellline"": ""cell_line"",  # Cell Finder"
"""species"": ""species"",  # Cell Finder"
"""protein"": ""gene"",  # BioID"
"Collect all texts of the document, each passage will be"
a text in our internal format
Sort passages by start offset
Transform all entity annotations into internal format
Find the passage of the entity (necessary for offset adaption)
Adapt entity offsets according to passage offsets
FIXME: This is just for debugging purposes
passage_text = id_to_text[passage_id]
doc_text = passage_text[entity_offset[0] : entity_offset[1]]
"mention_text = entity[""text""][0]"
if doc_text != mention_text:
"print(f""Annotation error ({document['document_id']}) - Doc: {doc_text} vs. Mention: {mention_text}"")"
Get element in the middle
Is the mention with the passage offsets?
"If element is smaller than mid, then it can only"
be present in left subarray
Else the element can only be present in right subarray
TODO whether cell or cell line is the correct tag
TODO whether cell or cell line is the correct tag
Special case for ProGene: We need to use the split_0_train and split_0_test splits
as they are currently provided in BigBio
cache Feidegger config file
cache Feidegger images
replace image URL with local cached file
append Sentence-Image data point
cast to list if necessary
cast to list if necessary
"first, check if pymongo is installed"
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
Expose base classses
Expose all biomedical data sets used for the evaluation of BioBERT
-
-
-
-
Expose all biomedical data sets using the HUNER splits
Expose all biomedical data sets
Expose all document classification datasets
word sense disambiguation
Expose all entity linking datasets
Expose all relation extraction datasets
universal proposition banks
keyphrase detection datasets
other NER datasets
standard NER datasets
Expose all sequence labeling datasets
Expose all text-image datasets
Expose all text-text datasets
Expose all treebanks
"find train, dev and test files if not specified"
get train data
get test data
get dev data
option 1: read only sentence boundaries as offset positions
option 2: keep everything in memory
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
current token ID
handling for the awful UD multiword format
end of sentence
comments or ellipsis
if token is a multi-word
normal single-word tokens
"if we don't split multiwords, skip over component words"
add token
add morphological tags
derive whitespace logic for multiwords
"if multi-word equals component tokens, there should be no whitespace"
go through all tokens in subword and set whitespace_after information
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
this dataset name
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
"finally, print model card for information"
Note: Multi-GPU can affect corpus loading
This code will run multiple times -- each GPU gets its own process and each process runs this code. We need to
"ensure that the corpus has the same elements and order on all processes, despite sampling. We do that by using"
the same seed on all processes.
Note: Multi-GPU can affect choice of batch size.
"In order to compare batch updates fairly between single and multi-GPU training, we should:"
1) Step the optimizer after the same number of examples to achieve com
2) Process the same number of examples in each forward pass
"e.g. Suppose your machine has 2 GPUs. If multi_gpu=False, the first gpu will process 32 examples, then the"
"first gpu will process another 32 examples, then the optimizer will step. If multi_gpu=True, each gpu will"
"process 32 examples at the same time, then the optimizer will step."
noqa: INP001
-- Project information -----------------------------------------------------
"The full version, including alpha/beta/rc tags"
use smv_current_version as the git url
-- General configuration ---------------------------------------------------
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Napoleon settings
Whitelist pattern for tags (set to None to ignore all tags)
Whitelist pattern for branches (set to None to ignore all branches)
Whitelist pattern for remotes (set to None to use local branches only)
Pattern for released versions
Format for versioned output directories inside the build directory
Determines whether remote or local git branches/tags are preferred if their output dirs conflict
test corpus
create a TARS classifier
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
test corpus
create a TARS classifier
switch to a new task (TARS can do multiple tasks so you must define one)
initialize the text classifier trainer
start the training
"With end symbol, without start symbol, padding in front"
"Without end symbol, with start symbol, padding in back"
"Without end symbol, without start symbol, padding in front"
initialize trainer
initialize trainer
initialize trainer
clean up directory
clean up directory
example sentence
set 4 labels for 2 tokens ('love' is tagged twice)
check if there are three POS labels with correct text and values
check if there are is one SENTIMENT label with correct text and values
check if all tokens are correctly labeled
remove the pos label from the last word
there should be 2 POS labels left
now remove all pos tags
set 3 labels for 2 spans (HU is tagged twice)
check if there are three labels with correct text and values
check if there are two spans with correct text and values
"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
should be only one NER label left
and only one NER span
set 3 labels for 2 spans (HU is tagged twice with different tags)
check if there are three labels with correct text and values
check if there are two spans with correct text and values
"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
should be only one NER label left
and only one NER span
but there is also one orgtype span and label
and only one NER span
let's add the NER tag back
check if there are three labels with correct text and values
check if there are two spans with correct text and values
now remove all NER tags
set 3 labels for 2 spans (HU is tagged twice with different tags)
create two relation label
there should be two relation labels
there should be one syntactic labels
"there should be two relations, one with two and one with one label"
example sentence
add another topic label
example sentence
has sentiment value
has 4 part of speech tags
has 1 NER tag
should be in total 6 labels
example sentence
add two NER labels
get the four labels
check that only two of the respective data points are equal
make a sentence and some right context
TODO: is this desirable? Or should two sentences with same text be considered same objects?
Initializing a Sentence this way assumes that there is a space after each token
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
now exclude -DOCSTART- sentences
now load whole documents as sentences
ban each boundary but set each sentence to be independent
"get training, test and dev data"
"get training, test and dev data"
load column dataset with one entry
load column dataset with two entries
load column dataset with three entries
"get training, test and dev data"
"get training, test and dev data"
check if Token labels are correct
"get training, test and dev data"
check if Token labels for frames are correct
"get training, test and dev data"
"get training, test and dev data"
get two corpora as one
"get training, test and dev data for full English UD corpus from web"
clean up data directory
"assert [token.get_tag(""head"").value for token in sent1.tokens] == ["
"""2"","
"""0"","
"""4"","
"""2"","
"""2"","
"""2"","
]
This test only covers basic universal dependencies datasets.
"For example, multi-word tokens or the ""deps"" column sentence annotations are not supported yet."
"Here, we use the default token annotation fields."
This test covers the complete HIPE 2022 dataset.
https://github.com/hipe-eval/HIPE-2022-data
"Includes variant with document separator, and all versions of the dataset."
"We have manually checked, that these numbers are correct:"
"+1 offset, because of missing EOS marker at EOD"
Test data for v2.1 release
This test covers the complete ICDAR Europeana corpus:
https://github.com/stefan-it/historic-domain-adaptation-icdar
"This test covers the complete MasakhaNER dataset, including support for v1 and v2."
This test covers the NERMuD dataset. Official stats can be found here:
https://github.com/dhfbk/KIND/tree/main/evalita-2023
Number of instances per dataset split are taken from https://huggingface.co/datasets/elenanereiss/german-ler
This test covers the complete MasakhaPOS dataset.
"See MobIE paper (https://aclanthology.org/2021.konvens-1.22/), table 2"
--- Embeddings that are shared by both models --- #
--- Task 1: Sentiment Analysis (5-class) --- #
Define corpus and model
-- Task 2: Binary Sentiment Analysis on Customer Reviews -- #
Define corpus and model
-- Define mapping (which tagger should train on which model) -- #
-- Create model trainer and train -- #
NOTE: Avoid emtpy string if mentions are just punctutations (e.g. `-` or `(`)
clean up file
get features from forward propagation
reverse sort all sequences by their length
remove previously predicted labels of this type
no need for label_dict
"pretrained_model = ""tars-ner""  # disabled due to too much space requirements."
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
Intel ----founded_by---> Gordon Moore
Intel ----founded_by---> Robert Noyce
"Ground truth is a set of tuples of (<Sentence Text>, <Relation Label Values>)"
Check sentence masking and relation label annotation on
"training, validation and test dataset (in this test the splits are the same)"
"Entity pair permutations of: ""Larry Page and Sergey Brin founded Google ."""
"Entity pair permutations of: ""Microsoft was founded by Bill Gates ."""
"Entity pair permutations of: ""Konrad Zuse was born in Berlin on 22 June 1910 ."""
"Entity pair permutations of: ""Joseph Weizenbaum , a professor at MIT , was born in Berlin , Germany ."""
"Entity pair permutations of: ""The German - American computer scientist Joseph Weizenbaum ( 8 January 1923 - 5 March 2008 ) was born in Berlin ."""
This sentence is only included if we transform the corpus with cross augmentation
"""The German - American computer scientist Joseph Weizenbaum ( 8 January 1923 - 5 March 2008 ) was born in Berlin ."""
"""The German - American computer scientist Joseph Weizenbaum ( 8 January 1923 - 5 March 2008 ) was born in Berlin ."""
Simulate training epoch
Simulate training batch
"pretrained_model = ""tars-base""  # disabled due to too much space requirements."
Ensure this is an example that predicts no classes in multilabel
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
ensure that the prepared tensors is what we expect
use a SequenceTagger to save and reload the embedding in the manner it is supposed to work
previous and next sentence as context
test expansion for sentence without context
test expansion for with previous and next as context
test expansion if first sentence is document boundary
test expansion if we don't use context
"apparently the precision is not that high on cuda, hence the absolute tolerance needs to be higher."
dummy model with embeddings
save the dummy and load it again
check that context_length and use_context_separator is the same for both
mmap seems to be much more memory efficient
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
Extract all the contents of zip file in current directory
use model name as subfolder
Lazy import
output information
Extract all the contents of zip file in current directory
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
load_big_file is a workaround byhttps://github.com/highway11git
to load models on some Mac/Windows setups
see https://github.com/zalandoresearch/flair/issues/351
first determine the distribution of classes in the dataset
weight for each sample
Create blocks
shuffle the blocks
concatenate the shuffled blocks
Create blocks
shuffle the blocks
concatenate the shuffled blocks
increment for last token in sentence if not followed by whitespace
this is the default init size of a lmdb database for embeddings
get db filename from embedding name
"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
SequenceTagger
TextClassifier
get db filename from embedding name
if embedding database already exists
"otherwise, push embedding to database"
if embedding database already exists
open the database in read mode
we need to set self.k
create and load the database in write mode
"no idea why, but we need to close and reopen the environment to avoid"
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
when opening new transaction !
init dictionaries
"in order to deal with unknown tokens, add <unk>"
set 'add_unk' if the dictionary was created with a version of Flair older than 0.9
set 'add_unk' depending on whether <unk> is a key
"if one embedding name, directly return it"
"if multiple embedding names, concatenate them"
First we remove any existing labels for this PartOfSentence in self.sentence
labels also need to be deleted at Sentence object
delete labels at object itself
The Token is a special _PartOfSentence in that it may be initialized without a Sentence.
"therefore, labels get added only to the Sentence if it exists"
The Token is a special _PartOfSentence in that it may be initialized without a Sentence.
"Therefore, labels get set only to the Sentence if it exists"
"check if the span already exists. If so, return it"
else make a new span
"check if the relation already exists. If so, return it"
else make a new relation
private field for all known spans
the tokenizer used for this sentence
some sentences represent a document boundary (but most do not)
internal variables to denote position inside dataset
"if text is passed, instantiate sentence with tokens (words)"
determine token positions and whitespace_after flag
the last token has no whitespace after
log a warning if the dataset is empty
data with zero-width characters cannot be handled
set token idx and sentence
append token to sentence
register token annotations on sentence
move sentence embeddings to device
also move token embeddings to device
clear token embeddings
infer whitespace after field
"if sentence has no tokens, return empty string"
"otherwise, return concatenation of tokens with the correct offsets"
The sentence's start position is not propagated to its tokens.
"Therefore, we need to add the sentence's start position to its last token's end position, including whitespaces."
No character at the corresponding code point: remove it
"if no label if specified, return all labels"
"if the label type exists in the Sentence, return it"
return empty list if none of the above
labels also need to be deleted at all tokens
labels also need to be deleted at all known spans
remove spans without labels
delete labels at object itself
set name
abort if no data is provided
sample test data from train if none is provided
sample dev data from train if none is provided
set train dev and test data
find out empty sentence indices
create subset of non-empty sentence indices
find out empty sentence indices
create subset of non-empty sentence indices
"first, determine the datapoint type by going through dataset until first label is found"
count all label types per sentence
go through all labels of label_type and count values
special handling for Token-level annotations. Add all untagged as 'O' label
"if an unk threshold is set, UNK all label values below this threshold"
sample randomly from a label distribution according to the probabilities defined by the noise transition matrix
replace the old label with the new one
keep track of the old (clean) label using another label type category
keep track of how many labels in total are flipped
sample randomly from a label distribution according to the probabilities defined by the desired noise share
replace the old label with the new one
keep track of the old (clean) label using another label type category
keep track of how many labels in total are flipped
"add a dummy ""O"" to close final prediction"
return complex list
internal variables
non-set tags are OUT tags
anything that is not OUT is IN
does this prediction start a new span?
B- and S- always start new spans
"if the predicted class changes, I- starts a new span"
"if the predicted class changes and S- was previous tag, start a new span"
if an existing span is ended (either by reaching O or starting a new span)
determine score and value
append to result list
reset for-loop variables for new span
remember previous tag
global variable: cache_root
Get the device from the environment variable
global variable: device
"No need for correctness checks, torch is doing it"
global variable: version
global variable: arrow symbol
dummy return to fulfill trainer.train() needs
print(vec)
Attach optimizer
"convert `metrics` to float, in case it's a zero-dim Tensor"
if memory mode option 'none' delete everything
"if dynamic embedding keys not passed, identify them automatically"
always delete dynamic embeddings
"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)"
optional metric space decoder if prototypes have different length than embedding
create initial prototypes for all classes (all initial prototypes are a vector of all 1s)
"if set, create initial prototypes from normal distribution"
"if set, use a radius"
all parameters will be pushed internally to the specified device
decode embeddings into prototype space
"if unlabeled distance is set, mask out loss to unlabeled class prototype"
verbalize BIOES labels
"if label is not BIOES, use label itself"
Always include the name of the Model class for which the state dict holds
"this seems to just return model name, not a model with that name"
"write out a ""model card"" if one is set"
save model
"if this class is abstract, go through all inheriting classes and try to fetch and load the model"
get all non-abstract subclasses
"try to fetch the model for each subclass. if fetching is possible, load model and return it"
"skip any invalid loadings, e.g. not found on HuggingFace hub"
"if the model cannot be fetched, load as a file"
try to get model class from state
"older (flair 11.3 and below) models do not contain cls information. In this case, try all subclasses"
"skip any invalid loadings, e.g. not found on HuggingFace hub"
"if this class is not abstract, fetch the model and load it"
"make sure <unk> is contained in gold_label_dictionary, if given"
"read Dataset into data loader, if list of sentences passed, make Dataset first"
loss calculation
variables for printing
variables for computing scores
remove any previously predicted labels
predict for batch
get the gold labels
add to all_predicted_values
make printout lines
convert true and predicted values to two span-aligned lists
delete excluded labels if exclude_labels is given
"if after excluding labels, no label is left, ignore the datapoint"
write all_predicted_values to out_file if set
make the evaluation dictionary
check if this is a multi-label problem
compute numbers by formatting true and predicted such that Scikit-Learn can use them
multi-label problems require a multi-hot vector for each true and predicted label
single-label problems can do with a single index for each true and predicted label
"now, calculate evaluation numbers"
there is at least one gold label or one prediction (default)
compute accuracy separately as it is not always in classification_report (e.g. when micro avg exists)
"if there is only one label, then ""micro avg"" = ""macro avg"""
"The ""micro avg"" appears only in the classification report if no prediction is possible."
"Otherwise, it is identical to the ""macro avg"". In this case, we add it to the report."
"Create and populate score object for logging with all evaluation values, plus the loss"
issue error and default all evaluation numbers to 0.
check if there is a label mismatch
print info
set the embeddings
initialize the label dictionary
initialize the decoder
set up multi-label logic
init dropouts
loss weights and loss function
Initialize the weight tensor
set up gradient reversal if so specified
embed sentences
get a tensor of data points
do dropout
make a forward pass to produce embedded data points and labels
get the data points for which to predict labels
get their gold labels as a tensor
pass data points through network to get encoded data point tensor
decode
an optional masking step (no masking in most cases)
calculate the loss
filter empty sentences
reverse sort all sequences by their length
progress bar for verbosity
filter data points in batch
stop if all sentences are empty
pass data points through network and decode
if anything could possibly be predicted
remove previously predicted labels of this type
filter data points that have labels outside of dictionary
add DefaultClassifier arguments
add variables of DefaultClassifier
Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23
Get projected 1st dimension
Compute bilinear form
Arcosh
Project the input data to n+1 dimensions
"The first dimension, is recomputed in the distance module"
header for 'weights.txt'
"determine the column index of loss, f-score and accuracy for"
"train, dev and test split"
then get all relevant values from the tsv
then get all relevant values from the tsv
plot i
save plots
save plots
plt.show()
save plot
auto-spawn on GPU if available
progress bar for verbosity
stop if all sentences are empty
clearing token embeddings to save memory
"read Dataset into data loader, if list of sentences passed, make Dataset first"
TODO: not saving lines yet
TODO: This closely shadows the RelationExtractor name. Maybe we need a better name here.
- MaskedRelationClassifier ?
This depends if this relation classification architecture should replace or offer as an alternative.
Set label type and prepare label dictionary
Initialize super default classifier
Add the special tokens from the encoding strategy
"Auto-spawn on GPU, if available"
Only use entities labelled with the specified labels for each label type
Only use entities above the specified threshold
Use a dictionary to find gold relation annotations for a given entity pair
Yield head and tail entity pairs from the cross product of all entities
Remove identity relation entity pairs
Remove entity pairs with labels that do not match any
of the specified relations in `self.entity_pair_labels`
"Obtain gold label, if existing"
Some sanity checks
Pre-compute non-leading head and tail tokens for entity masking
We can not use the plaintext of the head/tail span in the sentence as the mask/marker
since there may be multiple occurrences of the same entity mentioned in the sentence.
"Therefore, we use the span's position in the sentence."
Create masked sentence
Add gold relation annotation as sentence label
"Using the sentence label instead of annotating a separate `Relation` object is easier to manage since,"
"during prediction, the forward pass does not need any knowledge about the entities in the sentence."
"If we sample missing splits, the encoded sentences that correspond to the same original sentences"
"may get distributed into different splits. For training purposes, this is always undesired."
Ensure that all sentences are encoded properly
Deal with the case where all sentences are encoded sentences
"mypy does not infer the type of ""sentences"" restricted by the if statement"
Deal with the case where all sentences are standard (non-encoded) sentences
"For each encoded sentence, transfer its prediction onto the original relation"
auto-spawn on GPU if available
pad strings with whitespaces to longest sentence
cut up the input into chunks of max charlength = chunk_size
push each chunk through the RNN language model
concatenate all chunks to make final output
initial hidden state
get predicted weights
divide by temperature
"to prevent overflow problem with small temperature values, substract largest value from all"
this makes a vector in which the largest value is 0
compute word weights with exponential function
try sampling multinomial distribution for next character
print(word_idx)
input ids
push list of character IDs through model
the target is always the next character
use cross entropy loss to compare output of forward pass with targets
exponentiate cross-entropy loss to calculate perplexity
"""document_delimiter"" property may be missing in some older pre-trained models"
serialize the language models and the constructor arguments (but nothing else)
special handling for deserializing language models
re-initialize language model with constructor arguments
copy over state dictionary to self
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
"in their ""self.train()"" method)"
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
Transform input data into TARS format
"if there are no labels, return a random sample as negatives"
"otherwise, go through all labels"
make sure the probabilities always sum up to 1
get and embed all labels by making a Sentence object that contains only the label text
get each label embedding and scale between 0 and 1
compute similarity matrix
"the higher the similarity, the greater the chance that a label is"
sampled as negative example
make label dictionary if no Dictionary object is passed
prepare dictionary of tags (without B- I- prefixes and without UNK)
check if candidate_label_set is empty
make list if only one candidate label is passed
create label dictionary
note current task
create a temporary task
make zero shot predictions
switch to the pre-existing task
prepare TARS dictionary
initialize a bare-bones sequence tagger
transformer separator
Store task specific labels since TARS can handle multiple tasks
make a tars sentence where all labels are O by default
init new TARS classifier
set all task information
progress bar for verbosity
stop if all sentences are empty
always remove tags first
go through each sentence in the batch
always remove tags first
get the span and its label
determine whether tokens in this span already have a label
only add if all tokens have no label
make and add a corresponding predicted span
set indices so that no token can be tagged twice
clearing token embeddings to save memory
"all labels default to ""O"""
set gold token-level
set predicted token-level
now print labels in CoNLL format
prepare TARS dictionary
initialize a bare-bones sequence tagger
transformer separator
Store task specific labels since TARS can handle multiple tasks
get the serialized embeddings
remap state dict for models serialized with Flair <= 0.11.3
init new TARS classifier
set all task information
with torch.no_grad():
progress bar for verbosity
stop if all sentences are empty
always remove tags first
go through each sentence in the batch
always remove tags first
add all labels that according to TARS match the text and are above threshold
do not add labels below confidence threshold
only use label with the highest confidence if enforcing single-label predictions
add the label with the highest score even if below the threshold if force label is activated.
remove previously added labels and only add the best label
clearing token embeddings to save memory
set separator to concatenate three sentences
auto-spawn on GPU if available
set separator to concatenate two sentences
auto-spawn on GPU if available
"If the concatenated version of the text pair does not exist yet, create it"
pooling operation to get embeddings for entites
set embeddings
set relation and entity label types
"whether to use gold entity pairs, and whether to filter entity pairs by type"
filter entity pairs according to their tags if set
whether to encode characters and whether to use attention (attention can only be used if chars are encoded)
character dictionary for decoding and encoding
make sure <unk> is in dictionary for handling of unknown characters
add special symbols to dictionary if necessary and save respective indices
---- ENCODER ----
encoder character embeddings
encoder pre-trained embeddings
encoder RNN
additional encoder linear layer if bidirectional encoding
---- DECODER ----
decoder: linear layers to transform vectors to and from alphabet_size
when using attention we concatenate attention outcome and decoder hidden states
decoder RNN
loss and softmax
self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction
add additional columns for special symbols if necessary
initialize with dummy symbols
encode inputs
get labels (we assume each token has a lemma label)
get char indices for labels of sentence
"(batch_size, max_sequence_length) batch_size = #words in sentence,"
max_sequence_length = length of longest label of sentence + 1
get char embeddings
"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size"
take decoder input and initial hidden and pass through RNN
"if all encoder outputs are provided, use attention"
take convex combinations of encoder hidden states as new output using the computed attention coefficients
"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)"
get all tokens
encode input characters by sending them through RNN
get one-hots for characters and add special symbols / padding
determine length of each token
embed sentences
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
variable to store initial hidden states for decoder
encode input characters by sending them through RNN
test packing and padding
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
concatenate the final hidden states of the encoder. These will be projected to hidden_size of
decoder later with self.emb_to_hidden
mask out vectors that correspond to a dummy symbol (TODO: check attention masking)
use token embedding as initial hidden state for decoder
concatenate everything together and project to appropriate size for decoder
variable to store initial hidden states for decoder
encode input characters by sending them through RNN
note that we do not need to fill up with dummy symbols since we process each token seperately
embed character one-hots
send through encoder RNN (produces initial hidden for decoder)
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
project 2*hidden_size to hidden_size
concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder
later with self.emb_to_hidden
use token embedding as initial hidden state for decoder
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
concatenate everything together and project to appropriate size for decoder
"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)"
"create target vector (batch_size, max_label_seq_length + 1)"
filter empty sentences
max length of the predicted sequences
for printing
stop if all sentences are empty
remove previously predicted labels of this type
create list of tokens in batch
encode inputs
"create input for first pass (batch_size, 1, input_size), first letter is special character <S>"
sequence length is always set to one in prediction
option 1: greedy decoding
predictions
decode next character
pick top beam size many outputs with highest probabilities
option 2: beam search
out_probs = self.softmax(output_vectors).squeeze(1)
make sure no dummy symbol <> or start symbol <S> is predicted
pick top beam size many outputs with highest probabilities
"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1"
"leading_indices and probabilities have size (batch_size, beam_size)"
keep scores of beam_size many hypothesis for each token in the batch
stack all leading indices of all hypothesis and corresponding hidden states in two tensors
save sequences so far
keep track of how many hypothesis were completed for each token
"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)"
decode with log softmax
make sure no dummy symbol <> or start symbol <S> is predicted
"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside"
"if the sequence is already ended, do not record as candidate"
index of token in in list tokens_in_batch
print(token_number)
hypothesis score
TODO: remove token if number of completed hypothesis exceeds given value
set score of corresponding entry to -inf so it will not be expanded
get leading_indices for next expansion
find highest scoring hypothesis among beam_size*beam_size possible ones for each token
take beam_size many copies of scores vector and add scores of possible new extensions
"size (beam_size*batch_size, beam_size)"
print(hypothesis_scores)
"reshape to vector of size (batch_size, beam_size*beam_size),"
each row contains beam_size*beam_size scores of the new possible hypothesis
print(hypothesis_scores_per_token)
"choose beam_size best for each token - size (batch_size, beam_size)"
out of indices_per_token we now need to recompute the original indices of the hypothesis in
a list of length beam_size*batch_size
"where the first three inidices belong to the first token, the next three to the second token,"
and so on
with these indices we can compute the tensors for the next iteration
expand sequences with corresponding index
add log-probabilities to the scores
save new leading indices
save corresponding hidden states
it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations
in that case we append one of the final seuqences without end symbol to the final_candidates
get best final hypothesis for each token
get characters from index sequences and add predicted label to token
"Overwrites evaluate of parent class to remove the ""by class"" printout"
set separator to concatenate two sentences
init dropouts
auto-spawn on GPU if available
make a forward pass to produce embedded data points and labels
get their gold labels as a tensor
pass data points through network to get encoded data point tensor
decode
calculate the loss
get a tensor of data points
do dropout
"If the concatenated version of the text pair does not exist yet, create it"
add Model arguments
progress bar for verbosity
stop if all sentences are empty
clearing token embeddings to save memory
"read Dataset into data loader, if list of sentences passed, make Dataset first"
"if the classifier predicts BIO/BIOES span labels, the internal label dictionary must be computed"
fields in case this is a span-prediction problem
the label type
all parameters will be pushed internally to the specified device
special handling during training if this is a span prediction problem
internal variables
non-set tags are OUT tags
anything that is not OUT is IN
does this prediction start a new span?
B- and S- always start new spans
"if the predicted class changes, I- starts a new span"
"if the predicted class changes and S- was previous tag, start a new span"
if an existing span is ended (either by reaching O or starting a new span)
reset for-loop variables for new span
remember previous tag
"if there is a span at end of sentence, add it"
"all labels default to ""O"""
set gold token-level
set predicted token-level
now print labels in CoNLL format
print labels in CoNLL format
internal candidate lists of generator
load Zelda candidates if so passed
create candidate lists
"if lower casing is enabled, create candidate lists of lower cased versions"
create a new dictionary for lower cased mentions
go through each mention and its candidates
"check if backoff mention already seen. If so, add candidates. Else, create new entry."
set lowercased version as map
"only use span label type if there are predictions, otherwise search for output label type (training labels)"
remap state dict for models serialized with Flair <= 0.11.3
get the candidates
"during training, add the gold value as candidate"
----- Create the internal tag dictionary -----
span-labels need special encoding (BIO or BIOES)
the big question is whether the label dictionary should contain an UNK or not
"without UNK, we cannot evaluate on data that contains labels not seen in test"
"with UNK, the model learns less well if there are no UNK examples"
is this a span prediction problem?
----- Embeddings -----
----- Initial loss weights parameters -----
----- RNN specific parameters -----
----- Conditional Random Field parameters -----
"Previously trained models have been trained without an explicit CRF, thus it is required to check"
whether we are loading a model from state dict in order to skip or add START and STOP token
----- Dropout parameters -----
dropouts
remove word dropout if there is no contact over the sequence dimension.
----- Model layers -----
----- RNN layer -----
"If shared RNN provided, else create one for model"
Whether to train initial hidden state
final linear map to tag space
"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss"
"if using CRF, we also require a CRF and a Viterbi decoder"
"if there are no sentences, there is no loss"
forward pass to get scores
calculate loss given scores and labels
make a zero-padded tensor for the whole sentence
linear map to tag space
"Depending on whether we are using CRF or a linear layer, scores is either:"
"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF"
"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer"
spans need to be encoded as token-level predictions
all others are regular labels for each token
make sure it's a list
filter empty sentences
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
get features from forward propagation
remove previously predicted labels of this type
"if return_loss, get loss value"
make predictions
add predictions to Sentence
BIOES-labels need to be converted to spans
"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)"
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
core Flair models on Huggingface ModelHub
"Large NER models,"
Multilingual NER models
English POS models
Multilingual POS models
English SRL models
English chunking models
Language-specific NER models
Language-specific POS models
Historic German
English NER models
English SRL models
Danish models
German models
Arabic models
French models
Dutch models
Malayalam models
Portuguese models
Biomedical models
check if model name is a valid local file
"check if model key is remapped to HF key - if so, print out information"
get mapped name
"if not, check if model key is remapped to direct download location. If so, download model"
"for all other cases (not local file or special download location), use HF model hub"
## Demo: How to use in Flair
load tagger
make example sentence
predict NER tags
print sentence
print predicted NER spans
iterate over entities and print
Lazy import
Save model weight
Determine if model card already exists
Generate and save model card
Upload files
"all labels default to ""O"""
set gold token-level
set predicted token-level
now print labels in CoNLL format
print labels in CoNLL format
Dense + sparse retrieval
fetched from original repo to avoid download
"just in case we add: fuzzy search, Levenstein, ..."
"for now we always fall back to SapBERT,"
but we should train our own models at some point
NOTE: Avoid emtpy string if mentions are just punctutations (e.g. `-` or `(`)
NOTE: Avoid emtpy string if mentions are just punctuations (e.g. `-` or `(`)
Ab3P works on sentence-level and not on a single entity mention / name
- so we just apply the wrapped text pre-processing here (if configured)
NOTE: ensure correct similarity metric for pretrained model
empty cuda cache if device is a cuda device
"Sanity conversion: if flair.device was set as a string, convert to torch.device"
NOTE: This is a hacky workaround for the fact that
the `label_type`s in `Classifier.load('hunflair)` are
"'diseases', 'genes', 'species', 'chemical' instead of 'ner'."
We warn users once they need to update SequenceTagger model
See: https://github.com/flairNLP/flair/pull/3387
make sure sentences is a list of sentences
Make sure entity label types are represented as dict
Collect all entities based on entity type labels configuration
Preprocess entity mentions
Retrieve top-k concept / entity candidates
Add a label annotation for each candidate
load model by entity_type
check if we have a hybrid pre-trained model
the multi task model has several labels
biomedical models
entity linker
auto-spawn on GPU if available
remap state dict for models serialized with Flair <= 0.11.3
English sentiment models
Communicative Functions Model
"If we sample missing splits, the encoded sentences that correspond to the same original sentences"
"may get distributed into different splits. For training purposes, this is always undesired."
Prepend the task description prompt to the sentence text
Make sure it's a list
Reconstruct all annotations from the original sentence (necessary for learning classifiers)
If all sentences are not augmented -> augment them
"mypy does not infer the type of ""sentences"" restricted by the if statement"
"mypy does not infer the type of ""sentences"" restricted by code above"
Compute prediction label type
make sure it's a list
"If all sentences are already augmented (i.e. compatible with this class), just forward the sentences"
"mypy does not infer the type of ""sentences"" restricted by the if statement"
Remove existing labels
Augment sentences - copy all annotation of the given tag type
Predict on augmented sentence and store it in an internal annotation layer / label
Append predicted labels to the original sentences
check if model name is a valid local file
check if model name is a pre-configured hf model
"scores_at_targets[range(features.shape[0]), lengths.values -1]"
Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices
"Initially, get scores from <start> tag to all other tags"
"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp"
"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep"
Create a tensor to hold accumulated sequence scores at each current tag
Create a tensor to hold back-pointers
"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag"
"Let pads be the <end> tag index, since that was the last tag in the decoded sequence"
"We add scores at current timestep to scores accumulated up to previous timestep, and"
choose the previous timestep that corresponds to the max. accumulated score for each current timestep
"If sentence is over, add transition to STOP-tag"
Decode/trace best path backwards
Sanity check
remove start-tag and backscore to stop-tag
Max + Softmax to get confidence score for predicted label and append label to each token
"Transitions are used in the following way: transitions[to, from]."
"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag"
to START-tag and from STOP-tag to any other tag to -10000.
"if necessary, make batch_steps"
break up the batch into slices of size
mini_batch_chunk_size
"if training also uses dev/train data, include in training set"
evaluation and monitoring
sampling and shuffling
evaluation and monitoring
when and what to save
logging parameters
acceleration
plugins
activate annealing plugin
call self.train_custom with all parameters (minus the ones specific to the AnnealingPlugin)
training parameters
evaluation and monitoring
sampling and shuffling
evaluation and monitoring
when and what to save
logging parameters
acceleration
plugins
annealing logic
training parameters
evaluation and monitoring
sampling and shuffling
evaluation and monitoring
when and what to save
logging parameters
acceleration
plugins
training parameters
evaluation and monitoring
sampling and shuffling
evaluation and monitoring
when and what to save
logging parameters
acceleration
plugins
Create output folder
=== START BLOCK: ACTIVATE PLUGINS === #
We first activate all optional plugins. These take care of optional functionality such as various
logging techniques and checkpointing
log file plugin
loss file plugin
plugin for writing weights
plugin for checkpointing
=== END BLOCK: ACTIVATE PLUGINS === #
derive parameters the function was called with (or defaults)
initialize model card with these parameters
Prepare training data and get dataset size
"determine what splits (train, dev, test) to evaluate"
determine how to determine best model and whether to save it
instantiate the optimizer
initialize sampler if provided
init with default values if only class is provided
set dataset to sample from
configure special behavior to use multiple GPUs
Guard against each process initializing corpus differently due to e.g. different random seeds
this field stores the names of all dynamic embeddings in the model (determined after first forward pass)
Sanity checks
"Sanity conversion: if flair.device was set as a string, convert to torch.device"
-- AmpPlugin -> wraps with AMP
-- AnnealingPlugin -> initialize schedulers (requires instantiated optimizer)
At any point you can hit Ctrl + C to break out of training early.
"- SchedulerPlugin -> load state for anneal_with_restarts, batch_growth_annealing, logic for early stopping"
- LossFilePlugin -> get the current epoch for loss file logging
"if shuffle_first_epoch==False, the first epoch is not shuffled"
log infos on training progress every `log_modulo` batches
process mini-batches
zero the gradients on the model and optimizer
forward and backward for batch
forward pass
We need to __call__ ddp_model() because this triggers hooks that sync gradients.
But that calls forward rather than forward_loss. So we patch forward to redirect
to forward_loss. Then undo the patch in case forward_loss itself calls forward.
identify dynamic embeddings (always deleted) on first sentence
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
do the optimizer step
DDP averages across processes but we want the sum
- SchedulerPlugin -> do the scheduler step if one-cycle or linear decay
- WeightExtractorPlugin -> extracts weights
- CheckpointPlugin -> executes save_model_each_k_epochs
- SchedulerPlugin -> log bad epochs
Determine if this is the best model or if we need to anneal
log results
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
use DEV split to determine if this is the best model so far
"if not using DEV score, determine best model using train loss"
- LossFilePlugin -> somehow prints all relevant metrics
- AnnealPlugin -> scheduler step
- SWAPlugin -> restores SGD weights from SWA
"if we do not use dev data for model selection, save final model"
TensorboardLogger -> closes writer
test best model if test data is present
get and return the final test score of best model
MetricHistoryPlugin -> stores the loss history in return_values
"Store return values, as they will be erased by reset_training_attributes"
get a random sample of training sentences
create a model card for this model with Flair and PyTorch version
record Transformers version if library is loaded
remember all parameters used in train() call
"TextDataset returns a list. valid and test are only one file,"
so return the first element
cast string to Path
error message if the validation dataset is too small
Shuffle training files randomly after serially iterating
through corpus one
"iterate through training data, starting at"
self.split (for checkpointing)
off by one for printing
go into train mode
reset variables
not really sure what this does
do the forward pass in the model
try to predict the targets
Backward
`clip_grad_norm` helps prevent the exploding gradient
problem in RNNs / LSTMs.
We detach the hidden state from how it was
previously produced.
"If we didn't, the model would try backpropagating"
all the way to start of the dataset.
explicitly remove loss to clear up memory
#########################################################
Save the model if the validation loss is the best we've
seen so far.
#########################################################
print info
#########################################################
##############################################################################
final testing
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
"no need to check for MetricName, as __add__ of other would be called in this case"
"This flag tracks, whether an event is currently being processed (otherwise it is added to the queue)"
instantiate plugin
"Reset the flag, since an exception event might be dispatched"
"If there is no **kw argument in the callback, check if any of the passed kw args is not accepted by"
the callback
go through all attributes
get attribute hook events (may raise an AttributeError)
register function as a hook
"Decorator was used with parentheses, but no args"
Decorator was used with args (strings specifiying the events)
Decorator was used without args
path to store the model
special annealing modes
determine the min learning rate
"minimize training loss if training with dev data, else maximize dev score"
instantiate the scheduler
stop training if learning rate becomes too small
reload last best model if annealing with restarts is enabled
calculate warmup steps
skip if no optimization has happened.
saves the model with full vocab as checkpoints etc were created with reduced vocab.
TODO: check if metric is in tracked metrics
prepare loss logging file and set up header
set up all metrics to collect
set up headers
name: HEADER
Add all potentially relevant metrics. If a metric is not published
"after the first epoch (when the header is written), the column is"
removed at that point.
initialize the first log line
record is a list of scalars
output log file
remove columns where no value was found on the first epoch (could be != 1 if training was resumed)
make headers on epoch 1
write header
adjust alert level
"the default model for ELMo is the 'original' model, which is very large"
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
put on Cuda if available
embed a dummy sentence to determine embedding_length
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
"multilingual forward (English, German, French, Italian, Dutch, Polish)"
"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
Dutch forward
Dutch backward
Swedish forward
Swedish backward
French forward
French backward
Czech forward
Czech backward
Portuguese forward
Portuguese backward
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
"if only one sentence is passed, convert to list of sentence"
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
"legacy pickle-like saving for image embeddings, as implementation details are not obvious"
"legacy pickle-like loading for image embeddings, as implementation details are not obvious"
"<cls> token initially set to 1/D, so it attends to all image features equally"
add positional encodings
reshape the pixels into the sequence
layer norm after convolution and positional encodings
add <cls> token
"transformer requires input in the shape [h*w+1, b, d]"
the output is an embedding of <cls> token
this parameter is fixed
optional fine-tuning on top of embedding layer
"if only one sentence is passed, convert to list of sentence"
"if only one sentence is passed, convert to list of sentence"
bidirectional RNN on top of embedding layer
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push through RNN
after-RNN dropout
extract embeddings from RNN
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
serialize the language models and the constructor arguments (but nothing else)
re-initialize language model with constructor arguments
special handling for deserializing language models
copy over state dictionary to self
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
"in their ""self.train()"" method)"
IMPORTANT: add embeddings as torch modules
iterate over sentences
"if its a forward LM, take last state"
"convert to plain strings, embedded in a list for the encode function"
CNN
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push CNN
after-CNN dropout
extract embeddings from CNN
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
"if only one sentence is passed, convert to list of sentence"
Expose base classses
Expose document embedding classes
Expose image embedding classes
Expose legacy embedding classes
Expose token embedding classes
in some cases we need to insert zero vectors for tokens without embedding.
sum embeddings for each token
calculate the mean of subtokens
Create a mask for valid tokens based on token_lengths
padding
remove special markup
check if special tokens exist to circumvent error message
iterate over subtokens and reconstruct tokens
remove special markup
check if reconstructed token is special begin token ([CLS] or similar)
some BERT tokenizers somehow omit words - in such cases skip to next token
"we cannot handle unk_tokens perfectly, so let's assume that one unk_token corresponds to one token."
if tokens are unaccounted for
check if all tokens were matched to subtokens
The layoutlm tokenizer doesn't handle ocr themselves
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
"cannot run `.encode` if ocr boxes are required, assume"
"transformers returns the ""added_tokens.json"" even if it doesn't create it"
"transformers returns the ""added_tokens.json"" even if it doesn't create it"
in case of doubt: token embedding has higher priority than document embedding
random check some tokens to save performance.
Models such as FNet do not have an attention_mask
set language IDs for XLM-style transformers
"word_ids is only supported for fast rust tokenizers. Some models like ""xlm-mlm-ende-1024"" do not have"
"a fast tokenizer implementation, hence we need to fall back to our own reconstruction of word_ids."
set context if not set already
flair specific pre-tokenization
fields to store left and right context
expand context only if context_length is set
"if context_dropout is set, randomly deactivate left context during training"
"if context_dropout is set, randomly deactivate right context during training"
"if use_context_separator is set, add a [FLERT] token"
return expanded sentence and context length information
"onnx prepares numpy arrays, no mather if it runs on gpu or cpu, the input is on cpu first."
temporary fix to disable tokenizer parallelism warning
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
do not print transformer warnings as these are confusing in this case
load tokenizer and transformer model
load tokenizer from inmemory zip-file
if model is quantized by BitsAndBytes this will fail
add adapters for finetuning
peft_config: PeftConfig
model name
embedding parameters
send mini-token through to check how many layers the model has
return length
"If we use a context separator, add a new special token"
"most models have an initial BOS token, except for XLNet, T5 and GPT2"
"when initializing, embeddings are in eval mode by default"
in case of doubt: token embedding has higher priority than document embedding
in case of doubt: token embedding has higher priority than document embedding
legacy TransformerDocumentEmbedding
legacy TransformerTokenEmbedding
legacy Flair <= 0.12
legacy Flair <= 0.7
legacy TransformerTokenEmbedding
Legacy TransformerDocumentEmbedding
legacy TransformerTokenEmbedding
legacy TransformerDocumentEmbedding
some models like the tars model somehow lost this information.
copy values from new embedding
do not switch the attention implementation upon reload.
those parameters are only from the super class and will be recreated in the constructor.
cls first pooling can be done without recreating sentence hidden states
make the tuple a tensor; makes working with it easier.
"for multimodal models like layoutlmv3, we truncate the image embeddings as they are only used via attention"
only use layers that will be outputted
this parameter is fixed
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
make compatible with serialized models
gensim version 4
gensim version 3
"if no embedding is set, the vocab and embedding length is required"
GLOVE embeddings
TURIAN embeddings
KOMNINOS embeddings
pubmed embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
twitter embeddings
two-letter language code wiki embeddings
two-letter language code wiki embeddings
two-letter language code crawl embeddings
"this is required to force the module on the cpu,"
"if a parent module is put to gpu, the _apply is called to each sub_module"
self.to(..) actually sets the device properly
this ignores the get_cached_vec method when loading older versions
it is needed for compatibility reasons
gensim version 4
gensim version 3
"when loading the old versions from pickle, the embeddings might not be added as pytorch module."
"we do this delayed, when the weights are collected (e.g. for saving), as doing this earlier might"
lead to issues while loading (trying to load weights that weren't stored as python weights and therefore
not finding them)
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
multilingual models
English models
Arabic
Bulgarian
Czech
Danish
German
Spanish
Basque
Persian
Finnish
French
Hebrew
Hindi
Croatian
Indonesian
Italian
Japanese
Malayalam
Dutch
Norwegian
Polish
Portuguese
Pubmed
Slovenian
Swedish
Tamil
Spanish clinical
CLEF HIPE Shared task
Amharic
Ukrainian
load model if in pretrained model map
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
CLEF HIPE models are lowercased
embeddings are static if we don't do finetuning
embed a dummy sentence to determine embedding_length
set to eval mode
"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout"
gradients are enable if fine-tuning is enabled
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
offset mode that extracts at whitespace after last character
offset mode that extracts at last character
make compatible with old models
use the character language model embeddings as basis
length is twice the original character LM embedding length
these fields are for the embedding memory
whether to add only capitalized words to memory (faster runtime and lower memory consumption)
we re-compute embeddings dynamically at each epoch
set the memory method
memory is wiped each time we do a training run
"if we keep a pooling, it needs to be updated continuously"
update embedding
check token.text is empty or not
set aggregation operation
add embeddings after updating
model architecture
model architecture
"""pl"","
download if necessary
load the model
"this is required to force the module on the cpu,"
"if a parent module is put to gpu, the _apply is called to each sub_module"
self.to(..) actually sets the device properly
"when loading the old versions from pickle, the embeddings might not be added as pytorch module."
"we do this delayed, when the weights are collected (e.g. for saving), as doing this earlier might"
lead to issues while loading (trying to load weights that weren't stored as python weights and therefore
not finding them)
old embeddings do not have a torch-embedding and therefore do not store the weights in the saved torch state_dict
"however they are already initialized rightfully, so we just set the state dict from our current state dict"
GLOVE embeddings
no need to recreate as NILCEmbeddings
read in test file if exists
read in dev file if exists
"find train, dev and test files if not specified"
Add tags for each annotated span
Remove leading and trailing whitespaces from annotated spans
Search start and end token index for current span
If end index is not found set to last token
Throw error if indices are not valid
Add metadatas for sentence
Currently all Jsonl Datasets are stored in Memory
get train data
read in test file if exists
read in dev file if exists
"find train, dev and test files if not specified"
special key for space after
special key for feature columns
special key for dependency head id
"store either Sentence objects in memory, or only file offsets"
"most data sets have the token text in the first column, if not, pass 'text' as column"
determine encoding of text file
identify which columns are spans and which are word-level
now load all sentences
skip first line if to selected
option 1: keep Sentence objects in memory
pointer to previous
parse next sentence
quit if last sentence reached
skip banned sentences
set previous and next sentence for context
append parsed sentence to list in memory
option 2: keep source data in memory
"read lines for next sentence, but don't parse"
quit if last sentence reached
append raw lines for each sentence
we make a distinction between word-level tags and span-level tags
read first sentence to determine which columns are span-labels
skip first line if to selected
check the first 5 sentences
go through all annotations and identify word- and span-level annotations
- if a column has at least one BIES we know it's a Span label
"- if a column has at least one tag that is not BIOES, we know it's a Token label"
- problem cases are columns for which we see only O - in this case we default to Span
skip assigned columns
the space after key is always word-levels
"if at least one token has a BIES, we know it's a span label"
"if at least one token has a label other than BIOES, we know it's a token label"
all remaining columns that are not word-level are span-level
for column in self.word_level_tag_columns:
"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")"
"if sentence ends, break"
parse comments if possible
"otherwise, this line is a token. parse and add to sentence"
check if this sentence is a document boundary
add span labels
discard tags from tokens that are not added to the sentence
parse relations if they are set
head and tail span indices are 1-indexed and end index is inclusive
parse comments such as '# id cd27886d-6895-4d02-a8df-e5fa763fa88f	domain=de-orcas'
"to set the metadata ""domain"" to ""de-orcas"""
get fields from line
get head_id if exists (only in dependency parses)
initialize token
go through all columns
'feats' and 'misc' column should be split into different fields
special handling for whitespace after
add each other feature as label-value pair
get the task name (e.g. 'ner')
get the label value
add label
remap regular tag names
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
set sentence context using partials TODO: pointer to dataset is really inefficient
use all domains
iter over all domains / sources and create target files
The conll representation of coref spans allows spans to
"overlap. If spans end or begin at the same word, they are"
"separated by a ""|""."
The span begins at this word.
The span begins and ends at this word (single word span).
"The span is starting, so we record the index of the word."
"The span for this id is ending, but didn't start at this word."
Retrieve the start index from the document state and
add the span to the clusters for this id.
strip all bracketing information to
get the actual propbank label.
Entering into a span for a particular semantic role label.
We append the label and set the current span for this annotation.
"If there's no '(' token, but the current_span_label is not None,"
then we are inside a span.
We're outside a span.
"Exiting a span, so we reset the current span label for this annotation."
The words in the sentence.
The pos tags of the words in the sentence.
the pieces of the parse tree.
The lemmatised form of the words in the sentence which
have SRL or word sense information.
The FrameNet ID of the predicate.
"The sense of the word, if available."
"The current speaker, if available."
"Cluster id -> List of (start_index, end_index) spans."
Cluster id -> List of start_indices which are open for this id.
Replace brackets in text and pos tags
with a different token for parse trees.
only keep ')' if there are nested brackets with nothing in them.
There are some bad annotations in the CONLL data.
"They contain no information, so to make this explicit,"
we just set the parse piece to be None which will result
in the overall parse tree being None.
"If this is the first word in the sentence, create"
empty lists to collect the NER and SRL BIO labels.
"We can't do this upfront, because we don't know how many"
"components we are collecting, as a sentence can have"
variable numbers of SRL frames.
Create variables representing the current label for each label
sequence we are collecting.
"If any annotation marks this word as a verb predicate,"
we need to record its index. This also has the side effect
of ordering the verbal predicates by their location in the
"sentence, automatically aligning them with the annotations."
"this would not be reached if parse_pieces contained None, hence the cast"
Non-empty line. Collect the annotation.
Collect any stragglers or files which might not
have the '#end document' format for the end of the file.
this dataset name
check if data there
column format
this dataset name
check if data there
column format
this dataset name
download data if necessary
download files if not present locally
we need to slightly modify the original files by adding some new lines after document separators
column format
this dataset name
download data if necessary
Set the base path for the dataset
Define column format
Define dataset name
Define data folder path
"Check if the train data file exists, otherwise download and prepare the dataset"
Download and prepare the dataset
Initialize the parent class with the specified parameters
"Check if the line is a change, delete or add command (like 17721c17703,17705 or 5728d5727)"
Append the previous change block to the changes list
Start a new change block
"Capture original lines (those marked with ""<"")"
"Capture new lines (those marked with "">"")"
Append the last change block to the changes list
Apply each change in reverse order (important to avoid index shift issues)
"Determine the type of the change: `c` for change, `d` for delete, `a` for add"
"Example command: 17721c17703,17705"
Example command: 5728d5727
"Example command: 1000a1001,1002"
Write the modified content to the output file
Strip whitespace to check if the line is empty
Write the first token followed by a newline if the line is not empty
Write an empty line if the line is empty
Strip the leading '[TOKEN]\t' from the annotation
Create a temporary directory
Check the contents of the temporary directory
Extract only the tokens from the original CoNLL03 files
Apply the downloaded patch files to apply our token modifications (e.g. line breaks)
Merge the updated token files with the CleanCoNLL annotations
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
Remove CoNLL-U meta information in the last column
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
entity_mapping
this dataset name
download data if necessary
data validation
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
download files if not present locallys
column format
this dataset name
default dataset folder is the cache root
# download zip
merge the files in one as the zip is containing multiples files
column format
this dataset name
download data if necessary
"unzip the downloaded repo and merge the train, dev and test datasets"
column format
this dataset name
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
check if data there
create folder
download dataset
column format
this dataset name
download and parse data if necessary
create train test dev if not exist
column format
this dataset name
If the extracted corpus file is not yet present in dir
download zip if necessary
"extracted corpus is not present , so unpacking it."
column format
this dataset name
download zip
unpacking the zip
merge the files in one as the zip is containing multiples files
column format
this dataset name
"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)"
download files if not present locally
we need to modify the original files by adding new lines after after the end of each sentence
if only one language is given
column format
this dataset name
"use all languages if explicitly set to ""all"""
download data if necessary
initialize comlumncorpus and add it to list
column format
this dataset name
column format
this dataset name
column format
this dataset name
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
this list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
get google drive id from list
download from google drive
unzip
transform data into required format
"the processed dataset has the additional ending ""_new"""
remove the unprocessed dataset
initialize comlumncorpus and add it to list
if no languages are given as argument all languages used in XTREME will be loaded
if only one language is given
column format
this dataset name
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
This list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
download from HU Server
unzip
transform data into required format
initialize comlumncorpus and add it to list
if only one language is given
column format
this dataset name
download data if necessary
initialize comlumncorpus and add it to list
download data if necessary
unpack and write out in CoNLL column-like format
column format
this dataset name
download data if necessary
data is not in IOB2 format. Thus we transform it to IOB2
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
column format
this dataset name
rename according to train - test - dev - convention
column format
this dataset name
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
Add missing newline after header
Workaround for empty tokens
"Add ""real"" document marker"
Dataset split mapping
v2.0 only adds new language and splits for AJMC dataset
Special document marker for sample splits in AJMC dataset
column format
this dataset name
download data if necessary
column format
this dataset name
create dataset files from index and train/test splits
news date is usually in 3rd or 4th sentence of each article
"generate NoiseBench dataset variants, given CleanCoNLL, noisy label files and index file"
"os.makedirs(os.path.join('data','noisebench'), exist_ok=True)"
copy test set
if only one language is given
column format
this dataset name
"use all languages if explicitly set to ""all"""
download data if necessary
initialize comlumncorpus and add it to list
this dataset name
one name can map to multiple concepts
NOTE: EntityLinkingDictionary are lazy-loaded from a preprocessed file.
Use this class to load into memory all candidates
"if identifier == ""MESH:D013749"":"
# This MeSH ID was used by MeSH when this chemical was part of the MeSH controlled vocabulary.
continue
parse line
this dataset name
default dataset folder is the cache root
download and parse data if necessary
paths to train and test splits
init corpus
this dataset name
default dataset folder is the cache root
download and parse data if necessary
iterate over all html files
"get rid of html syntax, we only need the text"
between all documents we write a separator symbol
skip empty strings
"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)"
"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention"
sentence splitting and tokenization
iterate through all annotations and add to corresponding tokens
find sentence to which annotation belongs
position within corresponding sentence
set annotation for tokens of entity mention
write to out-file in column format
"in case something goes wrong, delete the dataset and raise error"
this dataset name
download and parse data if necessary
from qwikidata.linked_data_interface import get_entity_dict_from_api
generate qid wikiname dictionaries
merge dictionaries
ignore first line
commented and empty lines
read all Q-IDs
ignore first line
request
this dataset name
we use the wikiids in the data instead of directly utilizing the wikipedia urls.
like this we can quickly check if the corresponding page exists
if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi
delete unprocessed file
collect all wikiids
create the dictionary
request
this dataset name
names of raw text documents
open output_file
iterate through all documents
split sentences and tokenize
iterate through all annotations and add to corresponding tokens
find sentence to which annotation belongs
position within corresponding sentence
set annotation for tokens of entity mention
write to out file
annotation from one annotator or two agreeing annotators
this dataset name
download and parse data if necessary
this dataset name
download and parse data if necessary
First parse the post titles
Keep track of how many and which entity mentions does a given post title have
Check if the current post title has an entity link and parse accordingly
Post titles with entity mentions (if any) are handled via this function
Then parse the comments
"Iterate over the comments.tsv file, until the end is reached"
"Keep track of the current comment thread and its corresponding key, on which the annotations are matched."
Each comment thread is handled as one 'document'.
Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.
This if-condition is needed to handle this problem.
"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure"
"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above"
"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle."
"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,"
and not just single letters into single rows.
If there are annotated entity mentions for given post title or a comment thread
"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence"
Write the token with a corresponding tag to file
"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed"
"If a comment thread or a post title has no entity link, all tokens are assigned the O tag"
Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized
"incorrectly, in order to keep the desired format (empty line as a sentence separator)."
"Thrown when the second check above happens, but the last token of a sentence is reached."
"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below."
"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS"
Check if further annotations belong to the current post title or comment thread as well
Stop when the end of an annotation file is reached
Check if further annotations belong to the current sentence as well
"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)"
Docstart
if there is more than one word in the chunk we write each in a separate line
print(chunks)
empty line after each sentence
convert the file to CoNLL
this dataset name
"check if data there, if not, download the data"
create folder
download data
transform data into column format if necessary
if no filenames are specified we use all the data
"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled"
also we remove 'raganato_ALL' from filenames in case its in the list
generate the test file
make column file and save to data_folder
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
TODO: Adapt this following: https://github.com/flairNLP/flair/pull/3146
+1 assumes the title and abstract will be joined by a space.
"We need a unique identifier for this entity, so build it from the document id and entity id"
The user can provide a callable that returns the database name.
some entities are not linked and
some entities are linked to multiple normalized ids
passages must not overlap and spans must cover the entire document
entities
parse db ids
Some of the entities have a off-by-one error. Correct these annotations!
"passage offsets/lengths do not connect, recalculate them for this schema."
this dataset name
download data if necessary
if True:
write CoNLL-U Plus header
"Some special cases (e.g., missing spaces before entity marker)"
necessary if text should be whitespace tokenizeable
Handle case where tail may occur before the head
this dataset name
write CoNLL-U Plus header
this dataset name
TODO: change data source to original CoNLL04 -- this dataset has span formatting errors
download data if necessary
write CoNLL-U Plus header
The span has ended.
We are entering a new span; reset indices
and active tag to new span.
We're inside a span.
Last token might have been a part of a valid span.
this dataset name
write CoNLL-U Plus header
"for source_file_path, target_filename in zip(source_file_paths, target_filenames):"
"with zip_file.open(source_file_path, mode=""r"") as source_file:"
target_file_path = Path(data_folder) / target_filename
"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:"
# write CoNLL-U Plus header
"target_file.write(""# global.columns = id form ner\n"")"
for example in json.load(source_file):
token_list = self._tacred_example_to_token_list(example)
target_file.write(token_list.serialize())
check if first tag row is already occupied
"if first tag row is occupied, use second tag row"
hardcoded mapping TODO: perhaps find nicer solution
remap regular tag names
else skip to position in file where sentence begins
set sentence context using partials TODO: pointer to dataset is really inefficient
read in dev file if exists
read in test file if exists
the url is copied from https://huggingface.co/datasets/darentang/sroie/blob/main/sroie.py#L44
"find train, dev and test files if not specified"
use test_file to create test split if available
use dev_file to create test split if available
"if data point contains black-listed label, do not use"
first check if valid sentence
"if so, add to indices"
"find train, dev and test files if not specified"
variables
different handling of in_memory data than streaming data
"most data sets have the token text in the first column, if not, pass 'text' as column"
test if format is OK
test if at least one label given
make sentence from text (and filter for length)
"if a pair column is defined, make a sentence pair object"
noinspection PyDefaultArgument
dataset name includes the split size
default dataset folder is the cache root
download data if necessary
download each of the 28 splits
create dataset directory if necessary
download senteval datasets if necessary und unzip
this dataset name
download data if necessary
this dataset name
download data if necessary
download data from same source as in huggingface's implementations
read label order
"Original labels are [1, 2, 3, 4] -> ['World', 'Sports', 'Business', 'Sci/Tech']"
"Re-map to [0, 1, 2, 3]."
this dataset name
download data if necessary
handle labels file
handle data file
Create flair compatible labels
"by default, map point score to POSITIVE / NEGATIVE values"
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file from CSV
create test.txt file from CSV
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create train dev and test files in fasttext format
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
convert to FastText format
download data if necessary
"if data is not downloaded yet, download it"
get the zip file
move original .tsv files to another folder
create train and dev splits in fasttext format
create eval_dataset file with no labels
download zip archive
unpack file in datasets directory (zip archive contains a directory named SST-2)
this dataset name
default dataset folder is the cache root
download data if necessary
download datasets if necessary
create dataset directory if necessary
create correctly formated txt files
multiple labels are possible
this dataset name
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
download data if necessary
this dataset name
download data if necessary
create a separate directory for different tasks
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
check if dataset is supported
set file names
set file names
download and unzip in file structure if necessary
instantiate corpus
"find train, dev and test files if not specified"
"create DataPairDataset for train, test and dev file, if they are given"
stop if file does not exist
create a DataPair object from strings
"if in_memory is True we return a datapair, otherwise we create one from the lists of strings"
"find train, dev, and test files if not specified"
"create DataTripleDataset for train, test, and dev files, if they are given"
stop if the file does not exist
create a DataTriple object from strings
"if in_memory is True we return a DataTriple, otherwise we create one from the lists of strings"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"reorder dev datasets to have same columns as in train set: 8, 9, and 11"
dev sets include 5 different annotations but we will only keep the gold label
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get test and dev sets
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data not downloaded yet, download it"
get the zip file
"the downloaded files have json format, we transform them to tsv"
Function to transform JSON file to tsv for Recognizing Textual Entailment Data
remove json file
Uses dynamic programming approach to calculate maximum independent set in interval graph
with sum of all entity lengths as secondary key
calculate offset without current text
because we stick all passages of a document together
TODO For split entities we also annotate everything inbetween which might be a bad idea?
Try to fix incorrect annotations
print(
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
)
Ignore empty lines or relation annotations
FIX annotation of whitespaces (necessary for PDR)
Add task description for multi-task learning
One token may contain multiple entities -> deque all of them
column format
this dataset name
Create tokenization-dependent CONLL files. This is necessary to prevent
from caching issues (e.g. loading the same corpus with different sentence splitters)
column format
this dataset name
column format
this dataset name
Edge case: last token starts a new entity
Last document in file
column format
this dataset name
column format
this dataset name
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
Edge case: last token starts a new entity
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
column format
this dataset name
Read texts
Read annotations
column format
this dataset name
column format
this dataset name
column format
this dataset name
We need to apply a patch to correct the original training file
Articles title
Article abstract
Entity annotations
column format
this dataset name
Edge case: last token starts a new entity
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
Incomplete article
Invalid XML syntax
column format
this dataset name
column format
this dataset name
if len(mid) != 3:
continue
Try to fix entity offsets
column format
this dataset name
There is still one illegal annotation in the file ..
column format
this dataset name
"Abstract first, title second to prevent issues with sentence splitting"
column format
this dataset name
column format
this dataset name
column format
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
column format
this dataset name
"Filter for specific entity types, by default no entities will be filtered"
Get original HUNER splits to retrieve a list of all document ids contained in V2
train and dev split of V2 will be train in V4
test split of V2 will be dev in V4
New documents in V4 will become test documents
column format
this dataset name
column format
this dataset name
default dataset folder is the cache root
column format
build dataset name and full huggingface reference name
Download data if necessary
"Some datasets in BigBio only have train or test splits, not both"
"If only test split, assign it to train split"
"If only train split, sample other from it (sample_missing_splits=True)"
Not every dataset has a dev / validation set!
Perform type mapping if necessary
return None
TODO: Add entity type mapping for all remaining bigbio datasets not in HunFlair?
"""simple_chemical"": ""chemical"",  # BioNLP ST 2013 CG"
"""cancer"": ""disease"",  # BioNLP ST 2013 CG"
"""gene_or_gene_product"": ""gene"",  # BioNLP ST 2013 CG"
"""gene"": ""gene"",  # NLM Gene"
"""chemical"": ""chemical"",  # NLM Chem"
"""cellline"": ""cell_line"",  # Cell Finder"
"""species"": ""species"",  # Cell Finder"
"""protein"": ""gene"",  # BioID"
"Collect all texts of the document, each passage will be"
a text in our internal format
Sort passages by start offset
Transform all entity annotations into internal format
Find the passage of the entity (necessary for offset adaption)
Adapt entity offsets according to passage offsets
FIXME: This is just for debugging purposes
passage_text = id_to_text[passage_id]
doc_text = passage_text[entity_offset[0] : entity_offset[1]]
"mention_text = entity[""text""][0]"
if doc_text != mention_text:
"print(f""Annotation error ({document['document_id']}) - Doc: {doc_text} vs. Mention: {mention_text}"")"
Get element in the middle
Is the mention with the passage offsets?
"If element is smaller than mid, then it can only"
be present in left subarray
Else the element can only be present in right subarray
TODO whether cell or cell line is the correct tag
TODO whether cell or cell line is the correct tag
Special case for ProGene: We need to use the split_0_train and split_0_test splits
as they are currently provided in BigBio
cache Feidegger config file
cache Feidegger images
replace image URL with local cached file
append Sentence-Image data point
cast to list if necessary
cast to list if necessary
"first, check if pymongo is installed"
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
Expose base classses
Expose all biomedical data sets used for the evaluation of BioBERT
-
-
-
-
Expose all biomedical data sets using the HUNER splits
Expose all biomedical data sets
Expose all document classification datasets
word sense disambiguation
Expose all entity linking datasets
Expose all relation extraction datasets
universal proposition banks
keyphrase detection datasets
other NER datasets
standard NER datasets
Expose all sequence labeling datasets
Expose all text-image datasets
Expose all text-text datasets
Expose all treebanks
"find train, dev and test files if not specified"
get train data
get test data
get dev data
option 1: read only sentence boundaries as offset positions
option 2: keep everything in memory
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
current token ID
handling for the awful UD multiword format
end of sentence
comments or ellipsis
if token is a multi-word
normal single-word tokens
"if we don't split multiwords, skip over component words"
add token
add morphological tags
derive whitespace logic for multiwords
"if multi-word equals component tokens, there should be no whitespace"
go through all tokens in subword and set whitespace_after information
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
this dataset name
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
"finally, print model card for information"
Note: Multi-GPU can affect corpus loading
This code will run multiple times -- each GPU gets its own process and each process runs this code. We need to
"ensure that the corpus has the same elements and order on all processes, despite sampling. We do that by using"
the same seed on all processes.
Note: Multi-GPU can affect choice of batch size.
"In order to compare batch updates fairly between single and multi-GPU training, we should:"
1) Step the optimizer after the same number of examples to achieve com
2) Process the same number of examples in each forward pass
"e.g. Suppose your machine has 2 GPUs. If multi_gpu=False, the first gpu will process 32 examples, then the"
"first gpu will process another 32 examples, then the optimizer will step. If multi_gpu=True, each gpu will"
"process 32 examples at the same time, then the optimizer will step."
noqa: INP001
-- Project information -----------------------------------------------------
"The full version, including alpha/beta/rc tags"
use smv_current_version as the git url
-- General configuration ---------------------------------------------------
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Napoleon settings
Whitelist pattern for tags (set to None to ignore all tags)
Whitelist pattern for branches (set to None to ignore all branches)
Whitelist pattern for remotes (set to None to use local branches only)
Pattern for released versions
Format for versioned output directories inside the build directory
Determines whether remote or local git branches/tags are preferred if their output dirs conflict
test corpus
create a TARS classifier
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
test corpus
create a TARS classifier
switch to a new task (TARS can do multiple tasks so you must define one)
initialize the text classifier trainer
start the training
"With end symbol, without start symbol, padding in front"
"Without end symbol, with start symbol, padding in back"
"Without end symbol, without start symbol, padding in front"
initialize trainer
initialize trainer
initialize trainer
clean up directory
clean up directory
example sentence
set 4 labels for 2 tokens ('love' is tagged twice)
check if there are three POS labels with correct text and values
check if there are is one SENTIMENT label with correct text and values
check if all tokens are correctly labeled
remove the pos label from the last word
there should be 2 POS labels left
now remove all pos tags
set 3 labels for 2 spans (HU is tagged twice)
check if there are three labels with correct text and values
check if there are two spans with correct text and values
"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
should be only one NER label left
and only one NER span
set 3 labels for 2 spans (HU is tagged twice with different tags)
check if there are three labels with correct text and values
check if there are two spans with correct text and values
"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
should be only one NER label left
and only one NER span
but there is also one orgtype span and label
and only one NER span
let's add the NER tag back
check if there are three labels with correct text and values
check if there are two spans with correct text and values
now remove all NER tags
set 3 labels for 2 spans (HU is tagged twice with different tags)
create two relation label
there should be two relation labels
there should be one syntactic labels
"there should be two relations, one with two and one with one label"
example sentence
add another topic label
example sentence
has sentiment value
has 4 part of speech tags
has 1 NER tag
should be in total 6 labels
example sentence
add two NER labels
get the four labels
check that only two of the respective data points are equal
make a sentence and some right context
TODO: is this desirable? Or should two sentences with same text be considered same objects?
Initializing a Sentence this way assumes that there is a space after each token
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
load column dataset with one entry
load column dataset with two entries
load column dataset with three entries
"get training, test and dev data"
"get training, test and dev data"
check if Token labels are correct
"get training, test and dev data"
check if Token labels for frames are correct
"get training, test and dev data"
"get training, test and dev data"
get two corpora as one
"get training, test and dev data for full English UD corpus from web"
clean up data directory
"assert [token.get_tag(""head"").value for token in sent1.tokens] == ["
"""2"","
"""0"","
"""4"","
"""2"","
"""2"","
"""2"","
]
This test only covers basic universal dependencies datasets.
"For example, multi-word tokens or the ""deps"" column sentence annotations are not supported yet."
"Here, we use the default token annotation fields."
This test covers the complete HIPE 2022 dataset.
https://github.com/hipe-eval/HIPE-2022-data
"Includes variant with document separator, and all versions of the dataset."
"We have manually checked, that these numbers are correct:"
"+1 offset, because of missing EOS marker at EOD"
Test data for v2.1 release
This test covers the complete ICDAR Europeana corpus:
https://github.com/stefan-it/historic-domain-adaptation-icdar
"This test covers the complete MasakhaNER dataset, including support for v1 and v2."
This test covers the NERMuD dataset. Official stats can be found here:
https://github.com/dhfbk/KIND/tree/main/evalita-2023
Number of instances per dataset split are taken from https://huggingface.co/datasets/elenanereiss/german-ler
This test covers the complete MasakhaPOS dataset.
"See MobIE paper (https://aclanthology.org/2021.konvens-1.22/), table 2"
--- Embeddings that are shared by both models --- #
--- Task 1: Sentiment Analysis (5-class) --- #
Define corpus and model
-- Task 2: Binary Sentiment Analysis on Customer Reviews -- #
Define corpus and model
-- Define mapping (which tagger should train on which model) -- #
-- Create model trainer and train -- #
NOTE: Avoid emtpy string if mentions are just punctutations (e.g. `-` or `(`)
clean up file
get features from forward propagation
reverse sort all sequences by their length
remove previously predicted labels of this type
no need for label_dict
"pretrained_model = ""tars-ner""  # disabled due to too much space requirements."
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
Intel ----founded_by---> Gordon Moore
Intel ----founded_by---> Robert Noyce
"Ground truth is a set of tuples of (<Sentence Text>, <Relation Label Values>)"
Check sentence masking and relation label annotation on
"training, validation and test dataset (in this test the splits are the same)"
"Entity pair permutations of: ""Larry Page and Sergey Brin founded Google ."""
"Entity pair permutations of: ""Microsoft was founded by Bill Gates ."""
"Entity pair permutations of: ""Konrad Zuse was born in Berlin on 22 June 1910 ."""
"Entity pair permutations of: ""Joseph Weizenbaum , a professor at MIT , was born in Berlin , Germany."""
This sentence is only included if we transform the corpus with cross augmentation
"pretrained_model = ""tars-base""  # disabled due to too much space requirements."
Ensure this is an example that predicts no classes in multilabel
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
ensure that the prepared tensors is what we expect
use a SequenceTagger to save and reload the embedding in the manner it is supposed to work
previous and next sentence as context
test expansion for sentence without context
test expansion for with previous and next as context
test expansion if first sentence is document boundary
test expansion if we don't use context
"apparently the precision is not that high on cuda, hence the absolute tolerance needs to be higher."
dummy model with embeddings
save the dummy and load it again
check that context_length and use_context_separator is the same for both
mmap seems to be much more memory efficient
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
Extract all the contents of zip file in current directory
use model name as subfolder
Lazy import
output information
Extract all the contents of zip file in current directory
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
load_big_file is a workaround byhttps://github.com/highway11git
to load models on some Mac/Windows setups
see https://github.com/zalandoresearch/flair/issues/351
first determine the distribution of classes in the dataset
weight for each sample
Create blocks
shuffle the blocks
concatenate the shuffled blocks
Create blocks
shuffle the blocks
concatenate the shuffled blocks
increment for last token in sentence if not followed by whitespace
this is the default init size of a lmdb database for embeddings
get db filename from embedding name
"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
SequenceTagger
TextClassifier
get db filename from embedding name
if embedding database already exists
"otherwise, push embedding to database"
if embedding database already exists
open the database in read mode
we need to set self.k
create and load the database in write mode
"no idea why, but we need to close and reopen the environment to avoid"
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
when opening new transaction !
init dictionaries
"in order to deal with unknown tokens, add <unk>"
set 'add_unk' if the dictionary was created with a version of Flair older than 0.9
set 'add_unk' depending on whether <unk> is a key
"if one embedding name, directly return it"
"if multiple embedding names, concatenate them"
First we remove any existing labels for this PartOfSentence in self.sentence
labels also need to be deleted at Sentence object
delete labels at object itself
The Token is a special _PartOfSentence in that it may be initialized without a Sentence.
"therefore, labels get added only to the Sentence if it exists"
The Token is a special _PartOfSentence in that it may be initialized without a Sentence.
"Therefore, labels get set only to the Sentence if it exists"
"check if the span already exists. If so, return it"
else make a new span
"check if the relation already exists. If so, return it"
else make a new relation
private field for all known spans
the tokenizer used for this sentence
some sentences represent a document boundary (but most do not)
internal variables to denote position inside dataset
"if text is passed, instantiate sentence with tokens (words)"
determine token positions and whitespace_after flag
the last token has no whitespace after
log a warning if the dataset is empty
data with zero-width characters cannot be handled
set token idx and sentence
append token to sentence
register token annotations on sentence
move sentence embeddings to device
also move token embeddings to device
clear token embeddings
infer whitespace after field
"if sentence has no tokens, return empty string"
"otherwise, return concatenation of tokens with the correct offsets"
The sentence's start position is not propagated to its tokens.
"Therefore, we need to add the sentence's start position to its last token's end position, including whitespaces."
No character at the corresponding code point: remove it
"if no label if specified, return all labels"
"if the label type exists in the Sentence, return it"
return empty list if none of the above
labels also need to be deleted at all tokens
labels also need to be deleted at all known spans
remove spans without labels
delete labels at object itself
set name
abort if no data is provided
sample test data from train if none is provided
sample dev data from train if none is provided
set train dev and test data
find out empty sentence indices
create subset of non-empty sentence indices
find out empty sentence indices
create subset of non-empty sentence indices
"first, determine the datapoint type by going through dataset until first label is found"
count all label types per sentence
go through all labels of label_type and count values
special handling for Token-level annotations. Add all untagged as 'O' label
"if an unk threshold is set, UNK all label values below this threshold"
sample randomly from a label distribution according to the probabilities defined by the noise transition matrix
replace the old label with the new one
keep track of the old (clean) label using another label type category
keep track of how many labels in total are flipped
sample randomly from a label distribution according to the probabilities defined by the desired noise share
replace the old label with the new one
keep track of the old (clean) label using another label type category
keep track of how many labels in total are flipped
"add a dummy ""O"" to close final prediction"
return complex list
internal variables
non-set tags are OUT tags
anything that is not OUT is IN
does this prediction start a new span?
B- and S- always start new spans
"if the predicted class changes, I- starts a new span"
"if the predicted class changes and S- was previous tag, start a new span"
if an existing span is ended (either by reaching O or starting a new span)
determine score and value
append to result list
reset for-loop variables for new span
remember previous tag
global variable: cache_root
Get the device from the environment variable
global variable: device
"No need for correctness checks, torch is doing it"
global variable: version
global variable: arrow symbol
dummy return to fulfill trainer.train() needs
print(vec)
Attach optimizer
"convert `metrics` to float, in case it's a zero-dim Tensor"
if memory mode option 'none' delete everything
"if dynamic embedding keys not passed, identify them automatically"
always delete dynamic embeddings
"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)"
optional metric space decoder if prototypes have different length than embedding
create initial prototypes for all classes (all initial prototypes are a vector of all 1s)
"if set, create initial prototypes from normal distribution"
"if set, use a radius"
all parameters will be pushed internally to the specified device
decode embeddings into prototype space
"if unlabeled distance is set, mask out loss to unlabeled class prototype"
verbalize BIOES labels
"if label is not BIOES, use label itself"
Always include the name of the Model class for which the state dict holds
"write out a ""model card"" if one is set"
save model
"if this class is abstract, go through all inheriting classes and try to fetch and load the model"
get all non-abstract subclasses
"try to fetch the model for each subclass. if fetching is possible, load model and return it"
"skip any invalid loadings, e.g. not found on huggingface hub"
"if the model cannot be fetched, load as a file"
try to get model class from state
"older (flair 11.3 and below) models do not contain cls information. In this case, try all subclasses"
"if str(model_cls) == ""<class 'flair.models.pairwise_classification_model.TextPairClassifier'>"": continue"
"skip any invalid loadings, e.g. not found on huggingface hub"
"if this class is not abstract, fetch the model and load it"
"make sure <unk> is contained in gold_label_dictionary, if given"
"read Dataset into data loader, if list of sentences passed, make Dataset first"
loss calculation
variables for printing
variables for computing scores
remove any previously predicted labels
predict for batch
get the gold labels
add to all_predicted_values
make printout lines
convert true and predicted values to two span-aligned lists
delete exluded labels if exclude_labels is given
"if after excluding labels, no label is left, ignore the datapoint"
write all_predicted_values to out_file if set
make the evaluation dictionary
check if this is a multi-label problem
compute numbers by formatting true and predicted such that Scikit-Learn can use them
multi-label problems require a multi-hot vector for each true and predicted label
single-label problems can do with a single index for each true and predicted label
"now, calculate evaluation numbers"
there is at least one gold label or one prediction (default)
compute accuracy separately as it is not always in classification_report (e.. when micro avg exists)
"if there is only one label, then ""micro avg"" = ""macro avg"""
"The ""micro avg"" appears only in the classification report if no prediction is possible."
"Otherwise, it is identical to the ""macro avg"". In this case, we add it to the report."
"Create and populate score object for logging with all evaluation values, plus the loss"
issue error and default all evaluation numbers to 0.
check if there is a label mismatch
print info
set the embeddings
initialize the label dictionary
initialize the decoder
set up multi-label logic
init dropouts
loss weights and loss function
Initialize the weight tensor
set up gradient reversal if so specified
embed sentences
get a tensor of data points
do dropout
make a forward pass to produce embedded data points and labels
get the data points for which to predict labels
get their gold labels as a tensor
pass data points through network to get encoded data point tensor
decode
an optional masking step (no masking in most cases)
calculate the loss
filter empty sentences
reverse sort all sequences by their length
progress bar for verbosity
filter data points in batch
stop if all sentences are empty
pass data points through network and decode
if anything could possibly be predicted
remove previously predicted labels of this type
filter data points that have labels outside of dictionary
add DefaultClassifier arguments
add variables of DefaultClassifier
Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23
Get projected 1st dimension
Compute bilinear form
Arcosh
Project the input data to n+1 dimensions
"The first dimension, is recomputed in the distance module"
header for 'weights.txt'
"determine the column index of loss, f-score and accuracy for"
"train, dev and test split"
then get all relevant values from the tsv
then get all relevant values from the tsv
plot i
save plots
save plots
plt.show()
save plot
auto-spawn on GPU if available
progress bar for verbosity
stop if all sentences are empty
clearing token embeddings to save memory
"read Dataset into data loader, if list of sentences passed, make Dataset first"
TODO: not saving lines yet
TODO: This closely shadows the RelationExtractor name. Maybe we need a better name here.
- MaskedRelationClassifier ?
This depends if this relation classification architecture should replace or offer as an alternative.
Set label type and prepare label dictionary
Initialize super default classifier
Add the special tokens from the encoding strategy
"Auto-spawn on GPU, if available"
Only use entities labelled with the specified labels for each label type
Only use entities above the specified threshold
Use a dictionary to find gold relation annotations for a given entity pair
Yield head and tail entity pairs from the cross product of all entities
Remove identity relation entity pairs
Remove entity pairs with labels that do not match any
of the specified relations in `self.entity_pair_labels`
"Obtain gold label, if existing"
Some sanity checks
Pre-compute non-leading head and tail tokens for entity masking
We can not use the plaintext of the head/tail span in the sentence as the mask/marker
since there may be multiple occurrences of the same entity mentioned in the sentence.
"Therefore, we use the span's position in the sentence."
Create masked sentence
Add gold relation annotation as sentence label
"Using the sentence label instead of annotating a separate `Relation` object is easier to manage since,"
"during prediction, the forward pass does not need any knowledge about the entities in the sentence."
"If we sample missing splits, the encoded sentences that correspond to the same original sentences"
"may get distributed into different splits. For training purposes, this is always undesired."
Ensure that all sentences are encoded properly
Deal with the case where all sentences are encoded sentences
"mypy does not infer the type of ""sentences"" restricted by the if statement"
Deal with the case where all sentences are standard (non-encoded) sentences
"For each encoded sentence, transfer its prediction onto the original relation"
auto-spawn on GPU if available
pad strings with whitespaces to longest sentence
cut up the input into chunks of max charlength = chunk_size
push each chunk through the RNN language model
concatenate all chunks to make final output
initial hidden state
get predicted weights
divide by temperature
"to prevent overflow problem with small temperature values, substract largest value from all"
this makes a vector in which the largest value is 0
compute word weights with exponential function
try sampling multinomial distribution for next character
print(word_idx)
input ids
push list of character IDs through model
the target is always the next character
use cross entropy loss to compare output of forward pass with targets
exponentiate cross-entropy loss to calculate perplexity
"""document_delimiter"" property may be missing in some older pre-trained models"
serialize the language models and the constructor arguments (but nothing else)
special handling for deserializing language models
re-initialize language model with constructor arguments
copy over state dictionary to self
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
"in their ""self.train()"" method)"
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
Transform input data into TARS format
"if there are no labels, return a random sample as negatives"
"otherwise, go through all labels"
make sure the probabilities always sum up to 1
get and embed all labels by making a Sentence object that contains only the label text
get each label embedding and scale between 0 and 1
compute similarity matrix
"the higher the similarity, the greater the chance that a label is"
sampled as negative example
make label dictionary if no Dictionary object is passed
prepare dictionary of tags (without B- I- prefixes and without UNK)
check if candidate_label_set is empty
make list if only one candidate label is passed
create label dictionary
note current task
create a temporary task
make zero shot predictions
switch to the pre-existing task
prepare TARS dictionary
initialize a bare-bones sequence tagger
transformer separator
Store task specific labels since TARS can handle multiple tasks
make a tars sentence where all labels are O by default
init new TARS classifier
set all task information
progress bar for verbosity
stop if all sentences are empty
always remove tags first
go through each sentence in the batch
always remove tags first
get the span and its label
determine whether tokens in this span already have a label
only add if all tokens have no label
make and add a corresponding predicted span
set indices so that no token can be tagged twice
clearing token embeddings to save memory
"all labels default to ""O"""
set gold token-level
set predicted token-level
now print labels in CoNLL format
prepare TARS dictionary
initialize a bare-bones sequence tagger
transformer separator
Store task specific labels since TARS can handle multiple tasks
get the serialized embeddings
remap state dict for models serialized with Flair <= 0.11.3
init new TARS classifier
set all task information
with torch.no_grad():
progress bar for verbosity
stop if all sentences are empty
always remove tags first
go through each sentence in the batch
always remove tags first
add all labels that according to TARS match the text and are above threshold
do not add labels below confidence threshold
only use label with the highest confidence if enforcing single-label predictions
add the label with the highest score even if below the threshold if force label is activated.
remove previously added labels and only add the best label
clearing token embeddings to save memory
set separator to concatenate three sentences
auto-spawn on GPU if available
set separator to concatenate two sentences
auto-spawn on GPU if available
"If the concatenated version of the text pair does not exist yet, create it"
pooling operation to get embeddings for entites
set embeddings
set relation and entity label types
"whether to use gold entity pairs, and whether to filter entity pairs by type"
filter entity pairs according to their tags if set
whether to encode characters and whether to use attention (attention can only be used if chars are encoded)
character dictionary for decoding and encoding
make sure <unk> is in dictionary for handling of unknown characters
add special symbols to dictionary if necessary and save respective indices
---- ENCODER ----
encoder character embeddings
encoder pre-trained embeddings
encoder RNN
additional encoder linear layer if bidirectional encoding
---- DECODER ----
decoder: linear layers to transform vectors to and from alphabet_size
when using attention we concatenate attention outcome and decoder hidden states
decoder RNN
loss and softmax
self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction
add additional columns for special symbols if necessary
initialize with dummy symbols
encode inputs
get labels (we assume each token has a lemma label)
get char indices for labels of sentence
"(batch_size, max_sequence_length) batch_size = #words in sentence,"
max_sequence_length = length of longest label of sentence + 1
get char embeddings
"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size"
take decoder input and initial hidden and pass through RNN
"if all encoder outputs are provided, use attention"
take convex combinations of encoder hidden states as new output using the computed attention coefficients
"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)"
get all tokens
encode input characters by sending them through RNN
get one-hots for characters and add special symbols / padding
determine length of each token
embed sentences
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
variable to store initial hidden states for decoder
encode input characters by sending them through RNN
test packing and padding
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
concatenate the final hidden states of the encoder. These will be projected to hidden_size of
decoder later with self.emb_to_hidden
mask out vectors that correspond to a dummy symbol (TODO: check attention masking)
use token embedding as initial hidden state for decoder
concatenate everything together and project to appropriate size for decoder
variable to store initial hidden states for decoder
encode input characters by sending them through RNN
note that we do not need to fill up with dummy symbols since we process each token seperately
embed character one-hots
send through encoder RNN (produces initial hidden for decoder)
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
project 2*hidden_size to hidden_size
concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder
later with self.emb_to_hidden
use token embedding as initial hidden state for decoder
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
concatenate everything together and project to appropriate size for decoder
"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)"
"create target vector (batch_size, max_label_seq_length + 1)"
filter empty sentences
max length of the predicted sequences
for printing
stop if all sentences are empty
remove previously predicted labels of this type
create list of tokens in batch
encode inputs
"create input for first pass (batch_size, 1, input_size), first letter is special character <S>"
sequence length is always set to one in prediction
option 1: greedy decoding
predictions
decode next character
pick top beam size many outputs with highest probabilities
option 2: beam search
out_probs = self.softmax(output_vectors).squeeze(1)
make sure no dummy symbol <> or start symbol <S> is predicted
pick top beam size many outputs with highest probabilities
"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1"
"leading_indices and probabilities have size (batch_size, beam_size)"
keep scores of beam_size many hypothesis for each token in the batch
stack all leading indices of all hypothesis and corresponding hidden states in two tensors
save sequences so far
keep track of how many hypothesis were completed for each token
"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)"
decode with log softmax
make sure no dummy symbol <> or start symbol <S> is predicted
"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside"
"if the sequence is already ended, do not record as candidate"
index of token in in list tokens_in_batch
print(token_number)
hypothesis score
TODO: remove token if number of completed hypothesis exceeds given value
set score of corresponding entry to -inf so it will not be expanded
get leading_indices for next expansion
find highest scoring hypothesis among beam_size*beam_size possible ones for each token
take beam_size many copies of scores vector and add scores of possible new extensions
"size (beam_size*batch_size, beam_size)"
print(hypothesis_scores)
"reshape to vector of size (batch_size, beam_size*beam_size),"
each row contains beam_size*beam_size scores of the new possible hypothesis
print(hypothesis_scores_per_token)
"choose beam_size best for each token - size (batch_size, beam_size)"
out of indices_per_token we now need to recompute the original indices of the hypothesis in
a list of length beam_size*batch_size
"where the first three inidices belong to the first token, the next three to the second token,"
and so on
with these indices we can compute the tensors for the next iteration
expand sequences with corresponding index
add log-probabilities to the scores
save new leading indices
save corresponding hidden states
it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations
in that case we append one of the final seuqences without end symbol to the final_candidates
get best final hypothesis for each token
get characters from index sequences and add predicted label to token
"Overwrites evaluate of parent class to remove the ""by class"" printout"
set separator to concatenate two sentences
init dropouts
auto-spawn on GPU if available
make a forward pass to produce embedded data points and labels
get their gold labels as a tensor
pass data points through network to get encoded data point tensor
decode
calculate the loss
get a tensor of data points
do dropout
"If the concatenated version of the text pair does not exist yet, create it"
add DefaultClassifier arguments
progress bar for verbosity
stop if all sentences are empty
clearing token embeddings to save memory
"read Dataset into data loader, if list of sentences passed, make Dataset first"
"if the classifier predicts BIO/BIOES span labels, the internal label dictionary must be computed"
fields in case this is a span-prediction problem
the label type
all parameters will be pushed internally to the specified device
special handling during training if this is a span prediction problem
internal variables
non-set tags are OUT tags
anything that is not OUT is IN
does this prediction start a new span?
B- and S- always start new spans
"if the predicted class changes, I- starts a new span"
"if the predicted class changes and S- was previous tag, start a new span"
if an existing span is ended (either by reaching O or starting a new span)
reset for-loop variables for new span
remember previous tag
"if there is a span at end of sentence, add it"
"all labels default to ""O"""
set gold token-level
set predicted token-level
now print labels in CoNLL format
print labels in CoNLL format
internal candidate lists of generator
load Zelda candidates if so passed
create candidate lists
"if lower casing is enabled, create candidate lists of lower cased versions"
create a new dictionary for lower cased mentions
go through each mention and its candidates
"check if backoff mention already seen. If so, add candidates. Else, create new entry."
set lowercased version as map
"only use span label type if there are predictions, otherwise search for output label type (training labels)"
remap state dict for models serialized with Flair <= 0.11.3
get the candidates
"during training, add the gold value as candidate"
----- Create the internal tag dictionary -----
span-labels need special encoding (BIO or BIOES)
the big question is whether the label dictionary should contain an UNK or not
"without UNK, we cannot evaluate on data that contains labels not seen in test"
"with UNK, the model learns less well if there are no UNK examples"
is this a span prediction problem?
----- Embeddings -----
----- Initial loss weights parameters -----
----- RNN specific parameters -----
----- Conditional Random Field parameters -----
"Previously trained models have been trained without an explicit CRF, thus it is required to check"
whether we are loading a model from state dict in order to skip or add START and STOP token
----- Dropout parameters -----
dropouts
remove word dropout if there is no contact over the sequence dimension.
----- Model layers -----
----- RNN layer -----
"If shared RNN provided, else create one for model"
Whether to train initial hidden state
final linear map to tag space
"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss"
"if using CRF, we also require a CRF and a Viterbi decoder"
"if there are no sentences, there is no loss"
forward pass to get scores
calculate loss given scores and labels
make a zero-padded tensor for the whole sentence
linear map to tag space
"Depending on whether we are using CRF or a linear layer, scores is either:"
"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF"
"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer"
spans need to be encoded as token-level predictions
all others are regular labels for each token
make sure it's a list
filter empty sentences
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
get features from forward propagation
remove previously predicted labels of this type
"if return_loss, get loss value"
make predictions
add predictions to Sentence
BIOES-labels need to be converted to spans
"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)"
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
core Flair models on Huggingface ModelHub
"Large NER models,"
Multilingual NER models
English POS models
Multilingual POS models
English SRL models
English chunking models
Language-specific NER models
Language-specific POS models
Historic German
English NER models
English SRL models
Danish models
German models
Arabic models
French models
Dutch models
Malayalam models
Portuguese models
Biomedical models
check if model name is a valid local file
"check if model key is remapped to HF key - if so, print out information"
get mapped name
"if not, check if model key is remapped to direct download location. If so, download model"
"for all other cases (not local file or special download location), use HF model hub"
## Demo: How to use in Flair
load tagger
make example sentence
predict NER tags
print sentence
print predicted NER spans
iterate over entities and print
Lazy import
Save model weight
Determine if model card already exists
Generate and save model card
Upload files
"all labels default to ""O"""
set gold token-level
set predicted token-level
now print labels in CoNLL format
print labels in CoNLL format
Dense + sparse retrieval
fetched from original repo to avoid download
"just in case we add: fuzzy search, Levenstein, ..."
"for now we always fall back to SapBERT,"
but we should train our own models at some point
NOTE: Avoid emtpy string if mentions are just punctutations (e.g. `-` or `(`)
NOTE: Avoid emtpy string if mentions are just punctuations (e.g. `-` or `(`)
Ab3P works on sentence-level and not on a single entity mention / name
- so we just apply the wrapped text pre-processing here (if configured)
NOTE: ensure correct similarity metric for pretrained model
empty cuda cache if device is a cuda device
"Sanity conversion: if flair.device was set as a string, convert to torch.device"
NOTE: This is a hacky workaround for the fact that
the `label_type`s in `Classifier.load('hunflair)` are
"'diseases', 'genes', 'species', 'chemical' instead of 'ner'."
We warn users once they need to update SequenceTagger model
See: https://github.com/flairNLP/flair/pull/3387
make sure sentences is a list of sentences
Make sure entity label types are represented as dict
Collect all entities based on entity type labels configuration
Preprocess entity mentions
Retrieve top-k concept / entity candidates
Add a label annotation for each candidate
load model by entity_type
check if we have a hybrid pre-trained model
the multi task model has several labels
biomedical models
entity linker
auto-spawn on GPU if available
remap state dict for models serialized with Flair <= 0.11.3
English sentiment models
Communicative Functions Model
"If we sample missing splits, the encoded sentences that correspond to the same original sentences"
"may get distributed into different splits. For training purposes, this is always undesired."
Prepend the task description prompt to the sentence text
Make sure it's a list
Reconstruct all annotations from the original sentence (necessary for learning classifiers)
If all sentences are not augmented -> augment them
"mypy does not infer the type of ""sentences"" restricted by the if statement"
"mypy does not infer the type of ""sentences"" restricted by code above"
Compute prediction label type
make sure it's a list
"If all sentences are already augmented (i.e. compatible with this class), just forward the sentences"
"mypy does not infer the type of ""sentences"" restricted by the if statement"
Remove existing labels
Augment sentences - copy all annotation of the given tag type
Predict on augmented sentence and store it in an internal annotation layer / label
Append predicted labels to the original sentences
check if model name is a valid local file
check if model name is a pre-configured hf model
"scores_at_targets[range(features.shape[0]), lengths.values -1]"
Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices
"Initially, get scores from <start> tag to all other tags"
"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp"
"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep"
Create a tensor to hold accumulated sequence scores at each current tag
Create a tensor to hold back-pointers
"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag"
"Let pads be the <end> tag index, since that was the last tag in the decoded sequence"
"We add scores at current timestep to scores accumulated up to previous timestep, and"
choose the previous timestep that corresponds to the max. accumulated score for each current timestep
"If sentence is over, add transition to STOP-tag"
Decode/trace best path backwards
Sanity check
remove start-tag and backscore to stop-tag
Max + Softmax to get confidence score for predicted label and append label to each token
"Transitions are used in the following way: transitions[to, from]."
"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag"
to START-tag and from STOP-tag to any other tag to -10000.
"if necessary, make batch_steps"
break up the batch into slices of size
mini_batch_chunk_size
"if training also uses dev/train data, include in training set"
evaluation and monitoring
sampling and shuffling
evaluation and monitoring
when and what to save
logging parameters
plugins
activate annealing plugin
call self.train_custom with all parameters (minus the ones specific to the AnnealingPlugin)
training parameters
evaluation and monitoring
sampling and shuffling
evaluation and monitoring
when and what to save
logging parameters
amp
plugins
annealing logic
training parameters
evaluation and monitoring
sampling and shuffling
evaluation and monitoring
when and what to save
logging parameters
amp
plugins
training parameters
evaluation and monitoring
sampling and shuffling
evaluation and monitoring
when and what to save
logging parameters
amp
plugins
Create output folder
=== START BLOCK: ACTIVATE PLUGINS === #
We first activate all optional plugins. These take care of optional functionality such as various
logging techniques and checkpointing
log file plugin
loss file plugin
plugin for writing weights
plugin for checkpointing
=== END BLOCK: ACTIVATE PLUGINS === #
derive parameters the function was called with (or defaults)
initialize model card with these parameters
Prepare training data and get dataset size
"determine what splits (train, dev, test) to evaluate"
determine how to determine best model and whether to save it
instantiate the optimizer
initialize sampler if provided
init with default values if only class is provided
set dataset to sample from
this field stores the names of all dynamic embeddings in the model (determined after first forward pass)
Sanity checks
"Sanity conversion: if flair.device was set as a string, convert to torch.device"
-- AmpPlugin -> wraps with AMP
-- AnnealingPlugin -> initialize schedulers (requires instantiated optimizer)
At any point you can hit Ctrl + C to break out of training early.
"- SchedulerPlugin -> load state for anneal_with_restarts, batch_growth_annealing, logic for early stopping"
- LossFilePlugin -> get the current epoch for loss file logging
"if shuffle_first_epoch==False, the first epoch is not shuffled"
log infos on training progress every `log_modulo` batches
process mini-batches
zero the gradients on the model and optimizer
forward and backward for batch
forward pass
identify dynamic embeddings (always deleted) on first sentence
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
do the optimizer step
- SchedulerPlugin -> do the scheduler step if one-cycle or linear decay
- WeightExtractorPlugin -> extracts weights
- CheckpointPlugin -> executes save_model_each_k_epochs
- SchedulerPlugin -> log bad epochs
Determine if this is the best model or if we need to anneal
log results
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
use DEV split to determine if this is the best model so far
"if not using DEV score, determine best model using train loss"
- LossFilePlugin -> somehow prints all relevant metrics
- AnnealPlugin -> scheduler step
- SWAPlugin -> restores SGD weights from SWA
"if we do not use dev data for model selection, save final model"
TensorboardLogger -> closes writer
test best model if test data is present
get and return the final test score of best model
MetricHistoryPlugin -> stores the loss history in return_values
"Store return values, as they will be erased by reset_training_attributes"
get a random sample of training sentences
create a model card for this model with Flair and PyTorch version
record Transformers version if library is loaded
remember all parameters used in train() call
"TextDataset returns a list. valid and test are only one file,"
so return the first element
cast string to Path
error message if the validation dataset is too small
Shuffle training files randomly after serially iterating
through corpus one
"iterate through training data, starting at"
self.split (for checkpointing)
off by one for printing
go into train mode
reset variables
not really sure what this does
do the forward pass in the model
try to predict the targets
Backward
`clip_grad_norm` helps prevent the exploding gradient
problem in RNNs / LSTMs.
We detach the hidden state from how it was
previously produced.
"If we didn't, the model would try backpropagating"
all the way to start of the dataset.
explicitly remove loss to clear up memory
#########################################################
Save the model if the validation loss is the best we've
seen so far.
#########################################################
print info
#########################################################
##############################################################################
final testing
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
"no need to check for MetricName, as __add__ of other would be called in this case"
"This flag tracks, whether an event is currently being processed (otherwise it is added to the queue)"
instantiate plugin
"Reset the flag, since an exception event might be dispatched"
"If there is no **kw argument in the callback, check if any of the passed kw args is not accepted by"
the callback
go through all attributes
get attribute hook events (may raise an AttributeError)
register function as a hook
"Decorator was used with parentheses, but no args"
Decorator was used with args (strings specifiying the events)
Decorator was used without args
path to store the model
special annealing modes
determine the min learning rate
"minimize training loss if training with dev data, else maximize dev score"
instantiate the scheduler
stop training if learning rate becomes too small
reload last best model if annealing with restarts is enabled
calculate warmup steps
skip if no optimization has happened.
saves the model with full vocab as checkpoints etc were created with reduced vocab.
TODO: check if metric is in tracked metrics
prepare loss logging file and set up header
set up all metrics to collect
set up headers
name: HEADER
Add all potentially relevant metrics. If a metric is not published
"after the first epoch (when the header is written), the column is"
removed at that point.
initialize the first log line
record is a list of scalars
output log file
remove columns where no value was found on the first epoch (could be != 1 if training was resumed)
make headers on epoch 1
write header
adjust alert level
"the default model for ELMo is the 'original' model, which is very large"
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
put on Cuda if available
embed a dummy sentence to determine embedding_length
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
"multilingual forward (English, German, French, Italian, Dutch, Polish)"
"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
Dutch forward
Dutch backward
Swedish forward
Swedish backward
French forward
French backward
Czech forward
Czech backward
Portuguese forward
Portuguese backward
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
"if only one sentence is passed, convert to list of sentence"
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
"legacy pickle-like saving for image embeddings, as implementation details are not obvious"
"legacy pickle-like loading for image embeddings, as implementation details are not obvious"
"<cls> token initially set to 1/D, so it attends to all image features equally"
add positional encodings
reshape the pixels into the sequence
layer norm after convolution and positional encodings
add <cls> token
"transformer requires input in the shape [h*w+1, b, d]"
the output is an embedding of <cls> token
this parameter is fixed
optional fine-tuning on top of embedding layer
"if only one sentence is passed, convert to list of sentence"
"if only one sentence is passed, convert to list of sentence"
bidirectional RNN on top of embedding layer
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push through RNN
after-RNN dropout
extract embeddings from RNN
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
serialize the language models and the constructor arguments (but nothing else)
re-initialize language model with constructor arguments
special handling for deserializing language models
copy over state dictionary to self
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
"in their ""self.train()"" method)"
IMPORTANT: add embeddings as torch modules
iterate over sentences
"if its a forward LM, take last state"
"convert to plain strings, embedded in a list for the encode function"
CNN
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push CNN
after-CNN dropout
extract embeddings from CNN
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
"if only one sentence is passed, convert to list of sentence"
Expose base classses
Expose document embedding classes
Expose image embedding classes
Expose legacy embedding classes
Expose token embedding classes
in some cases we need to insert zero vectors for tokens without embedding.
padding
remove special markup
check if special tokens exist to circumvent error message
iterate over subtokens and reconstruct tokens
remove special markup
check if reconstructed token is special begin token ([CLS] or similar)
some BERT tokenizers somehow omit words - in such cases skip to next token
"we cannot handle unk_tokens perfectly, so let's assume that one unk_token corresponds to one token."
if tokens are unaccounted for
check if all tokens were matched to subtokens
The layoutlm tokenizer doesn't handle ocr themselves
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
"cannot run `.encode` if ocr boxes are required, assume"
"transformers returns the ""added_tokens.json"" even if it doesn't create it"
"transformers returns the ""added_tokens.json"" even if it doesn't create it"
in case of doubt: token embedding has higher priority than document embedding
random check some tokens to save performance.
Models such as FNet do not have an attention_mask
set language IDs for XLM-style transformers
"word_ids is only supported for fast rust tokenizers. Some models like ""xlm-mlm-ende-1024"" do not have"
"a fast tokenizer implementation, hence we need to fall back to our own reconstruction of word_ids."
set context if not set already
flair specific pre-tokenization
fields to store left and right context
expand context only if context_length is set
"if context_dropout is set, randomly deactivate left context during training"
"if context_dropout is set, randomly deactivate right context during training"
"if use_context_separator is set, add a [FLERT] token"
return expanded sentence and context length information
"onnx prepares numpy arrays, no mather if it runs on gpu or cpu, the input is on cpu first."
temporary fix to disable tokenizer parallelism warning
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
do not print transformer warnings as these are confusing in this case
load tokenizer and transformer model
load tokenizer from inmemory zip-file
if model is quantized by BitsAndBytes this will fail
add adapters for finetuning
peft_config: PeftConfig
model name
embedding parameters
send mini-token through to check how many layers the model has
return length
"If we use a context separator, add a new special token"
"most models have an initial BOS token, except for XLNet, T5 and GPT2"
"when initializing, embeddings are in eval mode by default"
in case of doubt: token embedding has higher priority than document embedding
in case of doubt: token embedding has higher priority than document embedding
legacy TransformerDocumentEmbedding
legacy TransformerTokenEmbedding
legacy Flair <= 0.12
legacy Flair <= 0.7
legacy TransformerTokenEmbedding
Legacy TransformerDocumentEmbedding
legacy TransformerTokenEmbedding
legacy TransformerDocumentEmbedding
some models like the tars model somehow lost this information.
copy values from new embedding
those parameters are only from the super class and will be recreated in the constructor.
cls first pooling can be done without recreating sentence hidden states
make the tuple a tensor; makes working with it easier.
"for multimodal models like layoutlmv3, we truncate the image embeddings as they are only used via attention"
only use layers that will be outputted
this parameter is fixed
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
make compatible with serialized models
gensim version 4
gensim version 3
"if no embedding is set, the vocab and embedding length is required"
GLOVE embeddings
TURIAN embeddings
KOMNINOS embeddings
pubmed embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
twitter embeddings
two-letter language code wiki embeddings
two-letter language code wiki embeddings
two-letter language code crawl embeddings
"this is required to force the module on the cpu,"
"if a parent module is put to gpu, the _apply is called to each sub_module"
self.to(..) actually sets the device properly
this ignores the get_cached_vec method when loading older versions
it is needed for compatibility reasons
gensim version 4
gensim version 3
"when loading the old versions from pickle, the embeddings might not be added as pytorch module."
"we do this delayed, when the weights are collected (e.g. for saving), as doing this earlier might"
lead to issues while loading (trying to load weights that weren't stored as python weights and therefore
not finding them)
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
multilingual models
English models
Arabic
Bulgarian
Czech
Danish
German
Spanish
Basque
Persian
Finnish
French
Hebrew
Hindi
Croatian
Indonesian
Italian
Japanese
Malayalam
Dutch
Norwegian
Polish
Portuguese
Pubmed
Slovenian
Swedish
Tamil
Spanish clinical
CLEF HIPE Shared task
Amharic
Ukrainian
load model if in pretrained model map
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
CLEF HIPE models are lowercased
embeddings are static if we don't do finetuning
embed a dummy sentence to determine embedding_length
set to eval mode
"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout"
gradients are enable if fine-tuning is enabled
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
offset mode that extracts at whitespace after last character
offset mode that extracts at last character
make compatible with old models
use the character language model embeddings as basis
length is twice the original character LM embedding length
these fields are for the embedding memory
whether to add only capitalized words to memory (faster runtime and lower memory consumption)
we re-compute embeddings dynamically at each epoch
set the memory method
memory is wiped each time we do a training run
"if we keep a pooling, it needs to be updated continuously"
update embedding
check token.text is empty or not
set aggregation operation
add embeddings after updating
model architecture
model architecture
"""pl"","
download if necessary
load the model
"this is required to force the module on the cpu,"
"if a parent module is put to gpu, the _apply is called to each sub_module"
self.to(..) actually sets the device properly
"when loading the old versions from pickle, the embeddings might not be added as pytorch module."
"we do this delayed, when the weights are collected (e.g. for saving), as doing this earlier might"
lead to issues while loading (trying to load weights that weren't stored as python weights and therefore
not finding them)
old embeddings do not have a torch-embedding and therefore do not store the weights in the saved torch state_dict
"however they are already initialized rightfully, so we just set the state dict from our current state dict"
GLOVE embeddings
no need to recreate as NILCEmbeddings
read in test file if exists
read in dev file if exists
"find train, dev and test files if not specified"
Add tags for each annotated span
Remove leading and trailing whitespaces from annotated spans
Search start and end token index for current span
If end index is not found set to last token
Throw error if indices are not valid
Add metadatas for sentence
Currently all Jsonl Datasets are stored in Memory
get train data
read in test file if exists
read in dev file if exists
"find train, dev and test files if not specified"
special key for space after
special key for feature columns
special key for dependency head id
"store either Sentence objects in memory, or only file offsets"
"most data sets have the token text in the first column, if not, pass 'text' as column"
determine encoding of text file
identify which columns are spans and which are word-level
now load all sentences
skip first line if to selected
option 1: keep Sentence objects in memory
pointer to previous
parse next sentence
quit if last sentence reached
skip banned sentences
set previous and next sentence for context
append parsed sentence to list in memory
option 2: keep source data in memory
"read lines for next sentence, but don't parse"
quit if last sentence reached
append raw lines for each sentence
we make a distinction between word-level tags and span-level tags
read first sentence to determine which columns are span-labels
skip first line if to selected
check the first 5 sentences
go through all annotations and identify word- and span-level annotations
- if a column has at least one BIES we know it's a Span label
"- if a column has at least one tag that is not BIOES, we know it's a Token label"
- problem cases are columns for which we see only O - in this case we default to Span
skip assigned columns
the space after key is always word-levels
"if at least one token has a BIES, we know it's a span label"
"if at least one token has a label other than BIOES, we know it's a token label"
all remaining columns that are not word-level are span-level
for column in self.word_level_tag_columns:
"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")"
"if sentence ends, break"
parse comments if possible
"otherwise, this line is a token. parse and add to sentence"
check if this sentence is a document boundary
add span labels
discard tags from tokens that are not added to the sentence
parse relations if they are set
head and tail span indices are 1-indexed and end index is inclusive
parse comments such as '# id cd27886d-6895-4d02-a8df-e5fa763fa88f	domain=de-orcas'
"to set the metadata ""domain"" to ""de-orcas"""
get fields from line
get head_id if exists (only in dependency parses)
initialize token
go through all columns
'feats' and 'misc' column should be split into different fields
special handling for whitespace after
add each other feature as label-value pair
get the task name (e.g. 'ner')
get the label value
add label
remap regular tag names
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
set sentence context using partials TODO: pointer to dataset is really inefficient
use all domains
iter over all domains / sources and create target files
The conll representation of coref spans allows spans to
"overlap. If spans end or begin at the same word, they are"
"separated by a ""|""."
The span begins at this word.
The span begins and ends at this word (single word span).
"The span is starting, so we record the index of the word."
"The span for this id is ending, but didn't start at this word."
Retrieve the start index from the document state and
add the span to the clusters for this id.
strip all bracketing information to
get the actual propbank label.
Entering into a span for a particular semantic role label.
We append the label and set the current span for this annotation.
"If there's no '(' token, but the current_span_label is not None,"
then we are inside a span.
We're outside a span.
"Exiting a span, so we reset the current span label for this annotation."
The words in the sentence.
The pos tags of the words in the sentence.
the pieces of the parse tree.
The lemmatised form of the words in the sentence which
have SRL or word sense information.
The FrameNet ID of the predicate.
"The sense of the word, if available."
"The current speaker, if available."
"Cluster id -> List of (start_index, end_index) spans."
Cluster id -> List of start_indices which are open for this id.
Replace brackets in text and pos tags
with a different token for parse trees.
only keep ')' if there are nested brackets with nothing in them.
There are some bad annotations in the CONLL data.
"They contain no information, so to make this explicit,"
we just set the parse piece to be None which will result
in the overall parse tree being None.
"If this is the first word in the sentence, create"
empty lists to collect the NER and SRL BIO labels.
"We can't do this upfront, because we don't know how many"
"components we are collecting, as a sentence can have"
variable numbers of SRL frames.
Create variables representing the current label for each label
sequence we are collecting.
"If any annotation marks this word as a verb predicate,"
we need to record its index. This also has the side effect
of ordering the verbal predicates by their location in the
"sentence, automatically aligning them with the annotations."
"this would not be reached if parse_pieces contained None, hence the cast"
Non-empty line. Collect the annotation.
Collect any stragglers or files which might not
have the '#end document' format for the end of the file.
this dataset name
check if data there
column format
this dataset name
check if data there
column format
this dataset name
download data if necessary
download files if not present locally
we need to slightly modify the original files by adding some new lines after document separators
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
Remove CoNLL-U meta information in the last column
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
entity_mapping
this dataset name
download data if necessary
data validation
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
download files if not present locallys
column format
this dataset name
default dataset folder is the cache root
# download zip
merge the files in one as the zip is containing multiples files
column format
this dataset name
download data if necessary
"unzip the downloaded repo and merge the train, dev and test datasets"
column format
this dataset name
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
check if data there
create folder
download dataset
column format
this dataset name
download and parse data if necessary
create train test dev if not exist
column format
this dataset name
If the extracted corpus file is not yet present in dir
download zip if necessary
"extracted corpus is not present , so unpacking it."
column format
this dataset name
download zip
unpacking the zip
merge the files in one as the zip is containing multiples files
column format
this dataset name
"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)"
download files if not present locally
we need to modify the original files by adding new lines after after the end of each sentence
if only one language is given
column format
this dataset name
"use all languages if explicitly set to ""all"""
download data if necessary
initialize comlumncorpus and add it to list
column format
this dataset name
column format
this dataset name
column format
this dataset name
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
this list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
get google drive id from list
download from google drive
unzip
"tar.extractall(language_folder,members=[tar.getmember(file_name)])"
transform data into required format
"the processed dataset has the additional ending ""_new"""
remove the unprocessed dataset
initialize comlumncorpus and add it to list
if no languages are given as argument all languages used in XTREME will be loaded
if only one language is given
column format
this dataset name
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
This list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
download from HU Server
unzip
transform data into required format
initialize comlumncorpus and add it to list
if only one language is given
column format
this dataset name
download data if necessary
initialize comlumncorpus and add it to list
download data if necessary
unpack and write out in CoNLL column-like format
column format
this dataset name
download data if necessary
data is not in IOB2 format. Thus we transform it to IOB2
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
column format
this dataset name
rename according to train - test - dev - convention
column format
this dataset name
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
Add missing newline after header
Workaround for empty tokens
"Add ""real"" document marker"
Dataset split mapping
v2.0 only adds new language and splits for AJMC dataset
Special document marker for sample splits in AJMC dataset
column format
this dataset name
download data if necessary
column format
this dataset name
if only one language is given
column format
this dataset name
"use all languages if explicitly set to ""all"""
download data if necessary
initialize comlumncorpus and add it to list
this dataset name
one name can map to multiple concepts
NOTE: EntityLinkingDictionary are lazy-loaded from a preprocessed file.
Use this class to load into memory all candidates
"if identifier == ""MESH:D013749"":"
# This MeSH ID was used by MeSH when this chemical was part of the MeSH controlled vocabulary.
continue
parse line
this dataset name
default dataset folder is the cache root
download and parse data if necessary
paths to train and test splits
init corpus
this dataset name
default dataset folder is the cache root
download and parse data if necessary
iterate over all html files
"get rid of html syntax, we only need the text"
between all documents we write a separator symbol
skip empty strings
"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)"
"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention"
sentence splitting and tokenization
iterate through all annotations and add to corresponding tokens
find sentence to which annotation belongs
position within corresponding sentence
set annotation for tokens of entity mention
write to out-file in column format
"in case something goes wrong, delete the dataset and raise error"
this dataset name
download and parse data if necessary
from qwikidata.linked_data_interface import get_entity_dict_from_api
generate qid wikiname dictionaries
merge dictionaries
ignore first line
commented and empty lines
read all Q-IDs
ignore first line
request
this dataset name
we use the wikiids in the data instead of directly utilizing the wikipedia urls.
like this we can quickly check if the corresponding page exists
if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi
delete unprocessed file
collect all wikiids
create the dictionary
request
this dataset name
names of raw text documents
open output_file
iterate through all documents
split sentences and tokenize
iterate through all annotations and add to corresponding tokens
find sentence to which annotation belongs
position within corresponding sentence
set annotation for tokens of entity mention
write to out file
annotation from one annotator or two agreeing annotators
this dataset name
download and parse data if necessary
this dataset name
download and parse data if necessary
First parse the post titles
Keep track of how many and which entity mentions does a given post title have
Check if the current post title has an entity link and parse accordingly
Post titles with entity mentions (if any) are handled via this function
Then parse the comments
"Iterate over the comments.tsv file, until the end is reached"
"Keep track of the current comment thread and its corresponding key, on which the annotations are matched."
Each comment thread is handled as one 'document'.
Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.
This if-condition is needed to handle this problem.
"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure"
"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above"
"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle."
"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,"
and not just single letters into single rows.
If there are annotated entity mentions for given post title or a comment thread
"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence"
Write the token with a corresponding tag to file
"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed"
"If a comment thread or a post title has no entity link, all tokens are assigned the O tag"
Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized
"incorrectly, in order to keep the desired format (empty line as a sentence separator)."
"Thrown when the second check above happens, but the last token of a sentence is reached."
"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below."
"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS"
Check if further annotations belong to the current post title or comment thread as well
Stop when the end of an annotation file is reached
Check if further annotations belong to the current sentence as well
"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)"
Docstart
if there is more than one word in the chunk we write each in a separate line
print(chunks)
empty line after each sentence
convert the file to CoNLL
this dataset name
"check if data there, if not, download the data"
create folder
download data
transform data into column format if necessary
if no filenames are specified we use all the data
"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled"
also we remove 'raganato_ALL' from filenames in case its in the list
generate the test file
make column file and save to data_folder
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
TODO: Adapt this following: https://github.com/flairNLP/flair/pull/3146
+1 assumes the title and abstract will be joined by a space.
"We need a unique identifier for this entity, so build it from the document id and entity id"
The user can provide a callable that returns the database name.
some entities are not linked and
some entities are linked to multiple normalized ids
passages must not overlap and spans must cover the entire document
entities
parse db ids
Some of the entities have a off-by-one error. Correct these annotations!
"passage offsets/lengths do not connect, recalculate them for this schema."
this dataset name
download data if necessary
if True:
write CoNLL-U Plus header
"Some special cases (e.g., missing spaces before entity marker)"
necessary if text should be whitespace tokenizeable
Handle case where tail may occur before the head
this dataset name
write CoNLL-U Plus header
this dataset name
TODO: change data source to original CoNLL04 -- this dataset has span formatting errors
download data if necessary
write CoNLL-U Plus header
The span has ended.
We are entering a new span; reset indices
and active tag to new span.
We're inside a span.
Last token might have been a part of a valid span.
this dataset name
write CoNLL-U Plus header
"for source_file_path, target_filename in zip(source_file_paths, target_filenames):"
"with zip_file.open(source_file_path, mode=""r"") as source_file:"
target_file_path = Path(data_folder) / target_filename
"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:"
# write CoNLL-U Plus header
"target_file.write(""# global.columns = id form ner\n"")"
for example in json.load(source_file):
token_list = self._tacred_example_to_token_list(example)
target_file.write(token_list.serialize())
check if first tag row is already occupied
"if first tag row is occupied, use second tag row"
hardcoded mapping TODO: perhaps find nicer solution
remap regular tag names
else skip to position in file where sentence begins
set sentence context using partials TODO: pointer to dataset is really inefficient
read in dev file if exists
read in test file if exists
the url is copied from https://huggingface.co/datasets/darentang/sroie/blob/main/sroie.py#L44
"find train, dev and test files if not specified"
use test_file to create test split if available
use dev_file to create test split if available
"if data point contains black-listed label, do not use"
first check if valid sentence
"if so, add to indices"
"find train, dev and test files if not specified"
variables
different handling of in_memory data than streaming data
"most data sets have the token text in the first column, if not, pass 'text' as column"
test if format is OK
test if at least one label given
make sentence from text (and filter for length)
"if a pair column is defined, make a sentence pair object"
noinspection PyDefaultArgument
dataset name includes the split size
default dataset folder is the cache root
download data if necessary
download each of the 28 splits
create dataset directory if necessary
download senteval datasets if necessary und unzip
this dataset name
download data if necessary
this dataset name
download data if necessary
download data from same source as in huggingface's implementations
read label order
"Original labels are [1, 2, 3, 4] -> ['World', 'Sports', 'Business', 'Sci/Tech']"
"Re-map to [0, 1, 2, 3]."
this dataset name
download data if necessary
handle labels file
handle data file
Create flair compatible labels
"by default, map point score to POSITIVE / NEGATIVE values"
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file from CSV
create test.txt file from CSV
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create train dev and test files in fasttext format
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
convert to FastText format
download data if necessary
"if data is not downloaded yet, download it"
get the zip file
move original .tsv files to another folder
create train and dev splits in fasttext format
create eval_dataset file with no labels
download zip archive
unpack file in datasets directory (zip archive contains a directory named SST-2)
this dataset name
default dataset folder is the cache root
download data if necessary
download datasets if necessary
create dataset directory if necessary
create correctly formated txt files
multiple labels are possible
this dataset name
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
download data if necessary
this dataset name
download data if necessary
create a separate directory for different tasks
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
check if dataset is supported
set file names
set file names
download and unzip in file structure if necessary
instantiate corpus
"find train, dev and test files if not specified"
"create DataPairDataset for train, test and dev file, if they are given"
stop if file does not exist
create a DataPair object from strings
"if in_memory is True we return a datapair, otherwise we create one from the lists of strings"
"find train, dev, and test files if not specified"
"create DataTripleDataset for train, test, and dev files, if they are given"
stop if the file does not exist
create a DataTriple object from strings
"if in_memory is True we return a DataTriple, otherwise we create one from the lists of strings"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"reorder dev datasets to have same columns as in train set: 8, 9, and 11"
dev sets include 5 different annotations but we will only keep the gold label
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get test and dev sets
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data not downloaded yet, download it"
get the zip file
"the downloaded files have json format, we transform them to tsv"
Function to transform JSON file to tsv for Recognizing Textual Entailment Data
remove json file
Uses dynamic programming approach to calculate maximum independent set in interval graph
with sum of all entity lengths as secondary key
calculate offset without current text
because we stick all passages of a document together
TODO For split entities we also annotate everything inbetween which might be a bad idea?
Try to fix incorrect annotations
print(
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
)
Ignore empty lines or relation annotations
FIX annotation of whitespaces (necessary for PDR)
Add task description for multi-task learning
One token may contain multiple entities -> deque all of them
column format
this dataset name
Create tokenization-dependent CONLL files. This is necessary to prevent
from caching issues (e.g. loading the same corpus with different sentence splitters)
column format
this dataset name
column format
this dataset name
Edge case: last token starts a new entity
Last document in file
column format
this dataset name
column format
this dataset name
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
Edge case: last token starts a new entity
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
column format
this dataset name
Read texts
Read annotations
column format
this dataset name
column format
this dataset name
column format
this dataset name
We need to apply a patch to correct the original training file
Articles title
Article abstract
Entity annotations
column format
this dataset name
Edge case: last token starts a new entity
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
Incomplete article
Invalid XML syntax
column format
this dataset name
column format
this dataset name
if len(mid) != 3:
continue
Try to fix entity offsets
column format
this dataset name
There is still one illegal annotation in the file ..
column format
this dataset name
"Abstract first, title second to prevent issues with sentence splitting"
column format
this dataset name
column format
this dataset name
column format
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
column format
this dataset name
"Filter for specific entity types, by default no entities will be filtered"
Get original HUNER splits to retrieve a list of all document ids contained in V2
train and dev split of V2 will be train in V4
test split of V2 will be dev in V4
New documents in V4 will become test documents
column format
this dataset name
column format
this dataset name
default dataset folder is the cache root
column format
build dataset name and full huggingface reference name
Download data if necessary
"Some datasets in BigBio only have train or test splits, not both"
"If only test split, assign it to train split"
"If only train split, sample other from it (sample_missing_splits=True)"
Not every dataset has a dev / validation set!
Perform type mapping if necessary
return None
TODO: Add entity type mapping for all remaining bigbio datasets not in HunFlair?
"""simple_chemical"": ""chemical"",  # BioNLP ST 2013 CG"
"""cancer"": ""disease"",  # BioNLP ST 2013 CG"
"""gene_or_gene_product"": ""gene"",  # BioNLP ST 2013 CG"
"""gene"": ""gene"",  # NLM Gene"
"""chemical"": ""chemical"",  # NLM Chem"
"""cellline"": ""cell_line"",  # Cell Finder"
"""species"": ""species"",  # Cell Finder"
"""protein"": ""gene"",  # BioID"
"Collect all texts of the document, each passage will be"
a text in our internal format
Sort passages by start offset
Transform all entity annotations into internal format
Find the passage of the entity (necessary for offset adaption)
Adapt entity offsets according to passage offsets
FIXME: This is just for debugging purposes
passage_text = id_to_text[passage_id]
doc_text = passage_text[entity_offset[0] : entity_offset[1]]
"mention_text = entity[""text""][0]"
if doc_text != mention_text:
"print(f""Annotation error ({document['document_id']}) - Doc: {doc_text} vs. Mention: {mention_text}"")"
Get element in the middle
Is the mention with the passage offsets?
"If element is smaller than mid, then it can only"
be present in left subarray
Else the element can only be present in right subarray
TODO whether cell or cell line is the correct tag
TODO whether cell or cell line is the correct tag
Special case for ProGene: We need to use the split_0_train and split_0_test splits
as they are currently provided in BigBio
cache Feidegger config file
cache Feidegger images
replace image URL with local cached file
append Sentence-Image data point
cast to list if necessary
cast to list if necessary
"first, check if pymongo is installed"
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
Expose base classses
Expose all biomedical data sets used for the evaluation of BioBERT
-
-
-
-
Expose all biomedical data sets using the HUNER splits
Expose all biomedical data sets
Expose all document classification datasets
word sense disambiguation
Expose all entity linking datasets
Expose all relation extraction datasets
universal proposition banks
keyphrase detection datasets
other NER datasets
standard NER datasets
Expose all sequence labeling datasets
Expose all text-image datasets
Expose all text-text datasets
Expose all treebanks
"find train, dev and test files if not specified"
get train data
get test data
get dev data
option 1: read only sentence boundaries as offset positions
option 2: keep everything in memory
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
current token ID
handling for the awful UD multiword format
end of sentence
comments or ellipsis
if token is a multi-word
normal single-word tokens
"if we don't split multiwords, skip over component words"
add token
add morphological tags
derive whitespace logic for multiwords
"if multi-word equals component tokens, there should be no whitespace"
go through all tokens in subword and set whitespace_after information
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
this dataset name
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
"finally, print model card for information"
noqa: INP001
-- Project information -----------------------------------------------------
"The full version, including alpha/beta/rc tags"
use smv_current_version as the git url
-- General configuration ---------------------------------------------------
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Napoleon settings
Whitelist pattern for tags (set to None to ignore all tags)
Whitelist pattern for branches (set to None to ignore all branches)
Whitelist pattern for remotes (set to None to use local branches only)
Pattern for released versions
Format for versioned output directories inside the build directory
Determines whether remote or local git branches/tags are preferred if their output dirs conflict
test corpus
create a TARS classifier
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
test corpus
create a TARS classifier
switch to a new task (TARS can do multiple tasks so you must define one)
initialize the text classifier trainer
start the training
"With end symbol, without start symbol, padding in front"
"Without end symbol, with start symbol, padding in back"
"Without end symbol, without start symbol, padding in front"
initialize trainer
initialize trainer
initialize trainer
increment for last token in sentence if not followed by whitespace
clean up directory
clean up directory
example sentence
set 4 labels for 2 tokens ('love' is tagged twice)
check if there are three POS labels with correct text and values
check if there are is one SENTIMENT label with correct text and values
check if all tokens are correctly labeled
remove the pos label from the last word
there should be 2 POS labels left
now remove all pos tags
set 3 labels for 2 spans (HU is tagged twice)
check if there are three labels with correct text and values
check if there are two spans with correct text and values
"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
should be only one NER label left
and only one NER span
set 3 labels for 2 spans (HU is tagged twice with different tags)
check if there are three labels with correct text and values
check if there are two spans with correct text and values
"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
should be only one NER label left
and only one NER span
but there is also one orgtype span and label
and only one NER span
let's add the NER tag back
check if there are three labels with correct text and values
check if there are two spans with correct text and values
now remove all NER tags
set 3 labels for 2 spans (HU is tagged twice with different tags)
create two relation label
there should be two relation labels
there should be one syntactic labels
"there should be two relations, one with two and one with one label"
example sentence
add another topic label
example sentence
has sentiment value
has 4 part of speech tags
has 1 NER tag
should be in total 6 labels
example sentence
add two NER labels
get the four labels
check that only two of the respective data points are equal
make a sentence and some right context
TODO: is this desirable? Or should two sentences with same text be considered same objects?
Initializing a Sentence this way assumes that there is a space after each token
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
load column dataset with one entry
load column dataset with two entries
load column dataset with three entries
"get training, test and dev data"
"get training, test and dev data"
check if Token labels are correct
"get training, test and dev data"
check if Token labels for frames are correct
"get training, test and dev data"
"get training, test and dev data"
get two corpora as one
"get training, test and dev data for full English UD corpus from web"
clean up data directory
"assert [token.get_tag(""head"").value for token in sent1.tokens] == ["
"""2"","
"""0"","
"""4"","
"""2"","
"""2"","
"""2"","
]
This test only covers basic universal dependencies datasets.
"For example, multi-word tokens or the ""deps"" column sentence annotations are not supported yet."
"Here, we use the default token annotation fields."
This test covers the complete HIPE 2022 dataset.
https://github.com/hipe-eval/HIPE-2022-data
"Includes variant with document separator, and all versions of the dataset."
"We have manually checked, that these numbers are correct:"
"+1 offset, because of missing EOS marker at EOD"
Test data for v2.1 release
This test covers the complete ICDAR Europeana corpus:
https://github.com/stefan-it/historic-domain-adaptation-icdar
"This test covers the complete MasakhaNER dataset, including support for v1 and v2."
This test covers the NERMuD dataset. Official stats can be found here:
https://github.com/dhfbk/KIND/tree/main/evalita-2023
Number of instances per dataset split are taken from https://huggingface.co/datasets/elenanereiss/german-ler
This test covers the complete MasakhaPOS dataset.
"See MobIE paper (https://aclanthology.org/2021.konvens-1.22/), table 2"
--- Embeddings that are shared by both models --- #
--- Task 1: Sentiment Analysis (5-class) --- #
Define corpus and model
-- Task 2: Binary Sentiment Analysis on Customer Reviews -- #
Define corpus and model
-- Define mapping (which tagger should train on which model) -- #
-- Create model trainer and train -- #
NOTE: Avoid emtpy string if mentions are just punctutations (e.g. `-` or `(`)
clean up file
no need for label_dict
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
Intel ----founded_by---> Gordon Moore
Intel ----founded_by---> Robert Noyce
"Ground truth is a set of tuples of (<Sentence Text>, <Relation Label Values>)"
Check sentence masking and relation label annotation on
"training, validation and test dataset (in this test the splits are the same)"
"Entity pair permutations of: ""Larry Page and Sergey Brin founded Google ."""
"Entity pair permutations of: ""Microsoft was founded by Bill Gates ."""
"Entity pair permutations of: ""Konrad Zuse was born in Berlin on 22 June 1910 ."""
"Entity pair permutations of: ""Joseph Weizenbaum , a professor at MIT , was born in Berlin , Germany."""
This sentence is only included if we transform the corpus with cross augmentation
Ensure this is an example that predicts no classes in multilabel
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
ensure that the prepared tensors is what we expect
use a SequenceTagger to save and reload the embedding in the manner it is supposed to work
previous and next sentence as context
test expansion for sentence without context
test expansion for with previous and next as context
test expansion if first sentence is document boundary
test expansion if we don't use context
"apparently the precision is not that high on cuda, hence the absolute tolerance needs to be higher."
dummy model with embeddings
save the dummy and load it again
check that context_length and use_context_separator is the same for both
mmap seems to be much more memory efficient
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
Extract all the contents of zip file in current directory
Extract all the contents of zip file in current directory
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
load_big_file is a workaround byhttps://github.com/highway11git
to load models on some Mac/Windows setups
see https://github.com/zalandoresearch/flair/issues/351
first determine the distribution of classes in the dataset
weight for each sample
Create blocks
shuffle the blocks
concatenate the shuffled blocks
Create blocks
shuffle the blocks
concatenate the shuffled blocks
increment for last token in sentence if not followed by whitespace
this is the default init size of a lmdb database for embeddings
get db filename from embedding name
"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
SequenceTagger
TextClassifier
get db filename from embedding name
if embedding database already exists
"otherwise, push embedding to database"
if embedding database already exists
open the database in read mode
we need to set self.k
create and load the database in write mode
"no idea why, but we need to close and reopen the environment to avoid"
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
when opening new transaction !
init dictionaries
"in order to deal with unknown tokens, add <unk>"
set 'add_unk' if the dictionary was created with a version of Flair older than 0.9
set 'add_unk' depending on whether <unk> is a key
"if one embedding name, directly return it"
"if multiple embedding names, concatenate them"
First we remove any existing labels for this PartOfSentence in self.sentence
labels also need to be deleted at Sentence object
delete labels at object itself
The Token is a special _PartOfSentence in that it may be initialized without a Sentence.
"therefore, labels get added only to the Sentence if it exists"
The Token is a special _PartOfSentence in that it may be initialized without a Sentence.
"Therefore, labels get set only to the Sentence if it exists"
"check if the span already exists. If so, return it"
else make a new span
"check if the relation already exists. If so, return it"
else make a new relation
private field for all known spans
the tokenizer used for this sentence
some sentences represent a document boundary (but most do not)
internal variables to denote position inside dataset
"if text is passed, instantiate sentence with tokens (words)"
determine token positions and whitespace_after flag
the last token has no whitespace after
log a warning if the dataset is empty
data with zero-width characters cannot be handled
set token idx and sentence
append token to sentence
register token annotations on sentence
move sentence embeddings to device
also move token embeddings to device
clear token embeddings
infer whitespace after field
"if sentence has no tokens, return empty string"
"otherwise, return concatenation of tokens with the correct offsets"
The sentence's start position is not propagated to its tokens.
"Therefore, we need to add the sentence's start position to its last token's end position, including whitespaces."
No character at the corresponding code point: remove it
"if no label if specified, return all labels"
"if the label type exists in the Sentence, return it"
return empty list if none of the above
labels also need to be deleted at all tokens
labels also need to be deleted at all known spans
remove spans without labels
delete labels at object itself
set name
abort if no data is provided
sample test data from train if none is provided
sample dev data from train if none is provided
set train dev and test data
find out empty sentence indices
create subset of non-empty sentence indices
find out empty sentence indices
create subset of non-empty sentence indices
"first, determine the datapoint type by going through dataset until first label is found"
count all label types per sentence
go through all labels of label_type and count values
special handling for Token-level annotations. Add all untagged as 'O' label
"if an unk threshold is set, UNK all label values below this threshold"
sample randomly from a label distribution according to the probabilities defined by the noise transition matrix
replace the old label with the new one
keep track of the old (clean) label using another label type category
keep track of how many labels in total are flipped
sample randomly from a label distribution according to the probabilities defined by the desired noise share
replace the old label with the new one
keep track of the old (clean) label using another label type category
keep track of how many labels in total are flipped
"add a dummy ""O"" to close final prediction"
return complex list
internal variables
non-set tags are OUT tags
anything that is not OUT is IN
does this prediction start a new span?
B- and S- always start new spans
"if the predicted class changes, I- starts a new span"
"if the predicted class changes and S- was previous tag, start a new span"
if an existing span is ended (either by reaching O or starting a new span)
determine score and value
append to result list
reset for-loop variables for new span
remember previous tag
global variable: cache_root
global variable: device
"No need for correctness checks, torch is doing it"
global variable: version
global variable: arrow symbol
dummy return to fulfill trainer.train() needs
print(vec)
Attach optimizer
"convert `metrics` to float, in case it's a zero-dim Tensor"
if memory mode option 'none' delete everything
"if dynamic embedding keys not passed, identify them automatically"
always delete dynamic embeddings
"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)"
optional metric space decoder if prototypes have different length than embedding
create initial prototypes for all classes (all initial prototypes are a vector of all 1s)
"if set, create initial prototypes from normal distribution"
"if set, use a radius"
all parameters will be pushed internally to the specified device
decode embeddings into prototype space
"if unlabeled distance is set, mask out loss to unlabeled class prototype"
verbalize BIOES labels
"if label is not BIOES, use label itself"
Always include the name of the Model class for which the state dict holds
"write out a ""model card"" if one is set"
save model
"if this class is abstract, go through all inheriting classes and try to fetch and load the model"
get all non-abstract subclasses
"try to fetch the model for each subclass. if fetching is possible, load model and return it"
"skip any invalid loadings, e.g. not found on huggingface hub"
"if the model cannot be fetched, load as a file"
try to get model class from state
"older (flair 11.3 and below) models do not contain cls information. In this case, try all subclasses"
"if str(model_cls) == ""<class 'flair.models.pairwise_classification_model.TextPairClassifier'>"": continue"
"skip any invalid loadings, e.g. not found on huggingface hub"
"if this class is not abstract, fetch the model and load it"
"make sure <unk> is contained in gold_label_dictionary, if given"
"read Dataset into data loader, if list of sentences passed, make Dataset first"
loss calculation
variables for printing
variables for computing scores
remove any previously predicted labels
predict for batch
get the gold labels
add to all_predicted_values
make printout lines
convert true and predicted values to two span-aligned lists
delete exluded labels if exclude_labels is given
"if after excluding labels, no label is left, ignore the datapoint"
write all_predicted_values to out_file if set
make the evaluation dictionary
check if this is a multi-label problem
compute numbers by formatting true and predicted such that Scikit-Learn can use them
multi-label problems require a multi-hot vector for each true and predicted label
single-label problems can do with a single index for each true and predicted label
"now, calculate evaluation numbers"
there is at least one gold label or one prediction (default)
compute accuracy separately as it is not always in classification_report (e.. when micro avg exists)
"if there is only one label, then ""micro avg"" = ""macro avg"""
"The ""micro avg"" appears only in the classification report if no prediction is possible."
"Otherwise, it is identical to the ""macro avg"". In this case, we add it to the report."
"Create and populate score object for logging with all evaluation values, plus the loss"
issue error and default all evaluation numbers to 0.
check if there is a label mismatch
print info
set the embeddings
initialize the label dictionary
initialize the decoder
set up multi-label logic
init dropouts
loss weights and loss function
Initialize the weight tensor
set up gradient reversal if so specified
embed sentences
get a tensor of data points
do dropout
make a forward pass to produce embedded data points and labels
get the data points for which to predict labels
get their gold labels as a tensor
pass data points through network to get encoded data point tensor
decode
an optional masking step (no masking in most cases)
calculate the loss
filter empty sentences
reverse sort all sequences by their length
progress bar for verbosity
filter data points in batch
stop if all sentences are empty
pass data points through network and decode
if anything could possibly be predicted
remove previously predicted labels of this type
filter data points that have labels outside of dictionary
add DefaultClassifier arguments
add variables of DefaultClassifier
Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23
Get projected 1st dimension
Compute bilinear form
Arcosh
Project the input data to n+1 dimensions
"The first dimension, is recomputed in the distance module"
header for 'weights.txt'
"determine the column index of loss, f-score and accuracy for"
"train, dev and test split"
then get all relevant values from the tsv
then get all relevant values from the tsv
plot i
save plots
save plots
plt.show()
save plot
auto-spawn on GPU if available
progress bar for verbosity
stop if all sentences are empty
clearing token embeddings to save memory
"read Dataset into data loader, if list of sentences passed, make Dataset first"
TODO: not saving lines yet
TODO: This closely shadows the RelationExtractor name. Maybe we need a better name here.
- MaskedRelationClassifier ?
This depends if this relation classification architecture should replace or offer as an alternative.
Set label type and prepare label dictionary
Initialize super default classifier
Add the special tokens from the encoding strategy
"Auto-spawn on GPU, if available"
Only use entities labelled with the specified labels for each label type
Only use entities above the specified threshold
Use a dictionary to find gold relation annotations for a given entity pair
Yield head and tail entity pairs from the cross product of all entities
Remove identity relation entity pairs
Remove entity pairs with labels that do not match any
of the specified relations in `self.entity_pair_labels`
"Obtain gold label, if existing"
Some sanity checks
Pre-compute non-leading head and tail tokens for entity masking
We can not use the plaintext of the head/tail span in the sentence as the mask/marker
since there may be multiple occurrences of the same entity mentioned in the sentence.
"Therefore, we use the span's position in the sentence."
Create masked sentence
Add gold relation annotation as sentence label
"Using the sentence label instead of annotating a separate `Relation` object is easier to manage since,"
"during prediction, the forward pass does not need any knowledge about the entities in the sentence."
"If we sample missing splits, the encoded sentences that correspond to the same original sentences"
"may get distributed into different splits. For training purposes, this is always undesired."
Ensure that all sentences are encoded properly
Deal with the case where all sentences are encoded sentences
"mypy does not infer the type of ""sentences"" restricted by the if statement"
Deal with the case where all sentences are standard (non-encoded) sentences
"For each encoded sentence, transfer its prediction onto the original relation"
auto-spawn on GPU if available
pad strings with whitespaces to longest sentence
cut up the input into chunks of max charlength = chunk_size
push each chunk through the RNN language model
concatenate all chunks to make final output
initial hidden state
get predicted weights
divide by temperature
"to prevent overflow problem with small temperature values, substract largest value from all"
this makes a vector in which the largest value is 0
compute word weights with exponential function
try sampling multinomial distribution for next character
print(word_idx)
input ids
push list of character IDs through model
the target is always the next character
use cross entropy loss to compare output of forward pass with targets
exponentiate cross-entropy loss to calculate perplexity
"""document_delimiter"" property may be missing in some older pre-trained models"
serialize the language models and the constructor arguments (but nothing else)
special handling for deserializing language models
re-initialize language model with constructor arguments
copy over state dictionary to self
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
"in their ""self.train()"" method)"
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
Transform input data into TARS format
"if there are no labels, return a random sample as negatives"
"otherwise, go through all labels"
make sure the probabilities always sum up to 1
get and embed all labels by making a Sentence object that contains only the label text
get each label embedding and scale between 0 and 1
compute similarity matrix
"the higher the similarity, the greater the chance that a label is"
sampled as negative example
make label dictionary if no Dictionary object is passed
prepare dictionary of tags (without B- I- prefixes and without UNK)
check if candidate_label_set is empty
make list if only one candidate label is passed
create label dictionary
note current task
create a temporary task
make zero shot predictions
switch to the pre-existing task
prepare TARS dictionary
initialize a bare-bones sequence tagger
transformer separator
Store task specific labels since TARS can handle multiple tasks
make a tars sentence where all labels are O by default
init new TARS classifier
set all task information
progress bar for verbosity
stop if all sentences are empty
always remove tags first
go through each sentence in the batch
always remove tags first
get the span and its label
determine whether tokens in this span already have a label
only add if all tokens have no label
make and add a corresponding predicted span
set indices so that no token can be tagged twice
clearing token embeddings to save memory
"all labels default to ""O"""
set gold token-level
set predicted token-level
now print labels in CoNLL format
prepare TARS dictionary
initialize a bare-bones sequence tagger
transformer separator
Store task specific labels since TARS can handle multiple tasks
get the serialized embeddings
remap state dict for models serialized with Flair <= 0.11.3
init new TARS classifier
set all task information
with torch.no_grad():
progress bar for verbosity
stop if all sentences are empty
always remove tags first
go through each sentence in the batch
always remove tags first
add all labels that according to TARS match the text and are above threshold
do not add labels below confidence threshold
only use label with the highest confidence if enforcing single-label predictions
add the label with the highest score even if below the threshold if force label is activated.
remove previously added labels and only add the best label
clearing token embeddings to save memory
set separator to concatenate two sentences
auto-spawn on GPU if available
pooling operation to get embeddings for entites
set embeddings
set relation and entity label types
"whether to use gold entity pairs, and whether to filter entity pairs by type"
filter entity pairs according to their tags if set
whether to encode characters and whether to use attention (attention can only be used if chars are encoded)
character dictionary for decoding and encoding
make sure <unk> is in dictionary for handling of unknown characters
add special symbols to dictionary if necessary and save respective indices
---- ENCODER ----
encoder character embeddings
encoder pre-trained embeddings
encoder RNN
additional encoder linear layer if bidirectional encoding
---- DECODER ----
decoder: linear layers to transform vectors to and from alphabet_size
when using attention we concatenate attention outcome and decoder hidden states
decoder RNN
loss and softmax
self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction
add additional columns for special symbols if necessary
initialize with dummy symbols
encode inputs
get labels (we assume each token has a lemma label)
get char indices for labels of sentence
"(batch_size, max_sequence_length) batch_size = #words in sentence,"
max_sequence_length = length of longest label of sentence + 1
get char embeddings
"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size"
take decoder input and initial hidden and pass through RNN
"if all encoder outputs are provided, use attention"
take convex combinations of encoder hidden states as new output using the computed attention coefficients
"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)"
get all tokens
encode input characters by sending them through RNN
get one-hots for characters and add special symbols / padding
determine length of each token
embed sentences
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
variable to store initial hidden states for decoder
encode input characters by sending them through RNN
test packing and padding
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
concatenate the final hidden states of the encoder. These will be projected to hidden_size of
decoder later with self.emb_to_hidden
mask out vectors that correspond to a dummy symbol (TODO: check attention masking)
use token embedding as initial hidden state for decoder
concatenate everything together and project to appropriate size for decoder
variable to store initial hidden states for decoder
encode input characters by sending them through RNN
note that we do not need to fill up with dummy symbols since we process each token seperately
embed character one-hots
send through encoder RNN (produces initial hidden for decoder)
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
project 2*hidden_size to hidden_size
concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder
later with self.emb_to_hidden
use token embedding as initial hidden state for decoder
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
concatenate everything together and project to appropriate size for decoder
"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)"
"create target vector (batch_size, max_label_seq_length + 1)"
filter empty sentences
max length of the predicted sequences
for printing
stop if all sentences are empty
remove previously predicted labels of this type
create list of tokens in batch
encode inputs
"create input for first pass (batch_size, 1, input_size), first letter is special character <S>"
sequence length is always set to one in prediction
option 1: greedy decoding
predictions
decode next character
pick top beam size many outputs with highest probabilities
option 2: beam search
out_probs = self.softmax(output_vectors).squeeze(1)
make sure no dummy symbol <> or start symbol <S> is predicted
pick top beam size many outputs with highest probabilities
"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1"
"leading_indices and probabilities have size (batch_size, beam_size)"
keep scores of beam_size many hypothesis for each token in the batch
stack all leading indices of all hypothesis and corresponding hidden states in two tensors
save sequences so far
keep track of how many hypothesis were completed for each token
"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)"
decode with log softmax
make sure no dummy symbol <> or start symbol <S> is predicted
"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside"
"if the sequence is already ended, do not record as candidate"
index of token in in list tokens_in_batch
print(token_number)
hypothesis score
TODO: remove token if number of completed hypothesis exceeds given value
set score of corresponding entry to -inf so it will not be expanded
get leading_indices for next expansion
find highest scoring hypothesis among beam_size*beam_size possible ones for each token
take beam_size many copies of scores vector and add scores of possible new extensions
"size (beam_size*batch_size, beam_size)"
print(hypothesis_scores)
"reshape to vector of size (batch_size, beam_size*beam_size),"
each row contains beam_size*beam_size scores of the new possible hypothesis
print(hypothesis_scores_per_token)
"choose beam_size best for each token - size (batch_size, beam_size)"
out of indices_per_token we now need to recompute the original indices of the hypothesis in
a list of length beam_size*batch_size
"where the first three inidices belong to the first token, the next three to the second token,"
and so on
with these indices we can compute the tensors for the next iteration
expand sequences with corresponding index
add log-probabilities to the scores
save new leading indices
save corresponding hidden states
it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations
in that case we append one of the final seuqences without end symbol to the final_candidates
get best final hypothesis for each token
get characters from index sequences and add predicted label to token
"Overwrites evaluate of parent class to remove the ""by class"" printout"
set separator to concatenate two sentences
init dropouts
auto-spawn on GPU if available
make a forward pass to produce embedded data points and labels
get their gold labels as a tensor
pass data points through network to get encoded data point tensor
decode
calculate the loss
get a tensor of data points
do dropout
add DefaultClassifier arguments
progress bar for verbosity
stop if all sentences are empty
clearing token embeddings to save memory
"read Dataset into data loader, if list of sentences passed, make Dataset first"
"if the classifier predicts BIO/BIOES span labels, the internal label dictionary must be computed"
fields in case this is a span-prediction problem
the label type
all parameters will be pushed internally to the specified device
special handling during training if this is a span prediction problem
internal variables
non-set tags are OUT tags
anything that is not OUT is IN
does this prediction start a new span?
B- and S- always start new spans
"if the predicted class changes, I- starts a new span"
"if the predicted class changes and S- was previous tag, start a new span"
if an existing span is ended (either by reaching O or starting a new span)
reset for-loop variables for new span
remember previous tag
"if there is a span at end of sentence, add it"
"all labels default to ""O"""
set gold token-level
set predicted token-level
now print labels in CoNLL format
print labels in CoNLL format
internal candidate lists of generator
load Zelda candidates if so passed
create candidate lists
"if lower casing is enabled, create candidate lists of lower cased versions"
create a new dictionary for lower cased mentions
go through each mention and its candidates
"check if backoff mention already seen. If so, add candidates. Else, create new entry."
set lowercased version as map
remap state dict for models serialized with Flair <= 0.11.3
get the candidates
"during training, add the gold value as candidate"
----- Create the internal tag dictionary -----
span-labels need special encoding (BIO or BIOES)
the big question is whether the label dictionary should contain an UNK or not
"without UNK, we cannot evaluate on data that contains labels not seen in test"
"with UNK, the model learns less well if there are no UNK examples"
is this a span prediction problem?
----- Embeddings -----
----- Initial loss weights parameters -----
----- RNN specific parameters -----
----- Conditional Random Field parameters -----
"Previously trained models have been trained without an explicit CRF, thus it is required to check"
whether we are loading a model from state dict in order to skip or add START and STOP token
----- Dropout parameters -----
dropouts
remove word dropout if there is no contact over the sequence dimension.
----- Model layers -----
----- RNN layer -----
"If shared RNN provided, else create one for model"
Whether to train initial hidden state
final linear map to tag space
"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss"
"if using CRF, we also require a CRF and a Viterbi decoder"
"if there are no sentences, there is no loss"
forward pass to get scores
calculate loss given scores and labels
make a zero-padded tensor for the whole sentence
linear map to tag space
"Depending on whether we are using CRF or a linear layer, scores is either:"
"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF"
"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer"
spans need to be encoded as token-level predictions
all others are regular labels for each token
make sure it's a list
filter empty sentences
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
get features from forward propagation
remove previously predicted labels of this type
"if return_loss, get loss value"
make predictions
add predictions to Sentence
BIOES-labels need to be converted to spans
"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)"
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
core Flair models on Huggingface ModelHub
"Large NER models,"
Multilingual NER models
English POS models
Multilingual POS models
English SRL models
English chunking models
Language-specific NER models
Language-specific POS models
English NER models
Multilingual NER models
English POS models
Multilingual POS models
English SRL models
English chunking models
Danish models
German models
French models
Dutch models
Malayalam models
Portuguese models
Keyphase models
Biomedical models
check if model name is a valid local file
"check if model key is remapped to HF key - if so, print out information"
get mapped name
use mapped name instead
"if not, check if model key is remapped to direct download location. If so, download model"
special handling for the taggers by the @redewiegergabe project (TODO: move to model hub)
"for all other cases (not local file or special download location), use HF model hub"
"if not a local file, get from model hub"
use model name as subfolder
Lazy import
output information
## Demo: How to use in Flair
load tagger
make example sentence
predict NER tags
print sentence
print predicted NER spans
iterate over entities and print
Lazy import
Save model weight
Determine if model card already exists
Generate and save model card
Upload files
"all labels default to ""O"""
set gold token-level
set predicted token-level
now print labels in CoNLL format
print labels in CoNLL format
the multi task model has several labels
biomedical models
entity linker
auto-spawn on GPU if available
remap state dict for models serialized with Flair <= 0.11.3
English sentiment models
Communicative Functions Model
"scores_at_targets[range(features.shape[0]), lengths.values -1]"
Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices
"Initially, get scores from <start> tag to all other tags"
"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp"
"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep"
Create a tensor to hold accumulated sequence scores at each current tag
Create a tensor to hold back-pointers
"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag"
"Let pads be the <end> tag index, since that was the last tag in the decoded sequence"
"We add scores at current timestep to scores accumulated up to previous timestep, and"
choose the previous timestep that corresponds to the max. accumulated score for each current timestep
"If sentence is over, add transition to STOP-tag"
Decode/trace best path backwards
Sanity check
remove start-tag and backscore to stop-tag
Max + Softmax to get confidence score for predicted label and append label to each token
"Transitions are used in the following way: transitions[to, from]."
"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag"
to START-tag and from STOP-tag to any other tag to -10000.
"if necessary, make batch_steps"
break up the batch into slices of size
mini_batch_chunk_size
"if training also uses dev/train data, include in training set"
evaluation and monitoring
sampling and shuffling
evaluation and monitoring
when and what to save
logging parameters
plugins
activate annealing plugin
call self.train_custom with all parameters (minus the ones specific to the AnnealingPlugin)
training parameters
evaluation and monitoring
sampling and shuffling
evaluation and monitoring
when and what to save
logging parameters
amp
plugins
annealing logic
training parameters
evaluation and monitoring
sampling and shuffling
evaluation and monitoring
when and what to save
logging parameters
amp
plugins
training parameters
evaluation and monitoring
sampling and shuffling
evaluation and monitoring
when and what to save
logging parameters
amp
plugins
Create output folder
=== START BLOCK: ACTIVATE PLUGINS === #
We first activate all optional plugins. These take care of optional functionality such as various
logging techniques and checkpointing
log file plugin
loss file plugin
plugin for writing weights
plugin for checkpointing
=== END BLOCK: ACTIVATE PLUGINS === #
derive parameters the function was called with (or defaults)
initialize model card with these parameters
Prepare training data and get dataset size
"determine what splits (train, dev, test) to evaluate"
determine how to determine best model and whether to save it
instantiate the optimizer
initialize sampler if provided
init with default values if only class is provided
set dataset to sample from
this field stores the names of all dynamic embeddings in the model (determined after first forward pass)
Sanity checks
"Sanity conversion: if flair.device was set as a string, convert to torch.device"
-- AmpPlugin -> wraps with AMP
-- AnnealingPlugin -> initialize schedulers (requires instantiated optimizer)
At any point you can hit Ctrl + C to break out of training early.
"- SchedulerPlugin -> load state for anneal_with_restarts, batch_growth_annealing, logic for early stopping"
- LossFilePlugin -> get the current epoch for loss file logging
"if shuffle_first_epoch==False, the first epoch is not shuffled"
log infos on training progress every `log_modulo` batches
process mini-batches
zero the gradients on the model and optimizer
forward and backward for batch
forward pass
identify dynamic embeddings (always deleted) on first sentence
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
do the optimizer step
- SchedulerPlugin -> do the scheduler step if one-cycle or linear decay
- WeightExtractorPlugin -> extracts weights
- CheckpointPlugin -> executes save_model_each_k_epochs
- SchedulerPlugin -> log bad epochs
Determine if this is the best model or if we need to anneal
log results
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
use DEV split to determine if this is the best model so far
"if not using DEV score, determine best model using train loss"
- LossFilePlugin -> somehow prints all relevant metrics
- AnnealPlugin -> scheduler step
- SWAPlugin -> restores SGD weights from SWA
"if we do not use dev data for model selection, save final model"
TensorboardLogger -> closes writer
test best model if test data is present
get and return the final test score of best model
MetricHistoryPlugin -> stores the loss history in return_values
"Store return values, as they will be erased by reset_training_attributes"
get a random sample of training sentences
create a model card for this model with Flair and PyTorch version
record Transformers version if library is loaded
remember all parameters used in train() call
"TextDataset returns a list. valid and test are only one file,"
so return the first element
cast string to Path
error message if the validation dataset is too small
Shuffle training files randomly after serially iterating
through corpus one
"iterate through training data, starting at"
self.split (for checkpointing)
off by one for printing
go into train mode
reset variables
not really sure what this does
do the forward pass in the model
try to predict the targets
Backward
`clip_grad_norm` helps prevent the exploding gradient
problem in RNNs / LSTMs.
We detach the hidden state from how it was
previously produced.
"If we didn't, the model would try backpropagating"
all the way to start of the dataset.
explicitly remove loss to clear up memory
#########################################################
Save the model if the validation loss is the best we've
seen so far.
#########################################################
print info
#########################################################
##############################################################################
final testing
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
"no need to check for MetricName, as __add__ of other would be called in this case"
"This flag tracks, whether an event is currently being processed (otherwise it is added to the queue)"
instantiate plugin
"Reset the flag, since an exception event might be dispatched"
"If there is no **kw argument in the callback, check if any of the passed kw args is not accepted by"
the callback
go through all attributes
get attribute hook events (may raise an AttributeError)
register function as a hook
"Decorator was used with parentheses, but no args"
Decorator was used with args (strings specifiying the events)
Decorator was used without args
path to store the model
special annealing modes
determine the min learning rate
"minimize training loss if training with dev data, else maximize dev score"
instantiate the scheduler
stop training if learning rate becomes too small
reload last best model if annealing with restarts is enabled
calculate warmup steps
skip if no optimization has happened.
saves the model with full vocab as checkpoints etc were created with reduced vocab.
TODO: check if metric is in tracked metrics
prepare loss logging file and set up header
set up all metrics to collect
set up headers
name: HEADER
Add all potentially relevant metrics. If a metric is not published
"after the first epoch (when the header is written), the column is"
removed at that point.
initialize the first log line
record is a list of scalars
output log file
remove columns where no value was found on the first epoch (could be != 1 if training was resumed)
make headers on epoch 1
write header
adjust alert level
"the default model for ELMo is the 'original' model, which is very large"
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
put on Cuda if available
embed a dummy sentence to determine embedding_length
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
"multilingual forward (English, German, French, Italian, Dutch, Polish)"
"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
Dutch forward
Dutch backward
Swedish forward
Swedish backward
French forward
French backward
Czech forward
Czech backward
Portuguese forward
Portuguese backward
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
"if only one sentence is passed, convert to list of sentence"
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
"legacy pickle-like saving for image embeddings, as implementation details are not obvious"
"legacy pickle-like loading for image embeddings, as implementation details are not obvious"
"<cls> token initially set to 1/D, so it attends to all image features equally"
add positional encodings
reshape the pixels into the sequence
layer norm after convolution and positional encodings
add <cls> token
"transformer requires input in the shape [h*w+1, b, d]"
the output is an embedding of <cls> token
this parameter is fixed
optional fine-tuning on top of embedding layer
"if only one sentence is passed, convert to list of sentence"
"if only one sentence is passed, convert to list of sentence"
bidirectional RNN on top of embedding layer
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push through RNN
after-RNN dropout
extract embeddings from RNN
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
serialize the language models and the constructor arguments (but nothing else)
re-initialize language model with constructor arguments
special handling for deserializing language models
copy over state dictionary to self
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
"in their ""self.train()"" method)"
IMPORTANT: add embeddings as torch modules
iterate over sentences
"if its a forward LM, take last state"
"convert to plain strings, embedded in a list for the encode function"
CNN
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push CNN
after-CNN dropout
extract embeddings from CNN
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
"if only one sentence is passed, convert to list of sentence"
Expose base classses
Expose document embedding classes
Expose image embedding classes
Expose legacy embedding classes
Expose token embedding classes
in some cases we need to insert zero vectors for tokens without embedding.
padding
remove special markup
check if special tokens exist to circumvent error message
iterate over subtokens and reconstruct tokens
remove special markup
check if reconstructed token is special begin token ([CLS] or similar)
some BERT tokenizers somehow omit words - in such cases skip to next token
"we cannot handle unk_tokens perfectly, so let's assume that one unk_token corresponds to one token."
if tokens are unaccounted for
check if all tokens were matched to subtokens
The layoutlm tokenizer doesn't handle ocr themselves
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
"cannot run `.encode` if ocr boxes are required, assume"
"transformers returns the ""added_tokens.json"" even if it doesn't create it"
"transformers returns the ""added_tokens.json"" even if it doesn't create it"
in case of doubt: token embedding has higher priority than document embedding
random check some tokens to save performance.
Models such as FNet do not have an attention_mask
set language IDs for XLM-style transformers
"word_ids is only supported for fast rust tokenizers. Some models like ""xlm-mlm-ende-1024"" do not have"
"a fast tokenizer implementation, hence we need to fall back to our own reconstruction of word_ids."
set context if not set already
flair specific pre-tokenization
fields to store left and right context
expand context only if context_length is set
"if context_dropout is set, randomly deactivate left context during training"
"if context_dropout is set, randomly deactivate right context during training"
"if use_context_separator is set, add a [FLERT] token"
return expanded sentence and context length information
"onnx prepares numpy arrays, no mather if it runs on gpu or cpu, the input is on cpu first."
temporary fix to disable tokenizer parallelism warning
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
do not print transformer warnings as these are confusing in this case
load tokenizer and transformer model
load tokenizer from inmemory zip-file
model name
embedding parameters
send mini-token through to check how many layers the model has
return length
"If we use a context separator, add a new special token"
"most models have an initial BOS token, except for XLNet, T5 and GPT2"
"when initializing, embeddings are in eval mode by default"
in case of doubt: token embedding has higher priority than document embedding
in case of doubt: token embedding has higher priority than document embedding
legacy TransformerDocumentEmbedding
legacy TransformerTokenEmbedding
legacy Flair <= 0.12
legacy Flair <= 0.7
legacy TransformerTokenEmbedding
Legacy TransformerDocumentEmbedding
legacy TransformerTokenEmbedding
legacy TransformerDocumentEmbedding
some models like the tars model somehow lost this information.
copy values from new embedding
those parameters are only from the super class and will be recreated in the constructor.
cls first pooling can be done without recreating sentence hidden states
make the tuple a tensor; makes working with it easier.
"for multimodal models like layoutlmv3, we truncate the image embeddings as they are only used via attention"
only use layers that will be outputted
this parameter is fixed
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
make compatible with serialized models
gensim version 4
gensim version 3
"if no embedding is set, the vocab and embedding length is requried"
GLOVE embeddings
TURIAN embeddings
KOMNINOS embeddings
pubmed embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
twitter embeddings
two-letter language code wiki embeddings
two-letter language code wiki embeddings
two-letter language code crawl embeddings
"this is required to force the module on the cpu,"
"if a parent module is put to gpu, the _apply is called to each sub_module"
self.to(..) actually sets the device properly
this ignores the get_cached_vec method when loading older versions
it is needed for compatibility reasons
gensim version 4
gensim version 3
"when loading the old versions from pickle, the embeddings might not be added as pytorch module."
"we do this delayed, when the weights are collected (e.g. for saving), as doing this earlier might"
lead to issues while loading (trying to load weights that weren't stored as python weights and therefore
not finding them)
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
multilingual models
English models
Arabic
Bulgarian
Czech
Danish
German
Spanish
Basque
Persian
Finnish
French
Hebrew
Hindi
Croatian
Indonesian
Italian
Japanese
Malayalam
Dutch
Norwegian
Polish
Portuguese
Pubmed
Slovenian
Swedish
Tamil
Spanish clinical
CLEF HIPE Shared task
Amharic
Ukrainian
load model if in pretrained model map
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
CLEF HIPE models are lowercased
embeddings are static if we don't do finetuning
embed a dummy sentence to determine embedding_length
set to eval mode
"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout"
gradients are enable if fine-tuning is enabled
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
offset mode that extracts at whitespace after last character
offset mode that extracts at last character
make compatible with old models
use the character language model embeddings as basis
length is twice the original character LM embedding length
these fields are for the embedding memory
whether to add only capitalized words to memory (faster runtime and lower memory consumption)
we re-compute embeddings dynamically at each epoch
set the memory method
memory is wiped each time we do a training run
"if we keep a pooling, it needs to be updated continuously"
update embedding
check token.text is empty or not
set aggregation operation
add embeddings after updating
model architecture
model architecture
"""pl"","
download if necessary
load the model
"TODO: keep for backwards compatibility, but remove in future"
save the sentence piece model as binary file (not as path which may change)
write out the binary sentence piece model into the expected directory
"if the model was saved as binary and it is not found on disk, write to appropriate path"
"otherwise, use normal process and potentially trigger another download"
"once the modes if there, load it with sentence piece"
empty words get no embedding
all other words get embedded
GLOVE embeddings
no need to recreate as NILCEmbeddings
read in test file if exists
read in dev file if exists
"find train, dev and test files if not specified"
Add tags for each annotated span
Remove leading and trailing whitespaces from annotated spans
Search start and end token index for current span
If end index is not found set to last token
Throw error if indices are not valid
Add metadatas for sentence
Currently all Jsonl Datasets are stored in Memory
get train data
read in test file if exists
read in dev file if exists
"find train, dev and test files if not specified"
special key for space after
special key for feature columns
special key for dependency head id
"store either Sentence objects in memory, or only file offsets"
"most data sets have the token text in the first column, if not, pass 'text' as column"
determine encoding of text file
identify which columns are spans and which are word-level
now load all sentences
skip first line if to selected
option 1: keep Sentence objects in memory
pointer to previous
parse next sentence
quit if last sentence reached
skip banned sentences
set previous and next sentence for context
append parsed sentence to list in memory
option 2: keep source data in memory
"read lines for next sentence, but don't parse"
quit if last sentence reached
append raw lines for each sentence
we make a distinction between word-level tags and span-level tags
read first sentence to determine which columns are span-labels
skip first line if to selected
check the first 5 sentences
go through all annotations and identify word- and span-level annotations
- if a column has at least one BIES we know it's a Span label
"- if a column has at least one tag that is not BIOES, we know it's a Token label"
- problem cases are columns for which we see only O - in this case we default to Span
skip assigned columns
the space after key is always word-levels
"if at least one token has a BIES, we know it's a span label"
"if at least one token has a label other than BIOES, we know it's a token label"
all remaining columns that are not word-level are span-level
for column in self.word_level_tag_columns:
"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")"
"if sentence ends, break"
parse comments if possible
"otherwise, this line is a token. parse and add to sentence"
check if this sentence is a document boundary
add span labels
discard tags from tokens that are not added to the sentence
parse relations if they are set
head and tail span indices are 1-indexed and end index is inclusive
parse comments such as '# id cd27886d-6895-4d02-a8df-e5fa763fa88f	domain=de-orcas'
"to set the metadata ""domain"" to ""de-orcas"""
get fields from line
get head_id if exists (only in dependency parses)
initialize token
go through all columns
'feats' and 'misc' column should be split into different fields
special handling for whitespace after
add each other feature as label-value pair
get the task name (e.g. 'ner')
get the label value
add label
remap regular tag names
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
set sentence context using partials TODO: pointer to dataset is really inefficient
use all domains
iter over all domains / sources and create target files
The conll representation of coref spans allows spans to
"overlap. If spans end or begin at the same word, they are"
"separated by a ""|""."
The span begins at this word.
The span begins and ends at this word (single word span).
"The span is starting, so we record the index of the word."
"The span for this id is ending, but didn't start at this word."
Retrieve the start index from the document state and
add the span to the clusters for this id.
strip all bracketing information to
get the actual propbank label.
Entering into a span for a particular semantic role label.
We append the label and set the current span for this annotation.
"If there's no '(' token, but the current_span_label is not None,"
then we are inside a span.
We're outside a span.
"Exiting a span, so we reset the current span label for this annotation."
The words in the sentence.
The pos tags of the words in the sentence.
the pieces of the parse tree.
The lemmatised form of the words in the sentence which
have SRL or word sense information.
The FrameNet ID of the predicate.
"The sense of the word, if available."
"The current speaker, if available."
"Cluster id -> List of (start_index, end_index) spans."
Cluster id -> List of start_indices which are open for this id.
Replace brackets in text and pos tags
with a different token for parse trees.
only keep ')' if there are nested brackets with nothing in them.
There are some bad annotations in the CONLL data.
"They contain no information, so to make this explicit,"
we just set the parse piece to be None which will result
in the overall parse tree being None.
"If this is the first word in the sentence, create"
empty lists to collect the NER and SRL BIO labels.
"We can't do this upfront, because we don't know how many"
"components we are collecting, as a sentence can have"
variable numbers of SRL frames.
Create variables representing the current label for each label
sequence we are collecting.
"If any annotation marks this word as a verb predicate,"
we need to record its index. This also has the side effect
of ordering the verbal predicates by their location in the
"sentence, automatically aligning them with the annotations."
"this would not be reached if parse_pieces contained None, hence the cast"
Non-empty line. Collect the annotation.
Collect any stragglers or files which might not
have the '#end document' format for the end of the file.
this dataset name
check if data there
column format
this dataset name
check if data there
column format
this dataset name
download data if necessary
download files if not present locally
we need to slightly modify the original files by adding some new lines after document separators
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
Remove CoNLL-U meta information in the last column
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
entity_mapping
this dataset name
download data if necessary
data validation
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
download files if not present locallys
column format
this dataset name
default dataset folder is the cache root
# download zip
merge the files in one as the zip is containing multiples files
column format
this dataset name
download data if necessary
"unzip the downloaded repo and merge the train, dev and test datasets"
column format
this dataset name
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
check if data there
create folder
download dataset
column format
this dataset name
download and parse data if necessary
create train test dev if not exist
column format
this dataset name
If the extracted corpus file is not yet present in dir
download zip if necessary
"extracted corpus is not present , so unpacking it."
column format
this dataset name
download zip
unpacking the zip
merge the files in one as the zip is containing multiples files
column format
this dataset name
"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)"
download files if not present locally
we need to modify the original files by adding new lines after after the end of each sentence
if only one language is given
column format
this dataset name
"use all languages if explicitly set to ""all"""
download data if necessary
initialize comlumncorpus and add it to list
column format
this dataset name
column format
this dataset name
column format
this dataset name
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
this list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
get google drive id from list
download from google drive
unzip
"tar.extractall(language_folder,members=[tar.getmember(file_name)])"
transform data into required format
"the processed dataset has the additional ending ""_new"""
remove the unprocessed dataset
initialize comlumncorpus and add it to list
if no languages are given as argument all languages used in XTREME will be loaded
if only one language is given
column format
this dataset name
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
This list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
download from HU Server
unzip
transform data into required format
initialize comlumncorpus and add it to list
if only one language is given
column format
this dataset name
download data if necessary
initialize comlumncorpus and add it to list
download data if necessary
unpack and write out in CoNLL column-like format
column format
this dataset name
download data if necessary
data is not in IOB2 format. Thus we transform it to IOB2
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
column format
this dataset name
rename according to train - test - dev - convention
column format
this dataset name
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
Add missing newline after header
Workaround for empty tokens
"Add ""real"" document marker"
Dataset split mapping
v2.0 only adds new language and splits for AJMC dataset
Special document marker for sample splits in AJMC dataset
column format
this dataset name
download data if necessary
column format
this dataset name
if only one language is given
column format
this dataset name
"use all languages if explicitly set to ""all"""
download data if necessary
initialize comlumncorpus and add it to list
this dataset name
default dataset folder is the cache root
download and parse data if necessary
paths to train and test splits
init corpus
this dataset name
default dataset folder is the cache root
download and parse data if necessary
iterate over all html files
"get rid of html syntax, we only need the text"
between all documents we write a separator symbol
skip empty strings
"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)"
"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention"
sentence splitting and tokenization
iterate through all annotations and add to corresponding tokens
find sentence to which annotation belongs
position within corresponding sentence
set annotation for tokens of entity mention
write to out-file in column format
"in case something goes wrong, delete the dataset and raise error"
this dataset name
download and parse data if necessary
from qwikidata.linked_data_interface import get_entity_dict_from_api
generate qid wikiname dictionaries
merge dictionaries
ignore first line
commented and empty lines
read all Q-IDs
ignore first line
request
this dataset name
we use the wikiids in the data instead of directly utilizing the wikipedia urls.
like this we can quickly check if the corresponding page exists
if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi
delete unprocessed file
collect all wikiids
create the dictionary
request
this dataset name
names of raw text documents
open output_file
iterate through all documents
split sentences and tokenize
iterate through all annotations and add to corresponding tokens
find sentence to which annotation belongs
position within corresponding sentence
set annotation for tokens of entity mention
write to out file
annotation from one annotator or two agreeing annotators
this dataset name
download and parse data if necessary
this dataset name
download and parse data if necessary
First parse the post titles
Keep track of how many and which entity mentions does a given post title have
Check if the current post title has an entity link and parse accordingly
Post titles with entity mentions (if any) are handled via this function
Then parse the comments
"Iterate over the comments.tsv file, until the end is reached"
"Keep track of the current comment thread and its corresponding key, on which the annotations are matched."
Each comment thread is handled as one 'document'.
Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.
This if-condition is needed to handle this problem.
"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure"
"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above"
"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle."
"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,"
and not just single letters into single rows.
If there are annotated entity mentions for given post title or a comment thread
"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence"
Write the token with a corresponding tag to file
"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed"
"If a comment thread or a post title has no entity link, all tokens are assigned the O tag"
Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized
"incorrectly, in order to keep the desired format (empty line as a sentence separator)."
"Thrown when the second check above happens, but the last token of a sentence is reached."
"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below."
"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS"
Check if further annotations belong to the current post title or comment thread as well
Stop when the end of an annotation file is reached
Check if further annotations belong to the current sentence as well
"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)"
Docstart
if there is more than one word in the chunk we write each in a separate line
print(chunks)
empty line after each sentence
convert the file to CoNLL
this dataset name
"check if data there, if not, download the data"
create folder
download data
transform data into column format if necessary
if no filenames are specified we use all the data
"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled"
also we remove 'raganato_ALL' from filenames in case its in the list
generate the test file
make column file and save to data_folder
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
this dataset name
download data if necessary
if True:
write CoNLL-U Plus header
"Some special cases (e.g., missing spaces before entity marker)"
necessary if text should be whitespace tokenizeable
Handle case where tail may occur before the head
this dataset name
write CoNLL-U Plus header
this dataset name
TODO: change data source to original CoNLL04 -- this dataset has span formatting errors
download data if necessary
write CoNLL-U Plus header
The span has ended.
We are entering a new span; reset indices
and active tag to new span.
We're inside a span.
Last token might have been a part of a valid span.
this dataset name
write CoNLL-U Plus header
"for source_file_path, target_filename in zip(source_file_paths, target_filenames):"
"with zip_file.open(source_file_path, mode=""r"") as source_file:"
target_file_path = Path(data_folder) / target_filename
"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:"
# write CoNLL-U Plus header
"target_file.write(""# global.columns = id form ner\n"")"
for example in json.load(source_file):
token_list = self._tacred_example_to_token_list(example)
target_file.write(token_list.serialize())
check if first tag row is already occupied
"if first tag row is occupied, use second tag row"
hardcoded mapping TODO: perhaps find nicer solution
remap regular tag names
else skip to position in file where sentence begins
set sentence context using partials TODO: pointer to dataset is really inefficient
read in dev file if exists
read in test file if exists
the url is copied from https://huggingface.co/datasets/darentang/sroie/blob/main/sroie.py#L44
"find train, dev and test files if not specified"
use test_file to create test split if available
use dev_file to create test split if available
"if data point contains black-listed label, do not use"
first check if valid sentence
"if so, add to indices"
"find train, dev and test files if not specified"
variables
different handling of in_memory data than streaming data
"most data sets have the token text in the first column, if not, pass 'text' as column"
test if format is OK
test if at least one label given
make sentence from text (and filter for length)
"if a pair column is defined, make a sentence pair object"
noinspection PyDefaultArgument
dataset name includes the split size
default dataset folder is the cache root
download data if necessary
download each of the 28 splits
create dataset directory if necessary
download senteval datasets if necessary und unzip
this dataset name
download data if necessary
this dataset name
download data if necessary
download data from same source as in huggingface's implementations
read label order
"Original labels are [1, 2, 3, 4] -> ['World', 'Sports', 'Business', 'Sci/Tech']"
"Re-map to [0, 1, 2, 3]."
this dataset name
download data if necessary
handle labels file
handle data file
Create flair compatible labels
"by default, map point score to POSITIVE / NEGATIVE values"
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file from CSV
create test.txt file from CSV
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create train dev and test files in fasttext format
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
convert to FastText format
download data if necessary
"if data is not downloaded yet, download it"
get the zip file
move original .tsv files to another folder
create train and dev splits in fasttext format
create eval_dataset file with no labels
download zip archive
unpack file in datasets directory (zip archive contains a directory named SST-2)
this dataset name
default dataset folder is the cache root
download data if necessary
download datasets if necessary
create dataset directory if necessary
create correctly formated txt files
multiple labels are possible
this dataset name
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
download data if necessary
this dataset name
download data if necessary
create a separate directory for different tasks
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
check if dataset is supported
set file names
set file names
download and unzip in file structure if necessary
instantiate corpus
"find train, dev and test files if not specified"
"create DataPairDataset for train, test and dev file, if they are given"
stop if file does not exist
create a DataPair object from strings
"if in_memory is True we return a datapair, otherwise we create one from the lists of strings"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"reorder dev datasets to have same columns as in train set: 8, 9, and 11"
dev sets include 5 different annotations but we will only keep the gold label
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get test and dev sets
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data not downloaded yet, download it"
get the zip file
"the downloaded files have json format, we transform them to tsv"
Function to transform JSON file to tsv for Recognizing Textual Entailment Data
remove json file
Uses dynamic programming approach to calculate maximum independent set in interval graph
with sum of all entity lengths as secondary key
calculate offset without current text
because we stick all passages of a document together
TODO For split entities we also annotate everything inbetween which might be a bad idea?
Try to fix incorrect annotations
print(
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
)
Ignore empty lines or relation annotations
FIX annotation of whitespaces (necessary for PDR)
One token may contain multiple entities -> deque all of them
column format
this dataset name
Create tokenization-dependent CONLL files. This is necessary to prevent
from caching issues (e.g. loading the same corpus with different sentence splitters)
column format
this dataset name
column format
this dataset name
Edge case: last token starts a new entity
Last document in file
column format
this dataset name
column format
this dataset name
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
Edge case: last token starts a new entity
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
column format
this dataset name
Read texts
Read annotations
column format
this dataset name
column format
this dataset name
column format
this dataset name
We need to apply a patch to correct the original training file
Articles title
Article abstract
Entity annotations
column format
this dataset name
Edge case: last token starts a new entity
Map all entities to chemicals
Map all entities to disease
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
Incomplete article
Invalid XML syntax
column format
this dataset name
column format
this dataset name
if len(mid) != 3:
continue
Try to fix entity offsets
column format
this dataset name
There is still one illegal annotation in the file ..
column format
this dataset name
"Abstract first, title second to prevent issues with sentence splitting"
column format
this dataset name
column format
this dataset name
column format
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
column format
this dataset name
"Filter for specific entity types, by default no entities will be filtered"
Get original HUNER splits to retrieve a list of all document ids contained in V2
train and dev split of V2 will be train in V4
test split of V2 will be dev in V4
New documents in V4 will become test documents
column format
this dataset name
column format
this dataset name
default dataset folder is the cache root
column format
build dataset name and full huggingface reference name
Download data if necessary
"Some datasets in BigBio only have train or test splits, not both"
"If only test split, assign it to train split"
"If only train split, sample other from it (sample_missing_splits=True)"
Not every dataset has a dev / validation set!
Perform type mapping if necessary
"Collect all texts of the document, each passage will be"
a text in our internal format
Sort passages by start offset
Transform all entity annotations into internal format
Find the passage of the entity (necessary for offset adaption)
Adapt entity offsets according to passage offsets
FIXME: This is just for debugging purposes
passage_text = id_to_text[passage_id]
doc_text = passage_text[entity_offset[0] : entity_offset[1]]
"mention_text = entity[""text""][0]"
if doc_text != mention_text:
"print(f""Annotation error ({document['document_id']}) - Doc: {doc_text} vs. Mention: {mention_text}"")"
Check base case
Get element in the middle
Is the mention with the passage offsets?
"If element is smaller than mid, then it can only"
be present in left subarray
Else the element can only be present in right subarray
Special case for ProGene: We need to use the split_0_train and split_0_test splits
as they are currently provided in BigBio
cache Feidegger config file
cache Feidegger images
replace image URL with local cached file
append Sentence-Image data point
cast to list if necessary
cast to list if necessary
"first, check if pymongo is installed"
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
Expose base classses
Expose all biomedical data sets used for the evaluation of BioBERT
-
-
-
-
Expose all biomedical data sets using the HUNER splits
Expose all biomedical data sets
Expose all document classification datasets
word sense disambiguation
Expose all entity linking datasets
Expose all relation extraction datasets
universal proposition banks
keyphrase detection datasets
other NER datasets
standard NER datasets
Expose all sequence labeling datasets
Expose all text-image datasets
Expose all text-text datasets
Expose all treebanks
"find train, dev and test files if not specified"
get train data
get test data
get dev data
option 1: read only sentence boundaries as offset positions
option 2: keep everything in memory
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
current token ID
handling for the awful UD multiword format
end of sentence
comments or ellipsis
if token is a multi-word
normal single-word tokens
"if we don't split multiwords, skip over component words"
add token
add morphological tags
derive whitespace logic for multiwords
print(token)
print(current_multiword_last_token)
print(current_multiword_first_token)
"if multi-word equals component tokens, there should be no whitespace"
go through all tokens in subword and set whitespace_after information
print(i)
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
this dataset name
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
"finally, print model card for information"
noqa: INP001
-- Project information -----------------------------------------------------
"The full version, including alpha/beta/rc tags"
use smv_current_version as the git url
-- General configuration ---------------------------------------------------
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Napoleon settings
Whitelist pattern for tags (set to None to ignore all tags)
Whitelist pattern for branches (set to None to ignore all branches)
Whitelist pattern for remotes (set to None to use local branches only)
Pattern for released versions
Format for versioned output directories inside the build directory
Determines whether remote or local git branches/tags are preferred if their output dirs conflict
test corpus
create a TARS classifier
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
test corpus
create a TARS classifier
switch to a new task (TARS can do multiple tasks so you must define one)
initialize the text classifier trainer
start the training
"With end symbol, without start symbol, padding in front"
"Without end symbol, with start symbol, padding in back"
"Without end symbol, without start symbol, padding in front"
initialize trainer
initialize trainer
initialize trainer
increment for last token in sentence if not followed by whitespace
clean up directory
clean up directory
example sentence
set 4 labels for 2 tokens ('love' is tagged twice)
check if there are three POS labels with correct text and values
check if there are is one SENTIMENT label with correct text and values
check if all tokens are correctly labeled
remove the pos label from the last word
there should be 2 POS labels left
now remove all pos tags
set 3 labels for 2 spans (HU is tagged twice)
check if there are three labels with correct text and values
check if there are two spans with correct text and values
"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
should be only one NER label left
and only one NER span
set 3 labels for 2 spans (HU is tagged twice with different tags)
check if there are three labels with correct text and values
check if there are two spans with correct text and values
"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
should be only one NER label left
and only one NER span
but there is also one orgtype span and label
and only one NER span
let's add the NER tag back
check if there are three labels with correct text and values
check if there are two spans with correct text and values
now remove all NER tags
set 3 labels for 2 spans (HU is tagged twice with different tags)
create two relation label
there should be two relation labels
there should be one syntactic labels
"there should be two relations, one with two and one with one label"
example sentence
add another topic label
example sentence
has sentiment value
has 4 part of speech tags
has 1 NER tag
should be in total 6 labels
example sentence
add two NER labels
get the four labels
check that only two of the respective data points are equal
make a sentence and some right context
TODO: is this desirable? Or should two sentences with same text be considered same objects?
Initializing a Sentence this way assumes that there is a space after each token
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
load column dataset with one entry
load column dataset with two entries
load column dataset with three entries
"get training, test and dev data"
"get training, test and dev data"
check if Token labels are correct
"get training, test and dev data"
check if Token labels for frames are correct
"get training, test and dev data"
"get training, test and dev data"
get two corpora as one
"get training, test and dev data for full English UD corpus from web"
clean up data directory
"assert [token.get_tag(""head"").value for token in sent1.tokens] == ["
"""2"","
"""0"","
"""4"","
"""2"","
"""2"","
"""2"","
]
This test only covers basic universal dependencies datasets.
"For example, multi-word tokens or the ""deps"" column sentence annotations are not supported yet."
"Here, we use the default token annotation fields."
This test covers the complete HIPE 2022 dataset.
https://github.com/hipe-eval/HIPE-2022-data
"Includes variant with document separator, and all versions of the dataset."
"We have manually checked, that these numbers are correct:"
"+1 offset, because of missing EOS marker at EOD"
Test data for v2.1 release
This test covers the complete ICDAR Europeana corpus:
https://github.com/stefan-it/historic-domain-adaptation-icdar
"This test covers the complete MasakhaNER dataset, including support for v1 and v2."
This test covers the NERMuD dataset. Official stats can be found here:
https://github.com/dhfbk/KIND/tree/main/evalita-2023
Number of instances per dataset split are taken from https://huggingface.co/datasets/elenanereiss/german-ler
This test covers the complete MasakhaPOS dataset.
"See MobIE paper (https://aclanthology.org/2021.konvens-1.22/), table 2"
--- Embeddings that are shared by both models --- #
--- Task 1: Sentiment Analysis (5-class) --- #
Define corpus and model
-- Task 2: Binary Sentiment Analysis on Customer Reviews -- #
Define corpus and model
-- Define mapping (which tagger should train on which model) -- #
-- Create model trainer and train -- #
clean up file
no need for label_dict
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
Intel ----founded_by---> Gordon Moore
Intel ----founded_by---> Robert Noyce
"Ground truth is a set of tuples of (<Sentence Text>, <Relation Label Values>)"
Check sentence masking and relation label annotation on
"training, validation and test dataset (in this test the splits are the same)"
"Entity pair permutations of: ""Larry Page and Sergey Brin founded Google ."""
"Entity pair permutations of: ""Microsoft was founded by Bill Gates ."""
"Entity pair permutations of: ""Konrad Zuse was born in Berlin on 22 June 1910 ."""
"Entity pair permutations of: ""Joseph Weizenbaum , a professor at MIT , was born in Berlin , Germany."""
This sentence is only included if we transform the corpus with cross augmentation
Ensure this is an example that predicts no classes in multilabel
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
ensure that the prepared tensors is what we expect
use a SequenceTagger to save and reload the embedding in the manner it is supposed to work
previous and next sentence as context
test expansion for sentence without context
test expansion for with previous and next as context
test expansion if first sentence is document boundary
test expansion if we don't use context
"apparently the precision is not that high on cuda, hence the absolute tolerance needs to be higher."
dummy model with embeddings
save the dummy and load it again
check that context_length and use_context_separator is the same for both
mmap seems to be much more memory efficient
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
Extract all the contents of zip file in current directory
Extract all the contents of zip file in current directory
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
load_big_file is a workaround byhttps://github.com/highway11git
to load models on some Mac/Windows setups
see https://github.com/zalandoresearch/flair/issues/351
first determine the distribution of classes in the dataset
weight for each sample
Create blocks
shuffle the blocks
concatenate the shuffled blocks
Create blocks
shuffle the blocks
concatenate the shuffled blocks
increment for last token in sentence if not followed by whitespace
this is the default init size of a lmdb database for embeddings
get db filename from embedding name
"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
SequenceTagger
TextClassifier
get db filename from embedding name
if embedding database already exists
"otherwise, push embedding to database"
if embedding database already exists
open the database in read mode
we need to set self.k
create and load the database in write mode
"no idea why, but we need to close and reopen the environment to avoid"
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
when opening new transaction !
init dictionaries
"in order to deal with unknown tokens, add <unk>"
set 'add_unk' if the dictionary was created with a version of Flair older than 0.9
set 'add_unk' depending on whether <unk> is a key
"if one embedding name, directly return it"
"if multiple embedding names, concatenate them"
First we remove any existing labels for this PartOfSentence in self.sentence
labels also need to be deleted at Sentence object
delete labels at object itself
The Token is a special _PartOfSentence in that it may be initialized without a Sentence.
"therefore, labels get added only to the Sentence if it exists"
The Token is a special _PartOfSentence in that it may be initialized without a Sentence.
"Therefore, labels get set only to the Sentence if it exists"
"check if the span already exists. If so, return it"
else make a new span
"check if the relation already exists. If so, return it"
else make a new relation
private field for all known spans
the tokenizer used for this sentence
some sentences represent a document boundary (but most do not)
internal variables to denote position inside dataset
"if text is passed, instantiate sentence with tokens (words)"
determine token positions and whitespace_after flag
the last token has no whitespace after
log a warning if the dataset is empty
data with zero-width characters cannot be handled
set token idx and sentence
append token to sentence
register token annotations on sentence
move sentence embeddings to device
also move token embeddings to device
clear token embeddings
infer whitespace after field
"if sentence has no tokens, return empty string"
"otherwise, return concatenation of tokens with the correct offsets"
The sentence's start position is not propagated to its tokens.
"Therefore, we need to add the sentence's start position to its last token's end position, including whitespaces."
No character at the corresponding code point: remove it
"if no label if specified, return all labels"
"if the label type exists in the Sentence, return it"
return empty list if none of the above
labels also need to be deleted at all tokens
labels also need to be deleted at all known spans
remove spans without labels
delete labels at object itself
set name
abort if no data is provided
sample test data from train if none is provided
sample dev data from train if none is provided
set train dev and test data
find out empty sentence indices
create subset of non-empty sentence indices
find out empty sentence indices
create subset of non-empty sentence indices
"first, determine the datapoint type by going through dataset until first label is found"
count all label types per sentence
go through all labels of label_type and count values
special handling for Token-level annotations. Add all untagged as 'O' label
"if an unk threshold is set, UNK all label values below this threshold"
sample randomly from a label distribution according to the probabilities defined by the noise transition matrix
replace the old label with the new one
keep track of the old (clean) label using another label type category
keep track of how many labels in total are flipped
sample randomly from a label distribution according to the probabilities defined by the desired noise share
replace the old label with the new one
keep track of the old (clean) label using another label type category
keep track of how many labels in total are flipped
"add a dummy ""O"" to close final prediction"
return complex list
internal variables
non-set tags are OUT tags
anything that is not OUT is IN
does this prediction start a new span?
B- and S- always start new spans
"if the predicted class changes, I- starts a new span"
"if the predicted class changes and S- was previous tag, start a new span"
if an existing span is ended (either by reaching O or starting a new span)
determine score and value
append to result list
reset for-loop variables for new span
remember previous tag
global variable: cache_root
global variable: device
"No need for correctness checks, torch is doing it"
global variable: version
global variable: arrow symbol
dummy return to fulfill trainer.train() needs
print(vec)
Attach optimizer
"convert `metrics` to float, in case it's a zero-dim Tensor"
if memory mode option 'none' delete everything
"if dynamic embedding keys not passed, identify them automatically"
always delete dynamic embeddings
"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)"
optional metric space decoder if prototypes have different length than embedding
create initial prototypes for all classes (all initial prototypes are a vector of all 1s)
"if set, create initial prototypes from normal distribution"
"if set, use a radius"
all parameters will be pushed internally to the specified device
decode embeddings into prototype space
"if unlabeled distance is set, mask out loss to unlabeled class prototype"
verbalize BIOES labels
"if label is not BIOES, use label itself"
Always include the name of the Model class for which the state dict holds
"write out a ""model card"" if one is set"
save model
"if this class is abstract, go through all inheriting classes and try to fetch and load the model"
get all non-abstract subclasses
"try to fetch the model for each subclass. if fetching is possible, load model and return it"
"skip any invalid loadings, e.g. not found on huggingface hub"
"if the model cannot be fetched, load as a file"
try to get model class from state
"older (flair 11.3 and below) models do not contain cls information. In this case, try all subclasses"
"if str(model_cls) == ""<class 'flair.models.pairwise_classification_model.TextPairClassifier'>"": continue"
"skip any invalid loadings, e.g. not found on huggingface hub"
"if this class is not abstract, fetch the model and load it"
"make sure <unk> is contained in gold_label_dictionary, if given"
"read Dataset into data loader, if list of sentences passed, make Dataset first"
loss calculation
variables for printing
variables for computing scores
remove any previously predicted labels
predict for batch
get the gold labels
add to all_predicted_values
make printout lines
convert true and predicted values to two span-aligned lists
delete exluded labels if exclude_labels is given
"if after excluding labels, no label is left, ignore the datapoint"
write all_predicted_values to out_file if set
make the evaluation dictionary
check if this is a multi-label problem
compute numbers by formatting true and predicted such that Scikit-Learn can use them
multi-label problems require a multi-hot vector for each true and predicted label
single-label problems can do with a single index for each true and predicted label
"now, calculate evaluation numbers"
there is at least one gold label or one prediction (default)
"if there is only one label, then ""micro avg"" = ""macro avg"""
"micro average is only computed if zero-label exists (for instance ""O"")"
if no zero-label exists (such as in POS tagging) micro average is equal to accuracy
same for the main score
issue error and default all evaluation numbers to 0.
check if there is a label mismatch
print info
set the embeddings
initialize the label dictionary
initialize the decoder
set up multi-label logic
init dropouts
loss weights and loss function
Initialize the weight tensor
set up gradient reversal if so specified
embed sentences
get a tensor of data points
do dropout
make a forward pass to produce embedded data points and labels
get the data points for which to predict labels
get their gold labels as a tensor
pass data points through network to get encoded data point tensor
decode
an optional masking step (no masking in most cases)
calculate the loss
filter empty sentences
reverse sort all sequences by their length
progress bar for verbosity
filter data points in batch
stop if all sentences are empty
pass data points through network and decode
if anything could possibly be predicted
remove previously predicted labels of this type
filter data points that have labels outside of dictionary
add DefaultClassifier arguments
add variables of DefaultClassifier
Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23
Get projected 1st dimension
Compute bilinear form
Arcosh
Project the input data to n+1 dimensions
"The first dimension, is recomputed in the distance module"
header for 'weights.txt'
"determine the column index of loss, f-score and accuracy for"
"train, dev and test split"
then get all relevant values from the tsv
then get all relevant values from the tsv
plot i
save plots
save plots
plt.show()
save plot
auto-spawn on GPU if available
progress bar for verbosity
stop if all sentences are empty
clearing token embeddings to save memory
"read Dataset into data loader, if list of sentences passed, make Dataset first"
TODO: not saving lines yet
TODO: This closely shadows the RelationExtractor name. Maybe we need a better name here.
- MaskedRelationClassifier ?
This depends if this relation classification architecture should replace or offer as an alternative.
Set label type and prepare label dictionary
Initialize super default classifier
Add the special tokens from the encoding strategy
"Auto-spawn on GPU, if available"
Only use entities labelled with the specified labels for each label type
Only use entities above the specified threshold
Use a dictionary to find gold relation annotations for a given entity pair
Yield head and tail entity pairs from the cross product of all entities
Remove identity relation entity pairs
Remove entity pairs with labels that do not match any
of the specified relations in `self.entity_pair_labels`
"Obtain gold label, if existing"
Some sanity checks
Pre-compute non-leading head and tail tokens for entity masking
We can not use the plaintext of the head/tail span in the sentence as the mask/marker
since there may be multiple occurrences of the same entity mentioned in the sentence.
"Therefore, we use the span's position in the sentence."
Create masked sentence
Add gold relation annotation as sentence label
"Using the sentence label instead of annotating a separate `Relation` object is easier to manage since,"
"during prediction, the forward pass does not need any knowledge about the entities in the sentence."
"If we sample missing splits, the encoded sentences that correspond to the same original sentences"
"may get distributed into different splits. For training purposes, this is always undesired."
Ensure that all sentences are encoded properly
Deal with the case where all sentences are encoded sentences
"mypy does not infer the type of ""sentences"" restricted by the if statement"
Deal with the case where all sentences are standard (non-encoded) sentences
"For each encoded sentence, transfer its prediction onto the original relation"
auto-spawn on GPU if available
pad strings with whitespaces to longest sentence
cut up the input into chunks of max charlength = chunk_size
push each chunk through the RNN language model
concatenate all chunks to make final output
initial hidden state
get predicted weights
divide by temperature
"to prevent overflow problem with small temperature values, substract largest value from all"
this makes a vector in which the largest value is 0
compute word weights with exponential function
try sampling multinomial distribution for next character
print(word_idx)
input ids
push list of character IDs through model
the target is always the next character
use cross entropy loss to compare output of forward pass with targets
exponentiate cross-entropy loss to calculate perplexity
"""document_delimiter"" property may be missing in some older pre-trained models"
serialize the language models and the constructor arguments (but nothing else)
special handling for deserializing language models
re-initialize language model with constructor arguments
copy over state dictionary to self
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
"in their ""self.train()"" method)"
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
Transform input data into TARS format
"if there are no labels, return a random sample as negatives"
"otherwise, go through all labels"
make sure the probabilities always sum up to 1
get and embed all labels by making a Sentence object that contains only the label text
get each label embedding and scale between 0 and 1
compute similarity matrix
"the higher the similarity, the greater the chance that a label is"
sampled as negative example
make label dictionary if no Dictionary object is passed
prepare dictionary of tags (without B- I- prefixes and without UNK)
check if candidate_label_set is empty
make list if only one candidate label is passed
create label dictionary
note current task
create a temporary task
make zero shot predictions
switch to the pre-existing task
prepare TARS dictionary
initialize a bare-bones sequence tagger
transformer separator
Store task specific labels since TARS can handle multiple tasks
make a tars sentence where all labels are O by default
init new TARS classifier
set all task information
progress bar for verbosity
stop if all sentences are empty
always remove tags first
go through each sentence in the batch
always remove tags first
get the span and its label
determine whether tokens in this span already have a label
only add if all tokens have no label
make and add a corresponding predicted span
set indices so that no token can be tagged twice
clearing token embeddings to save memory
"all labels default to ""O"""
set gold token-level
set predicted token-level
now print labels in CoNLL format
prepare TARS dictionary
initialize a bare-bones sequence tagger
transformer separator
Store task specific labels since TARS can handle multiple tasks
get the serialized embeddings
remap state dict for models serialized with Flair <= 0.11.3
init new TARS classifier
set all task information
with torch.no_grad():
progress bar for verbosity
stop if all sentences are empty
always remove tags first
go through each sentence in the batch
always remove tags first
add all labels that according to TARS match the text and are above threshold
do not add labels below confidence threshold
only use label with the highest confidence if enforcing single-label predictions
add the label with the highest score even if below the threshold if force label is activated.
remove previously added labels and only add the best label
clearing token embeddings to save memory
set separator to concatenate two sentences
auto-spawn on GPU if available
pooling operation to get embeddings for entites
set embeddings
set relation and entity label types
"whether to use gold entity pairs, and whether to filter entity pairs by type"
filter entity pairs according to their tags if set
whether to encode characters and whether to use attention (attention can only be used if chars are encoded)
character dictionary for decoding and encoding
make sure <unk> is in dictionary for handling of unknown characters
add special symbols to dictionary if necessary and save respective indices
---- ENCODER ----
encoder character embeddings
encoder pre-trained embeddings
encoder RNN
additional encoder linear layer if bidirectional encoding
---- DECODER ----
decoder: linear layers to transform vectors to and from alphabet_size
when using attention we concatenate attention outcome and decoder hidden states
decoder RNN
loss and softmax
self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction
add additional columns for special symbols if necessary
initialize with dummy symbols
encode inputs
get labels (we assume each token has a lemma label)
get char indices for labels of sentence
"(batch_size, max_sequence_length) batch_size = #words in sentence,"
max_sequence_length = length of longest label of sentence + 1
get char embeddings
"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size"
take decoder input and initial hidden and pass through RNN
"if all encoder outputs are provided, use attention"
take convex combinations of encoder hidden states as new output using the computed attention coefficients
"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)"
get all tokens
encode input characters by sending them through RNN
get one-hots for characters and add special symbols / padding
determine length of each token
embed sentences
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
variable to store initial hidden states for decoder
encode input characters by sending them through RNN
test packing and padding
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
concatenate the final hidden states of the encoder. These will be projected to hidden_size of
decoder later with self.emb_to_hidden
mask out vectors that correspond to a dummy symbol (TODO: check attention masking)
use token embedding as initial hidden state for decoder
concatenate everything together and project to appropriate size for decoder
variable to store initial hidden states for decoder
encode input characters by sending them through RNN
note that we do not need to fill up with dummy symbols since we process each token seperately
embed character one-hots
send through encoder RNN (produces initial hidden for decoder)
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
project 2*hidden_size to hidden_size
concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder
later with self.emb_to_hidden
use token embedding as initial hidden state for decoder
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
concatenate everything together and project to appropriate size for decoder
"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)"
"create target vector (batch_size, max_label_seq_length + 1)"
filter empty sentences
max length of the predicted sequences
for printing
stop if all sentences are empty
remove previously predicted labels of this type
create list of tokens in batch
encode inputs
"create input for first pass (batch_size, 1, input_size), first letter is special character <S>"
sequence length is always set to one in prediction
option 1: greedy decoding
predictions
decode next character
pick top beam size many outputs with highest probabilities
option 2: beam search
out_probs = self.softmax(output_vectors).squeeze(1)
make sure no dummy symbol <> or start symbol <S> is predicted
pick top beam size many outputs with highest probabilities
"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1"
"leading_indices and probabilities have size (batch_size, beam_size)"
keep scores of beam_size many hypothesis for each token in the batch
stack all leading indices of all hypothesis and corresponding hidden states in two tensors
save sequences so far
keep track of how many hypothesis were completed for each token
"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)"
decode with log softmax
make sure no dummy symbol <> or start symbol <S> is predicted
"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside"
"if the sequence is already ended, do not record as candidate"
index of token in in list tokens_in_batch
print(token_number)
hypothesis score
TODO: remove token if number of completed hypothesis exceeds given value
set score of corresponding entry to -inf so it will not be expanded
get leading_indices for next expansion
find highest scoring hypothesis among beam_size*beam_size possible ones for each token
take beam_size many copies of scores vector and add scores of possible new extensions
"size (beam_size*batch_size, beam_size)"
print(hypothesis_scores)
"reshape to vector of size (batch_size, beam_size*beam_size),"
each row contains beam_size*beam_size scores of the new possible hypothesis
print(hypothesis_scores_per_token)
"choose beam_size best for each token - size (batch_size, beam_size)"
out of indices_per_token we now need to recompute the original indices of the hypothesis in
a list of length beam_size*batch_size
"where the first three inidices belong to the first token, the next three to the second token,"
and so on
with these indices we can compute the tensors for the next iteration
expand sequences with corresponding index
add log-probabilities to the scores
save new leading indices
save corresponding hidden states
it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations
in that case we append one of the final seuqences without end symbol to the final_candidates
get best final hypothesis for each token
get characters from index sequences and add predicted label to token
"Overwrites evaluate of parent class to remove the ""by class"" printout"
set separator to concatenate two sentences
init dropouts
auto-spawn on GPU if available
make a forward pass to produce embedded data points and labels
get their gold labels as a tensor
pass data points through network to get encoded data point tensor
decode
calculate the loss
get a tensor of data points
do dropout
add DefaultClassifier arguments
progress bar for verbosity
stop if all sentences are empty
clearing token embeddings to save memory
"read Dataset into data loader, if list of sentences passed, make Dataset first"
"if the classifier predicts BIO/BIOES span labels, the internal label dictionary must be computed"
fields in case this is a span-prediction problem
the label type
all parameters will be pushed internally to the specified device
special handling during training if this is a span prediction problem
internal variables
non-set tags are OUT tags
anything that is not OUT is IN
does this prediction start a new span?
B- and S- always start new spans
"if the predicted class changes, I- starts a new span"
"if the predicted class changes and S- was previous tag, start a new span"
if an existing span is ended (either by reaching O or starting a new span)
reset for-loop variables for new span
remember previous tag
"if there is a span at end of sentence, add it"
"all labels default to ""O"""
set gold token-level
set predicted token-level
now print labels in CoNLL format
print labels in CoNLL format
internal candidate lists of generator
load Zelda candidates if so passed
create candidate lists
"if lower casing is enabled, create candidate lists of lower cased versions"
create a new dictionary for lower cased mentions
go through each mention and its candidates
"check if backoff mention already seen. If so, add candidates. Else, create new entry."
set lowercased version as map
remap state dict for models serialized with Flair <= 0.11.3
get the candidates
"during training, add the gold value as candidate"
----- Create the internal tag dictionary -----
span-labels need special encoding (BIO or BIOES)
the big question is whether the label dictionary should contain an UNK or not
"without UNK, we cannot evaluate on data that contains labels not seen in test"
"with UNK, the model learns less well if there are no UNK examples"
is this a span prediction problem?
----- Embeddings -----
----- Initial loss weights parameters -----
----- RNN specific parameters -----
----- Conditional Random Field parameters -----
"Previously trained models have been trained without an explicit CRF, thus it is required to check"
whether we are loading a model from state dict in order to skip or add START and STOP token
----- Dropout parameters -----
dropouts
remove word dropout if there is no contact over the sequence dimension.
----- Model layers -----
----- RNN layer -----
"If shared RNN provided, else create one for model"
Whether to train initial hidden state
final linear map to tag space
"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss"
"if using CRF, we also require a CRF and a Viterbi decoder"
"if there are no sentences, there is no loss"
forward pass to get scores
calculate loss given scores and labels
make a zero-padded tensor for the whole sentence
linear map to tag space
"Depending on whether we are using CRF or a linear layer, scores is either:"
"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF"
"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer"
spans need to be encoded as token-level predictions
all others are regular labels for each token
make sure it's a list
filter empty sentences
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
get features from forward propagation
remove previously predicted labels of this type
"if return_loss, get loss value"
make predictions
add predictions to Sentence
BIOES-labels need to be converted to spans
"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)"
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
core Flair models on Huggingface ModelHub
"Large NER models,"
Multilingual NER models
English POS models
Multilingual POS models
English SRL models
English chunking models
Language-specific NER models
Language-specific POS models
English NER models
Multilingual NER models
English POS models
Multilingual POS models
English SRL models
English chunking models
Danish models
German models
French models
Dutch models
Malayalam models
Portuguese models
Keyphase models
Biomedical models
check if model name is a valid local file
"check if model key is remapped to HF key - if so, print out information"
get mapped name
use mapped name instead
"if not, check if model key is remapped to direct download location. If so, download model"
special handling for the taggers by the @redewiegergabe project (TODO: move to model hub)
"for all other cases (not local file or special download location), use HF model hub"
"if not a local file, get from model hub"
use model name as subfolder
Lazy import
output information
## Demo: How to use in Flair
load tagger
make example sentence
predict NER tags
print sentence
print predicted NER spans
iterate over entities and print
Lazy import
Save model weight
Determine if model card already exists
Generate and save model card
Upload files
"all labels default to ""O"""
set gold token-level
set predicted token-level
now print labels in CoNLL format
print labels in CoNLL format
the multi task model has several labels
biomedical models
entity linker
auto-spawn on GPU if available
remap state dict for models serialized with Flair <= 0.11.3
English sentiment models
Communicative Functions Model
"scores_at_targets[range(features.shape[0]), lengths.values -1]"
Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices
"Initially, get scores from <start> tag to all other tags"
"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp"
"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep"
Create a tensor to hold accumulated sequence scores at each current tag
Create a tensor to hold back-pointers
"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag"
"Let pads be the <end> tag index, since that was the last tag in the decoded sequence"
"We add scores at current timestep to scores accumulated up to previous timestep, and"
choose the previous timestep that corresponds to the max. accumulated score for each current timestep
"If sentence is over, add transition to STOP-tag"
Decode/trace best path backwards
Sanity check
remove start-tag and backscore to stop-tag
Max + Softmax to get confidence score for predicted label and append label to each token
"Transitions are used in the following way: transitions[to, from]."
"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag"
to START-tag and from STOP-tag to any other tag to -10000.
"if necessary, make batch_steps"
break up the batch into slices of size
mini_batch_chunk_size
"if training also uses dev/train data, include in training set"
evaluation and monitoring
sampling and shuffling
evaluation and monitoring
when and what to save
logging parameters
plugins
activate annealing plugin
call self.train_custom with all parameters (minus the ones specific to the AnnealingPlugin)
training parameters
evaluation and monitoring
sampling and shuffling
evaluation and monitoring
when and what to save
logging parameters
amp
plugins
annealing logic
training parameters
evaluation and monitoring
sampling and shuffling
evaluation and monitoring
when and what to save
logging parameters
amp
plugins
training parameters
evaluation and monitoring
sampling and shuffling
evaluation and monitoring
when and what to save
logging parameters
amp
plugins
Create output folder
=== START BLOCK: ACTIVATE PLUGINS === #
We first activate all optional plugins. These take care of optional functionality such as various
logging techniques and checkpointing
log file plugin
loss file plugin
plugin for writing weights
plugin for checkpointing
=== END BLOCK: ACTIVATE PLUGINS === #
derive parameters the function was called with (or defaults)
initialize model card with these parameters
Prepare training data and get dataset size
"determine what splits (train, dev, test) to evaluate"
determine how to determine best model and whether to save it
instantiate the optimizer
initialize sampler if provided
init with default values if only class is provided
set dataset to sample from
this field stores the names of all dynamic embeddings in the model (determined after first forward pass)
Sanity checks
"Sanity conversion: if flair.device was set as a string, convert to torch.device"
-- AmpPlugin -> wraps with AMP
-- AnnealingPlugin -> initialize schedulers (requires instantiated optimizer)
At any point you can hit Ctrl + C to break out of training early.
"- SchedulerPlugin -> load state for anneal_with_restarts, batch_growth_annealing, logic for early stopping"
- LossFilePlugin -> get the current epoch for loss file logging
"if shuffle_first_epoch==False, the first epoch is not shuffled"
log infos on training progress every `log_modulo` batches
process mini-batches
zero the gradients on the model and optimizer
forward and backward for batch
forward pass
identify dynamic embeddings (always deleted) on first sentence
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
do the optimizer step
- SchedulerPlugin -> do the scheduler step if one-cycle or linear decay
- WeightExtractorPlugin -> extracts weights
- CheckpointPlugin -> executes save_model_each_k_epochs
- SchedulerPlugin -> log bad epochs
Determine if this is the best model or if we need to anneal
log results
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
use DEV split to determine if this is the best model so far
"if not using DEV score, determine best model using train loss"
- LossFilePlugin -> somehow prints all relevant metrics
- AnnealPlugin -> scheduler step
- SWAPlugin -> restores SGD weights from SWA
"if we do not use dev data for model selection, save final model"
TensorboardLogger -> closes writer
test best model if test data is present
get and return the final test score of best model
MetricHistoryPlugin -> stores the loss history in return_values
"Store return values, as they will be erased by reset_training_attributes"
get a random sample of training sentences
create a model card for this model with Flair and PyTorch version
record Transformers version if library is loaded
remember all parameters used in train() call
"TextDataset returns a list. valid and test are only one file,"
so return the first element
cast string to Path
error message if the validation dataset is too small
Shuffle training files randomly after serially iterating
through corpus one
"iterate through training data, starting at"
self.split (for checkpointing)
off by one for printing
go into train mode
reset variables
not really sure what this does
do the forward pass in the model
try to predict the targets
Backward
`clip_grad_norm` helps prevent the exploding gradient
problem in RNNs / LSTMs.
We detach the hidden state from how it was
previously produced.
"If we didn't, the model would try backpropagating"
all the way to start of the dataset.
explicitly remove loss to clear up memory
#########################################################
Save the model if the validation loss is the best we've
seen so far.
#########################################################
print info
#########################################################
##############################################################################
final testing
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
"no need to check for MetricName, as __add__ of other would be called in this case"
"This flag tracks, whether an event is currently being processed (otherwise it is added to the queue)"
instantiate plugin
"Reset the flag, since an exception event might be dispatched"
"If there is no **kw argument in the callback, check if any of the passed kw args is not accepted by"
the callback
go through all attributes
get attribute hook events (may raise an AttributeError)
register function as a hook
"Decorator was used with parentheses, but no args"
Decorator was used with args (strings specifiying the events)
Decorator was used without args
path to store the model
special annealing modes
determine the min learning rate
"minimize training loss if training with dev data, else maximize dev score"
instantiate the scheduler
stop training if learning rate becomes too small
reload last best model if annealing with restarts is enabled
calculate warmup steps
skip if no optimization has happened.
saves the model with full vocab as checkpoints etc were created with reduced vocab.
TODO: check if metric is in tracked metrics
prepare loss logging file and set up header
set up all metrics to collect
set up headers
name: HEADER
Add all potentially relevant metrics. If a metric is not published
"after the first epoch (when the header is written), the column is"
removed at that point.
initialize the first log line
record is a list of scalars
output log file
remove columns where no value was found on the first epoch (could be != 1 if training was resumed)
make headers on epoch 1
write header
adjust alert level
"the default model for ELMo is the 'original' model, which is very large"
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
put on Cuda if available
embed a dummy sentence to determine embedding_length
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
"multilingual forward (English, German, French, Italian, Dutch, Polish)"
"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
Dutch forward
Dutch backward
Swedish forward
Swedish backward
French forward
French backward
Czech forward
Czech backward
Portuguese forward
Portuguese backward
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
"if only one sentence is passed, convert to list of sentence"
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
"legacy pickle-like saving for image embeddings, as implementation details are not obvious"
"legacy pickle-like loading for image embeddings, as implementation details are not obvious"
"<cls> token initially set to 1/D, so it attends to all image features equally"
add positional encodings
reshape the pixels into the sequence
layer norm after convolution and positional encodings
add <cls> token
"transformer requires input in the shape [h*w+1, b, d]"
the output is an embedding of <cls> token
this parameter is fixed
optional fine-tuning on top of embedding layer
"if only one sentence is passed, convert to list of sentence"
"if only one sentence is passed, convert to list of sentence"
bidirectional RNN on top of embedding layer
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push through RNN
after-RNN dropout
extract embeddings from RNN
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
serialize the language models and the constructor arguments (but nothing else)
re-initialize language model with constructor arguments
special handling for deserializing language models
copy over state dictionary to self
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
"in their ""self.train()"" method)"
IMPORTANT: add embeddings as torch modules
iterate over sentences
"if its a forward LM, take last state"
"convert to plain strings, embedded in a list for the encode function"
CNN
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push CNN
after-CNN dropout
extract embeddings from CNN
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
"if only one sentence is passed, convert to list of sentence"
Expose base classses
Expose document embedding classes
Expose image embedding classes
Expose legacy embedding classes
Expose token embedding classes
in some cases we need to insert zero vectors for tokens without embedding.
padding
remove special markup
check if special tokens exist to circumvent error message
iterate over subtokens and reconstruct tokens
remove special markup
check if reconstructed token is special begin token ([CLS] or similar)
some BERT tokenizers somehow omit words - in such cases skip to next token
"we cannot handle unk_tokens perfectly, so let's assume that one unk_token corresponds to one token."
if tokens are unaccounted for
check if all tokens were matched to subtokens
The layoutlm tokenizer doesn't handle ocr themselves
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
"cannot run `.encode` if ocr boxes are required, assume"
"transformers returns the ""added_tokens.json"" even if it doesn't create it"
"transformers returns the ""added_tokens.json"" even if it doesn't create it"
in case of doubt: token embedding has higher priority than document embedding
random check some tokens to save performance.
Models such as FNet do not have an attention_mask
set language IDs for XLM-style transformers
"word_ids is only supported for fast rust tokenizers. Some models like ""xlm-mlm-ende-1024"" do not have"
"a fast tokenizer implementation, hence we need to fall back to our own reconstruction of word_ids."
set context if not set already
flair specific pre-tokenization
fields to store left and right context
expand context only if context_length is set
"if context_dropout is set, randomly deactivate left context during training"
"if context_dropout is set, randomly deactivate right context during training"
"if use_context_separator is set, add a [FLERT] token"
return expanded sentence and context length information
"onnx prepares numpy arrays, no mather if it runs on gpu or cpu, the input is on cpu first."
temporary fix to disable tokenizer parallelism warning
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
do not print transformer warnings as these are confusing in this case
load tokenizer and transformer model
load tokenizer from inmemory zip-file
model name
embedding parameters
send mini-token through to check how many layers the model has
return length
"If we use a context separator, add a new special token"
"most models have an initial BOS token, except for XLNet, T5 and GPT2"
"when initializing, embeddings are in eval mode by default"
in case of doubt: token embedding has higher priority than document embedding
in case of doubt: token embedding has higher priority than document embedding
legacy TransformerDocumentEmbedding
legacy TransformerTokenEmbedding
legacy Flair <= 0.12
legacy Flair <= 0.7
legacy TransformerTokenEmbedding
Legacy TransformerDocumentEmbedding
legacy TransformerTokenEmbedding
legacy TransformerDocumentEmbedding
some models like the tars model somehow lost this information.
copy values from new embedding
those parameters are only from the super class and will be recreated in the constructor.
cls first pooling can be done without recreating sentence hidden states
make the tuple a tensor; makes working with it easier.
"for multimodal models like layoutlmv3, we truncate the image embeddings as they are only used via attention"
only use layers that will be outputted
this parameter is fixed
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
make compatible with serialized models
gensim version 4
gensim version 3
"if no embedding is set, the vocab and embedding length is requried"
GLOVE embeddings
TURIAN embeddings
KOMNINOS embeddings
pubmed embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
twitter embeddings
two-letter language code wiki embeddings
two-letter language code wiki embeddings
two-letter language code crawl embeddings
"this is required to force the module on the cpu,"
"if a parent module is put to gpu, the _apply is called to each sub_module"
self.to(..) actually sets the device properly
this ignores the get_cached_vec method when loading older versions
it is needed for compatibility reasons
gensim version 4
gensim version 3
"when loading the old versions from pickle, the embeddings might not be added as pytorch module."
"we do this delayed, when the weights are collected (e.g. for saving), as doing this earlier might"
lead to issues while loading (trying to load weights that weren't stored as python weights and therefore
not finding them)
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
multilingual models
English models
Arabic
Bulgarian
Czech
Danish
German
Spanish
Basque
Persian
Finnish
French
Hebrew
Hindi
Croatian
Indonesian
Italian
Japanese
Malayalam
Dutch
Norwegian
Polish
Portuguese
Pubmed
Slovenian
Swedish
Tamil
Spanish clinical
CLEF HIPE Shared task
Amharic
Ukrainian
load model if in pretrained model map
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
CLEF HIPE models are lowercased
embeddings are static if we don't do finetuning
embed a dummy sentence to determine embedding_length
set to eval mode
"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout"
gradients are enable if fine-tuning is enabled
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
offset mode that extracts at whitespace after last character
offset mode that extracts at last character
make compatible with old models
use the character language model embeddings as basis
length is twice the original character LM embedding length
these fields are for the embedding memory
whether to add only capitalized words to memory (faster runtime and lower memory consumption)
we re-compute embeddings dynamically at each epoch
set the memory method
memory is wiped each time we do a training run
"if we keep a pooling, it needs to be updated continuously"
update embedding
check token.text is empty or not
set aggregation operation
add embeddings after updating
model architecture
model architecture
"""pl"","
download if necessary
load the model
"TODO: keep for backwards compatibility, but remove in future"
save the sentence piece model as binary file (not as path which may change)
write out the binary sentence piece model into the expected directory
"if the model was saved as binary and it is not found on disk, write to appropriate path"
"otherwise, use normal process and potentially trigger another download"
"once the modes if there, load it with sentence piece"
empty words get no embedding
all other words get embedded
GLOVE embeddings
no need to recreate as NILCEmbeddings
read in test file if exists
read in dev file if exists
"find train, dev and test files if not specified"
Add tags for each annotated span
Remove leading and trailing whitespaces from annotated spans
Search start and end token index for current span
If end index is not found set to last token
Throw error if indices are not valid
Add metadatas for sentence
Currently all Jsonl Datasets are stored in Memory
get train data
read in test file if exists
read in dev file if exists
"find train, dev and test files if not specified"
special key for space after
special key for feature columns
special key for dependency head id
"store either Sentence objects in memory, or only file offsets"
"most data sets have the token text in the first column, if not, pass 'text' as column"
determine encoding of text file
identify which columns are spans and which are word-level
now load all sentences
skip first line if to selected
option 1: keep Sentence objects in memory
pointer to previous
parse next sentence
quit if last sentence reached
skip banned sentences
set previous and next sentence for context
append parsed sentence to list in memory
option 2: keep source data in memory
"read lines for next sentence, but don't parse"
quit if last sentence reached
append raw lines for each sentence
we make a distinction between word-level tags and span-level tags
read first sentence to determine which columns are span-labels
skip first line if to selected
check the first 5 sentences
go through all annotations and identify word- and span-level annotations
- if a column has at least one BIES we know it's a Span label
"- if a column has at least one tag that is not BIOES, we know it's a Token label"
- problem cases are columns for which we see only O - in this case we default to Span
skip assigned columns
the space after key is always word-levels
"if at least one token has a BIES, we know it's a span label"
"if at least one token has a label other than BIOES, we know it's a token label"
all remaining columns that are not word-level are span-level
for column in self.word_level_tag_columns:
"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")"
"if sentence ends, break"
parse comments if possible
"otherwise, this line is a token. parse and add to sentence"
check if this sentence is a document boundary
add span labels
discard tags from tokens that are not added to the sentence
parse relations if they are set
head and tail span indices are 1-indexed and end index is inclusive
parse comments such as '# id cd27886d-6895-4d02-a8df-e5fa763fa88f	domain=de-orcas'
"to set the metadata ""domain"" to ""de-orcas"""
get fields from line
get head_id if exists (only in dependency parses)
initialize token
go through all columns
'feats' and 'misc' column should be split into different fields
special handling for whitespace after
add each other feature as label-value pair
get the task name (e.g. 'ner')
get the label value
add label
remap regular tag names
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
set sentence context using partials TODO: pointer to dataset is really inefficient
use all domains
iter over all domains / sources and create target files
The conll representation of coref spans allows spans to
"overlap. If spans end or begin at the same word, they are"
"separated by a ""|""."
The span begins at this word.
The span begins and ends at this word (single word span).
"The span is starting, so we record the index of the word."
"The span for this id is ending, but didn't start at this word."
Retrieve the start index from the document state and
add the span to the clusters for this id.
strip all bracketing information to
get the actual propbank label.
Entering into a span for a particular semantic role label.
We append the label and set the current span for this annotation.
"If there's no '(' token, but the current_span_label is not None,"
then we are inside a span.
We're outside a span.
"Exiting a span, so we reset the current span label for this annotation."
The words in the sentence.
The pos tags of the words in the sentence.
the pieces of the parse tree.
The lemmatised form of the words in the sentence which
have SRL or word sense information.
The FrameNet ID of the predicate.
"The sense of the word, if available."
"The current speaker, if available."
"Cluster id -> List of (start_index, end_index) spans."
Cluster id -> List of start_indices which are open for this id.
Replace brackets in text and pos tags
with a different token for parse trees.
only keep ')' if there are nested brackets with nothing in them.
There are some bad annotations in the CONLL data.
"They contain no information, so to make this explicit,"
we just set the parse piece to be None which will result
in the overall parse tree being None.
"If this is the first word in the sentence, create"
empty lists to collect the NER and SRL BIO labels.
"We can't do this upfront, because we don't know how many"
"components we are collecting, as a sentence can have"
variable numbers of SRL frames.
Create variables representing the current label for each label
sequence we are collecting.
"If any annotation marks this word as a verb predicate,"
we need to record its index. This also has the side effect
of ordering the verbal predicates by their location in the
"sentence, automatically aligning them with the annotations."
"this would not be reached if parse_pieces contained None, hence the cast"
Non-empty line. Collect the annotation.
Collect any stragglers or files which might not
have the '#end document' format for the end of the file.
this dataset name
check if data there
column format
this dataset name
check if data there
column format
this dataset name
download data if necessary
download files if not present locally
we need to slightly modify the original files by adding some new lines after document separators
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
Remove CoNLL-U meta information in the last column
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
entity_mapping
this dataset name
download data if necessary
data validation
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
download files if not present locallys
column format
this dataset name
default dataset folder is the cache root
# download zip
merge the files in one as the zip is containing multiples files
column format
this dataset name
download data if necessary
"unzip the downloaded repo and merge the train, dev and test datasets"
column format
this dataset name
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
check if data there
create folder
download dataset
column format
this dataset name
download and parse data if necessary
create train test dev if not exist
column format
this dataset name
If the extracted corpus file is not yet present in dir
download zip if necessary
"extracted corpus is not present , so unpacking it."
column format
this dataset name
download zip
unpacking the zip
merge the files in one as the zip is containing multiples files
column format
this dataset name
"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)"
download files if not present locally
we need to modify the original files by adding new lines after after the end of each sentence
if only one language is given
column format
this dataset name
"use all languages if explicitly set to ""all"""
download data if necessary
initialize comlumncorpus and add it to list
column format
this dataset name
column format
this dataset name
column format
this dataset name
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
this list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
get google drive id from list
download from google drive
unzip
"tar.extractall(language_folder,members=[tar.getmember(file_name)])"
transform data into required format
"the processed dataset has the additional ending ""_new"""
remove the unprocessed dataset
initialize comlumncorpus and add it to list
if no languages are given as argument all languages used in XTREME will be loaded
if only one language is given
column format
this dataset name
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
This list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
download from HU Server
unzip
transform data into required format
initialize comlumncorpus and add it to list
if only one language is given
column format
this dataset name
download data if necessary
initialize comlumncorpus and add it to list
download data if necessary
unpack and write out in CoNLL column-like format
column format
this dataset name
download data if necessary
data is not in IOB2 format. Thus we transform it to IOB2
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
column format
this dataset name
rename according to train - test - dev - convention
column format
this dataset name
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
Add missing newline after header
Workaround for empty tokens
"Add ""real"" document marker"
Dataset split mapping
v2.0 only adds new language and splits for AJMC dataset
Special document marker for sample splits in AJMC dataset
column format
this dataset name
download data if necessary
column format
this dataset name
if only one language is given
column format
this dataset name
"use all languages if explicitly set to ""all"""
download data if necessary
initialize comlumncorpus and add it to list
this dataset name
default dataset folder is the cache root
download and parse data if necessary
paths to train and test splits
init corpus
this dataset name
default dataset folder is the cache root
download and parse data if necessary
iterate over all html files
"get rid of html syntax, we only need the text"
between all documents we write a separator symbol
skip empty strings
"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)"
"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention"
sentence splitting and tokenization
iterate through all annotations and add to corresponding tokens
find sentence to which annotation belongs
position within corresponding sentence
set annotation for tokens of entity mention
write to out-file in column format
"in case something goes wrong, delete the dataset and raise error"
this dataset name
download and parse data if necessary
from qwikidata.linked_data_interface import get_entity_dict_from_api
generate qid wikiname dictionaries
merge dictionaries
ignore first line
commented and empty lines
read all Q-IDs
ignore first line
request
this dataset name
we use the wikiids in the data instead of directly utilizing the wikipedia urls.
like this we can quickly check if the corresponding page exists
if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi
delete unprocessed file
collect all wikiids
create the dictionary
request
this dataset name
names of raw text documents
open output_file
iterate through all documents
split sentences and tokenize
iterate through all annotations and add to corresponding tokens
find sentence to which annotation belongs
position within corresponding sentence
set annotation for tokens of entity mention
write to out file
annotation from one annotator or two agreeing annotators
this dataset name
download and parse data if necessary
this dataset name
download and parse data if necessary
First parse the post titles
Keep track of how many and which entity mentions does a given post title have
Check if the current post title has an entity link and parse accordingly
Post titles with entity mentions (if any) are handled via this function
Then parse the comments
"Iterate over the comments.tsv file, until the end is reached"
"Keep track of the current comment thread and its corresponding key, on which the annotations are matched."
Each comment thread is handled as one 'document'.
Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.
This if-condition is needed to handle this problem.
"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure"
"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above"
"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle."
"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,"
and not just single letters into single rows.
If there are annotated entity mentions for given post title or a comment thread
"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence"
Write the token with a corresponding tag to file
"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed"
"If a comment thread or a post title has no entity link, all tokens are assigned the O tag"
Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized
"incorrectly, in order to keep the desired format (empty line as a sentence separator)."
"Thrown when the second check above happens, but the last token of a sentence is reached."
"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below."
"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS"
Check if further annotations belong to the current post title or comment thread as well
Stop when the end of an annotation file is reached
Check if further annotations belong to the current sentence as well
"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)"
Docstart
if there is more than one word in the chunk we write each in a separate line
print(chunks)
empty line after each sentence
convert the file to CoNLL
this dataset name
"check if data there, if not, download the data"
create folder
download data
transform data into column format if necessary
if no filenames are specified we use all the data
"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled"
also we remove 'raganato_ALL' from filenames in case its in the list
generate the test file
make column file and save to data_folder
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
this dataset name
download data if necessary
if True:
write CoNLL-U Plus header
"Some special cases (e.g., missing spaces before entity marker)"
necessary if text should be whitespace tokenizeable
Handle case where tail may occur before the head
this dataset name
write CoNLL-U Plus header
this dataset name
TODO: change data source to original CoNLL04 -- this dataset has span formatting errors
download data if necessary
write CoNLL-U Plus header
The span has ended.
We are entering a new span; reset indices
and active tag to new span.
We're inside a span.
Last token might have been a part of a valid span.
this dataset name
write CoNLL-U Plus header
"for source_file_path, target_filename in zip(source_file_paths, target_filenames):"
"with zip_file.open(source_file_path, mode=""r"") as source_file:"
target_file_path = Path(data_folder) / target_filename
"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:"
# write CoNLL-U Plus header
"target_file.write(""# global.columns = id form ner\n"")"
for example in json.load(source_file):
token_list = self._tacred_example_to_token_list(example)
target_file.write(token_list.serialize())
check if first tag row is already occupied
"if first tag row is occupied, use second tag row"
hardcoded mapping TODO: perhaps find nicer solution
remap regular tag names
else skip to position in file where sentence begins
set sentence context using partials TODO: pointer to dataset is really inefficient
read in dev file if exists
read in test file if exists
the url is copied from https://huggingface.co/datasets/darentang/sroie/blob/main/sroie.py#L44
"find train, dev and test files if not specified"
use test_file to create test split if available
use dev_file to create test split if available
"if data point contains black-listed label, do not use"
first check if valid sentence
"if so, add to indices"
"find train, dev and test files if not specified"
variables
different handling of in_memory data than streaming data
"most data sets have the token text in the first column, if not, pass 'text' as column"
test if format is OK
test if at least one label given
make sentence from text (and filter for length)
"if a pair column is defined, make a sentence pair object"
noinspection PyDefaultArgument
dataset name includes the split size
default dataset folder is the cache root
download data if necessary
download each of the 28 splits
create dataset directory if necessary
download senteval datasets if necessary und unzip
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
handle labels file
handle data file
Create flair compatible labels
"by default, map point score to POSITIVE / NEGATIVE values"
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file from CSV
create test.txt file from CSV
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create train dev and test files in fasttext format
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
convert to FastText format
download data if necessary
"if data is not downloaded yet, download it"
get the zip file
move original .tsv files to another folder
create train and dev splits in fasttext format
create eval_dataset file with no labels
download zip archive
unpack file in datasets directory (zip archive contains a directory named SST-2)
this dataset name
default dataset folder is the cache root
download data if necessary
download datasets if necessary
create dataset directory if necessary
create correctly formated txt files
multiple labels are possible
this dataset name
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
download data if necessary
this dataset name
download data if necessary
create a separate directory for different tasks
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
check if dataset is supported
set file names
set file names
download and unzip in file structure if necessary
instantiate corpus
"find train, dev and test files if not specified"
"create DataPairDataset for train, test and dev file, if they are given"
stop if file does not exist
create a DataPair object from strings
"if in_memory is True we return a datapair, otherwise we create one from the lists of strings"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"reorder dev datasets to have same columns as in train set: 8, 9, and 11"
dev sets include 5 different annotations but we will only keep the gold label
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get test and dev sets
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data not downloaded yet, download it"
get the zip file
"the downloaded files have json format, we transform them to tsv"
Function to transform JSON file to tsv for Recognizing Textual Entailment Data
remove json file
Uses dynamic programming approach to calculate maximum independent set in interval graph
with sum of all entity lengths as secondary key
calculate offset without current text
because we stick all passages of a document together
TODO For split entities we also annotate everything inbetween which might be a bad idea?
Try to fix incorrect annotations
print(
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
)
Ignore empty lines or relation annotations
FIX annotation of whitespaces (necessary for PDR)
One token may contain multiple entities -> deque all of them
column format
this dataset name
Create tokenization-dependent CONLL files. This is necessary to prevent
from caching issues (e.g. loading the same corpus with different sentence splitters)
column format
this dataset name
column format
this dataset name
Edge case: last token starts a new entity
Last document in file
column format
this dataset name
column format
this dataset name
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
Edge case: last token starts a new entity
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
column format
this dataset name
Read texts
Read annotations
column format
this dataset name
column format
this dataset name
column format
this dataset name
We need to apply a patch to correct the original training file
Articles title
Article abstract
Entity annotations
column format
this dataset name
Edge case: last token starts a new entity
Map all entities to chemicals
Map all entities to disease
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
Incomplete article
Invalid XML syntax
column format
this dataset name
column format
this dataset name
if len(mid) != 3:
continue
Try to fix entity offsets
column format
this dataset name
There is still one illegal annotation in the file ..
column format
this dataset name
"Abstract first, title second to prevent issues with sentence splitting"
column format
this dataset name
column format
this dataset name
column format
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
column format
this dataset name
"Filter for specific entity types, by default no entities will be filtered"
Get original HUNER splits to retrieve a list of all document ids contained in V2
train and dev split of V2 will be train in V4
test split of V2 will be dev in V4
New documents in V4 will become test documents
column format
this dataset name
column format
this dataset name
default dataset folder is the cache root
column format
build dataset name and full huggingface reference name
Download data if necessary
"Some datasets in BigBio only have train or test splits, not both"
"If only test split, assign it to train split"
"If only train split, sample other from it (sample_missing_splits=True)"
Not every dataset has a dev / validation set!
Perform type mapping if necessary
"Collect all texts of the document, each passage will be"
a text in our internal format
Sort passages by start offset
Transform all entity annotations into internal format
Find the passage of the entity (necessary for offset adaption)
Adapt entity offsets according to passage offsets
FIXME: This is just for debugging purposes
passage_text = id_to_text[passage_id]
doc_text = passage_text[entity_offset[0] : entity_offset[1]]
"mention_text = entity[""text""][0]"
if doc_text != mention_text:
"print(f""Annotation error ({document['document_id']}) - Doc: {doc_text} vs. Mention: {mention_text}"")"
Check base case
Get element in the middle
Is the mention with the passage offsets?
"If element is smaller than mid, then it can only"
be present in left subarray
Else the element can only be present in right subarray
Special case for ProGene: We need to use the split_0_train and split_0_test splits
as they are currently provided in BigBio
cache Feidegger config file
cache Feidegger images
replace image URL with local cached file
append Sentence-Image data point
cast to list if necessary
cast to list if necessary
"first, check if pymongo is installed"
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
Expose base classses
Expose all biomedical data sets used for the evaluation of BioBERT
-
-
-
-
Expose all biomedical data sets using the HUNER splits
Expose all biomedical data sets
Expose all document classification datasets
word sense disambiguation
Expose all entity linking datasets
Expose all relation extraction datasets
universal proposition banks
keyphrase detection datasets
other NER datasets
standard NER datasets
Expose all sequence labeling datasets
Expose all text-image datasets
Expose all text-text datasets
Expose all treebanks
"find train, dev and test files if not specified"
get train data
get test data
get dev data
option 1: read only sentence boundaries as offset positions
option 2: keep everything in memory
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
current token ID
handling for the awful UD multiword format
end of sentence
comments or ellipsis
if token is a multi-word
normal single-word tokens
"if we don't split multiwords, skip over component words"
add token
add morphological tags
derive whitespace logic for multiwords
print(token)
print(current_multiword_last_token)
print(current_multiword_first_token)
"if multi-word equals component tokens, there should be no whitespace"
go through all tokens in subword and set whitespace_after information
print(i)
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
this dataset name
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
"finally, print model card for information"
noqa: INP001
-- Project information -----------------------------------------------------
"The full version, including alpha/beta/rc tags"
use smv_current_version as the git url
-- General configuration ---------------------------------------------------
"Add any Sphinx extension module names here, as strings. They can be"
extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
ones.
"Add any paths that contain templates here, relative to this directory."
"List of patterns, relative to source directory, that match files and"
directories to ignore when looking for source files.
This pattern also affects html_static_path and html_extra_path.
-- Options for HTML output -------------------------------------------------
The theme to use for HTML and HTML Help pages.  See the documentation for
a list of builtin themes.
""
"Add any paths that contain custom static files (such as style sheets) here,"
"relative to this directory. They are copied after the builtin static files,"
"so a file named ""default.css"" will overwrite the builtin ""default.css""."
Napoleon settings
Whitelist pattern for tags (set to None to ignore all tags)
Whitelist pattern for branches (set to None to ignore all branches)
Whitelist pattern for remotes (set to None to use local branches only)
Pattern for released versions
Format for versioned output directories inside the build directory
Determines whether remote or local git branches/tags are preferred if their output dirs conflict
test corpus
create a TARS classifier
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
test corpus
create a TARS classifier
switch to a new task (TARS can do multiple tasks so you must define one)
initialize the text classifier trainer
start the training
"With end symbol, without start symbol, padding in front"
"Without end symbol, with start symbol, padding in back"
"Without end symbol, without start symbol, padding in front"
initialize trainer
initialize trainer
initialize trainer
increment for last token in sentence if not followed by whitespace
clean up directory
clean up directory
example sentence
set 4 labels for 2 tokens ('love' is tagged twice)
check if there are three POS labels with correct text and values
check if there are is one SENTIMENT label with correct text and values
check if all tokens are correctly labeled
remove the pos label from the last word
there should be 2 POS labels left
now remove all pos tags
set 3 labels for 2 spans (HU is tagged twice)
check if there are three labels with correct text and values
check if there are two spans with correct text and values
"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
should be only one NER label left
and only one NER span
set 3 labels for 2 spans (HU is tagged twice with different tags)
check if there are three labels with correct text and values
check if there are two spans with correct text and values
"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
should be only one NER label left
and only one NER span
but there is also one orgtype span and label
and only one NER span
let's add the NER tag back
check if there are three labels with correct text and values
check if there are two spans with correct text and values
now remove all NER tags
set 3 labels for 2 spans (HU is tagged twice with different tags)
create two relation label
there should be two relation labels
there should be one syntactic labels
"there should be two relations, one with two and one with one label"
example sentence
add another topic label
example sentence
has sentiment value
has 4 part of speech tags
has 1 NER tag
should be in total 6 labels
example sentence
add two NER labels
get the four labels
check that only two of the respective data points are equal
make a sentence and some right context
TODO: is this desirable? Or should two sentences with same text be considered same objects?
Initializing a Sentence this way assumes that there is a space after each token
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
load column dataset with one entry
load column dataset with two entries
load column dataset with three entries
"get training, test and dev data"
"get training, test and dev data"
check if Token labels are correct
"get training, test and dev data"
check if Token labels for frames are correct
"get training, test and dev data"
"get training, test and dev data"
get two corpora as one
"get training, test and dev data for full English UD corpus from web"
clean up data directory
"assert [token.get_tag(""head"").value for token in sent1.tokens] == ["
"""2"","
"""0"","
"""4"","
"""2"","
"""2"","
"""2"","
]
This test only covers basic universal dependencies datasets.
"For example, multi-word tokens or the ""deps"" column sentence annotations are not supported yet."
"Here, we use the default token annotation fields."
This test covers the complete HIPE 2022 dataset.
https://github.com/hipe-eval/HIPE-2022-data
"Includes variant with document separator, and all versions of the dataset."
"We have manually checked, that these numbers are correct:"
"+1 offset, because of missing EOS marker at EOD"
Test data for v2.1 release
This test covers the complete ICDAR Europeana corpus:
https://github.com/stefan-it/historic-domain-adaptation-icdar
"This test covers the complete MasakhaNER dataset, including support for v1 and v2."
This test covers the NERMuD dataset. Official stats can be found here:
https://github.com/dhfbk/KIND/tree/main/evalita-2023
Number of instances per dataset split are taken from https://huggingface.co/datasets/elenanereiss/german-ler
This test covers the complete MasakhaPOS dataset.
"See MobIE paper (https://aclanthology.org/2021.konvens-1.22/), table 2"
--- Embeddings that are shared by both models --- #
--- Task 1: Sentiment Analysis (5-class) --- #
Define corpus and model
-- Task 2: Binary Sentiment Analysis on Customer Reviews -- #
Define corpus and model
-- Define mapping (which tagger should train on which model) -- #
-- Create model trainer and train -- #
clean up file
no need for label_dict
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
Intel ----founded_by---> Gordon Moore
Intel ----founded_by---> Robert Noyce
"Ground truth is a set of tuples of (<Sentence Text>, <Relation Label Values>)"
Check sentence masking and relation label annotation on
"training, validation and test dataset (in this test the splits are the same)"
"Entity pair permutations of: ""Larry Page and Sergey Brin founded Google ."""
"Entity pair permutations of: ""Microsoft was founded by Bill Gates ."""
"Entity pair permutations of: ""Konrad Zuse was born in Berlin on 22 June 1910 ."""
"Entity pair permutations of: ""Joseph Weizenbaum , a professor at MIT , was born in Berlin , Germany."""
This sentence is only included if we transform the corpus with cross augmentation
Ensure this is an example that predicts no classes in multilabel
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
ensure that the prepared tensors is what we expect
use a SequenceTagger to save and reload the embedding in the manner it is supposed to work
previous and next sentence as context
test expansion for sentence without context
test expansion for with previous and next as context
test expansion if first sentence is document boundary
test expansion if we don't use context
"apparently the precision is not that high on cuda, hence the absolute tolerance needs to be higher."
dummy model with embeddings
save the dummy and load it again
check that context_length and use_context_separator is the same for both
mmap seems to be much more memory efficient
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
Extract all the contents of zip file in current directory
Extract all the contents of zip file in current directory
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
load_big_file is a workaround byhttps://github.com/highway11git
to load models on some Mac/Windows setups
see https://github.com/zalandoresearch/flair/issues/351
first determine the distribution of classes in the dataset
weight for each sample
Create blocks
shuffle the blocks
concatenate the shuffled blocks
Create blocks
shuffle the blocks
concatenate the shuffled blocks
increment for last token in sentence if not followed by whitespace
this is the default init size of a lmdb database for embeddings
get db filename from embedding name
"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
SequenceTagger
TextClassifier
get db filename from embedding name
if embedding database already exists
"otherwise, push embedding to database"
if embedding database already exists
open the database in read mode
we need to set self.k
create and load the database in write mode
"no idea why, but we need to close and reopen the environment to avoid"
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
when opening new transaction !
init dictionaries
"in order to deal with unknown tokens, add <unk>"
set 'add_unk' if the dictionary was created with a version of Flair older than 0.9
set 'add_unk' depending on whether <unk> is a key
"if one embedding name, directly return it"
"if multiple embedding names, concatenate them"
First we remove any existing labels for this PartOfSentence in self.sentence
labels also need to be deleted at Sentence object
delete labels at object itself
"check if the span already exists. If so, return it"
else make a new span
"check if the relation already exists. If so, return it"
else make a new relation
private field for all known spans
the tokenizer used for this sentence
some sentences represent a document boundary (but most do not)
internal variables to denote position inside dataset
"if text is passed, instantiate sentence with tokens (words)"
determine token positions and whitespace_after flag
the last token has no whitespace after
log a warning if the dataset is empty
data with zero-width characters cannot be handled
set token idx and sentence
append token to sentence
register token annotations on sentence
move sentence embeddings to device
also move token embeddings to device
clear token embeddings
infer whitespace after field
"if sentence has no tokens, return empty string"
"otherwise, return concatenation of tokens with the correct offsets"
The sentence's start position is not propagated to its tokens.
"Therefore, we need to add the sentence's start position to its last token's end position, including whitespaces."
No character at the corresponding code point: remove it
"if no label if specified, return all labels"
"if the label type exists in the Sentence, return it"
return empty list if none of the above
labels also need to be deleted at all tokens
labels also need to be deleted at all known spans
remove spans without labels
delete labels at object itself
set name
abort if no data is provided
sample test data from train if none is provided
sample dev data from train if none is provided
set train dev and test data
find out empty sentence indices
create subset of non-empty sentence indices
find out empty sentence indices
create subset of non-empty sentence indices
count all label types per sentence
go through all labels of label_type and count values
check if there are any span labels
"if an unk threshold is set, UNK all label values below this threshold"
sample randomly from a label distribution according to the probabilities defined by the noise transition matrix
replace the old label with the new one
keep track of the old (clean) label using another label type category
keep track of how many labels in total are flipped
sample randomly from a label distribution according to the probabilities defined by the desired noise share
replace the old label with the new one
keep track of the old (clean) label using another label type category
keep track of how many labels in total are flipped
Make the tag dictionary
"add a dummy ""O"" to close final prediction"
return complex list
internal variables
non-set tags are OUT tags
anything that is not OUT is IN
does this prediction start a new span?
begin and single tags start new spans
"in IOB format, an I tag starts a span if it follows an O or is a different span"
single tags that change prediction start new spans
if an existing span is ended (either by reaching O or starting a new span)
determine score and value
append to result list
reset for-loop variables for new span
remember previous tag
global variable: cache_root
global variable: device
"No need for correctness checks, torch is doing it"
global variable: version
global variable: arrow symbol
dummy return to fulfill trainer.train() needs
print(vec)
Attach optimizer
"convert `metrics` to float, in case it's a zero-dim Tensor"
if memory mode option 'none' delete everything
"if dynamic embedding keys not passed, identify them automatically"
always delete dynamic embeddings
"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)"
optional metric space decoder if prototypes have different length than embedding
create initial prototypes for all classes (all initial prototypes are a vector of all 1s)
"if set, create initial prototypes from normal distribution"
"if set, use a radius"
all parameters will be pushed internally to the specified device
decode embeddings into prototype space
"if unlabeled distance is set, mask out loss to unlabeled class prototype"
Always include the name of the Model class for which the state dict holds
"in Flair <0.9.1, optimizer and scheduler used to train model are not saved"
"write out a ""model card"" if one is set"
special handling for optimizer:
remember optimizer class and state dictionary
save model
restore optimizer and scheduler to model card if set
"if this class is abstract, go through all inheriting classes and try to fetch and load the model"
get all non-abstract subclasses
"try to fetch the model for each subclass. if fetching is possible, load model and return it"
"skip any invalid loadings, e.g. not found on huggingface hub"
"if the model cannot be fetched, load as a file"
try to get model class from state
"older (flair 11.3 and below) models do not contain cls information. In this case, try all subclasses"
"if str(model_cls) == ""<class 'flair.models.pairwise_classification_model.TextPairClassifier'>"": continue"
"skip any invalid loadings, e.g. not found on huggingface hub"
"if this class is not abstract, fetch the model and load it"
"make sure <unk> is contained in gold_label_dictionary, if given"
"read Dataset into data loader, if list of sentences passed, make Dataset first"
loss calculation
variables for printing
variables for computing scores
remove any previously predicted labels
predict for batch
get the gold labels
add to all_predicted_values
make printout lines
convert true and predicted values to two span-aligned lists
delete exluded labels if exclude_labels is given
"if after excluding labels, no label is left, ignore the datapoint"
write all_predicted_values to out_file if set
make the evaluation dictionary
check if this is a multi-label problem
compute numbers by formatting true and predicted such that Scikit-Learn can use them
multi-label problems require a multi-hot vector for each true and predicted label
single-label problems can do with a single index for each true and predicted label
"now, calculate evaluation numbers"
there is at least one gold label or one prediction (default)
"if there is only one label, then ""micro avg"" = ""macro avg"""
"micro average is only computed if zero-label exists (for instance ""O"")"
if no zero-label exists (such as in POS tagging) micro average is equal to accuracy
same for the main score
issue error and default all evaluation numbers to 0.
line for log file
check if there is a label mismatch
print info
set the embeddings
initialize the label dictionary
initialize the decoder
set up multi-label logic
init dropouts
loss weights and loss function
Initialize the weight tensor
set up gradient reversal if so specified
embed sentences
get a tensor of data points
do dropout
make a forward pass to produce embedded data points and labels
get the data points for which to predict labels
get their gold labels as a tensor
pass data points through network to get encoded data point tensor
decode
an optional masking step (no masking in most cases)
calculate the loss
filter empty sentences
reverse sort all sequences by their length
progress bar for verbosity
filter data points in batch
stop if all sentences are empty
pass data points through network and decode
if anything could possibly be predicted
remove previously predicted labels of this type
add DefaultClassifier arguments
add variables of DefaultClassifier
Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23
Get projected 1st dimension
Compute bilinear form
Arcosh
Project the input data to n+1 dimensions
"The first dimension, is recomputed in the distance module"
header for 'weights.txt'
"determine the column index of loss, f-score and accuracy for"
"train, dev and test split"
then get all relevant values from the tsv
then get all relevant values from the tsv
plot i
save plots
save plots
plt.show()
save plot
take the average over the last three scores of training
take average over the scores from the different training runs
auto-spawn on GPU if available
progress bar for verbosity
stop if all sentences are empty
clearing token embeddings to save memory
"read Dataset into data loader, if list of sentences passed, make Dataset first"
TODO: not saving lines yet
TODO: This closely shadows the RelationExtractor name. Maybe we need a better name here.
- MaskedRelationClassifier ?
This depends if this relation classification architecture should replace or offer as an alternative.
Set label type and prepare label dictionary
Initialize super default classifier
Add the special tokens from the encoding strategy
"Auto-spawn on GPU, if available"
Only use entities labelled with the specified labels for each label type
Only use entities above the specified threshold
Use a dictionary to find gold relation annotations for a given entity pair
Yield head and tail entity pairs from the cross product of all entities
Remove identity relation entity pairs
Remove entity pairs with labels that do not match any
of the specified relations in `self.entity_pair_labels`
"Obtain gold label, if existing"
Some sanity checks
Pre-compute non-leading head and tail tokens for entity masking
We can not use the plaintext of the head/tail span in the sentence as the mask/marker
since there may be multiple occurrences of the same entity mentioned in the sentence.
"Therefore, we use the span's position in the sentence."
Create masked sentence
Add gold relation annotation as sentence label
"Using the sentence label instead of annotating a separate `Relation` object is easier to manage since,"
"during prediction, the forward pass does not need any knowledge about the entities in the sentence."
"If we sample missing splits, the encoded sentences that correspond to the same original sentences"
"may get distributed into different splits. For training purposes, this is always undesired."
Ensure that all sentences are encoded properly
Deal with the case where all sentences are encoded sentences
"mypy does not infer the type of ""sentences"" restricted by the if statement"
Deal with the case where all sentences are standard (non-encoded) sentences
"For each encoded sentence, transfer its prediction onto the original relation"
auto-spawn on GPU if available
pad strings with whitespaces to longest sentence
cut up the input into chunks of max charlength = chunk_size
push each chunk through the RNN language model
concatenate all chunks to make final output
initial hidden state
get predicted weights
divide by temperature
"to prevent overflow problem with small temperature values, substract largest value from all"
this makes a vector in which the largest value is 0
compute word weights with exponential function
try sampling multinomial distribution for next character
print(word_idx)
input ids
push list of character IDs through model
the target is always the next character
use cross entropy loss to compare output of forward pass with targets
exponentiate cross-entropy loss to calculate perplexity
"""document_delimiter"" property may be missing in some older pre-trained models"
serialize the language models and the constructor arguments (but nothing else)
special handling for deserializing language models
re-initialize language model with constructor arguments
copy over state dictionary to self
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
"in their ""self.train()"" method)"
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
Transform input data into TARS format
"if there are no labels, return a random sample as negatives"
"otherwise, go through all labels"
make sure the probabilities always sum up to 1
get and embed all labels by making a Sentence object that contains only the label text
get each label embedding and scale between 0 and 1
compute similarity matrix
"the higher the similarity, the greater the chance that a label is"
sampled as negative example
make label dictionary if no Dictionary object is passed
prepare dictionary of tags (without B- I- prefixes and without UNK)
check if candidate_label_set is empty
make list if only one candidate label is passed
create label dictionary
note current task
create a temporary task
make zero shot predictions
switch to the pre-existing task
prepare TARS dictionary
initialize a bare-bones sequence tagger
transformer separator
Store task specific labels since TARS can handle multiple tasks
make a tars sentence where all labels are O by default
init new TARS classifier
set all task information
return
with torch.no_grad():
progress bar for verbosity
stop if all sentences are empty
go through each sentence in the batch
always remove tags first
get the span and its label
determine whether tokens in this span already have a label
only add if all tokens have no label
make and add a corresponding predicted span
set indices so that no token can be tagged twice
clearing token embeddings to save memory
"all labels default to ""O"""
set gold token-level
set predicted token-level
now print labels in CoNLL format
prepare TARS dictionary
initialize a bare-bones sequence tagger
transformer separator
Store task specific labels since TARS can handle multiple tasks
get the serialized embeddings
remap state dict for models serialized with Flair <= 0.11.3
init new TARS classifier
set all task information
with torch.no_grad():
progress bar for verbosity
stop if all sentences are empty
go through each sentence in the batch
always remove tags first
add all labels that according to TARS match the text and are above threshold
do not add labels below confidence threshold
only use label with highest confidence if enforcing single-label predictions
get all label scores and do an argmax to get the best label
remove previously added labels and only add the best label
clearing token embeddings to save memory
set separator to concatenate two sentences
auto-spawn on GPU if available
pooling operation to get embeddings for entites
set embeddings
set relation and entity label types
"whether to use gold entity pairs, and whether to filter entity pairs by type"
filter entity pairs according to their tags if set
whether to encode characters and whether to use attention (attention can only be used if chars are encoded)
character dictionary for decoding and encoding
make sure <unk> is in dictionary for handling of unknown characters
add special symbols to dictionary if necessary and save respective indices
---- ENCODER ----
encoder character embeddings
encoder pre-trained embeddings
encoder RNN
additional encoder linear layer if bidirectional encoding
---- DECODER ----
decoder: linear layers to transform vectors to and from alphabet_size
when using attention we concatenate attention outcome and decoder hidden states
decoder RNN
loss and softmax
self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction
add additional columns for special symbols if necessary
initialize with dummy symbols
encode inputs
get labels (we assume each token has a lemma label)
get char indices for labels of sentence
"(batch_size, max_sequence_length) batch_size = #words in sentence,"
max_sequence_length = length of longest label of sentence + 1
get char embeddings
"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size"
take decoder input and initial hidden and pass through RNN
"if all encoder outputs are provided, use attention"
take convex combinations of encoder hidden states as new output using the computed attention coefficients
"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)"
get all tokens
encode input characters by sending them through RNN
get one-hots for characters and add special symbols / padding
determine length of each token
embed sentences
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
variable to store initial hidden states for decoder
encode input characters by sending them through RNN
test packing and padding
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
concatenate the final hidden states of the encoder. These will be projected to hidden_size of
decoder later with self.emb_to_hidden
mask out vectors that correspond to a dummy symbol (TODO: check attention masking)
use token embedding as initial hidden state for decoder
concatenate everything together and project to appropriate size for decoder
variable to store initial hidden states for decoder
encode input characters by sending them through RNN
note that we do not need to fill up with dummy symbols since we process each token seperately
embed character one-hots
send through encoder RNN (produces initial hidden for decoder)
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
project 2*hidden_size to hidden_size
concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder
later with self.emb_to_hidden
use token embedding as initial hidden state for decoder
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
concatenate everything together and project to appropriate size for decoder
"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)"
"create target vector (batch_size, max_label_seq_length + 1)"
filter empty sentences
max length of the predicted sequences
for printing
stop if all sentences are empty
remove previously predicted labels of this type
create list of tokens in batch
encode inputs
"create input for first pass (batch_size, 1, input_size), first letter is special character <S>"
sequence length is always set to one in prediction
option 1: greedy decoding
predictions
decode next character
pick top beam size many outputs with highest probabilities
option 2: beam search
out_probs = self.softmax(output_vectors).squeeze(1)
make sure no dummy symbol <> or start symbol <S> is predicted
pick top beam size many outputs with highest probabilities
"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1"
"leading_indices and probabilities have size (batch_size, beam_size)"
keep scores of beam_size many hypothesis for each token in the batch
stack all leading indices of all hypothesis and corresponding hidden states in two tensors
save sequences so far
keep track of how many hypothesis were completed for each token
"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)"
decode with log softmax
make sure no dummy symbol <> or start symbol <S> is predicted
"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside"
"if the sequence is already ended, do not record as candidate"
index of token in in list tokens_in_batch
print(token_number)
hypothesis score
TODO: remove token if number of completed hypothesis exceeds given value
set score of corresponding entry to -inf so it will not be expanded
get leading_indices for next expansion
find highest scoring hypothesis among beam_size*beam_size possible ones for each token
take beam_size many copies of scores vector and add scores of possible new extensions
"size (beam_size*batch_size, beam_size)"
print(hypothesis_scores)
"reshape to vector of size (batch_size, beam_size*beam_size),"
each row contains beam_size*beam_size scores of the new possible hypothesis
print(hypothesis_scores_per_token)
"choose beam_size best for each token - size (batch_size, beam_size)"
out of indices_per_token we now need to recompute the original indices of the hypothesis in
a list of length beam_size*batch_size
"where the first three inidices belong to the first token, the next three to the second token,"
and so on
with these indices we can compute the tensors for the next iteration
expand sequences with corresponding index
add log-probabilities to the scores
save new leading indices
save corresponding hidden states
it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations
in that case we append one of the final seuqences without end symbol to the final_candidates
get best final hypothesis for each token
get characters from index sequences and add predicted label to token
dictionaries
all parameters will be pushed internally to the specified device
now print labels in CoNLL format
internal candidate lists of generator
load Zelda candidates if so passed
create candidate lists
"if lower casing is enabled, create candidate lists of lower cased versions"
create a new dictionary for lower cased mentions
go through each mention and its candidates
"check if backoff mention already seen. If so, add candidates. Else, create new entry."
set lowercased version as map
remap state dict for models serialized with Flair <= 0.11.3
get the candidates
"during training, add the gold value as candidate"
----- Create the internal tag dictionary -----
span-labels need special encoding (BIO or BIOES)
the big question is whether the label dictionary should contain an UNK or not
"without UNK, we cannot evaluate on data that contains labels not seen in test"
"with UNK, the model learns less well if there are no UNK examples"
is this a span prediction problem?
----- Embeddings -----
----- Initial loss weights parameters -----
----- RNN specific parameters -----
----- Conditional Random Field parameters -----
"Previously trained models have been trained without an explicit CRF, thus it is required to check"
whether we are loading a model from state dict in order to skip or add START and STOP token
----- Dropout parameters -----
dropouts
remove word dropout if there is no contact over the sequence dimension.
----- Model layers -----
----- RNN layer -----
"If shared RNN provided, else create one for model"
Whether to train initial hidden state
final linear map to tag space
"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss"
"if using CRF, we also require a CRF and a Viterbi decoder"
"if there are no sentences, there is no loss"
forward pass to get scores
calculate loss given scores and labels
make a zero-padded tensor for the whole sentence
linear map to tag space
"Depending on whether we are using CRF or a linear layer, scores is either:"
"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF"
"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer"
spans need to be encoded as token-level predictions
all others are regular labels for each token
make sure it's a list
filter empty sentences
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
get features from forward propagation
remove previously predicted labels of this type
"if return_loss, get loss value"
make predictions
add predictions to Sentence
BIOES-labels need to be converted to spans
"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)"
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
core Flair models on Huggingface ModelHub
"Large NER models,"
Multilingual NER models
English POS models
Multilingual POS models
English SRL models
English chunking models
Language-specific NER models
Language-specific POS models
English NER models
Multilingual NER models
English POS models
Multilingual POS models
English SRL models
English chunking models
Danish models
German models
French models
Dutch models
Malayalam models
Portuguese models
Keyphase models
Biomedical models
check if model name is a valid local file
"check if model key is remapped to HF key - if so, print out information"
get mapped name
use mapped name instead
"if not, check if model key is remapped to direct download location. If so, download model"
special handling for the taggers by the @redewiegergabe project (TODO: move to model hub)
"for all other cases (not local file or special download location), use HF model hub"
"if not a local file, get from model hub"
use model name as subfolder
Lazy import
output information
## Demo: How to use in Flair
load tagger
make example sentence
predict NER tags
print sentence
print predicted NER spans
iterate over entities and print
Lazy import
Save model weight
Determine if model card already exists
Generate and save model card
Upload files
"all labels default to ""O"""
set gold token-level
set predicted token-level
now print labels in CoNLL format
print labels in CoNLL format
the multi task model has several labels
biomedical models
entity linker
auto-spawn on GPU if available
remap state dict for models serialized with Flair <= 0.11.3
English sentiment models
Communicative Functions Model
"scores_at_targets[range(features.shape[0]), lengths.values -1]"
Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices
"Initially, get scores from <start> tag to all other tags"
"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp"
"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep"
Create a tensor to hold accumulated sequence scores at each current tag
Create a tensor to hold back-pointers
"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag"
"Let pads be the <end> tag index, since that was the last tag in the decoded sequence"
"We add scores at current timestep to scores accumulated up to previous timestep, and"
choose the previous timestep that corresponds to the max. accumulated score for each current timestep
"If sentence is over, add transition to STOP-tag"
Decode/trace best path backwards
Sanity check
remove start-tag and backscore to stop-tag
Max + Softmax to get confidence score for predicted label and append label to each token
"Transitions are used in the following way: transitions[to, from]."
"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag"
to START-tag and from STOP-tag to any other tag to -10000.
create a model card for this model with Flair and PyTorch version
also record Transformers version if library is loaded
remember all parameters used in train() call
add model card to model
"if optimizer class is passed, instantiate:"
"determine what splits (train, dev, test) to evaluate and log"
prepare loss logging file and set up header
"from here on, use list of learning rates"
load existing optimizer state dictionary if it exists
"minimize training loss if training with dev data, else maximize dev score"
"if scheduler is passed as a class, instantiate"
"if we load a checkpoint, we have already trained for epoch"
"Determine whether to log ""bad epochs"" information"
load existing scheduler state dictionary if it exists
update optimizer and scheduler in model card
"if training also uses dev/train data, include in training set"
initialize sampler if provided
init with default values if only class is provided
set dataset to sample from
this field stores the names of all dynamic embeddings in the model (determined after first forward pass)
At any point you can hit Ctrl + C to break out of training early.
update epoch in model card
get new learning rate
reload last best model if annealing with restarts is enabled
stop training if learning rate becomes too small
"if shuffle_first_epoch==False, the first epoch is not shuffled"
process mini-batches
zero the gradients on the model and optimizer
"if necessary, make batch_steps"
forward and backward for batch
forward pass
Backward
identify dynamic embeddings (always deleted) on first sentence
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
do the optimizer step
do the scheduler step if one-cycle or linear decay
get new learning rate
evaluate on train / dev / test split depending on training settings
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
calculate scores using dev data if available
append dev score to score history
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
determine if this is the best model or if we need to anneal
default mode: anneal against dev score
alternative: anneal against dev loss
alternative: anneal against train loss
determine bad epoch number
lr unchanged
log bad epochs
output log file
make headers on first epoch
"if checkpoint is enabled, save model at each epoch"
Check whether to save best model
"if we do not use dev data for model selection, save final model"
test best model if test data is present
recover all arguments that were used to train this model
you can overwrite params with your own
surface nested arguments
resume training with these parameters
"If set, add a factor to the learning rate of all parameters with 'embeddings' not in name"
get and return the final test score of best model
cast string to Path
forward pass
update optimizer and scheduler
"TextDataset returns a list. valid and test are only one file,"
so return the first element
cast string to Path
error message if the validation dataset is too small
Shuffle training files randomly after serially iterating
through corpus one
"iterate through training data, starting at"
self.split (for checkpointing)
off by one for printing
go into train mode
reset variables
not really sure what this does
do the forward pass in the model
try to predict the targets
Backward
`clip_grad_norm` helps prevent the exploding gradient
problem in RNNs / LSTMs.
We detach the hidden state from how it was
previously produced.
"If we didn't, the model would try backpropagating"
all the way to start of the dataset.
explicitly remove loss to clear up memory
#########################################################
Save the model if the validation loss is the best we've
seen so far.
#########################################################
print info
#########################################################
##############################################################################
final testing
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
"the default model for ELMo is the 'original' model, which is very large"
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
put on Cuda if available
embed a dummy sentence to determine embedding_length
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
"multilingual forward (English, German, French, Italian, Dutch, Polish)"
"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
Dutch forward
Dutch backward
Swedish forward
Swedish backward
French forward
French backward
Czech forward
Czech backward
Portuguese forward
Portuguese backward
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
1-camembert-base -> camembert-base
1-xlm-roberta-large -> xlm-roberta-large
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
The mask has 1 for real tokens and 0 for padding tokens. Only real
tokens are attended to.
Zero-pad up to the sequence length.
"first, find longest sentence in batch"
prepare id maps for BERT model
put encoded batch through BERT model to get all hidden states of all encoder layers
get aggregated embeddings for each BERT-subtoken in sentence
get the current sentence object
add concatenated embedding to sentence
use first subword embedding if pooling operation is 'first'
"otherwise, do a mean over all subwords in token"
"if only one sentence is passed, convert to list of sentence"
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
embed a dummy sentence to determine embedding_length
Avoid conflicts with flair's Token class
"legacy pickle-like saving for image embeddings, as implementation details are not obvious"
"legacy pickle-like loading for image embeddings, as implementation details are not obvious"
"<cls> token initially set to 1/D, so it attends to all image features equally"
add positional encodings
reshape the pixels into the sequence
layer norm after convolution and positional encodings
add <cls> token
"transformer requires input in the shape [h*w+1, b, d]"
the output is an embedding of <cls> token
this parameter is fixed
optional fine-tuning on top of embedding layer
"if only one sentence is passed, convert to list of sentence"
"if only one sentence is passed, convert to list of sentence"
bidirectional RNN on top of embedding layer
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push through RNN
after-RNN dropout
extract embeddings from RNN
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
serialize the language models and the constructor arguments (but nothing else)
re-initialize language model with constructor arguments
special handling for deserializing language models
copy over state dictionary to self
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
"in their ""self.train()"" method)"
IMPORTANT: add embeddings as torch modules
iterate over sentences
"if its a forward LM, take last state"
"convert to plain strings, embedded in a list for the encode function"
CNN
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push CNN
after-CNN dropout
extract embeddings from CNN
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
"if only one sentence is passed, convert to list of sentence"
Expose base classses
Expose document embedding classes
Expose image embedding classes
Expose legacy embedding classes
Expose token embedding classes
in some cases we need to insert zero vectors for tokens without embedding.
padding
remove special markup
check if special tokens exist to circumvent error message
iterate over subtokens and reconstruct tokens
remove special markup
check if reconstructed token is special begin token ([CLS] or similar)
some BERT tokenizers somehow omit words - in such cases skip to next token
"we cannot handle unk_tokens perfectly, so let's assume that one unk_token corresponds to one token."
if tokens are unaccounted for
check if all tokens were matched to subtokens
The layoutlm tokenizer doesn't handle ocr themselves
"transformers returns the ""added_tokens.json"" even if it doesn't create it"
"transformers returns the ""added_tokens.json"" even if it doesn't create it"
in case of doubt: token embedding has higher priority than document embedding
random check some tokens to save performance.
Models such as FNet do not have an attention_mask
set language IDs for XLM-style transformers
"word_ids is only supported for fast rust tokenizers. Some models like ""xlm-mlm-ende-1024"" do not have"
"a fast tokenizer implementation, hence we need to fall back to our own reconstruction of word_ids."
set context if not set already
flair specific pre-tokenization
fields to store left and right context
expand context only if context_length is set
"if context_dropout is set, randomly deactivate left context during training"
"if context_dropout is set, randomly deactivate right context during training"
"if use_context_separator is set, add a [FLERT] token"
return expanded sentence and context length information
"onnx prepares numpy arrays, no mather if it runs on gpu or cpu, the input is on cpu first."
temporary fix to disable tokenizer parallelism warning
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
do not print transformer warnings as these are confusing in this case
load tokenizer and transformer model
load tokenizer from inmemory zip-file
model name
embedding parameters
send mini-token through to check how many layers the model has
return length
"If we use a context separator, add a new special token"
"most models have an initial BOS token, except for XLNet, T5 and GPT2"
"when initializing, embeddings are in eval mode by default"
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
"cannot run `.encode` if ocr boxes are required, assume"
in case of doubt: token embedding has higher priority than document embedding
in case of doubt: token embedding has higher priority than document embedding
legacy TransformerDocumentEmbedding
legacy TransformerTokenEmbedding
legacy Flair <= 0.12
legacy Flair <= 0.7
legacy TransformerTokenEmbedding
Legacy TransformerDocumentEmbedding
legacy TransformerTokenEmbedding
legacy TransformerDocumentEmbedding
copy values from new embedding
cls first pooling can be done without recreating sentence hidden states
make the tuple a tensor; makes working with it easier.
"for multimodal models like layoutlmv3, we truncate the image embeddings as they are only used via attention"
only use layers that will be outputted
this parameter is fixed
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
make compatible with serialized models
gensim version 4
gensim version 3
"if no embedding is set, the vocab and embedding length is requried"
GLOVE embeddings
TURIAN embeddings
KOMNINOS embeddings
pubmed embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
twitter embeddings
two-letter language code wiki embeddings
two-letter language code wiki embeddings
two-letter language code crawl embeddings
fix serialized models
"this is required to force the module on the cpu,"
"if a parent module is put to gpu, the _apply is called to each sub_module"
self.to(..) actually sets the device properly
this ignores the get_cached_vec method when loading older versions
it is needed for compatibility reasons
gensim version 4
gensim version 3
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
multilingual models
English models
Arabic
Bulgarian
Czech
Danish
German
Spanish
Basque
Persian
Finnish
French
Hebrew
Hindi
Croatian
Indonesian
Italian
Japanese
Malayalam
Dutch
Norwegian
Polish
Portuguese
Pubmed
Slovenian
Swedish
Tamil
Spanish clinical
CLEF HIPE Shared task
Amharic
Ukrainian
load model if in pretrained model map
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
CLEF HIPE models are lowercased
embeddings are static if we don't do finetuning
embed a dummy sentence to determine embedding_length
set to eval mode
make compatible with serialized models (TODO: remove)
"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout"
make compatible with serialized models (TODO: remove)
gradients are enable if fine-tuning is enabled
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
offset mode that extracts at whitespace after last character
offset mode that extracts at last character
use the character language model embeddings as basis
length is twice the original character LM embedding length
these fields are for the embedding memory
whether to add only capitalized words to memory (faster runtime and lower memory consumption)
we re-compute embeddings dynamically at each epoch
set the memory method
memory is wiped each time we do a training run
"if we keep a pooling, it needs to be updated continuously"
update embedding
check token.text is empty or not
set aggregation operation
add embeddings after updating
model architecture
model architecture
"""pl"","
download if necessary
load the model
"TODO: keep for backwards compatibility, but remove in future"
save the sentence piece model as binary file (not as path which may change)
write out the binary sentence piece model into the expected directory
"if the model was saved as binary and it is not found on disk, write to appropriate path"
"otherwise, use normal process and potentially trigger another download"
"once the modes if there, load it with sentence piece"
empty words get no embedding
all other words get embedded
GLOVE embeddings
no need to recreate as NILCEmbeddings
read in test file if exists
read in dev file if exists
"find train, dev and test files if not specified"
Add tags for each annotated span
Remove leading and trailing whitespaces from annotated spans
Search start and end token index for current span
If end index is not found set to last token
Throw error if indices are not valid
get train data
read in test file if exists
read in dev file if exists
"find train, dev and test files if not specified"
special key for space after
special key for feature columns
special key for dependency head id
"store either Sentence objects in memory, or only file offsets"
"most data sets have the token text in the first column, if not, pass 'text' as column"
determine encoding of text file
identify which columns are spans and which are word-level
now load all sentences
skip first line if to selected
option 1: keep Sentence objects in memory
pointer to previous
parse next sentence
quit if last sentence reached
skip banned sentences
set previous and next sentence for context
append parsed sentence to list in memory
option 2: keep source data in memory
"read lines for next sentence, but don't parse"
quit if last sentence reached
append raw lines for each sentence
we make a distinction between word-level tags and span-level tags
read first sentence to determine which columns are span-labels
skip first line if to selected
check the first 5 sentences
go through all annotations and identify word- and span-level annotations
- if a column has at least one BIES we know it's a Span label
"- if a column has at least one tag that is not BIOES, we know it's a Token label"
- problem cases are columns for which we see only O - in this case we default to Span
skip assigned columns
the space after key is always word-levels
"if at least one token has a BIES, we know it's a span label"
"if at least one token has a label other than BIOES, we know it's a token label"
all remaining columns that are not word-level are span-level
for column in self.word_level_tag_columns:
"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")"
"if sentence ends, break"
parse comments if possible
"otherwise, this line is a token. parse and add to sentence"
check if this sentence is a document boundary
add span labels
discard tags from tokens that are not added to the sentence
parse relations if they are set
head and tail span indices are 1-indexed and end index is inclusive
parse comments such as '# id cd27886d-6895-4d02-a8df-e5fa763fa88f	domain=de-orcas'
"to set the metadata ""domain"" to ""de-orcas"""
get fields from line
get head_id if exists (only in dependency parses)
initialize token
go through all columns
'feats' and 'misc' column should be split into different fields
special handling for whitespace after
add each other feature as label-value pair
get the task name (e.g. 'ner')
get the label value
add label
remap regular tag names
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
set sentence context using partials TODO: pointer to dataset is really inefficient
use all domains
iter over all domains / sources and create target files
Parameters
The conll representation of coref spans allows spans to
"overlap. If spans end or begin at the same word, they are"
"separated by a ""|""."
The span begins at this word.
The span begins and ends at this word (single word span).
"The span is starting, so we record the index of the word."
"The span for this id is ending, but didn't start at this word."
Retrieve the start index from the document state and
add the span to the clusters for this id.
Parameters
strip all bracketing information to
get the actual propbank label.
Entering into a span for a particular semantic role label.
We append the label and set the current span for this annotation.
"If there's no '(' token, but the current_span_label is not None,"
then we are inside a span.
We're outside a span.
"Exiting a span, so we reset the current span label for this annotation."
The words in the sentence.
The pos tags of the words in the sentence.
the pieces of the parse tree.
The lemmatised form of the words in the sentence which
have SRL or word sense information.
The FrameNet ID of the predicate.
"The sense of the word, if available."
"The current speaker, if available."
"Cluster id -> List of (start_index, end_index) spans."
Cluster id -> List of start_indices which are open for this id.
Replace brackets in text and pos tags
with a different token for parse trees.
only keep ')' if there are nested brackets with nothing in them.
There are some bad annotations in the CONLL data.
"They contain no information, so to make this explicit,"
we just set the parse piece to be None which will result
in the overall parse tree being None.
"If this is the first word in the sentence, create"
empty lists to collect the NER and SRL BIO labels.
"We can't do this upfront, because we don't know how many"
"components we are collecting, as a sentence can have"
variable numbers of SRL frames.
Create variables representing the current label for each label
sequence we are collecting.
"If any annotation marks this word as a verb predicate,"
we need to record its index. This also has the side effect
of ordering the verbal predicates by their location in the
"sentence, automatically aligning them with the annotations."
"this would not be reached if parse_pieces contained None, hence the cast"
Non-empty line. Collect the annotation.
Collect any stragglers or files which might not
have the '#end document' format for the end of the file.
this dataset name
check if data there
column format
this dataset name
check if data there
column format
this dataset name
download data if necessary
download files if not present locally
we need to slightly modify the original files by adding some new lines after document separators
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
Remove CoNLL-U meta information in the last column
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
entity_mapping
this dataset name
download data if necessary
data validation
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
download files if not present locallys
column format
this dataset name
default dataset folder is the cache root
# download zip
merge the files in one as the zip is containing multiples files
column format
this dataset name
download data if necessary
"unzip the downloaded repo and merge the train, dev and test datasets"
column format
this dataset name
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
check if data there
create folder
download dataset
column format
this dataset name
download and parse data if necessary
create train test dev if not exist
column format
this dataset name
If the extracted corpus file is not yet present in dir
download zip if necessary
"extracted corpus is not present , so unpacking it."
column format
this dataset name
download zip
unpacking the zip
merge the files in one as the zip is containing multiples files
column format
this dataset name
"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)"
download files if not present locally
we need to modify the original files by adding new lines after after the end of each sentence
if only one language is given
column format
this dataset name
"use all languages if explicitly set to ""all"""
download data if necessary
initialize comlumncorpus and add it to list
column format
this dataset name
column format
this dataset name
column format
this dataset name
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
this list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
get google drive id from list
download from google drive
unzip
"tar.extractall(language_folder,members=[tar.getmember(file_name)])"
transform data into required format
"the processed dataset has the additional ending ""_new"""
remove the unprocessed dataset
initialize comlumncorpus and add it to list
if no languages are given as argument all languages used in XTREME will be loaded
if only one language is given
column format
this dataset name
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
This list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
download from HU Server
unzip
transform data into required format
initialize comlumncorpus and add it to list
if only one language is given
column format
this dataset name
download data if necessary
initialize comlumncorpus and add it to list
download data if necessary
unpack and write out in CoNLL column-like format
column format
this dataset name
download data if necessary
data is not in IOB2 format. Thus we transform it to IOB2
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
column format
this dataset name
rename according to train - test - dev - convention
column format
this dataset name
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
Add missing newline after header
Workaround for empty tokens
"Add ""real"" document marker"
Dataset split mapping
v2.0 only adds new language and splits for AJMC dataset
Special document marker for sample splits in AJMC dataset
column format
this dataset name
download data if necessary
column format
this dataset name
this dataset name
default dataset folder is the cache root
download and parse data if necessary
paths to train and test splits
init corpus
this dataset name
default dataset folder is the cache root
download and parse data if necessary
iterate over all html files
"get rid of html syntax, we only need the text"
between all documents we write a separator symbol
skip empty strings
"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)"
"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention"
sentence splitting and tokenization
iterate through all annotations and add to corresponding tokens
find sentence to which annotation belongs
position within corresponding sentence
set annotation for tokens of entity mention
write to out-file in column format
"in case something goes wrong, delete the dataset and raise error"
this dataset name
download and parse data if necessary
from qwikidata.linked_data_interface import get_entity_dict_from_api
generate qid wikiname dictionaries
merge dictionaries
ignore first line
commented and empty lines
read all Q-IDs
ignore first line
request
this dataset name
we use the wikiids in the data instead of directly utilizing the wikipedia urls.
like this we can quickly check if the corresponding page exists
if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi
delete unprocessed file
collect all wikiids
create the dictionary
request
this dataset name
names of raw text documents
open output_file
iterate through all documents
split sentences and tokenize
iterate through all annotations and add to corresponding tokens
find sentence to which annotation belongs
position within corresponding sentence
set annotation for tokens of entity mention
write to out file
this dataset name
download and parse data if necessary
this dataset name
download and parse data if necessary
First parse the post titles
Keep track of how many and which entity mentions does a given post title have
Check if the current post title has an entity link and parse accordingly
Post titles with entity mentions (if any) are handled via this function
Then parse the comments
"Iterate over the comments.tsv file, until the end is reached"
"Keep track of the current comment thread and its corresponding key, on which the annotations are matched."
Each comment thread is handled as one 'document'.
Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.
This if-condition is needed to handle this problem.
"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure"
"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above"
"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle."
"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,"
and not just single letters into single rows.
If there are annotated entity mentions for given post title or a comment thread
"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence"
Write the token with a corresponding tag to file
"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed"
"If a comment thread or a post title has no entity link, all tokens are assigned the O tag"
Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized
"incorrectly, in order to keep the desired format (empty line as a sentence separator)."
"Thrown when the second check above happens, but the last token of a sentence is reached."
"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below."
"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS"
Check if further annotations belong to the current post title or comment thread as well
Stop when the end of an annotation file is reached
Check if further annotations belong to the current sentence as well
"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)"
Docstart
if there is more than one word in the chunk we write each in a separate line
print(chunks)
empty line after each sentence
convert the file to CoNLL
this dataset name
"check if data there, if not, download the data"
create folder
download data
transform data into column format if necessary
if no filenames are specified we use all the data
"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled"
also we remove 'raganato_ALL' from filenames in case its in the list
generate the test file
make column file and save to data_folder
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
this dataset name
download data if necessary
if True:
write CoNLL-U Plus header
"Some special cases (e.g., missing spaces before entity marker)"
necessary if text should be whitespace tokenizeable
Handle case where tail may occur before the head
this dataset name
write CoNLL-U Plus header
this dataset name
TODO: change data source to original CoNLL04 -- this dataset has span formatting errors
download data if necessary
write CoNLL-U Plus header
The span has ended.
We are entering a new span; reset indices
and active tag to new span.
We're inside a span.
Last token might have been a part of a valid span.
this dataset name
write CoNLL-U Plus header
"for source_file_path, target_filename in zip(source_file_paths, target_filenames):"
"with zip_file.open(source_file_path, mode=""r"") as source_file:"
target_file_path = Path(data_folder) / target_filename
"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:"
# write CoNLL-U Plus header
"target_file.write(""# global.columns = id form ner\n"")"
for example in json.load(source_file):
token_list = self._tacred_example_to_token_list(example)
target_file.write(token_list.serialize())
check if first tag row is already occupied
"if first tag row is occupied, use second tag row"
hardcoded mapping TODO: perhaps find nicer solution
remap regular tag names
else skip to position in file where sentence begins
set sentence context using partials TODO: pointer to dataset is really inefficient
read in dev file if exists
read in test file if exists
the url is copied from https://huggingface.co/datasets/darentang/sroie/blob/main/sroie.py#L44
"find train, dev and test files if not specified"
use test_file to create test split if available
use dev_file to create test split if available
"if data point contains black-listed label, do not use"
first check if valid sentence
"if so, add to indices"
"find train, dev and test files if not specified"
variables
different handling of in_memory data than streaming data
"most data sets have the token text in the first column, if not, pass 'text' as column"
test if format is OK
test if at least one label given
make sentence from text (and filter for length)
"if a pair column is defined, make a sentence pair object"
noinspection PyDefaultArgument
dataset name includes the split size
default dataset folder is the cache root
download data if necessary
download each of the 28 splits
create dataset directory if necessary
download senteval datasets if necessary und unzip
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
handle labels file
handle data file
Create flair compatible labels
"by defaut, map point score to POSITIVE / NEGATIVE values"
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file from CSV
create test.txt file from CSV
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create train dev and test files in fasttext format
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
convert to FastText format
download data if necessary
"if data is not downloaded yet, download it"
get the zip file
move original .tsv files to another folder
create train and dev splits in fasttext format
create eval_dataset file with no labels
this dataset name
default dataset folder is the cache root
download data if necessary
download datasets if necessary
create dataset directory if necessary
create correctly formated txt files
multiple labels are possible
this dataset name
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
download data if necessary
this dataset name
download data if necessary
create a separate directory for different tasks
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
check if dataset is supported
set file names
set file names
download and unzip in file structure if necessary
instantiate corpus
"find train, dev and test files if not specified"
"create DataPairDataset for train, test and dev file, if they are given"
stop if file does not exist
create a DataPair object from strings
"if in_memory is True we return a datapair, otherwise we create one from the lists of strings"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"reorder dev datasets to have same columns as in train set: 8, 9, and 11"
dev sets include 5 different annotations but we will only keep the gold label
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get test and dev sets
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data not downloaded yet, download it"
get the zip file
"the downloaded files have json format, we transform them to tsv"
Function to transform JSON file to tsv for Recognizing Textual Entailment Data
remove json file
Uses dynamic programming approach to calculate maximum independent set in interval graph
with sum of all entity lengths as secondary key
calculate offset without current text
because we stick all passages of a document together
TODO For split entities we also annotate everything inbetween which might be a bad idea?
Try to fix incorrect annotations
print(
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
)
Ignore empty lines or relation annotations
FIX annotation of whitespaces (necessary for PDR)
One token may contain multiple entities -> deque all of them
column format
this dataset name
Create tokenization-dependent CONLL files. This is necessary to prevent
from caching issues (e.g. loading the same corpus with different sentence splitters)
column format
this dataset name
column format
this dataset name
Edge case: last token starts a new entity
Last document in file
column format
this dataset name
column format
this dataset name
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
Edge case: last token starts a new entity
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
column format
this dataset name
Read texts
Read annotations
column format
this dataset name
column format
this dataset name
column format
this dataset name
We need to apply a patch to correct the original training file
Articles title
Article abstract
Entity annotations
column format
this dataset name
Edge case: last token starts a new entity
Map all entities to chemicals
Map all entities to disease
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
Incomplete article
Invalid XML syntax
column format
this dataset name
column format
this dataset name
if len(mid) != 3:
continue
Try to fix entity offsets
column format
this dataset name
There is still one illegal annotation in the file ..
column format
this dataset name
"Abstract first, title second to prevent issues with sentence splitting"
column format
this dataset name
column format
this dataset name
column format
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
column format
this dataset name
"Filter for specific entity types, by default no entities will be filtered"
Get original HUNER splits to retrieve a list of all document ids contained in V2
train and dev split of V2 will be train in V4
test split of V2 will be dev in V4
New documents in V4 will become test documents
column format
this dataset name
column format
this dataset name
default dataset folder is the cache root
cache Feidegger config file
cache Feidegger images
replace image URL with local cached file
append Sentence-Image data point
"in certain cases, multi-CPU data loading makes no sense and slows"
"everything down. For this reason, we detect if a dataset is in-memory:"
"if so, num_workers is set to 0 for faster processing"
cast to list if necessary
cast to list if necessary
"first, check if pymongo is installed"
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
Expose base classses
Expose all biomedical data sets used for the evaluation of BioBERT
-
-
-
-
Expose all biomedical data sets using the HUNER splits
Expose all biomedical data sets
Expose all document classification datasets
word sense disambiguation
Expose all entity linking datasets
Expose all relation extraction datasets
universal proposition banks
keyphrase detection datasets
other NER datasets
standard NER datasets
Expose all sequence labeling datasets
Expose all text-image datasets
Expose all text-text datasets
Expose all treebanks
"find train, dev and test files if not specified"
get train data
get test data
get dev data
option 1: read only sentence boundaries as offset positions
option 2: keep everything in memory
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
current token ID
handling for the awful UD multiword format
end of sentence
comments
ellipsis
if token is a multi-word
normal single-word tokens
"if we don't split multiwords, skip over component words"
add token
add morphological tags
derive whitespace logic for multiwords
print(token)
print(current_multiword_last_token)
print(current_multiword_first_token)
"if multi-word equals component tokens, there should be no whitespace"
go through all tokens in subword and set whitespace_after information
print(i)
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
this dataset name
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
"finally, print model card for information"
test corpus
create a TARS classifier
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
test corpus
create a TARS classifier
switch to a new task (TARS can do multiple tasks so you must define one)
initialize the text classifier trainer
start the training
"With end symbol, without start symbol, padding in front"
"Without end symbol, with start symbol, padding in back"
"Without end symbol, without start symbol, padding in front"
initialize trainer
initialize trainer
train model for 2 epochs
load the checkpoint model and train until epoch 4
clean up results directory
initialize trainer
initialize trainer
increment for last token in sentence if not followed by whitespace
clean up directory
clean up directory
example sentence
set 4 labels for 2 tokens ('love' is tagged twice)
check if there are three POS labels with correct text and values
check if there are is one SENTIMENT label with correct text and values
check if all tokens are correctly labeled
remove the pos label from the last word
there should be 2 POS labels left
now remove all pos tags
set 3 labels for 2 spans (HU is tagged twice)
check if there are three labels with correct text and values
check if there are two spans with correct text and values
"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
should be only one NER label left
and only one NER span
set 3 labels for 2 spans (HU is tagged twice with different tags)
check if there are three labels with correct text and values
check if there are two spans with correct text and values
"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
should be only one NER label left
and only one NER span
but there is also one orgtype span and label
and only one NER span
let's add the NER tag back
check if there are three labels with correct text and values
check if there are two spans with correct text and values
now remove all NER tags
set 3 labels for 2 spans (HU is tagged twice with different tags)
create two relation label
there should be two relation labels
there should be one syntactic labels
"there should be two relations, one with two and one with one label"
example sentence
add another topic label
example sentence
has sentiment value
has 4 part of speech tags
has 1 NER tag
should be in total 6 labels
example sentence
add two NER labels
get the four labels
check that only two of the respective data points are equal
make a sentence and some right context
TODO: is this desirable? Or should two sentences with same text be considered same objects?
Initializing a Sentence this way assumes that there is a space after each token
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
define search space
sequence tagger parameter
model trainer parameter
training parameter
find best parameter settings
clean up results directory
document embeddings parameter
training parameter
clean up results directory
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
load column dataset with one entry
load column dataset with two entries
load column dataset with three entries
"get training, test and dev data"
"get training, test and dev data"
check if Token labels are correct
"get training, test and dev data"
check if Token labels for frames are correct
"get training, test and dev data"
"get training, test and dev data"
get two corpora as one
"get training, test and dev data for full English UD corpus from web"
clean up data directory
"assert [token.get_tag(""head"").value for token in sent1.tokens] == ["
"""2"","
"""0"","
"""4"","
"""2"","
"""2"","
"""2"","
]
"Here, we use the default token annotation fields."
"We have manually checked, that these numbers are correct:"
"+1 offset, because of missing EOS marker at EOD"
Test data for v2.1 release
--- Embeddings that are shared by both models --- #
--- Task 1: Sentiment Analysis (5-class) --- #
Define corpus and model
-- Task 2: Binary Sentiment Analysis on Customer Reviews -- #
Define corpus and model
-- Define mapping (which tagger should train on which model) -- #
-- Create model trainer and train -- #
clean up file
no need for label_dict
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
Intel ----founded_by---> Gordon Moore
Intel ----founded_by---> Robert Noyce
Check sentence masking and relation label annotation on
"training, validation and test dataset (in this test the splits are the same)"
"Entity pair permutations of: ""Larry Page and Sergey Brin founded Google ."""
"Entity pair permutations of: ""Microsoft was founded by Bill Gates ."""
"Entity pair permutations of: ""Konrad Zuse was born in Berlin on 22 June 1910 ."""
"Entity pair permutations of: ""Joseph Weizenbaum , a professor at MIT , was born in Berlin , Germany."""
This sentence is only included if we transform the corpus with cross augmentation
Ensure this is an example that predicts no classes in multilabel
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
ensure that the prepared tensors is what we expect
use a SequenceTagger to save and reload the embedding in the manner it is supposed to work
previous and next sentence as context
test expansion for sentence without context
test expansion for with previous and next as context
test expansion if first sentence is document boundary
test expansion if we don't use context
"apparently the precision is not that high on cuda, hence the absolute tolerance needs to be higher."
dummy model with embeddings
save the dummy and load it again
check that context_length and use_context_separator is the same for both
mmap seems to be much more memory efficient
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
Extract all the contents of zip file in current directory
Extract all the contents of zip file in current directory
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
load_big_file is a workaround byhttps://github.com/highway11git
to load models on some Mac/Windows setups
see https://github.com/zalandoresearch/flair/issues/351
first determine the distribution of classes in the dataset
weight for each sample
Create blocks
shuffle the blocks
concatenate the shuffled blocks
Create blocks
shuffle the blocks
concatenate the shuffled blocks
increment for last token in sentence if not followed by whitespace
this is the default init size of a lmdb database for embeddings
get db filename from embedding name
"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
SequenceTagger
TextClassifier
get db filename from embedding name
if embedding database already exists
"otherwise, push embedding to database"
if embedding database already exists
open the database in read mode
we need to set self.k
create and load the database in write mode
"no idea why, but we need to close and reopen the environment to avoid"
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
when opening new transaction !
init dictionaries
"in order to deal with unknown tokens, add <unk>"
set 'add_unk' if the dictionary was created with a version of Flair older than 0.9
set 'add_unk' depending on whether <unk> is a key
"if one embedding name, directly return it"
"if multiple embedding names, concatenate them"
TODO: does it make sense to exclude labels? Two data points of identical text (but different labels)
would be equal now.
First we remove any existing labels for this PartOfSentence in self.sentence
labels also need to be deleted at Sentence object
delete labels at object itself
private field for all known spans
the tokenizer used for this sentence
some sentences represent a document boundary (but most do not)
internal variables to denote position inside dataset
"if text is passed, instantiate sentence with tokens (words)"
determine token positions and whitespace_after flag
the last token has no whitespace after
log a warning if the dataset is empty
data with zero-width characters cannot be handled
set token idx and sentence
append token to sentence
register token annotations on sentence
move sentence embeddings to device
also move token embeddings to device
clear token embeddings
infer whitespace after field
"if sentence has no tokens, return empty string"
"otherwise, return concatenation of tokens with the correct offsets"
The sentence's start position is not propagated to its tokens.
"Therefore, we need to add the sentence's start position to its last token's end position, including whitespaces."
No character at the corresponding code point: remove it
"if no label if specified, return all labels"
"if the label type exists in the Sentence, return it"
return empty list if none of the above
labels also need to be deleted at all tokens
labels also need to be deleted at all known spans
remove spans without labels
delete labels at object itself
set name
abort if no data is provided
sample test data from train if none is provided
sample dev data from train if none is provided
set train dev and test data
find out empty sentence indices
create subset of non-empty sentence indices
find out empty sentence indices
create subset of non-empty sentence indices
count all label types per sentence
go through all labels of label_type and count values
check if there are any span labels
"if an unk threshold is set, UNK all label values below this threshold"
sample randomly from a label distribution according to the probabilities defined by the desired noise share
replace the old label with the new one
keep track of the old (clean) label using another label type category
keep track of how many labels in total are flipped
Make the tag dictionary
"add a dummy ""O"" to close final prediction"
return complex list
internal variables
non-set tags are OUT tags
anything that is not OUT is IN
does this prediction start a new span?
begin and single tags start new spans
"in IOB format, an I tag starts a span if it follows an O or is a different span"
single tags that change prediction start new spans
if an existing span is ended (either by reaching O or starting a new span)
determine score and value
append to result list
reset for-loop variables for new span
remember previous tag
global variable: cache_root
global variable: device
global variable: version
global variable: arrow symbol
dummy return to fulfill trainer.train() needs
print(vec)
Attach optimizer
"convert `metrics` to float, in case it's a zero-dim Tensor"
if memory mode option 'none' delete everything
"if dynamic embedding keys not passed, identify them automatically"
always delete dynamic embeddings
"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)"
optional metric space decoder if prototypes have different length than embedding
create initial prototypes for all classes (all initial prototypes are a vector of all 1s)
"if set, create initial prototypes from normal distribution"
"if set, use a radius"
all parameters will be pushed internally to the specified device
decode embeddings into prototype space
"if unlabeled distance is set, mask out loss to unlabeled class prototype"
Always include the name of the Model class for which the state dict holds
"in Flair <0.9.1, optimizer and scheduler used to train model are not saved"
"write out a ""model card"" if one is set"
special handling for optimizer:
remember optimizer class and state dictionary
save model
restore optimizer and scheduler to model card if set
"if this class is abstract, go through all inheriting classes and try to fetch and load the model"
get all non-abstract subclasses
"try to fetch the model for each subclass. if fetching is possible, load model and return it"
"skip any invalid loadings, e.g. not found on huggingface hub"
"if the model cannot be fetched, load as a file"
try to get model class from state
"older (flair 11.3 and below) models do not contain cls information. In this case, try all subclasses"
"if str(model_cls) == ""<class 'flair.models.pairwise_classification_model.TextPairClassifier'>"": continue"
"skip any invalid loadings, e.g. not found on huggingface hub"
"if this class is not abstract, fetch the model and load it"
"make sure <unk> is contained in gold_label_dictionary, if given"
"read Dataset into data loader, if list of sentences passed, make Dataset first"
loss calculation
variables for printing
variables for computing scores
remove any previously predicted labels
predict for batch
get the gold labels
add to all_predicted_values
make printout lines
convert true and predicted values to two span-aligned lists
delete exluded labels if exclude_labels is given
"if after excluding labels, no label is left, ignore the datapoint"
write all_predicted_values to out_file if set
make the evaluation dictionary
check if this is a multi-label problem
compute numbers by formatting true and predicted such that Scikit-Learn can use them
multi-label problems require a multi-hot vector for each true and predicted label
single-label problems can do with a single index for each true and predicted label
"now, calculate evaluation numbers"
there is at least one gold label or one prediction (default)
"if there is only one label, then ""micro avg"" = ""macro avg"""
"micro average is only computed if zero-label exists (for instance ""O"")"
if no zero-label exists (such as in POS tagging) micro average is equal to accuracy
same for the main score
issue error and default all evaluation numbers to 0.
line for log file
check if there is a label mismatch
print info
set the embeddings
initialize the label dictionary
initialize the decoder
set up multi-label logic
init dropouts
loss weights and loss function
Initialize the weight tensor
set up gradient reversal if so specified
embed sentences
get a tensor of data points
do dropout
make a forward pass to produce embedded data points and labels
get the data points for which to predict labels
get their gold labels as a tensor
pass data points through network to get encoded data point tensor
decode
an optional masking step (no masking in most cases)
calculate the loss
filter empty sentences
reverse sort all sequences by their length
progress bar for verbosity
filter data points in batch
stop if all sentences are empty
pass data points through network and decode
if anything could possibly be predicted
remove previously predicted labels of this type
add DefaultClassifier arguments
add variables of DefaultClassifier
Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23
Get projected 1st dimension
Compute bilinear form
Arcosh
Project the input data to n+1 dimensions
"The first dimension, is recomputed in the distance module"
header for 'weights.txt'
"determine the column index of loss, f-score and accuracy for"
"train, dev and test split"
then get all relevant values from the tsv
then get all relevant values from the tsv
plot i
save plots
save plots
plt.show()
save plot
take the average over the last three scores of training
take average over the scores from the different training runs
auto-spawn on GPU if available
progress bar for verbosity
stop if all sentences are empty
clearing token embeddings to save memory
"read Dataset into data loader, if list of sentences passed, make Dataset first"
TODO: not saving lines yet
TODO: This closely shadows the RelationExtractor name. Maybe we need a better name here.
- MaskedRelationClassifier ?
This depends if this relation classification architecture should replace or offer as an alternative.
Set label type and prepare label dictionary
Initialize super default classifier
Add the special tokens from the encoding strategy
"Auto-spawn on GPU, if available"
Only use entities labelled with the specified labels for each label type
Only use entities above the specified threshold
Use a dictionary to find gold relation annotations for a given entity pair
Yield head and tail entity pairs from the cross product of all entities
Remove identity relation entity pairs
Remove entity pairs with labels that do not match any
of the specified relations in `self.entity_pair_labels`
"Obtain gold label, if existing"
Some sanity checks
Pre-compute non-leading head and tail tokens for entity masking
We can not use the plaintext of the head/tail span in the sentence as the mask/marker
since there may be multiple occurrences of the same entity mentioned in the sentence.
"Therefore, we use the span's position in the sentence."
Create masked sentence
Add gold relation annotation as sentence label
"Using the sentence label instead of annotating a separate `Relation` object is easier to manage since,"
"during prediction, the forward pass does not need any knowledge about the entities in the sentence."
"If we sample missing splits, the encoded sentences that correspond to the same original sentences"
"may get distributed into different splits. For training purposes, this is always undesired."
Ensure that all sentences are encoded properly
Deal with the case where all sentences are encoded sentences
"mypy does not infer the type of ""sentences"" restricted by the if statement"
Deal with the case where all sentences are standard (non-encoded) sentences
"For each encoded sentence, transfer its prediction onto the original relation"
auto-spawn on GPU if available
pad strings with whitespaces to longest sentence
cut up the input into chunks of max charlength = chunk_size
push each chunk through the RNN language model
concatenate all chunks to make final output
initial hidden state
get predicted weights
divide by temperature
"to prevent overflow problem with small temperature values, substract largest value from all"
this makes a vector in which the largest value is 0
compute word weights with exponential function
try sampling multinomial distribution for next character
print(word_idx)
input ids
push list of character IDs through model
the target is always the next character
use cross entropy loss to compare output of forward pass with targets
exponentiate cross-entropy loss to calculate perplexity
"""document_delimiter"" property may be missing in some older pre-trained models"
serialize the language models and the constructor arguments (but nothing else)
special handling for deserializing language models
re-initialize language model with constructor arguments
copy over state dictionary to self
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
"in their ""self.train()"" method)"
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
Transform input data into TARS format
"if there are no labels, return a random sample as negatives"
"otherwise, go through all labels"
make sure the probabilities always sum up to 1
get and embed all labels by making a Sentence object that contains only the label text
get each label embedding and scale between 0 and 1
compute similarity matrix
"the higher the similarity, the greater the chance that a label is"
sampled as negative example
make label dictionary if no Dictionary object is passed
prepare dictionary of tags (without B- I- prefixes and without UNK)
check if candidate_label_set is empty
make list if only one candidate label is passed
create label dictionary
note current task
create a temporary task
make zero shot predictions
switch to the pre-existing task
prepare TARS dictionary
initialize a bare-bones sequence tagger
transformer separator
Store task specific labels since TARS can handle multiple tasks
make a tars sentence where all labels are O by default
init new TARS classifier
set all task information
return
with torch.no_grad():
progress bar for verbosity
stop if all sentences are empty
go through each sentence in the batch
always remove tags first
get the span and its label
determine whether tokens in this span already have a label
only add if all tokens have no label
make and add a corresponding predicted span
set indices so that no token can be tagged twice
clearing token embeddings to save memory
"all labels default to ""O"""
set gold token-level
set predicted token-level
now print labels in CoNLL format
prepare TARS dictionary
initialize a bare-bones sequence tagger
transformer separator
Store task specific labels since TARS can handle multiple tasks
get the serialized embeddings
remap state dict for models serialized with Flair <= 0.11.3
init new TARS classifier
set all task information
with torch.no_grad():
progress bar for verbosity
stop if all sentences are empty
go through each sentence in the batch
always remove tags first
add all labels that according to TARS match the text and are above threshold
do not add labels below confidence threshold
only use label with highest confidence if enforcing single-label predictions
get all label scores and do an argmax to get the best label
remove previously added labels and only add the best label
clearing token embeddings to save memory
set separator to concatenate two sentences
auto-spawn on GPU if available
pooling operation to get embeddings for entites
set embeddings
set relation and entity label types
"whether to use gold entity pairs, and whether to filter entity pairs by type"
filter entity pairs according to their tags if set
whether to encode characters and whether to use attention (attention can only be used if chars are encoded)
character dictionary for decoding and encoding
make sure <unk> is in dictionary for handling of unknown characters
add special symbols to dictionary if necessary and save respective indices
---- ENCODER ----
encoder character embeddings
encoder pre-trained embeddings
encoder RNN
additional encoder linear layer if bidirectional encoding
---- DECODER ----
decoder: linear layers to transform vectors to and from alphabet_size
when using attention we concatenate attention outcome and decoder hidden states
decoder RNN
loss and softmax
self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction
add additional columns for special symbols if necessary
initialize with dummy symbols
encode inputs
get labels (we assume each token has a lemma label)
get char indices for labels of sentence
"(batch_size, max_sequence_length) batch_size = #words in sentence,"
max_sequence_length = length of longest label of sentence + 1
get char embeddings
"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size"
take decoder input and initial hidden and pass through RNN
"if all encoder outputs are provided, use attention"
take convex combinations of encoder hidden states as new output using the computed attention coefficients
"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)"
get all tokens
encode input characters by sending them through RNN
get one-hots for characters and add special symbols / padding
determine length of each token
embed sentences
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
variable to store initial hidden states for decoder
encode input characters by sending them through RNN
test packing and padding
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
concatenate the final hidden states of the encoder. These will be projected to hidden_size of
decoder later with self.emb_to_hidden
mask out vectors that correspond to a dummy symbol (TODO: check attention masking)
use token embedding as initial hidden state for decoder
concatenate everything together and project to appropriate size for decoder
variable to store initial hidden states for decoder
encode input characters by sending them through RNN
note that we do not need to fill up with dummy symbols since we process each token seperately
embed character one-hots
send through encoder RNN (produces initial hidden for decoder)
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
project 2*hidden_size to hidden_size
concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder
later with self.emb_to_hidden
use token embedding as initial hidden state for decoder
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
concatenate everything together and project to appropriate size for decoder
"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)"
"create target vector (batch_size, max_label_seq_length + 1)"
filter empty sentences
max length of the predicted sequences
for printing
stop if all sentences are empty
remove previously predicted labels of this type
create list of tokens in batch
encode inputs
"create input for first pass (batch_size, 1, input_size), first letter is special character <S>"
sequence length is always set to one in prediction
option 1: greedy decoding
predictions
decode next character
pick top beam size many outputs with highest probabilities
option 2: beam search
out_probs = self.softmax(output_vectors).squeeze(1)
make sure no dummy symbol <> or start symbol <S> is predicted
pick top beam size many outputs with highest probabilities
"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1"
"leading_indices and probabilities have size (batch_size, beam_size)"
keep scores of beam_size many hypothesis for each token in the batch
stack all leading indices of all hypothesis and corresponding hidden states in two tensors
save sequences so far
keep track of how many hypothesis were completed for each token
"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)"
decode with log softmax
make sure no dummy symbol <> or start symbol <S> is predicted
"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside"
"if the sequence is already ended, do not record as candidate"
index of token in in list tokens_in_batch
print(token_number)
hypothesis score
TODO: remove token if number of completed hypothesis exceeds given value
set score of corresponding entry to -inf so it will not be expanded
get leading_indices for next expansion
find highest scoring hypothesis among beam_size*beam_size possible ones for each token
take beam_size many copies of scores vector and add scores of possible new extensions
"size (beam_size*batch_size, beam_size)"
print(hypothesis_scores)
"reshape to vector of size (batch_size, beam_size*beam_size),"
each row contains beam_size*beam_size scores of the new possible hypothesis
print(hypothesis_scores_per_token)
"choose beam_size best for each token - size (batch_size, beam_size)"
out of indices_per_token we now need to recompute the original indices of the hypothesis in
a list of length beam_size*batch_size
"where the first three inidices belong to the first token, the next three to the second token,"
and so on
with these indices we can compute the tensors for the next iteration
expand sequences with corresponding index
add log-probabilities to the scores
save new leading indices
save corresponding hidden states
it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations
in that case we append one of the final seuqences without end symbol to the final_candidates
get best final hypothesis for each token
get characters from index sequences and add predicted label to token
dictionaries
all parameters will be pushed internally to the specified device
now print labels in CoNLL format
internal candidate lists of generator
load Zelda candidates if so passed
create candidate lists
"if lower casing is enabled, create candidate lists of lower cased versions"
create a new dictionary for lower cased mentions
go through each mention and its candidates
"check if backoff mention already seen. If so, add candidates. Else, create new entry."
set lowercased version as map
remap state dict for models serialized with Flair <= 0.11.3
get the candidates
"during training, add the gold value as candidate"
----- Create the internal tag dictionary -----
span-labels need special encoding (BIO or BIOES)
the big question is whether the label dictionary should contain an UNK or not
"without UNK, we cannot evaluate on data that contains labels not seen in test"
"with UNK, the model learns less well if there are no UNK examples"
is this a span prediction problem?
----- Embeddings -----
----- Initial loss weights parameters -----
----- RNN specific parameters -----
----- Conditional Random Field parameters -----
"Previously trained models have been trained without an explicit CRF, thus it is required to check"
whether we are loading a model from state dict in order to skip or add START and STOP token
----- Dropout parameters -----
dropouts
remove word dropout if there is no contact over the sequence dimension.
----- Model layers -----
----- RNN layer -----
"If shared RNN provided, else create one for model"
Whether to train initial hidden state
final linear map to tag space
"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss"
"if using CRF, we also require a CRF and a Viterbi decoder"
"if there are no sentences, there is no loss"
forward pass to get scores
calculate loss given scores and labels
make a zero-padded tensor for the whole sentence
linear map to tag space
"Depending on whether we are using CRF or a linear layer, scores is either:"
"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF"
"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer"
spans need to be encoded as token-level predictions
all others are regular labels for each token
make sure it's a list
filter empty sentences
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
get features from forward propagation
remove previously predicted labels of this type
"if return_loss, get loss value"
make predictions
add predictions to Sentence
BIOES-labels need to be converted to spans
"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)"
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
core Flair models on Huggingface ModelHub
"Large NER models,"
Multilingual NER models
English POS models
Multilingual POS models
English SRL models
English chunking models
Language-specific NER models
Language-specific POS models
English NER models
Multilingual NER models
English POS models
Multilingual POS models
English SRL models
English chunking models
Danish models
German models
French models
Dutch models
Malayalam models
Portuguese models
Keyphase models
Biomedical models
check if model name is a valid local file
"check if model key is remapped to HF key - if so, print out information"
get mapped name
use mapped name instead
"if not, check if model key is remapped to direct download location. If so, download model"
special handling for the taggers by the @redewiegergabe project (TODO: move to model hub)
"for all other cases (not local file or special download location), use HF model hub"
"if not a local file, get from model hub"
use model name as subfolder
Lazy import
output information
## Demo: How to use in Flair
load tagger
make example sentence
predict NER tags
print sentence
print predicted NER spans
iterate over entities and print
Lazy import
Save model weight
Determine if model card already exists
Generate and save model card
Upload files
"all labels default to ""O"""
set gold token-level
set predicted token-level
now print labels in CoNLL format
print labels in CoNLL format
the multi task model has several labels
biomedical models
entity linker
auto-spawn on GPU if available
remap state dict for models serialized with Flair <= 0.11.3
English sentiment models
Communicative Functions Model
"scores_at_targets[range(features.shape[0]), lengths.values -1]"
Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices
"Initially, get scores from <start> tag to all other tags"
"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp"
"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep"
Create a tensor to hold accumulated sequence scores at each current tag
Create a tensor to hold back-pointers
"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag"
"Let pads be the <end> tag index, since that was the last tag in the decoded sequence"
"We add scores at current timestep to scores accumulated up to previous timestep, and"
choose the previous timestep that corresponds to the max. accumulated score for each current timestep
"If sentence is over, add transition to STOP-tag"
Decode/trace best path backwards
Sanity check
remove start-tag and backscore to stop-tag
Max + Softmax to get confidence score for predicted label and append label to each token
"Transitions are used in the following way: transitions[to, from]."
"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag"
to START-tag and from STOP-tag to any other tag to -10000.
create a model card for this model with Flair and PyTorch version
also record Transformers version if library is loaded
remember all parameters used in train() call
add model card to model
"if optimizer class is passed, instantiate:"
"determine what splits (train, dev, test) to evaluate and log"
prepare loss logging file and set up header
"from here on, use list of learning rates"
load existing optimizer state dictionary if it exists
"minimize training loss if training with dev data, else maximize dev score"
"if scheduler is passed as a class, instantiate"
"if we load a checkpoint, we have already trained for epoch"
"Determine whether to log ""bad epochs"" information"
load existing scheduler state dictionary if it exists
update optimizer and scheduler in model card
"if training also uses dev/train data, include in training set"
initialize sampler if provided
init with default values if only class is provided
set dataset to sample from
this field stores the names of all dynamic embeddings in the model (determined after first forward pass)
At any point you can hit Ctrl + C to break out of training early.
update epoch in model card
get new learning rate
reload last best model if annealing with restarts is enabled
stop training if learning rate becomes too small
"if shuffle_first_epoch==False, the first epoch is not shuffled"
process mini-batches
zero the gradients on the model and optimizer
"if necessary, make batch_steps"
forward and backward for batch
forward pass
Backward
identify dynamic embeddings (always deleted) on first sentence
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
do the optimizer step
do the scheduler step if one-cycle or linear decay
get new learning rate
evaluate on train / dev / test split depending on training settings
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
calculate scores using dev data if available
append dev score to score history
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
determine if this is the best model or if we need to anneal
default mode: anneal against dev score
alternative: anneal against dev loss
alternative: anneal against train loss
determine bad epoch number
lr unchanged
log bad epochs
output log file
make headers on first epoch
"if checkpoint is enabled, save model at each epoch"
Check whether to save best model
"if we do not use dev data for model selection, save final model"
test best model if test data is present
recover all arguments that were used to train this model
you can overwrite params with your own
surface nested arguments
resume training with these parameters
"If set, add a factor to the learning rate of all parameters with 'embeddings' not in name"
get and return the final test score of best model
cast string to Path
forward pass
update optimizer and scheduler
"TextDataset returns a list. valid and test are only one file,"
so return the first element
cast string to Path
error message if the validation dataset is too small
Shuffle training files randomly after serially iterating
through corpus one
"iterate through training data, starting at"
self.split (for checkpointing)
off by one for printing
go into train mode
reset variables
not really sure what this does
do the forward pass in the model
try to predict the targets
Backward
`clip_grad_norm` helps prevent the exploding gradient
problem in RNNs / LSTMs.
We detach the hidden state from how it was
previously produced.
"If we didn't, the model would try backpropagating"
all the way to start of the dataset.
explicitly remove loss to clear up memory
#########################################################
Save the model if the validation loss is the best we've
seen so far.
#########################################################
print info
#########################################################
##############################################################################
final testing
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
"the default model for ELMo is the 'original' model, which is very large"
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
put on Cuda if available
embed a dummy sentence to determine embedding_length
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
"multilingual forward (English, German, French, Italian, Dutch, Polish)"
"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
Dutch forward
Dutch backward
Swedish forward
Swedish backward
French forward
French backward
Czech forward
Czech backward
Portuguese forward
Portuguese backward
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
1-camembert-base -> camembert-base
1-xlm-roberta-large -> xlm-roberta-large
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
The mask has 1 for real tokens and 0 for padding tokens. Only real
tokens are attended to.
Zero-pad up to the sequence length.
"first, find longest sentence in batch"
prepare id maps for BERT model
put encoded batch through BERT model to get all hidden states of all encoder layers
get aggregated embeddings for each BERT-subtoken in sentence
get the current sentence object
add concatenated embedding to sentence
use first subword embedding if pooling operation is 'first'
"otherwise, do a mean over all subwords in token"
"if only one sentence is passed, convert to list of sentence"
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
embed a dummy sentence to determine embedding_length
Avoid conflicts with flair's Token class
"legacy pickle-like saving for image embeddings, as implementation details are not obvious"
"legacy pickle-like loading for image embeddings, as implementation details are not obvious"
"<cls> token initially set to 1/D, so it attends to all image features equally"
add positional encodings
reshape the pixels into the sequence
layer norm after convolution and positional encodings
add <cls> token
"transformer requires input in the shape [h*w+1, b, d]"
the output is an embedding of <cls> token
this parameter is fixed
optional fine-tuning on top of embedding layer
"if only one sentence is passed, convert to list of sentence"
"if only one sentence is passed, convert to list of sentence"
bidirectional RNN on top of embedding layer
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push through RNN
after-RNN dropout
extract embeddings from RNN
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
serialize the language models and the constructor arguments (but nothing else)
re-initialize language model with constructor arguments
special handling for deserializing language models
copy over state dictionary to self
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
"in their ""self.train()"" method)"
IMPORTANT: add embeddings as torch modules
iterate over sentences
"if its a forward LM, take last state"
"convert to plain strings, embedded in a list for the encode function"
CNN
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push CNN
after-CNN dropout
extract embeddings from CNN
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
"if only one sentence is passed, convert to list of sentence"
Expose base classses
Expose document embedding classes
Expose image embedding classes
Expose legacy embedding classes
Expose token embedding classes
in some cases we need to insert zero vectors for tokens without embedding.
padding
remove special markup
check if special tokens exist to circumvent error message
iterate over subtokens and reconstruct tokens
remove special markup
check if reconstructed token is special begin token ([CLS] or similar)
some BERT tokenizers somehow omit words - in such cases skip to next token
"we cannot handle unk_tokens perfectly, so let's assume that one unk_token corresponds to one token."
if tokens are unaccounted for
check if all tokens were matched to subtokens
The layoutlm tokenizer doesn't handle ocr themselves
"transformers returns the ""added_tokens.json"" even if it doesn't create it"
"transformers returns the ""added_tokens.json"" even if it doesn't create it"
in case of doubt: token embedding has higher priority than document embedding
random check some tokens to save performance.
Models such as FNet do not have an attention_mask
set language IDs for XLM-style transformers
"word_ids is only supported for fast rust tokenizers. Some models like ""xlm-mlm-ende-1024"" do not have"
"a fast tokenizer implementation, hence we need to fall back to our own reconstruction of word_ids."
set context if not set already
flair specific pre-tokenization
"onnx prepares numpy arrays, no mather if it runs on gpu or cpu, the input is on cpu first."
temporary fix to disable tokenizer parallelism warning
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
do not print transformer warnings as these are confusing in this case
load tokenizer and transformer model
load tokenizer from inmemory zip-file
model name
embedding parameters
send mini-token through to check how many layers the model has
return length
"If we use a context separator, add a new special token"
"most models have an initial BOS token, except for XLNet, T5 and GPT2"
"when initializing, embeddings are in eval mode by default"
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
"cannot run `.encode` if ocr boxes are required, assume"
in case of doubt: token embedding has higher priority than document embedding
in case of doubt: token embedding has higher priority than document embedding
legacy TransformerDocumentEmbedding
legacy TransformerTokenEmbedding
legacy Flair <= 0.12
legacy Flair <= 0.7
legacy TransformerTokenEmbedding
Legacy TransformerDocumentEmbedding
legacy TransformerTokenEmbedding
legacy TransformerDocumentEmbedding
copy values from new embedding
cls first pooling can be done without recreating sentence hidden states
make the tuple a tensor; makes working with it easier.
"for multimodal models like layoutlmv3, we truncate the image embeddings as they are only used via attention"
only use layers that will be outputted
this parameter is fixed
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
make compatible with serialized models
gensim version 4
gensim version 3
"if no embedding is set, the vocab and embedding length is requried"
GLOVE embeddings
TURIAN embeddings
KOMNINOS embeddings
pubmed embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
twitter embeddings
two-letter language code wiki embeddings
two-letter language code wiki embeddings
two-letter language code crawl embeddings
fix serialized models
"this is required to force the module on the cpu,"
"if a parent module is put to gpu, the _apply is called to each sub_module"
self.to(..) actually sets the device properly
this ignores the get_cached_vec method when loading older versions
it is needed for compatibility reasons
gensim version 4
gensim version 3
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
multilingual models
English models
Arabic
Bulgarian
Czech
Danish
German
Spanish
Basque
Persian
Finnish
French
Hebrew
Hindi
Croatian
Indonesian
Italian
Japanese
Malayalam
Dutch
Norwegian
Polish
Portuguese
Pubmed
Slovenian
Swedish
Tamil
Spanish clinical
CLEF HIPE Shared task
Amharic
Ukrainian
load model if in pretrained model map
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
CLEF HIPE models are lowercased
embeddings are static if we don't do finetuning
embed a dummy sentence to determine embedding_length
set to eval mode
make compatible with serialized models (TODO: remove)
"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout"
make compatible with serialized models (TODO: remove)
gradients are enable if fine-tuning is enabled
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
offset mode that extracts at whitespace after last character
offset mode that extracts at last character
use the character language model embeddings as basis
length is twice the original character LM embedding length
these fields are for the embedding memory
whether to add only capitalized words to memory (faster runtime and lower memory consumption)
we re-compute embeddings dynamically at each epoch
set the memory method
memory is wiped each time we do a training run
"if we keep a pooling, it needs to be updated continuously"
update embedding
check token.text is empty or not
set aggregation operation
add embeddings after updating
model architecture
model architecture
"""pl"","
download if necessary
load the model
"TODO: keep for backwards compatibility, but remove in future"
save the sentence piece model as binary file (not as path which may change)
write out the binary sentence piece model into the expected directory
"if the model was saved as binary and it is not found on disk, write to appropriate path"
"otherwise, use normal process and potentially trigger another download"
"once the modes if there, load it with sentence piece"
empty words get no embedding
all other words get embedded
GLOVE embeddings
no need to recreate as NILCEmbeddings
read in test file if exists
read in dev file if exists
"find train, dev and test files if not specified"
Add tags for each annotated span
Remove leading and trailing whitespaces from annotated spans
Search start and end token index for current span
If end index is not found set to last token
Throw error if indices are not valid
get train data
read in test file if exists
read in dev file if exists
"find train, dev and test files if not specified"
special key for space after
special key for feature columns
special key for dependency head id
"store either Sentence objects in memory, or only file offsets"
"most data sets have the token text in the first column, if not, pass 'text' as column"
determine encoding of text file
identify which columns are spans and which are word-level
now load all sentences
skip first line if to selected
option 1: keep Sentence objects in memory
pointer to previous
parse next sentence
quit if last sentence reached
skip banned sentences
set previous and next sentence for context
append parsed sentence to list in memory
option 2: keep source data in memory
"read lines for next sentence, but don't parse"
quit if last sentence reached
append raw lines for each sentence
we make a distinction between word-level tags and span-level tags
read first sentence to determine which columns are span-labels
skip first line if to selected
check the first 5 sentences
go through all annotations and identify word- and span-level annotations
- if a column has at least one BIES we know it's a Span label
"- if a column has at least one tag that is not BIOES, we know it's a Token label"
- problem cases are columns for which we see only O - in this case we default to Span
skip assigned columns
the space after key is always word-levels
"if at least one token has a BIES, we know it's a span label"
"if at least one token has a label other than BIOES, we know it's a token label"
all remaining columns that are not word-level are span-level
for column in self.word_level_tag_columns:
"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")"
"if sentence ends, break"
parse comments if possible
"otherwise, this line is a token. parse and add to sentence"
check if this sentence is a document boundary
add span labels
discard tags from tokens that are not added to the sentence
parse relations if they are set
head and tail span indices are 1-indexed and end index is inclusive
parse comments such as '# id cd27886d-6895-4d02-a8df-e5fa763fa88f	domain=de-orcas'
"to set the metadata ""domain"" to ""de-orcas"""
get fields from line
get head_id if exists (only in dependency parses)
initialize token
go through all columns
'feats' and 'misc' column should be split into different fields
special handling for whitespace after
add each other feature as label-value pair
get the task name (e.g. 'ner')
get the label value
add label
remap regular tag names
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
set sentence context using partials TODO: pointer to dataset is really inefficient
use all domains
iter over all domains / sources and create target files
Parameters
The conll representation of coref spans allows spans to
"overlap. If spans end or begin at the same word, they are"
"separated by a ""|""."
The span begins at this word.
The span begins and ends at this word (single word span).
"The span is starting, so we record the index of the word."
"The span for this id is ending, but didn't start at this word."
Retrieve the start index from the document state and
add the span to the clusters for this id.
Parameters
strip all bracketing information to
get the actual propbank label.
Entering into a span for a particular semantic role label.
We append the label and set the current span for this annotation.
"If there's no '(' token, but the current_span_label is not None,"
then we are inside a span.
We're outside a span.
"Exiting a span, so we reset the current span label for this annotation."
The words in the sentence.
The pos tags of the words in the sentence.
the pieces of the parse tree.
The lemmatised form of the words in the sentence which
have SRL or word sense information.
The FrameNet ID of the predicate.
"The sense of the word, if available."
"The current speaker, if available."
"Cluster id -> List of (start_index, end_index) spans."
Cluster id -> List of start_indices which are open for this id.
Replace brackets in text and pos tags
with a different token for parse trees.
only keep ')' if there are nested brackets with nothing in them.
There are some bad annotations in the CONLL data.
"They contain no information, so to make this explicit,"
we just set the parse piece to be None which will result
in the overall parse tree being None.
"If this is the first word in the sentence, create"
empty lists to collect the NER and SRL BIO labels.
"We can't do this upfront, because we don't know how many"
"components we are collecting, as a sentence can have"
variable numbers of SRL frames.
Create variables representing the current label for each label
sequence we are collecting.
"If any annotation marks this word as a verb predicate,"
we need to record its index. This also has the side effect
of ordering the verbal predicates by their location in the
"sentence, automatically aligning them with the annotations."
"this would not be reached if parse_pieces contained None, hence the cast"
Non-empty line. Collect the annotation.
Collect any stragglers or files which might not
have the '#end document' format for the end of the file.
this dataset name
check if data there
column format
this dataset name
check if data there
column format
this dataset name
download data if necessary
download files if not present locally
we need to slightly modify the original files by adding some new lines after document separators
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
Remove CoNLL-U meta information in the last column
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
entity_mapping
this dataset name
download data if necessary
data validation
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
download files if not present locallys
column format
this dataset name
default dataset folder is the cache root
# download zip
merge the files in one as the zip is containing multiples files
column format
this dataset name
download data if necessary
"unzip the downloaded repo and merge the train, dev and test datasets"
column format
this dataset name
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
check if data there
create folder
download dataset
column format
this dataset name
download and parse data if necessary
create train test dev if not exist
column format
this dataset name
If the extracted corpus file is not yet present in dir
download zip if necessary
"extracted corpus is not present , so unpacking it."
column format
this dataset name
download zip
unpacking the zip
merge the files in one as the zip is containing multiples files
column format
this dataset name
"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)"
download files if not present locally
we need to modify the original files by adding new lines after after the end of each sentence
if only one language is given
column format
this dataset name
"use all languages if explicitly set to ""all"""
download data if necessary
initialize comlumncorpus and add it to list
column format
this dataset name
column format
this dataset name
column format
this dataset name
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
this list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
get google drive id from list
download from google drive
unzip
"tar.extractall(language_folder,members=[tar.getmember(file_name)])"
transform data into required format
"the processed dataset has the additional ending ""_new"""
remove the unprocessed dataset
initialize comlumncorpus and add it to list
if no languages are given as argument all languages used in XTREME will be loaded
if only one language is given
column format
this dataset name
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
This list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
download from HU Server
unzip
transform data into required format
initialize comlumncorpus and add it to list
if only one language is given
column format
this dataset name
download data if necessary
initialize comlumncorpus and add it to list
download data if necessary
unpack and write out in CoNLL column-like format
column format
this dataset name
download data if necessary
data is not in IOB2 format. Thus we transform it to IOB2
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
column format
this dataset name
rename according to train - test - dev - convention
column format
this dataset name
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
Add missing newline after header
Workaround for empty tokens
"Add ""real"" document marker"
Dataset split mapping
v2.0 only adds new language and splits for AJMC dataset
Special document marker for sample splits in AJMC dataset
column format
this dataset name
download data if necessary
column format
this dataset name
this dataset name
default dataset folder is the cache root
download and parse data if necessary
paths to train and test splits
init corpus
this dataset name
default dataset folder is the cache root
download and parse data if necessary
iterate over all html files
"get rid of html syntax, we only need the text"
between all documents we write a separator symbol
skip empty strings
"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)"
"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention"
sentence splitting and tokenization
iterate through all annotations and add to corresponding tokens
find sentence to which annotation belongs
position within corresponding sentence
set annotation for tokens of entity mention
write to out-file in column format
"in case something goes wrong, delete the dataset and raise error"
this dataset name
download and parse data if necessary
from qwikidata.linked_data_interface import get_entity_dict_from_api
generate qid wikiname dictionaries
merge dictionaries
ignore first line
commented and empty lines
read all Q-IDs
ignore first line
request
this dataset name
we use the wikiids in the data instead of directly utilizing the wikipedia urls.
like this we can quickly check if the corresponding page exists
if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi
delete unprocessed file
collect all wikiids
create the dictionary
request
this dataset name
names of raw text documents
open output_file
iterate through all documents
split sentences and tokenize
iterate through all annotations and add to corresponding tokens
find sentence to which annotation belongs
position within corresponding sentence
set annotation for tokens of entity mention
write to out file
this dataset name
download and parse data if necessary
this dataset name
download and parse data if necessary
First parse the post titles
Keep track of how many and which entity mentions does a given post title have
Check if the current post title has an entity link and parse accordingly
Post titles with entity mentions (if any) are handled via this function
Then parse the comments
"Iterate over the comments.tsv file, until the end is reached"
"Keep track of the current comment thread and its corresponding key, on which the annotations are matched."
Each comment thread is handled as one 'document'.
Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.
This if-condition is needed to handle this problem.
"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure"
"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above"
"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle."
"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,"
and not just single letters into single rows.
If there are annotated entity mentions for given post title or a comment thread
"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence"
Write the token with a corresponding tag to file
"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed"
"If a comment thread or a post title has no entity link, all tokens are assigned the O tag"
Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized
"incorrectly, in order to keep the desired format (empty line as a sentence separator)."
"Thrown when the second check above happens, but the last token of a sentence is reached."
"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below."
"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS"
Check if further annotations belong to the current post title or comment thread as well
Stop when the end of an annotation file is reached
Check if further annotations belong to the current sentence as well
"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)"
Docstart
if there is more than one word in the chunk we write each in a separate line
print(chunks)
empty line after each sentence
convert the file to CoNLL
this dataset name
"check if data there, if not, download the data"
create folder
download data
transform data into column format if necessary
if no filenames are specified we use all the data
"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled"
also we remove 'raganato_ALL' from filenames in case its in the list
generate the test file
make column file and save to data_folder
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
this dataset name
download data if necessary
if True:
write CoNLL-U Plus header
"Some special cases (e.g., missing spaces before entity marker)"
necessary if text should be whitespace tokenizeable
Handle case where tail may occur before the head
this dataset name
write CoNLL-U Plus header
this dataset name
TODO: change data source to original CoNLL04 -- this dataset has span formatting errors
download data if necessary
write CoNLL-U Plus header
The span has ended.
We are entering a new span; reset indices
and active tag to new span.
We're inside a span.
Last token might have been a part of a valid span.
this dataset name
write CoNLL-U Plus header
"for source_file_path, target_filename in zip(source_file_paths, target_filenames):"
"with zip_file.open(source_file_path, mode=""r"") as source_file:"
target_file_path = Path(data_folder) / target_filename
"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:"
# write CoNLL-U Plus header
"target_file.write(""# global.columns = id form ner\n"")"
for example in json.load(source_file):
token_list = self._tacred_example_to_token_list(example)
target_file.write(token_list.serialize())
check if first tag row is already occupied
"if first tag row is occupied, use second tag row"
hardcoded mapping TODO: perhaps find nicer solution
remap regular tag names
else skip to position in file where sentence begins
set sentence context using partials TODO: pointer to dataset is really inefficient
read in dev file if exists
read in test file if exists
the url is copied from https://huggingface.co/datasets/darentang/sroie/blob/main/sroie.py#L44
"find train, dev and test files if not specified"
use test_file to create test split if available
use dev_file to create test split if available
"if data point contains black-listed label, do not use"
first check if valid sentence
"if so, add to indices"
"find train, dev and test files if not specified"
variables
different handling of in_memory data than streaming data
"most data sets have the token text in the first column, if not, pass 'text' as column"
test if format is OK
test if at least one label given
make sentence from text (and filter for length)
"if a pair column is defined, make a sentence pair object"
noinspection PyDefaultArgument
dataset name includes the split size
default dataset folder is the cache root
download data if necessary
download each of the 28 splits
create dataset directory if necessary
download senteval datasets if necessary und unzip
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
handle labels file
handle data file
Create flair compatible labels
"by defaut, map point score to POSITIVE / NEGATIVE values"
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file from CSV
create test.txt file from CSV
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create train dev and test files in fasttext format
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
convert to FastText format
download data if necessary
"if data is not downloaded yet, download it"
get the zip file
move original .tsv files to another folder
create train and dev splits in fasttext format
create eval_dataset file with no labels
this dataset name
default dataset folder is the cache root
download data if necessary
download datasets if necessary
create dataset directory if necessary
create correctly formated txt files
multiple labels are possible
this dataset name
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
download data if necessary
this dataset name
download data if necessary
create a separate directory for different tasks
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
check if dataset is supported
set file names
set file names
download and unzip in file structure if necessary
instantiate corpus
"find train, dev and test files if not specified"
"create DataPairDataset for train, test and dev file, if they are given"
stop if file does not exist
create a DataPair object from strings
"if in_memory is True we return a datapair, otherwise we create one from the lists of strings"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"reorder dev datasets to have same columns as in train set: 8, 9, and 11"
dev sets include 5 different annotations but we will only keep the gold label
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get test and dev sets
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data not downloaded yet, download it"
get the zip file
"the downloaded files have json format, we transform them to tsv"
Function to transform JSON file to tsv for Recognizing Textual Entailment Data
remove json file
Uses dynamic programming approach to calculate maximum independent set in interval graph
with sum of all entity lengths as secondary key
calculate offset without current text
because we stick all passages of a document together
TODO For split entities we also annotate everything inbetween which might be a bad idea?
Try to fix incorrect annotations
print(
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
)
Ignore empty lines or relation annotations
FIX annotation of whitespaces (necessary for PDR)
One token may contain multiple entities -> deque all of them
column format
this dataset name
Create tokenization-dependent CONLL files. This is necessary to prevent
from caching issues (e.g. loading the same corpus with different sentence splitters)
column format
this dataset name
column format
this dataset name
Edge case: last token starts a new entity
Last document in file
column format
this dataset name
column format
this dataset name
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
Edge case: last token starts a new entity
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
column format
this dataset name
Read texts
Read annotations
column format
this dataset name
column format
this dataset name
column format
this dataset name
We need to apply a patch to correct the original training file
Articles title
Article abstract
Entity annotations
column format
this dataset name
Edge case: last token starts a new entity
Map all entities to chemicals
Map all entities to disease
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
Incomplete article
Invalid XML syntax
column format
this dataset name
column format
this dataset name
if len(mid) != 3:
continue
Try to fix entity offsets
column format
this dataset name
There is still one illegal annotation in the file ..
column format
this dataset name
"Abstract first, title second to prevent issues with sentence splitting"
column format
this dataset name
column format
this dataset name
column format
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
column format
this dataset name
"Filter for specific entity types, by default no entities will be filtered"
Get original HUNER splits to retrieve a list of all document ids contained in V2
train and dev split of V2 will be train in V4
test split of V2 will be dev in V4
New documents in V4 will become test documents
column format
this dataset name
column format
this dataset name
default dataset folder is the cache root
cache Feidegger config file
cache Feidegger images
replace image URL with local cached file
append Sentence-Image data point
"in certain cases, multi-CPU data loading makes no sense and slows"
"everything down. For this reason, we detect if a dataset is in-memory:"
"if so, num_workers is set to 0 for faster processing"
cast to list if necessary
cast to list if necessary
"first, check if pymongo is installed"
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
Expose base classses
Expose all biomedical data sets used for the evaluation of BioBERT
-
-
-
-
Expose all biomedical data sets using the HUNER splits
Expose all biomedical data sets
Expose all document classification datasets
word sense disambiguation
Expose all entity linking datasets
Expose all relation extraction datasets
universal proposition banks
keyphrase detection datasets
other NER datasets
standard NER datasets
Expose all sequence labeling datasets
Expose all text-image datasets
Expose all text-text datasets
Expose all treebanks
"find train, dev and test files if not specified"
get train data
get test data
get dev data
option 1: read only sentence boundaries as offset positions
option 2: keep everything in memory
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
current token ID
handling for the awful UD multiword format
end of sentence
comments
ellipsis
if token is a multi-word
normal single-word tokens
"if we don't split multiwords, skip over component words"
add token
add morphological tags
derive whitespace logic for multiwords
print(token)
print(current_multiword_last_token)
print(current_multiword_first_token)
"if multi-word equals component tokens, there should be no whitespace"
go through all tokens in subword and set whitespace_after information
print(i)
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
this dataset name
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
"finally, print model card for information"
test corpus
create a TARS classifier
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
test corpus
create a TARS classifier
switch to a new task (TARS can do multiple tasks so you must define one)
initialize the text classifier trainer
start the training
"With end symbol, without start symbol, padding in front"
"Without end symbol, with start symbol, padding in back"
"Without end symbol, without start symbol, padding in front"
initialize trainer
initialize trainer
train model for 2 epochs
load the checkpoint model and train until epoch 4
clean up results directory
initialize trainer
initialize trainer
increment for last token in sentence if not followed by whitespace
clean up directory
clean up directory
example sentence
set 4 labels for 2 tokens ('love' is tagged twice)
check if there are three POS labels with correct text and values
check if there are is one SENTIMENT label with correct text and values
check if all tokens are correctly labeled
remove the pos label from the last word
there should be 2 POS labels left
now remove all pos tags
set 3 labels for 2 spans (HU is tagged twice)
check if there are three labels with correct text and values
check if there are two spans with correct text and values
"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
should be only one NER label left
and only one NER span
set 3 labels for 2 spans (HU is tagged twice with different tags)
check if there are three labels with correct text and values
check if there are two spans with correct text and values
"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
should be only one NER label left
and only one NER span
but there is also one orgtype span and label
and only one NER span
let's add the NER tag back
check if there are three labels with correct text and values
check if there are two spans with correct text and values
now remove all NER tags
set 3 labels for 2 spans (HU is tagged twice with different tags)
create two relation label
there should be two relation labels
there should be one syntactic labels
"there should be two relations, one with two and one with one label"
example sentence
add another topic label
example sentence
has sentiment value
has 4 part of speech tags
has 1 NER tag
should be in total 6 labels
example sentence
add two NER labels
get the four labels
check that only two of the respective data points are equal
make a sentence and some right context
TODO: is this desirable? Or should two sentences with same text still be considered different objects?
Initializing a Sentence this way assumes that there is a space after each token
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
define search space
sequence tagger parameter
model trainer parameter
training parameter
find best parameter settings
clean up results directory
document embeddings parameter
training parameter
clean up results directory
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
load column dataset with one entry
load column dataset with two entries
load column dataset with three entries
"get training, test and dev data"
"get training, test and dev data"
check if Token labels are correct
"get training, test and dev data"
check if Token labels for frames are correct
"get training, test and dev data"
"get training, test and dev data"
get two corpora as one
"get training, test and dev data for full English UD corpus from web"
clean up data directory
"assert [token.get_tag(""head"").value for token in sent1.tokens] == ["
"""2"","
"""0"","
"""4"","
"""2"","
"""2"","
"""2"","
]
"Here, we use the default token annotation fields."
"We have manually checked, that these numbers are correct:"
"+1 offset, because of missing EOS marker at EOD"
Test data for v2.1 release
--- Embeddings that are shared by both models --- #
--- Task 1: Sentiment Analysis (5-class) --- #
Define corpus and model
-- Task 2: Binary Sentiment Analysis on Customer Reviews -- #
Define corpus and model
-- Define mapping (which tagger should train on which model) -- #
-- Create model trainer and train -- #
load dataset
tagger without CRF
train
check if loaded model can predict
check if loaded model successfully fit the training data
load dataset
tagger without CRF
train
check if loaded model can predict
check if loaded model successfully fit the training data
load dataset
tagger without CRF
train
check if loaded model can predict
check if loaded model successfully fit the training data
load dataset
tagger without CRF
train
check if loaded model can predict
check if loaded model successfully fit the training data
check if model can predict
load model
chcek if model predicts correct label
check if loaded model successfully fit the training data
check if model can predict
load model
chcek if model predicts correct label
check if loaded model successfully fit the training data
check if model can predict
load model
chcek if model predicts correct label
check if loaded model successfully fit the training data
clean up file
no need for label_dict
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
Intel ----founded_by---> Gordon Moore
Intel ----founded_by---> Robert Noyce
Check sentence masking and relation label annotation on
"training, validation and test dataset (in this test the splits are the same)"
"Entity pair permutations of: ""Larry Page and Sergey Brin founded Google ."""
"Entity pair permutations of: ""Microsoft was founded by Bill Gates ."""
"Entity pair permutations of: ""Konrad Zuse was born in Berlin on 22 June 1910 ."""
"Entity pair permutations of: ""Joseph Weizenbaum , a professor at MIT , was born in Berlin , Germany."""
This sentence is only included if we transform the corpus with cross augmentation
Ensure this is an example that predicts no classes in multilabel
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
ensure that the prepared tensors is what we expect
use a SequenceTagger to save and reload the embedding in the manner it is supposed to work
previous and next sentence as context
test expansion for sentence without context
test expansion for with previous and next as context
test expansion if first sentence is document boundary
test expansion if we don't use context
"apparently the precision is not that high on cuda, hence the absolute tolerance needs to be higher."
dummy model with embeddings
save the dummy and load it again
check that context_length and use_context_separator is the same for both
mmap seems to be much more memory efficient
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
Extract all the contents of zip file in current directory
Extract all the contents of zip file in current directory
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
load_big_file is a workaround byhttps://github.com/highway11git
to load models on some Mac/Windows setups
see https://github.com/zalandoresearch/flair/issues/351
first determine the distribution of classes in the dataset
weight for each sample
Create blocks
shuffle the blocks
concatenate the shuffled blocks
Create blocks
shuffle the blocks
concatenate the shuffled blocks
increment for last token in sentence if not followed by whitespace
this is the default init size of a lmdb database for embeddings
get db filename from embedding name
"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
SequenceTagger
TextClassifier
get db filename from embedding name
if embedding database already exists
"otherwise, push embedding to database"
if embedding database already exists
open the database in read mode
we need to set self.k
create and load the database in write mode
"no idea why, but we need to close and reopen the environment to avoid"
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
when opening new transaction !
init dictionaries
"in order to deal with unknown tokens, add <unk>"
set 'add_unk' if the dictionary was created with a version of Flair older than 0.9
set 'add_unk' depending on whether <unk> is a key
"if one embedding name, directly return it"
"if multiple embedding names, concatenate them"
TODO: does it make sense to exclude labels? Two data points of identical text (but different labels)
would be equal now.
First we remove any existing labels for this PartOfSentence in self.sentence
labels also need to be deleted at Sentence object
delete labels at object itself
private field for all known spans
the tokenizer used for this sentence
some sentences represent a document boundary (but most do not)
internal variables to denote position inside dataset
"if text is passed, instantiate sentence with tokens (words)"
determine token positions and whitespace_after flag
the last token has no whitespace after
log a warning if the dataset is empty
data with zero-width characters cannot be handled
set token idx and sentence
append token to sentence
register token annotations on sentence
move sentence embeddings to device
also move token embeddings to device
clear token embeddings
infer whitespace after field
"if sentence has no tokens, return empty string"
"otherwise, return concatenation of tokens with the correct offsets"
The sentence's start position is not propagated to its tokens.
"Therefore, we need to add the sentence's start position to its last token's end position, including whitespaces."
No character at the corresponding code point: remove it
"if no label if specified, return all labels"
"if the label type exists in the Sentence, return it"
return empty list if none of the above
labels also need to be deleted at all tokens
labels also need to be deleted at all known spans
remove spans without labels
delete labels at object itself
set name
abort if no data is provided
sample test data from train if none is provided
sample dev data from train if none is provided
set train dev and test data
find out empty sentence indices
create subset of non-empty sentence indices
find out empty sentence indices
create subset of non-empty sentence indices
count all label types per sentence
go through all labels of label_type and count values
check if there are any span labels
"if an unk threshold is set, UNK all label values below this threshold"
sample randomly from a label distribution according to the probabilities defined by the desired noise share
replace the old label with the new one
keep track of the old (clean) label using another label type category
keep track of how many labels in total are flipped
Make the tag dictionary
"add a dummy ""O"" to close final prediction"
return complex list
internal variables
non-set tags are OUT tags
anything that is not OUT is IN
does this prediction start a new span?
begin and single tags start new spans
"in IOB format, an I tag starts a span if it follows an O or is a different span"
single tags that change prediction start new spans
if an existing span is ended (either by reaching O or starting a new span)
determine score and value
append to result list
reset for-loop variables for new span
remember previous tag
global variable: cache_root
global variable: device
global variable: version
global variable: arrow symbol
dummy return to fulfill trainer.train() needs
print(vec)
Attach optimizer
"convert `metrics` to float, in case it's a zero-dim Tensor"
if memory mode option 'none' delete everything
"if dynamic embedding keys not passed, identify them automatically"
always delete dynamic embeddings
"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)"
optional metric space decoder if prototypes have different length than embedding
create initial prototypes for all classes (all initial prototypes are a vector of all 1s)
"if set, create initial prototypes from normal distribution"
"if set, use a radius"
all parameters will be pushed internally to the specified device
decode embeddings into prototype space
"if unlabeled distance is set, mask out loss to unlabeled class prototype"
Always include the name of the Model class for which the state dict holds
"in Flair <0.9.1, optimizer and scheduler used to train model are not saved"
"write out a ""model card"" if one is set"
special handling for optimizer:
remember optimizer class and state dictionary
save model
restore optimizer and scheduler to model card if set
"if this class is abstract, go through all inheriting classes and try to fetch and load the model"
get all non-abstract subclasses
"try to fetch the model for each subclass. if fetching is possible, load model and return it"
"skip any invalid loadings, e.g. not found on huggingface hub"
"if the model cannot be fetched, load as a file"
try to get model class from state
"older (flair 11.3 and below) models do not contain cls information. In this case, try all subclasses"
"if str(model_cls) == ""<class 'flair.models.pairwise_classification_model.TextPairClassifier'>"": continue"
"skip any invalid loadings, e.g. not found on huggingface hub"
"if this class is not abstract, fetch the model and load it"
"make sure <unk> is contained in gold_label_dictionary, if given"
"read Dataset into data loader, if list of sentences passed, make Dataset first"
loss calculation
variables for printing
variables for computing scores
remove any previously predicted labels
predict for batch
get the gold labels
add to all_predicted_values
make printout lines
convert true and predicted values to two span-aligned lists
delete exluded labels if exclude_labels is given
"if after excluding labels, no label is left, ignore the datapoint"
write all_predicted_values to out_file if set
make the evaluation dictionary
check if this is a multi-label problem
compute numbers by formatting true and predicted such that Scikit-Learn can use them
multi-label problems require a multi-hot vector for each true and predicted label
single-label problems can do with a single index for each true and predicted label
"now, calculate evaluation numbers"
there is at least one gold label or one prediction (default)
"if there is only one label, then ""micro avg"" = ""macro avg"""
"micro average is only computed if zero-label exists (for instance ""O"")"
if no zero-label exists (such as in POS tagging) micro average is equal to accuracy
same for the main score
issue error and default all evaluation numbers to 0.
line for log file
check if there is a label mismatch
print info
set the embeddings
initialize the label dictionary
initialize the decoder
set up multi-label logic
init dropouts
loss weights and loss function
Initialize the weight tensor
set up gradient reversal if so specified
embed sentences
get a tensor of data points
do dropout
make a forward pass to produce embedded data points and labels
get the data points for which to predict labels
get their gold labels as a tensor
pass data points through network to get encoded data point tensor
decode
an optional masking step (no masking in most cases)
calculate the loss
filter empty sentences
reverse sort all sequences by their length
progress bar for verbosity
filter data points in batch
stop if all sentences are empty
pass data points through network and decode
if anything could possibly be predicted
remove previously predicted labels of this type
add DefaultClassifier arguments
add variables of DefaultClassifier
Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23
Get projected 1st dimension
Compute bilinear form
Arcosh
Project the input data to n+1 dimensions
"The first dimension, is recomputed in the distance module"
header for 'weights.txt'
"determine the column index of loss, f-score and accuracy for"
"train, dev and test split"
then get all relevant values from the tsv
then get all relevant values from the tsv
plot i
save plots
save plots
plt.show()
save plot
take the average over the last three scores of training
take average over the scores from the different training runs
auto-spawn on GPU if available
progress bar for verbosity
stop if all sentences are empty
clearing token embeddings to save memory
"read Dataset into data loader, if list of sentences passed, make Dataset first"
TODO: not saving lines yet
TODO: This closely shadows the RelationExtractor name. Maybe we need a better name here.
- MaskedRelationClassifier ?
This depends if this relation classification architecture should replace or offer as an alternative.
Set label type and prepare label dictionary
Initialize super default classifier
Add the special tokens from the encoding strategy
"Auto-spawn on GPU, if available"
Only use entities labelled with the specified labels for each label type
Only use entities above the specified threshold
Use a dictionary to find gold relation annotations for a given entity pair
Yield head and tail entity pairs from the cross product of all entities
Remove identity relation entity pairs
Remove entity pairs with labels that do not match any
of the specified relations in `self.entity_pair_labels`
"Obtain gold label, if existing"
Some sanity checks
Pre-compute non-leading head and tail tokens for entity masking
We can not use the plaintext of the head/tail span in the sentence as the mask/marker
since there may be multiple occurrences of the same entity mentioned in the sentence.
"Therefore, we use the span's position in the sentence."
Create masked sentence
Add gold relation annotation as sentence label
"Using the sentence label instead of annotating a separate `Relation` object is easier to manage since,"
"during prediction, the forward pass does not need any knowledge about the entities in the sentence."
"If we sample missing splits, the encoded sentences that correspond to the same original sentences"
"may get distributed into different splits. For training purposes, this is always undesired."
Ensure that all sentences are encoded properly
Deal with the case where all sentences are encoded sentences
"mypy does not infer the type of ""sentences"" restricted by the if statement"
Deal with the case where all sentences are standard (non-encoded) sentences
"For each encoded sentence, transfer its prediction onto the original relation"
auto-spawn on GPU if available
pad strings with whitespaces to longest sentence
cut up the input into chunks of max charlength = chunk_size
push each chunk through the RNN language model
concatenate all chunks to make final output
initial hidden state
get predicted weights
divide by temperature
"to prevent overflow problem with small temperature values, substract largest value from all"
this makes a vector in which the largest value is 0
compute word weights with exponential function
try sampling multinomial distribution for next character
print(word_idx)
input ids
push list of character IDs through model
the target is always the next character
use cross entropy loss to compare output of forward pass with targets
exponentiate cross-entropy loss to calculate perplexity
"""document_delimiter"" property may be missing in some older pre-trained models"
serialize the language models and the constructor arguments (but nothing else)
special handling for deserializing language models
re-initialize language model with constructor arguments
copy over state dictionary to self
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
"in their ""self.train()"" method)"
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
Transform input data into TARS format
"if there are no labels, return a random sample as negatives"
"otherwise, go through all labels"
make sure the probabilities always sum up to 1
get and embed all labels by making a Sentence object that contains only the label text
get each label embedding and scale between 0 and 1
compute similarity matrix
"the higher the similarity, the greater the chance that a label is"
sampled as negative example
make label dictionary if no Dictionary object is passed
prepare dictionary of tags (without B- I- prefixes and without UNK)
check if candidate_label_set is empty
make list if only one candidate label is passed
create label dictionary
note current task
create a temporary task
make zero shot predictions
switch to the pre-existing task
prepare TARS dictionary
initialize a bare-bones sequence tagger
transformer separator
Store task specific labels since TARS can handle multiple tasks
make a tars sentence where all labels are O by default
init new TARS classifier
set all task information
return
with torch.no_grad():
progress bar for verbosity
stop if all sentences are empty
go through each sentence in the batch
always remove tags first
get the span and its label
determine whether tokens in this span already have a label
only add if all tokens have no label
make and add a corresponding predicted span
set indices so that no token can be tagged twice
clearing token embeddings to save memory
"all labels default to ""O"""
set gold token-level
set predicted token-level
now print labels in CoNLL format
prepare TARS dictionary
initialize a bare-bones sequence tagger
transformer separator
Store task specific labels since TARS can handle multiple tasks
get the serialized embeddings
remap state dict for models serialized with Flair <= 0.11.3
init new TARS classifier
set all task information
with torch.no_grad():
progress bar for verbosity
stop if all sentences are empty
go through each sentence in the batch
always remove tags first
add all labels that according to TARS match the text and are above threshold
do not add labels below confidence threshold
only use label with highest confidence if enforcing single-label predictions
get all label scores and do an argmax to get the best label
remove previously added labels and only add the best label
clearing token embeddings to save memory
set separator to concatenate two sentences
auto-spawn on GPU if available
pooling operation to get embeddings for entites
set embeddings
set relation and entity label types
"whether to use gold entity pairs, and whether to filter entity pairs by type"
filter entity pairs according to their tags if set
whether to encode characters and whether to use attention (attention can only be used if chars are encoded)
character dictionary for decoding and encoding
make sure <unk> is in dictionary for handling of unknown characters
add special symbols to dictionary if necessary and save respective indices
---- ENCODER ----
encoder character embeddings
encoder pre-trained embeddings
encoder RNN
additional encoder linear layer if bidirectional encoding
---- DECODER ----
decoder: linear layers to transform vectors to and from alphabet_size
when using attention we concatenate attention outcome and decoder hidden states
decoder RNN
loss and softmax
self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction
add additional columns for special symbols if necessary
initialize with dummy symbols
encode inputs
get labels (we assume each token has a lemma label)
get char indices for labels of sentence
"(batch_size, max_sequence_length) batch_size = #words in sentence,"
max_sequence_length = length of longest label of sentence + 1
get char embeddings
"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size"
take decoder input and initial hidden and pass through RNN
"if all encoder outputs are provided, use attention"
take convex combinations of encoder hidden states as new output using the computed attention coefficients
"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)"
get all tokens
encode input characters by sending them through RNN
get one-hots for characters and add special symbols / padding
determine length of each token
embed sentences
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
variable to store initial hidden states for decoder
encode input characters by sending them through RNN
test packing and padding
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
concatenate the final hidden states of the encoder. These will be projected to hidden_size of
decoder later with self.emb_to_hidden
mask out vectors that correspond to a dummy symbol (TODO: check attention masking)
use token embedding as initial hidden state for decoder
concatenate everything together and project to appropriate size for decoder
variable to store initial hidden states for decoder
encode input characters by sending them through RNN
note that we do not need to fill up with dummy symbols since we process each token seperately
embed character one-hots
send through encoder RNN (produces initial hidden for decoder)
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
project 2*hidden_size to hidden_size
concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder
later with self.emb_to_hidden
use token embedding as initial hidden state for decoder
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
concatenate everything together and project to appropriate size for decoder
"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)"
"create target vector (batch_size, max_label_seq_length + 1)"
filter empty sentences
max length of the predicted sequences
for printing
stop if all sentences are empty
remove previously predicted labels of this type
create list of tokens in batch
encode inputs
"create input for first pass (batch_size, 1, input_size), first letter is special character <S>"
sequence length is always set to one in prediction
option 1: greedy decoding
predictions
decode next character
pick top beam size many outputs with highest probabilities
option 2: beam search
out_probs = self.softmax(output_vectors).squeeze(1)
make sure no dummy symbol <> or start symbol <S> is predicted
pick top beam size many outputs with highest probabilities
"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1"
"leading_indices and probabilities have size (batch_size, beam_size)"
keep scores of beam_size many hypothesis for each token in the batch
stack all leading indices of all hypothesis and corresponding hidden states in two tensors
save sequences so far
keep track of how many hypothesis were completed for each token
"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)"
decode with log softmax
make sure no dummy symbol <> or start symbol <S> is predicted
"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside"
"if the sequence is already ended, do not record as candidate"
index of token in in list tokens_in_batch
print(token_number)
hypothesis score
TODO: remove token if number of completed hypothesis exceeds given value
set score of corresponding entry to -inf so it will not be expanded
get leading_indices for next expansion
find highest scoring hypothesis among beam_size*beam_size possible ones for each token
take beam_size many copies of scores vector and add scores of possible new extensions
"size (beam_size*batch_size, beam_size)"
print(hypothesis_scores)
"reshape to vector of size (batch_size, beam_size*beam_size),"
each row contains beam_size*beam_size scores of the new possible hypothesis
print(hypothesis_scores_per_token)
"choose beam_size best for each token - size (batch_size, beam_size)"
out of indices_per_token we now need to recompute the original indices of the hypothesis in
a list of length beam_size*batch_size
"where the first three inidices belong to the first token, the next three to the second token,"
and so on
with these indices we can compute the tensors for the next iteration
expand sequences with corresponding index
add log-probabilities to the scores
save new leading indices
save corresponding hidden states
it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations
in that case we append one of the final seuqences without end symbol to the final_candidates
get best final hypothesis for each token
get characters from index sequences and add predicted label to token
dictionaries
all parameters will be pushed internally to the specified device
now print labels in CoNLL format
internal candidate lists of generator
load Zelda candidates if so passed
create candidate lists
"if lower casing is enabled, create candidate lists of lower cased versions"
create a new dictionary for lower cased mentions
go through each mention and its candidates
"check if backoff mention already seen. If so, add candidates. Else, create new entry."
set lowercased version as map
remap state dict for models serialized with Flair <= 0.11.3
get the candidates
"during training, add the gold value as candidate"
----- Create the internal tag dictionary -----
span-labels need special encoding (BIO or BIOES)
the big question is whether the label dictionary should contain an UNK or not
"without UNK, we cannot evaluate on data that contains labels not seen in test"
"with UNK, the model learns less well if there are no UNK examples"
is this a span prediction problem?
----- Embeddings -----
----- Initial loss weights parameters -----
----- RNN specific parameters -----
----- Conditional Random Field parameters -----
"Previously trained models have been trained without an explicit CRF, thus it is required to check"
whether we are loading a model from state dict in order to skip or add START and STOP token
----- Dropout parameters -----
dropouts
remove word dropout if there is no contact over the sequence dimension.
----- Model layers -----
----- RNN layer -----
"If shared RNN provided, else create one for model"
Whether to train initial hidden state
final linear map to tag space
"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss"
"if using CRF, we also require a CRF and a Viterbi decoder"
"if there are no sentences, there is no loss"
forward pass to get scores
calculate loss given scores and labels
make a zero-padded tensor for the whole sentence
linear map to tag space
"Depending on whether we are using CRF or a linear layer, scores is either:"
"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF"
"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer"
spans need to be encoded as token-level predictions
all others are regular labels for each token
make sure it's a list
filter empty sentences
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
get features from forward propagation
remove previously predicted labels of this type
"if return_loss, get loss value"
make predictions
add predictions to Sentence
BIOES-labels need to be converted to spans
"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)"
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
core Flair models on Huggingface ModelHub
"Large NER models,"
Multilingual NER models
English POS models
Multilingual POS models
English SRL models
English chunking models
Language-specific NER models
Language-specific POS models
English NER models
Multilingual NER models
English POS models
Multilingual POS models
English SRL models
English chunking models
Danish models
German models
French models
Dutch models
Malayalam models
Portuguese models
Keyphase models
Biomedical models
check if model name is a valid local file
"check if model key is remapped to HF key - if so, print out information"
get mapped name
use mapped name instead
"if not, check if model key is remapped to direct download location. If so, download model"
special handling for the taggers by the @redewiegergabe project (TODO: move to model hub)
"for all other cases (not local file or special download location), use HF model hub"
"if not a local file, get from model hub"
use model name as subfolder
Lazy import
output information
## Demo: How to use in Flair
load tagger
make example sentence
predict NER tags
print sentence
print predicted NER spans
iterate over entities and print
Lazy import
Save model weight
Determine if model card already exists
Generate and save model card
Upload files
"all labels default to ""O"""
set gold token-level
set predicted token-level
now print labels in CoNLL format
print labels in CoNLL format
the multi task model has several labels
biomedical models
entity linker
auto-spawn on GPU if available
remap state dict for models serialized with Flair <= 0.11.3
English sentiment models
Communicative Functions Model
"scores_at_targets[range(features.shape[0]), lengths.values -1]"
Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices
"Initially, get scores from <start> tag to all other tags"
"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp"
"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep"
Create a tensor to hold accumulated sequence scores at each current tag
Create a tensor to hold back-pointers
"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag"
"Let pads be the <end> tag index, since that was the last tag in the decoded sequence"
"We add scores at current timestep to scores accumulated up to previous timestep, and"
choose the previous timestep that corresponds to the max. accumulated score for each current timestep
"If sentence is over, add transition to STOP-tag"
Decode/trace best path backwards
Sanity check
remove start-tag and backscore to stop-tag
Max + Softmax to get confidence score for predicted label and append label to each token
"Transitions are used in the following way: transitions[to, from]."
"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag"
to START-tag and from STOP-tag to any other tag to -10000.
create a model card for this model with Flair and PyTorch version
also record Transformers version if library is loaded
remember all parameters used in train() call
add model card to model
"if optimizer class is passed, instantiate:"
"determine what splits (train, dev, test) to evaluate and log"
prepare loss logging file and set up header
"from here on, use list of learning rates"
load existing optimizer state dictionary if it exists
"minimize training loss if training with dev data, else maximize dev score"
"if scheduler is passed as a class, instantiate"
"if we load a checkpoint, we have already trained for epoch"
"Determine whether to log ""bad epochs"" information"
load existing scheduler state dictionary if it exists
update optimizer and scheduler in model card
"if training also uses dev/train data, include in training set"
initialize sampler if provided
init with default values if only class is provided
set dataset to sample from
this field stores the names of all dynamic embeddings in the model (determined after first forward pass)
At any point you can hit Ctrl + C to break out of training early.
update epoch in model card
get new learning rate
reload last best model if annealing with restarts is enabled
stop training if learning rate becomes too small
"if shuffle_first_epoch==False, the first epoch is not shuffled"
process mini-batches
zero the gradients on the model and optimizer
"if necessary, make batch_steps"
forward and backward for batch
forward pass
Backward
identify dynamic embeddings (always deleted) on first sentence
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
do the optimizer step
do the scheduler step if one-cycle or linear decay
get new learning rate
evaluate on train / dev / test split depending on training settings
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
calculate scores using dev data if available
append dev score to score history
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
determine if this is the best model or if we need to anneal
default mode: anneal against dev score
alternative: anneal against dev loss
alternative: anneal against train loss
determine bad epoch number
lr unchanged
log bad epochs
output log file
make headers on first epoch
"if checkpoint is enabled, save model at each epoch"
Check whether to save best model
"if we do not use dev data for model selection, save final model"
test best model if test data is present
recover all arguments that were used to train this model
you can overwrite params with your own
surface nested arguments
resume training with these parameters
"If set, add a factor to the learning rate of all parameters with 'embeddings' not in name"
get and return the final test score of best model
cast string to Path
forward pass
update optimizer and scheduler
"TextDataset returns a list. valid and test are only one file,"
so return the first element
cast string to Path
error message if the validation dataset is too small
Shuffle training files randomly after serially iterating
through corpus one
"iterate through training data, starting at"
self.split (for checkpointing)
off by one for printing
go into train mode
reset variables
not really sure what this does
do the forward pass in the model
try to predict the targets
Backward
`clip_grad_norm` helps prevent the exploding gradient
problem in RNNs / LSTMs.
We detach the hidden state from how it was
previously produced.
"If we didn't, the model would try backpropagating"
all the way to start of the dataset.
explicitly remove loss to clear up memory
#########################################################
Save the model if the validation loss is the best we've
seen so far.
#########################################################
print info
#########################################################
##############################################################################
final testing
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
"the default model for ELMo is the 'original' model, which is very large"
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
put on Cuda if available
embed a dummy sentence to determine embedding_length
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
"multilingual forward (English, German, French, Italian, Dutch, Polish)"
"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
Dutch forward
Dutch backward
Swedish forward
Swedish backward
French forward
French backward
Czech forward
Czech backward
Portuguese forward
Portuguese backward
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
1-camembert-base -> camembert-base
1-xlm-roberta-large -> xlm-roberta-large
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
The mask has 1 for real tokens and 0 for padding tokens. Only real
tokens are attended to.
Zero-pad up to the sequence length.
"first, find longest sentence in batch"
prepare id maps for BERT model
put encoded batch through BERT model to get all hidden states of all encoder layers
get aggregated embeddings for each BERT-subtoken in sentence
get the current sentence object
add concatenated embedding to sentence
use first subword embedding if pooling operation is 'first'
"otherwise, do a mean over all subwords in token"
"if only one sentence is passed, convert to list of sentence"
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
embed a dummy sentence to determine embedding_length
Avoid conflicts with flair's Token class
"legacy pickle-like saving for image embeddings, as implementation details are not obvious"
"legacy pickle-like loading for image embeddings, as implementation details are not obvious"
"<cls> token initially set to 1/D, so it attends to all image features equally"
add positional encodings
reshape the pixels into the sequence
layer norm after convolution and positional encodings
add <cls> token
"transformer requires input in the shape [h*w+1, b, d]"
the output is an embedding of <cls> token
this parameter is fixed
optional fine-tuning on top of embedding layer
"if only one sentence is passed, convert to list of sentence"
"if only one sentence is passed, convert to list of sentence"
bidirectional RNN on top of embedding layer
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push through RNN
after-RNN dropout
extract embeddings from RNN
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
serialize the language models and the constructor arguments (but nothing else)
re-initialize language model with constructor arguments
special handling for deserializing language models
copy over state dictionary to self
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
"in their ""self.train()"" method)"
IMPORTANT: add embeddings as torch modules
iterate over sentences
"if its a forward LM, take last state"
"convert to plain strings, embedded in a list for the encode function"
CNN
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push CNN
after-CNN dropout
extract embeddings from CNN
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
"if only one sentence is passed, convert to list of sentence"
Expose base classses
Expose document embedding classes
Expose image embedding classes
Expose legacy embedding classes
Expose token embedding classes
in some cases we need to insert zero vectors for tokens without embedding.
padding
remove special markup
check if special tokens exist to circumvent error message
iterate over subtokens and reconstruct tokens
remove special markup
check if reconstructed token is special begin token ([CLS] or similar)
some BERT tokenizers somehow omit words - in such cases skip to next token
"we cannot handle unk_tokens perfectly, so let's assume that one unk_token corresponds to one token."
if tokens are unaccounted for
check if all tokens were matched to subtokens
The layoutlm tokenizer doesn't handle ocr themselves
"transformers returns the ""added_tokens.json"" even if it doesn't create it"
"transformers returns the ""added_tokens.json"" even if it doesn't create it"
in case of doubt: token embedding has higher priority than document embedding
random check some tokens to save performance.
Models such as FNet do not have an attention_mask
set language IDs for XLM-style transformers
"word_ids is only supported for fast rust tokenizers. Some models like ""xlm-mlm-ende-1024"" do not have"
"a fast tokenizer implementation, hence we need to fall back to our own reconstruction of word_ids."
set context if not set already
flair specific pre-tokenization
"onnx prepares numpy arrays, no mather if it runs on gpu or cpu, the input is on cpu first."
temporary fix to disable tokenizer parallelism warning
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
do not print transformer warnings as these are confusing in this case
load tokenizer and transformer model
load tokenizer from inmemory zip-file
model name
embedding parameters
send mini-token through to check how many layers the model has
return length
"If we use a context separator, add a new special token"
"most models have an initial BOS token, except for XLNet, T5 and GPT2"
"when initializing, embeddings are in eval mode by default"
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
"cannot run `.encode` if ocr boxes are required, assume"
in case of doubt: token embedding has higher priority than document embedding
in case of doubt: token embedding has higher priority than document embedding
legacy TransformerDocumentEmbedding
legacy TransformerTokenEmbedding
legacy Flair <= 0.12
legacy Flair <= 0.7
legacy TransformerTokenEmbedding
Legacy TransformerDocumentEmbedding
legacy TransformerTokenEmbedding
legacy TransformerDocumentEmbedding
copy values from new embedding
cls first pooling can be done without recreating sentence hidden states
make the tuple a tensor; makes working with it easier.
"for multimodal models like layoutlmv3, we truncate the image embeddings as they are only used via attention"
only use layers that will be outputted
this parameter is fixed
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
make compatible with serialized models
gensim version 4
gensim version 3
"if no embedding is set, the vocab and embedding length is requried"
GLOVE embeddings
TURIAN embeddings
KOMNINOS embeddings
pubmed embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
twitter embeddings
two-letter language code wiki embeddings
two-letter language code wiki embeddings
two-letter language code crawl embeddings
fix serialized models
"this is required to force the module on the cpu,"
"if a parent module is put to gpu, the _apply is called to each sub_module"
self.to(..) actually sets the device properly
this ignores the get_cached_vec method when loading older versions
it is needed for compatibility reasons
gensim version 4
gensim version 3
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
multilingual models
English models
Arabic
Bulgarian
Czech
Danish
German
Spanish
Basque
Persian
Finnish
French
Hebrew
Hindi
Croatian
Indonesian
Italian
Japanese
Malayalam
Dutch
Norwegian
Polish
Portuguese
Pubmed
Slovenian
Swedish
Tamil
Spanish clinical
CLEF HIPE Shared task
Amharic
Ukrainian
load model if in pretrained model map
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
CLEF HIPE models are lowercased
embeddings are static if we don't do finetuning
embed a dummy sentence to determine embedding_length
set to eval mode
make compatible with serialized models (TODO: remove)
"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout"
make compatible with serialized models (TODO: remove)
gradients are enable if fine-tuning is enabled
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
offset mode that extracts at whitespace after last character
offset mode that extracts at last character
use the character language model embeddings as basis
length is twice the original character LM embedding length
these fields are for the embedding memory
whether to add only capitalized words to memory (faster runtime and lower memory consumption)
we re-compute embeddings dynamically at each epoch
set the memory method
memory is wiped each time we do a training run
"if we keep a pooling, it needs to be updated continuously"
update embedding
check token.text is empty or not
set aggregation operation
add embeddings after updating
model architecture
model architecture
"""pl"","
download if necessary
load the model
"TODO: keep for backwards compatibility, but remove in future"
save the sentence piece model as binary file (not as path which may change)
write out the binary sentence piece model into the expected directory
"if the model was saved as binary and it is not found on disk, write to appropriate path"
"otherwise, use normal process and potentially trigger another download"
"once the modes if there, load it with sentence piece"
empty words get no embedding
all other words get embedded
GLOVE embeddings
no need to recreate as NILCEmbeddings
read in test file if exists
read in dev file if exists
"find train, dev and test files if not specified"
Add tags for each annotated span
Remove leading and trailing whitespaces from annotated spans
Search start and end token index for current span
If end index is not found set to last token
Throw error if indices are not valid
get train data
read in test file if exists
read in dev file if exists
"find train, dev and test files if not specified"
special key for space after
special key for feature columns
special key for dependency head id
"store either Sentence objects in memory, or only file offsets"
"most data sets have the token text in the first column, if not, pass 'text' as column"
determine encoding of text file
identify which columns are spans and which are word-level
now load all sentences
skip first line if to selected
option 1: keep Sentence objects in memory
pointer to previous
parse next sentence
quit if last sentence reached
skip banned sentences
set previous and next sentence for context
append parsed sentence to list in memory
option 2: keep source data in memory
"read lines for next sentence, but don't parse"
quit if last sentence reached
append raw lines for each sentence
we make a distinction between word-level tags and span-level tags
read first sentence to determine which columns are span-labels
skip first line if to selected
check the first 5 sentences
go through all annotations and identify word- and span-level annotations
- if a column has at least one BIES we know it's a Span label
"- if a column has at least one tag that is not BIOES, we know it's a Token label"
- problem cases are columns for which we see only O - in this case we default to Span
skip assigned columns
the space after key is always word-levels
"if at least one token has a BIES, we know it's a span label"
"if at least one token has a label other than BIOES, we know it's a token label"
all remaining columns that are not word-level are span-level
for column in self.word_level_tag_columns:
"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")"
"if sentence ends, break"
parse comments if possible
"otherwise, this line is a token. parse and add to sentence"
check if this sentence is a document boundary
add span labels
discard tags from tokens that are not added to the sentence
parse relations if they are set
head and tail span indices are 1-indexed and end index is inclusive
parse comments such as '# id cd27886d-6895-4d02-a8df-e5fa763fa88f	domain=de-orcas'
"to set the metadata ""domain"" to ""de-orcas"""
get fields from line
get head_id if exists (only in dependency parses)
initialize token
go through all columns
'feats' and 'misc' column should be split into different fields
special handling for whitespace after
add each other feature as label-value pair
get the task name (e.g. 'ner')
get the label value
add label
remap regular tag names
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
set sentence context using partials TODO: pointer to dataset is really inefficient
use all domains
iter over all domains / sources and create target files
Parameters
The conll representation of coref spans allows spans to
"overlap. If spans end or begin at the same word, they are"
"separated by a ""|""."
The span begins at this word.
The span begins and ends at this word (single word span).
"The span is starting, so we record the index of the word."
"The span for this id is ending, but didn't start at this word."
Retrieve the start index from the document state and
add the span to the clusters for this id.
Parameters
strip all bracketing information to
get the actual propbank label.
Entering into a span for a particular semantic role label.
We append the label and set the current span for this annotation.
"If there's no '(' token, but the current_span_label is not None,"
then we are inside a span.
We're outside a span.
"Exiting a span, so we reset the current span label for this annotation."
The words in the sentence.
The pos tags of the words in the sentence.
the pieces of the parse tree.
The lemmatised form of the words in the sentence which
have SRL or word sense information.
The FrameNet ID of the predicate.
"The sense of the word, if available."
"The current speaker, if available."
"Cluster id -> List of (start_index, end_index) spans."
Cluster id -> List of start_indices which are open for this id.
Replace brackets in text and pos tags
with a different token for parse trees.
only keep ')' if there are nested brackets with nothing in them.
There are some bad annotations in the CONLL data.
"They contain no information, so to make this explicit,"
we just set the parse piece to be None which will result
in the overall parse tree being None.
"If this is the first word in the sentence, create"
empty lists to collect the NER and SRL BIO labels.
"We can't do this upfront, because we don't know how many"
"components we are collecting, as a sentence can have"
variable numbers of SRL frames.
Create variables representing the current label for each label
sequence we are collecting.
"If any annotation marks this word as a verb predicate,"
we need to record its index. This also has the side effect
of ordering the verbal predicates by their location in the
"sentence, automatically aligning them with the annotations."
"this would not be reached if parse_pieces contained None, hence the cast"
Non-empty line. Collect the annotation.
Collect any stragglers or files which might not
have the '#end document' format for the end of the file.
this dataset name
check if data there
column format
this dataset name
check if data there
column format
this dataset name
download data if necessary
download files if not present locally
we need to slightly modify the original files by adding some new lines after document separators
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
Remove CoNLL-U meta information in the last column
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
entity_mapping
this dataset name
download data if necessary
data validation
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
download files if not present locallys
column format
this dataset name
default dataset folder is the cache root
# download zip
merge the files in one as the zip is containing multiples files
column format
this dataset name
download data if necessary
"unzip the downloaded repo and merge the train, dev and test datasets"
column format
this dataset name
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
check if data there
create folder
download dataset
column format
this dataset name
download and parse data if necessary
create train test dev if not exist
column format
this dataset name
If the extracted corpus file is not yet present in dir
download zip if necessary
"extracted corpus is not present , so unpacking it."
column format
this dataset name
download zip
unpacking the zip
merge the files in one as the zip is containing multiples files
column format
this dataset name
"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)"
download files if not present locally
we need to modify the original files by adding new lines after after the end of each sentence
if only one language is given
column format
this dataset name
"use all languages if explicitly set to ""all"""
download data if necessary
initialize comlumncorpus and add it to list
column format
this dataset name
column format
this dataset name
column format
this dataset name
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
this list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
get google drive id from list
download from google drive
unzip
"tar.extractall(language_folder,members=[tar.getmember(file_name)])"
transform data into required format
"the processed dataset has the additional ending ""_new"""
remove the unprocessed dataset
initialize comlumncorpus and add it to list
if no languages are given as argument all languages used in XTREME will be loaded
if only one language is given
column format
this dataset name
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
This list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
download from HU Server
unzip
transform data into required format
initialize comlumncorpus and add it to list
if only one language is given
column format
this dataset name
download data if necessary
initialize comlumncorpus and add it to list
download data if necessary
unpack and write out in CoNLL column-like format
column format
this dataset name
download data if necessary
data is not in IOB2 format. Thus we transform it to IOB2
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
column format
this dataset name
rename according to train - test - dev - convention
column format
this dataset name
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
Add missing newline after header
Workaround for empty tokens
"Add ""real"" document marker"
Dataset split mapping
v2.0 only adds new language and splits for AJMC dataset
Special document marker for sample splits in AJMC dataset
column format
this dataset name
download data if necessary
column format
this dataset name
this dataset name
default dataset folder is the cache root
download and parse data if necessary
paths to train and test splits
init corpus
this dataset name
default dataset folder is the cache root
download and parse data if necessary
iterate over all html files
"get rid of html syntax, we only need the text"
between all documents we write a separator symbol
skip empty strings
"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)"
"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention"
sentence splitting and tokenization
iterate through all annotations and add to corresponding tokens
find sentence to which annotation belongs
position within corresponding sentence
set annotation for tokens of entity mention
write to out-file in column format
"in case something goes wrong, delete the dataset and raise error"
this dataset name
download and parse data if necessary
from qwikidata.linked_data_interface import get_entity_dict_from_api
generate qid wikiname dictionaries
merge dictionaries
ignore first line
commented and empty lines
read all Q-IDs
ignore first line
request
this dataset name
we use the wikiids in the data instead of directly utilizing the wikipedia urls.
like this we can quickly check if the corresponding page exists
if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi
delete unprocessed file
collect all wikiids
create the dictionary
request
this dataset name
names of raw text documents
open output_file
iterate through all documents
split sentences and tokenize
iterate through all annotations and add to corresponding tokens
find sentence to which annotation belongs
position within corresponding sentence
set annotation for tokens of entity mention
write to out file
this dataset name
download and parse data if necessary
this dataset name
download and parse data if necessary
First parse the post titles
Keep track of how many and which entity mentions does a given post title have
Check if the current post title has an entity link and parse accordingly
Post titles with entity mentions (if any) are handled via this function
Then parse the comments
"Iterate over the comments.tsv file, until the end is reached"
"Keep track of the current comment thread and its corresponding key, on which the annotations are matched."
Each comment thread is handled as one 'document'.
Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.
This if-condition is needed to handle this problem.
"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure"
"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above"
"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle."
"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,"
and not just single letters into single rows.
If there are annotated entity mentions for given post title or a comment thread
"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence"
Write the token with a corresponding tag to file
"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed"
"If a comment thread or a post title has no entity link, all tokens are assigned the O tag"
Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized
"incorrectly, in order to keep the desired format (empty line as a sentence separator)."
"Thrown when the second check above happens, but the last token of a sentence is reached."
"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below."
"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS"
Check if further annotations belong to the current post title or comment thread as well
Stop when the end of an annotation file is reached
Check if further annotations belong to the current sentence as well
"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)"
Docstart
if there is more than one word in the chunk we write each in a separate line
print(chunks)
empty line after each sentence
convert the file to CoNLL
this dataset name
"check if data there, if not, download the data"
create folder
download data
transform data into column format if necessary
if no filenames are specified we use all the data
"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled"
also we remove 'raganato_ALL' from filenames in case its in the list
generate the test file
make column file and save to data_folder
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
this dataset name
download data if necessary
if True:
write CoNLL-U Plus header
"Some special cases (e.g., missing spaces before entity marker)"
necessary if text should be whitespace tokenizeable
Handle case where tail may occur before the head
this dataset name
write CoNLL-U Plus header
this dataset name
TODO: change data source to original CoNLL04 -- this dataset has span formatting errors
download data if necessary
write CoNLL-U Plus header
The span has ended.
We are entering a new span; reset indices
and active tag to new span.
We're inside a span.
Last token might have been a part of a valid span.
this dataset name
write CoNLL-U Plus header
"for source_file_path, target_filename in zip(source_file_paths, target_filenames):"
"with zip_file.open(source_file_path, mode=""r"") as source_file:"
target_file_path = Path(data_folder) / target_filename
"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:"
# write CoNLL-U Plus header
"target_file.write(""# global.columns = id form ner\n"")"
for example in json.load(source_file):
token_list = self._tacred_example_to_token_list(example)
target_file.write(token_list.serialize())
check if first tag row is already occupied
"if first tag row is occupied, use second tag row"
hardcoded mapping TODO: perhaps find nicer solution
remap regular tag names
else skip to position in file where sentence begins
set sentence context using partials TODO: pointer to dataset is really inefficient
read in dev file if exists
read in test file if exists
the url is copied from https://huggingface.co/datasets/darentang/sroie/blob/main/sroie.py#L44
"find train, dev and test files if not specified"
use test_file to create test split if available
use dev_file to create test split if available
"if data point contains black-listed label, do not use"
first check if valid sentence
"if so, add to indices"
"find train, dev and test files if not specified"
variables
different handling of in_memory data than streaming data
"most data sets have the token text in the first column, if not, pass 'text' as column"
test if format is OK
test if at least one label given
make sentence from text (and filter for length)
"if a pair column is defined, make a sentence pair object"
noinspection PyDefaultArgument
dataset name includes the split size
default dataset folder is the cache root
download data if necessary
download each of the 28 splits
create dataset directory if necessary
download senteval datasets if necessary und unzip
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
handle labels file
handle data file
Create flair compatible labels
"by defaut, map point score to POSITIVE / NEGATIVE values"
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file from CSV
create test.txt file from CSV
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create train dev and test files in fasttext format
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
convert to FastText format
download data if necessary
"if data is not downloaded yet, download it"
get the zip file
move original .tsv files to another folder
create train and dev splits in fasttext format
create eval_dataset file with no labels
this dataset name
default dataset folder is the cache root
download data if necessary
download datasets if necessary
create dataset directory if necessary
create correctly formated txt files
multiple labels are possible
this dataset name
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
download data if necessary
this dataset name
download data if necessary
create a separate directory for different tasks
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
check if dataset is supported
set file names
set file names
download and unzip in file structure if necessary
instantiate corpus
"find train, dev and test files if not specified"
"create DataPairDataset for train, test and dev file, if they are given"
stop if file does not exist
create a DataPair object from strings
"if in_memory is True we return a datapair, otherwise we create one from the lists of strings"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"reorder dev datasets to have same columns as in train set: 8, 9, and 11"
dev sets include 5 different annotations but we will only keep the gold label
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get test and dev sets
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data not downloaded yet, download it"
get the zip file
"the downloaded files have json format, we transform them to tsv"
Function to transform JSON file to tsv for Recognizing Textual Entailment Data
remove json file
Uses dynamic programming approach to calculate maximum independent set in interval graph
with sum of all entity lengths as secondary key
calculate offset without current text
because we stick all passages of a document together
TODO For split entities we also annotate everything inbetween which might be a bad idea?
Try to fix incorrect annotations
print(
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
)
Ignore empty lines or relation annotations
FIX annotation of whitespaces (necessary for PDR)
One token may contain multiple entities -> deque all of them
column format
this dataset name
Create tokenization-dependent CONLL files. This is necessary to prevent
from caching issues (e.g. loading the same corpus with different sentence splitters)
column format
this dataset name
column format
this dataset name
Edge case: last token starts a new entity
Last document in file
column format
this dataset name
column format
this dataset name
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
Edge case: last token starts a new entity
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
column format
this dataset name
Read texts
Read annotations
column format
this dataset name
column format
this dataset name
column format
this dataset name
We need to apply a patch to correct the original training file
Articles title
Article abstract
Entity annotations
column format
this dataset name
Edge case: last token starts a new entity
Map all entities to chemicals
Map all entities to disease
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
Incomplete article
Invalid XML syntax
column format
this dataset name
column format
this dataset name
if len(mid) != 3:
continue
Try to fix entity offsets
column format
this dataset name
There is still one illegal annotation in the file ..
column format
this dataset name
"Abstract first, title second to prevent issues with sentence splitting"
column format
this dataset name
column format
this dataset name
column format
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
column format
this dataset name
"Filter for specific entity types, by default no entities will be filtered"
Get original HUNER splits to retrieve a list of all document ids contained in V2
train and dev split of V2 will be train in V4
test split of V2 will be dev in V4
New documents in V4 will become test documents
column format
this dataset name
column format
this dataset name
default dataset folder is the cache root
cache Feidegger config file
cache Feidegger images
replace image URL with local cached file
append Sentence-Image data point
"in certain cases, multi-CPU data loading makes no sense and slows"
"everything down. For this reason, we detect if a dataset is in-memory:"
"if so, num_workers is set to 0 for faster processing"
cast to list if necessary
cast to list if necessary
"first, check if pymongo is installed"
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
Expose base classses
Expose all biomedical data sets used for the evaluation of BioBERT
-
-
-
-
Expose all biomedical data sets using the HUNER splits
Expose all biomedical data sets
Expose all document classification datasets
word sense disambiguation
Expose all entity linking datasets
Expose all relation extraction datasets
universal proposition banks
keyphrase detection datasets
other NER datasets
standard NER datasets
Expose all sequence labeling datasets
Expose all text-image datasets
Expose all text-text datasets
Expose all treebanks
"find train, dev and test files if not specified"
get train data
get test data
get dev data
option 1: read only sentence boundaries as offset positions
option 2: keep everything in memory
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
current token ID
handling for the awful UD multiword format
end of sentence
comments
ellipsis
if token is a multi-word
normal single-word tokens
"if we don't split multiwords, skip over component words"
add token
add morphological tags
derive whitespace logic for multiwords
print(token)
print(current_multiword_last_token)
print(current_multiword_first_token)
"if multi-word equals component tokens, there should be no whitespace"
go through all tokens in subword and set whitespace_after information
print(i)
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
this dataset name
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
"finally, print model card for information"
test corpus
create a TARS classifier
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
test corpus
create a TARS classifier
switch to a new task (TARS can do multiple tasks so you must define one)
initialize the text classifier trainer
start the training
"With end symbol, without start symbol, padding in front"
"Without end symbol, with start symbol, padding in back"
"Without end symbol, without start symbol, padding in front"
initialize trainer
initialize trainer
train model for 2 epochs
load the checkpoint model and train until epoch 4
clean up results directory
initialize trainer
initialize trainer
increment for last token in sentence if not followed by whitespace
clean up directory
clean up directory
example sentence
set 4 labels for 2 tokens ('love' is tagged twice)
check if there are three POS labels with correct text and values
check if there are is one SENTIMENT label with correct text and values
check if all tokens are correctly labeled
remove the pos label from the last word
there should be 2 POS labels left
now remove all pos tags
set 3 labels for 2 spans (HU is tagged twice)
check if there are three labels with correct text and values
check if there are two spans with correct text and values
"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
should be only one NER label left
and only one NER span
set 3 labels for 2 spans (HU is tagged twice with different tags)
check if there are three labels with correct text and values
check if there are two spans with correct text and values
"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
should be only one NER label left
and only one NER span
but there is also one orgtype span and label
and only one NER span
let's add the NER tag back
check if there are three labels with correct text and values
check if there are two spans with correct text and values
now remove all NER tags
set 3 labels for 2 spans (HU is tagged twice with different tags)
create two relation label
there should be two relation labels
there should be one syntactic labels
"there should be two relations, one with two and one with one label"
example sentence
add another topic label
example sentence
has sentiment value
has 4 part of speech tags
has 1 NER tag
should be in total 6 labels
example sentence
add two NER labels
get the four labels
check that only two of the respective data points are equal
make a sentence and some right context
TODO: is this desirable? Or should two sentences with same text still be considered different objects?
Initializing a Sentence this way assumes that there is a space after each token
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
define search space
sequence tagger parameter
model trainer parameter
training parameter
find best parameter settings
clean up results directory
document embeddings parameter
training parameter
clean up results directory
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
load column dataset with one entry
load column dataset with two entries
load column dataset with three entries
"get training, test and dev data"
"get training, test and dev data"
check if Token labels are correct
"get training, test and dev data"
check if Token labels for frames are correct
"get training, test and dev data"
"get training, test and dev data"
get two corpora as one
"get training, test and dev data for full English UD corpus from web"
clean up data directory
"assert [token.get_tag(""head"").value for token in sent1.tokens] == ["
"""2"","
"""0"","
"""4"","
"""2"","
"""2"","
"""2"","
]
"Here, we use the default token annotation fields."
"We have manually checked, that these numbers are correct:"
"+1 offset, because of missing EOS marker at EOD"
Test data for v2.1 release
--- Embeddings that are shared by both models --- #
--- Task 1: Sentiment Analysis (5-class) --- #
Define corpus and model
-- Task 2: Binary Sentiment Analysis on Customer Reviews -- #
Define corpus and model
-- Define mapping (which tagger should train on which model) -- #
-- Create model trainer and train -- #
load dataset
tagger without CRF
train
check if loaded model can predict
check if loaded model successfully fit the training data
load dataset
tagger without CRF
train
check if loaded model can predict
check if loaded model successfully fit the training data
load dataset
tagger without CRF
train
check if loaded model can predict
check if loaded model successfully fit the training data
load dataset
tagger without CRF
train
check if loaded model can predict
check if loaded model successfully fit the training data
check if model can predict
load model
chcek if model predicts correct label
check if loaded model successfully fit the training data
check if model can predict
load model
chcek if model predicts correct label
check if loaded model successfully fit the training data
check if model can predict
load model
chcek if model predicts correct label
check if loaded model successfully fit the training data
clean up file
no need for label_dict
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
Intel ----founded_by---> Gordon Moore
Intel ----founded_by---> Robert Noyce
Check sentence masking and relation label annotation on
"training, validation and test dataset (in this test the splits are the same)"
"Entity pair permutations of: ""Larry Page and Sergey Brin founded Google ."""
"Entity pair permutations of: ""Microsoft was founded by Bill Gates ."""
"Entity pair permutations of: ""Konrad Zuse was born in Berlin on 22 June 1910 ."""
"Entity pair permutations of: ""Joseph Weizenbaum , a professor at MIT , was born in Berlin , Germany."""
This sentence is only included if we transform the corpus with cross augmentation
Ensure this is an example that predicts no classes in multilabel
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
ensure that the prepared tensors is what we expect
use a SequenceTagger to save and reload the embedding in the manner it is supposed to work
previous and next sentence as context
test expansion for sentence without context
test expansion for with previous and next as context
test expansion if first sentence is document boundary
test expansion if we don't use context
"apparently the precision is not that high on cuda, hence the absolute tolerance needs to be higher."
dummy model with embeddings
save the dummy and load it again
check that context_length and use_context_separator is the same for both
mmap seems to be much more memory efficient
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
Extract all the contents of zip file in current directory
Extract all the contents of zip file in current directory
get cache path to put the file
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
first determine the distribution of classes in the dataset
weight for each sample
Create blocks
shuffle the blocks
concatenate the shuffled blocks
Create blocks
shuffle the blocks
concatenate the shuffled blocks
increment for last token in sentence if not followed by whitespace
this is the default init size of a lmdb database for embeddings
get db filename from embedding name
"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
SequenceTagger
TextClassifier
get db filename from embedding name
if embedding database already exists
"otherwise, push embedding to database"
if embedding database already exists
open the database in read mode
we need to set self.k
create and load the database in write mode
"no idea why, but we need to close and reopen the environment to avoid"
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
when opening new transaction !
init dictionaries
"in order to deal with unknown tokens, add <unk>"
set 'add_unk' if the dictionary was created with a version of Flair older than 0.9
set 'add_unk' depending on whether <unk> is a key
TODO: does it make sense to exclude labels? Two data points of identical text (but different labels)
would be equal now.
labels also need to be deleted at Sentence object
delete labels at object itself
private field for all known spans
the tokenizer used for this sentence
"if text is passed, instantiate sentence with tokens (words)"
determine token positions and whitespace_after flag
the last token has no whitespace after
log a warning if the dataset is empty
some sentences represent a document boundary (but most do not)
internal variables to denote position inside dataset
data with zero-width characters cannot be handled
set token idx and sentence
append token to sentence
register token annotations on sentence
move sentence embeddings to device
also move token embeddings to device
clear token embeddings
infer whitespace after field
No character at the corresponding code point: remove it
"if no label if specified, return all labels"
"if the label type exists in the Sentence, return it"
return empty list if none of the above
labels also need to be deleted at all tokens
labels also need to be deleted at all known spans
remove spans without labels
delete labels at object itself
set name
abort if no data is provided
sample test data from train if none is provided
sample dev data from train if none is provided
set train dev and test data
find out empty sentence indices
create subset of non-empty sentence indices
find out empty sentence indices
create subset of non-empty sentence indices
count all label types per sentence
go through all labels of label_type and count values
check if there are any span labels
"if an unk threshold is set, UNK all label values below this threshold"
Make the tag dictionary
global variable: cache_root
global variable: device
global variable: version
global variable: arrow symbol
dummy return to fulfill trainer.train() needs
print(vec)
Attach optimizer
"convert `metrics` to float, in case it's a zero-dim Tensor"
if memory mode option 'none' delete everything
"if dynamic embedding keys not passed, identify them automatically"
always delete dynamic embeddings
"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)"
optional metric space decoder if prototypes have different length than embedding
create initial prototypes for all classes (all initial prototypes are a vector of all 1s)
"if set, create initial prototypes from normal distribution"
"if set, use a radius"
all parameters will be pushed internally to the specified device
decode embeddings into prototype space
"if unlabeled distance is set, mask out loss to unlabeled class prototype"
Monkey-patching is problematic for mypy (https://github.com/python/mypy/issues/2427)
gradients are not required for prototype computation
reset prototypes for all classes
decode embeddings into prototype space
embeddings need to be removed so that memory doesn't fill up
TODO: changes required
"in Flair <0.9.1, optimizer and scheduler used to train model are not saved"
"write out a ""model card"" if one is set"
special handling for optimizer:
remember optimizer class and state dictionary
save model
restore optimizer and scheduler to model card if set
load_big_file is a workaround byhttps://github.com/highway11git
to load models on some Mac/Windows setups
see https://github.com/zalandoresearch/flair/issues/351
"read Dataset into data loader, if list of sentences passed, make Dataset first"
loss calculation
variables for printing
variables for computing scores
remove any previously predicted labels
predict for batch
get the gold labels
add to all_predicted_values
make printout lines
convert true and predicted values to two span-aligned lists
delete exluded labels if exclude_labels is given
"if after excluding labels, no label is left, ignore the datapoint"
write all_predicted_values to out_file if set
make the evaluation dictionary
check if this is a multi-label problem
compute numbers by formatting true and predicted such that Scikit-Learn can use them
multi-label problems require a multi-hot vector for each true and predicted label
single-label problems can do with a single index for each true and predicted label
"now, calculate evaluation numbers"
there is at least one gold label or one prediction (default)
"if there is only one label, then ""micro avg"" = ""macro avg"""
"micro average is only computed if zero-label exists (for instance ""O"")"
if no zero-label exists (such as in POS tagging) micro average is equal to accuracy
same for the main score
issue error and default all evaluation numbers to 0.
line for log file
check if there is a label mismatch
print info
initialize the label dictionary
initialize the decoder
set up multi-label logic
init dropouts
loss weights and loss function
Initialize the weight tensor
make a forward pass to produce embedded data points and labels
no loss can be calculated if there are no labels
use dropout
push embedded_data_points through decoder to get the scores
calculate the loss
filter empty sentences
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
if anything could possibly be predicted
remove previously predicted labels of this type
add DefaultClassifier arguments
add variables of DefaultClassifier
Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23
Get projected 1st dimension
Compute bilinear form
Arcosh
Project the input data to n+1 dimensions
"The first dimension, is recomputed in the distance module"
header for 'weights.txt'
"determine the column index of loss, f-score and accuracy for"
"train, dev and test split"
then get all relevant values from the tsv
then get all relevant values from the tsv
plot i
save plots
save plots
plt.show()
save plot
take the average over the last three scores of training
take average over the scores from the different training runs
auto-spawn on GPU if available
progress bar for verbosity
stop if all sentences are empty
clearing token embeddings to save memory
"read Dataset into data loader, if list of sentences passed, make Dataset first"
TODO: not saving lines yet
== similarity measures ==
helper class for ModelSimilarity
-- works with binary cross entropy loss --
"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}"
-- works with ranking/triplet loss --
normalize the embeddings
== similarity losses ==
"we want that logits for corresponding pairs are high, and for non-corresponding low"
TODO: this assumes eye matrix
"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa"
== similarity learner ==
"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both"
assumes that for each data pair there's at least one embedding per modality
pre-compute embeddings for all targets in evaluation dataset
compute the similarity
sort the similarity matrix across modality 1
"get the ranks, so +1 to start counting ranks from 1"
The conversion from old model's constructor interface
auto-spawn on GPU if available
pad strings with whitespaces to longest sentence
cut up the input into chunks of max charlength = chunk_size
push each chunk through the RNN language model
concatenate all chunks to make final output
initial hidden state
get predicted weights
divide by temperature
"to prevent overflow problem with small temperature values, substract largest value from all"
this makes a vector in which the largest value is 0
compute word weights with exponential function
try sampling multinomial distribution for next character
print(word_idx)
input ids
push list of character IDs through model
the target is always the next character
use cross entropy loss to compare output of forward pass with targets
exponentiate cross-entropy loss to calculate perplexity
"""document_delimiter"" property may be missing in some older pre-trained models"
serialize the language models and the constructor arguments (but nothing else)
special handling for deserializing language models
re-initialize language model with constructor arguments
copy over state dictionary to self
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
"in their ""self.train()"" method)"
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
Transform input data into TARS format
"if there are no labels, return a random sample as negatives"
"otherwise, go through all labels"
make sure the probabilities always sum up to 1
get and embed all labels by making a Sentence object that contains only the label text
get each label embedding and scale between 0 and 1
compute similarity matrix
"the higher the similarity, the greater the chance that a label is"
sampled as negative example
make label dictionary if no Dictionary object is passed
prepare dictionary of tags (without B- I- prefixes and without UNK)
check if candidate_label_set is empty
make list if only one candidate label is passed
create label dictionary
note current task
create a temporary task
make zero shot predictions
switch to the pre-existing task
prepare TARS dictionary
initialize a bare-bones sequence tagger
transformer separator
Store task specific labels since TARS can handle multiple tasks
make a tars sentence where all labels are O by default
init new TARS classifier
set all task information
return
with torch.no_grad():
progress bar for verbosity
stop if all sentences are empty
go through each sentence in the batch
always remove tags first
get the span and its label
"label = span.get_labels(""tars_temp_label"")[0].value"
determine whether tokens in this span already have a label
only add if all tokens have no label
clearing token embeddings to save memory
"all labels default to ""O"""
set gold token-level
set predicted token-level
now print labels in CoNLL format
prepare TARS dictionary
initialize a bare-bones sequence tagger
transformer separator
Store task specific labels since TARS can handle multiple tasks
init new TARS classifier
set all task information
with torch.no_grad():
set context if not set already
progress bar for verbosity
stop if all sentences are empty
go through each sentence in the batch
always remove tags first
add all labels that according to TARS match the text and are above threshold
do not add labels below confidence threshold
only use label with highest confidence if enforcing single-label predictions
get all label scores and do an argmax to get the best label
remove previously added labels and only add the best label
clearing token embeddings to save memory
set separator to concatenate two sentences
auto-spawn on GPU if available
pooling operation to get embeddings for entites
set embeddings
set relation and entity label types
"whether to use gold entity pairs, and whether to filter entity pairs by type"
super lame: make dictionary to find relation annotations for a given entity pair
get all entity spans
"go through cross product of entities, for each pair concat embeddings"
filter entity pairs according to their tags if set
get gold label for this relation (if one exists)
"if there is no gold label for this entity pair, set to 'O' (no relation)"
"if predicting, also remember sentences and label candidates"
if there's at least one entity pair in the sentence
embed sentences and get embeddings for each entity pair
get embeddings
whether to encode characters and whether to use attention (attention can only be used if chars are encoded)
character dictionary for decoding and encoding
make sure <unk> is in dictionary for handling of unknown characters
add special symbols to dictionary if necessary and save respective indices
---- ENCODER ----
encoder character embeddings
encoder pre-trained embeddings
encoder RNN
additional encoder linear layer if bidirectional encoding
---- DECODER ----
decoder: linear layers to transform vectors to and from alphabet_size
when using attention we concatenate attention outcome and decoder hidden states
decoder RNN
loss and softmax
self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction
add additional columns for special symbols if necessary
initialize with dummy symbols
encode inputs
get labels (we assume each token has a lemma label)
get char indices for labels of sentence
"(batch_size, max_sequence_length) batch_size = #words in sentence,"
max_sequence_length = length of longest label of sentence + 1
get char embeddings
"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size"
take decoder input and initial hidden and pass through RNN
"if all encoder outputs are provided, use attention"
take convex combinations of encoder hidden states as new output using the computed attention coefficients
"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)"
get all tokens
variable to store initial hidden states for decoder
encode input characters by sending them through RNN
get one-hots for characters and add special symbols / padding
determine length of each token
embed character one-hots
test packing and padding
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
concatenate the final hidden states of the encoder. These will be projected to hidden_size of
decoder later with self.emb_to_hidden
mask out vectors that correspond to a dummy symbol (TODO: check attention masking)
use token embedding as initial hidden state for decoder
embed sentences
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
concatenate everything together and project to appropriate size for decoder
variable to store initial hidden states for decoder
encode input characters by sending them through RNN
note that we do not need to fill up with dummy symbols since we process each token seperately
embed character one-hots
send through encoder RNN (produces initial hidden for decoder)
since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
project 2*hidden_size to hidden_size
concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder
later with self.emb_to_hidden
use token embedding as initial hidden state for decoder
"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
concatenate everything together and project to appropriate size for decoder
"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)"
"create target vector (batch_size, max_label_seq_length + 1)"
filter empty sentences
max length of the predicted sequences
for printing
stop if all sentences are empty
remove previously predicted labels of this type
create list of tokens in batch
encode inputs
"create input for first pass (batch_size, 1, input_size), first letter is special character <S>"
sequence length is always set to one in prediction
option 1: greedy decoding
predictions
decode next character
pick top beam size many outputs with highest probabilities
option 2: beam search
out_probs = self.softmax(output_vectors).squeeze(1)
make sure no dummy symbol <> or start symbol <S> is predicted
pick top beam size many outputs with highest probabilities
"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1"
"leading_indices and probabilities have size (batch_size, beam_size)"
keep scores of beam_size many hypothesis for each token in the batch
stack all leading indices of all hypothesis and corresponding hidden states in two tensors
save sequences so far
keep track of how many hypothesis were completed for each token
"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)"
decode with log softmax
make sure no dummy symbol <> or start symbol <S> is predicted
"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside"
"if the sequence is already ended, do not record as candidate"
index of token in in list tokens_in_batch
print(token_number)
hypothesis score
TODO: remove token if number of completed hypothesis exceeds given value
set score of corresponding entry to -inf so it will not be expanded
get leading_indices for next expansion
find highest scoring hypothesis among beam_size*beam_size possible ones for each token
take beam_size many copies of scores vector and add scores of possible new extensions
"size (beam_size*batch_size, beam_size)"
print(hypothesis_scores)
"reshape to vector of size (batch_size, beam_size*beam_size),"
each row contains beam_size*beam_size scores of the new possible hypothesis
print(hypothesis_scores_per_token)
"choose beam_size best for each token - size (batch_size, beam_size)"
out of indices_per_token we now need to recompute the original indices of the hypothesis in
a list of length beam_size*batch_size
"where the first three inidices belong to the first token, the next three to the second token,"
and so on
with these indices we can compute the tensors for the next iteration
expand sequences with corresponding index
add log-probabilities to the scores
save new leading indices
save corresponding hidden states
it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations
in that case we append one of the final seuqences without end symbol to the final_candidates
get best final hypothesis for each token
get characters from index sequences and add predicted label to token
embeddings
dictionaries
all parameters will be pushed internally to the specified device
get all tokens in this mini-batch
now print labels in CoNLL format
filter sentences with no candidates (no candidates means nothing can be linked anyway)
fields to return
embed sentences and send through prediction head
embed all tokens
get the embeddings of the entity mentions
get the label of the entity
if there is no RNN
embed sentences
"Main model implementation drops words and tags (independently), instead, we use word dropout!"
apply MLPs for arc and relations to the BiLSTM output states
get scores from the biaffine attentions
"[batch_size, seq_len, seq_len]"
"[batch_size, seq_len, seq_len, n_rels]"
append both to file for evaluation
"if head AND deprel correct, augment correct_rels score"
----- Create the internal tag dictionary -----
span-labels need special encoding (BIO or BIOES)
the big question is whether the label dictionary should contain an UNK or not
"without UNK, we cannot evaluate on data that contains labels not seen in test"
"with UNK, the model learns less well if there are no UNK examples"
is this a span prediction problem?
----- Embeddings -----
----- Initial loss weights parameters -----
----- RNN specific parameters -----
----- Conditional Random Field parameters -----
"Previously trained models have been trained without an explicit CRF, thus it is required to check"
whether we are loading a model from state dict in order to skip or add START and STOP token
----- Dropout parameters -----
dropouts
----- Model layers -----
----- RNN layer -----
"If shared RNN provided, else create one for model"
Whether to train initial hidden state
final linear map to tag space
"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss"
"if using CRF, we also require a CRF and a Viterbi decoder"
"if there are no sentences, there is no loss"
forward pass to get scores
calculate loss given scores and labels
make a zero-padded tensor for the whole sentence
sort tensor in decreasing order based on lengths of sentences in batch
----- Forward Propagation -----
linear map to tag space
"Depending on whether we are using CRF or a linear layer, scores is either:"
"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF"
"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer"
get the gold labels
spans need to be encoded as token-level predictions
all others are regular labels for each token
make sure its a list
filter empty sentences
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
get features from forward propagation
remove previously predicted labels of this type
"if return_loss, get loss value"
Sort batch in same way as forward propagation
make predictions
add predictions to Sentence
BIOES-labels need to be converted to spans
"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)"
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
core Flair models on Huggingface ModelHub
"Large NER models,"
Multilingual NER models
English POS models
Multilingual POS models
English SRL models
English chunking models
Language-specific NER models
English NER models
Multilingual NER models
English POS models
Multilingual POS models
English SRL models
English chunking models
Danish models
German models
French models
Dutch models
Malayalam models
Portuguese models
Keyphase models
Biomedical models
check if model name is a valid local file
"check if model key is remapped to HF key - if so, print out information"
get mapped name
use mapped name instead
"if not, check if model key is remapped to direct download location. If so, download model"
special handling for the taggers by the @redewiegergabe project (TODO: move to model hub)
"for all other cases (not local file or special download location), use HF model hub"
"if not a local file, get from model hub"
use model name as subfolder
Lazy import
output information
"log.error(f"" - Error message: {e}"")"
"all labels default to ""O"""
set gold token-level
set predicted token-level
now print labels in CoNLL format
print labels in CoNLL format
clear embeddings after predicting
load each model
check if the same embeddings were already loaded previously
"if the model uses StackedEmbedding, make a new stack with previous objects"
sort embeddings by key alphabetically
check previous embeddings and add if found
only re-use static embeddings
"if not found, use existing embedding"
initialize new stack
"of the model uses regular embedding, re-load if previous version found"
auto-spawn on GPU if available
embed sentences
make tensor for all embedded sentences in batch
English sentiment models
Communicative Functions Model
"scores_at_targets[range(features.shape[0]), lengths.values -1]"
Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices
"Initially, get scores from <start> tag to all other tags"
"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp"
"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep"
Create a tensor to hold accumulated sequence scores at each current tag
Create a tensor to hold back-pointers
"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag"
"Let pads be the <end> tag index, since that was the last tag in the decoded sequence"
"We add scores at current timestep to scores accumulated up to previous timestep, and"
choose the previous timestep that corresponds to the max. accumulated score for each current timestep
"If sentence is over, add transition to STOP-tag"
Decode/trace best path backwards
Sanity check
remove start-tag and backscore to stop-tag
Max + Softmax to get confidence score for predicted label and append label to each token
"add a dummy ""O"" to close final prediction"
return complex list
internal variables
non-set tags are OUT tags
anything that is not OUT is IN
does this prediction start a new span?
begin and single tags start new spans
"in IOB format, an I tag starts a span if it follows an O or is a different span"
single tags that change prediction start new spans
if an existing span is ended (either by reaching O or starting a new span)
determine score and value
append to result list
reset for-loop variables for new span
remember previous tag
"Transitions are used in the following way: transitions[to, from]."
"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag"
to START-tag and from STOP-tag to any other tag to -10000.
weights for loss function
iput size is two times wordembedding size since we use pair of words as input
"the output size is max_distance + 1, i.e. we allow 0,1,...,max_distance words between pairs"
regression
input size is two times word embedding size since we use pair of words as input
the output size is 1
auto-spawn on GPU if available
forward allows only a single sentcence!!
embed words of sentence
go through all pairs of words with a maximum number of max_distance in between
go through all pairs
2-dim matrix whose rows are the embeddings of word pairs of the sentence
So far only one sentence allowed
If list of sentences is handed the function works with the first sentence of the list
Assume data_points is a single sentence!!!
scores are the predictions for each word pair
"classification needs labels to be integers, regression needs labels to be float"
this is due to the different loss functions
only single sentences as input
gold labels
for output text file
for buckets
for average prediction
add some statistics to the output
use scikit-learn to evaluate
"we iterate over each sentence, instead of batches"
get single labels from scores
gold labels
for output text file
hot one vector of true value
hot one vector of predicted value
"speichert embeddings, falls embedding_storage!= 'None'"
"make ""classification report"""
get scores
"precision_score = round(metrics.precision_score(y_true, y_pred, average='macro', zero_division=0), 4)"
"recall_score = round(metrics.recall_score(y_true, y_pred, average='macro', zero_division=0), 4)"
line for log file
create a model card for this model with Flair and PyTorch version
also record Transformers version if library is loaded
remember all parameters used in train() call
add model card to model
"if optimizer is class, trainer will create a single parameter group"
"determine what splits (train, dev, test) to evaluate and log"
prepare loss logging file and set up header
"if optimizer class is passed, instantiate:"
"from here on, use list of learning rates"
load existing optimizer state dictionary if it exists
"minimize training loss if training with dev data, else maximize dev score"
"if scheduler is passed as a class, instantiate"
"if we load a checkpoint, we have already trained for epoch"
load existing scheduler state dictionary if it exists
update optimizer and scheduler in model card
"if training also uses dev/train data, include in training set"
initialize sampler if provided
init with default values if only class is provided
set dataset to sample from
this field stores the names of all dynamic embeddings in the model (determined after first forward pass)
At any point you can hit Ctrl + C to break out of training early.
update epoch in model card
get new learning rate
reload last best model if annealing with restarts is enabled
stop training if learning rate becomes too small
process mini-batches
zero the gradients on the model and optimizer
"if necessary, make batch_steps"
forward and backward for batch
forward pass
Backward
identify dynamic embeddings (always deleted) on first sentence
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
do the optimizer step
do the scheduler step if one-cycle or linear decay
get new learning rate
evaluate on train / dev / test split depending on training settings
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
calculate scores using dev data if available
append dev score to score history
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
determine if this is the best model or if we need to anneal
default mode: anneal against dev score
alternative: anneal against dev loss
alternative: anneal against train loss
determine bad epoch number
lr unchanged
log bad epochs
output log file
make headers on first epoch
"if checkpoint is enabled, save model at each epoch"
Check whether to save best model
"if we do not use dev data for model selection, save final model"
test best model if test data is present
recover all arguments that were used to train this model
you can overwrite params with your own
surface nested arguments
resume training with these parameters
"if we are training over multiple datasets, do evaluation for each"
get and return the final test score of best model
cast string to Path
forward pass
update optimizer and scheduler
"TextDataset returns a list. valid and test are only one file,"
so return the first element
cast string to Path
error message if the validation dataset is too small
Shuffle training files randomly after serially iterating
through corpus one
"iterate through training data, starting at"
self.split (for checkpointing)
off by one for printing
go into train mode
reset variables
not really sure what this does
do the forward pass in the model
try to predict the targets
Backward
`clip_grad_norm` helps prevent the exploding gradient
problem in RNNs / LSTMs.
We detach the hidden state from how it was
previously produced.
"If we didn't, the model would try backpropagating"
all the way to start of the dataset.
explicitly remove loss to clear up memory
#########################################################
Save the model if the validation loss is the best we've
seen so far.
#########################################################
print info
#########################################################
##############################################################################
final testing
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
"multilingual forward (English, German, French, Italian, Dutch, Polish)"
"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
Dutch forward
Dutch backward
Swedish forward
Swedish backward
French forward
French backward
Czech forward
Czech backward
Portuguese forward
Portuguese backward
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
1-camembert-base -> camembert-base
1-xlm-roberta-large -> xlm-roberta-large
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
The mask has 1 for real tokens and 0 for padding tokens. Only real
tokens are attended to.
Zero-pad up to the sequence length.
"first, find longest sentence in batch"
prepare id maps for BERT model
put encoded batch through BERT model to get all hidden states of all encoder layers
get aggregated embeddings for each BERT-subtoken in sentence
get the current sentence object
add concatenated embedding to sentence
use first subword embedding if pooling operation is 'first'
"otherwise, do a mean over all subwords in token"
"if only one sentence is passed, convert to list of sentence"
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
embed a dummy sentence to determine embedding_length
Avoid conflicts with flair's Token class
"<cls> token initially set to 1/D, so it attends to all image features equally"
add positional encodings
reshape the pixels into the sequence
layer norm after convolution and positional encodings
add <cls> token
"transformer requires input in the shape [h*w+1, b, d]"
the output is an embedding of <cls> token
this parameter is fixed
optional fine-tuning on top of embedding layer
"if only one sentence is passed, convert to list of sentence"
"if only one sentence is passed, convert to list of sentence"
bidirectional RNN on top of embedding layer
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push through RNN
after-RNN dropout
extract embeddings from RNN
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
serialize the language models and the constructor arguments (but nothing else)
special handling for deserializing language models
re-initialize language model with constructor arguments
copy over state dictionary to self
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
"in their ""self.train()"" method)"
IMPORTANT: add embeddings as torch modules
iterate over sentences
"if its a forward LM, take last state"
"convert to plain strings, embedded in a list for the encode function"
CNN
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push CNN
after-CNN dropout
extract embeddings from CNN
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
"if only one sentence is passed, convert to list of sentence"
temporary fix to disable tokenizer parallelism warning
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
do not print transformer warnings as these are confusing in this case
load tokenizer and transformer model
load tokenizer from inmemory zip-file
model name
embedding parameters
send mini-token through to check how many layers the model has
return length
check if special tokens exist to circumvent error message
"most models have an initial BOS token, except for XLNet, T5 and GPT2"
"when initializing, embeddings are in eval mode by default"
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
in case of doubt: token embedding has higher priority than document embedding
in case of doubt: token embedding has higher priority than document embedding
"transformers returns the ""added_tokens.json"" even if it doesn't create it"
remove special markup
legacy TransformerDocumentEmbedding
legacy TransformerTokenEmbedding
legacy Flair <= 0.7
legacy TransformerTokenEmbedding
Legacy TransformerDocumentEmbedding
legacy TransformerTokenEmbedding
legacy TransformerDocumentEmbedding
copy values from new embedding
iterate over subtokens and reconstruct tokens
remove special markup
TODO check if this is necessary is this method is called before prepare_for_model
check if reconstructed token is special begin token ([CLS] or similar)
some BERT tokenizers somehow omit words - in such cases skip to next token
append subtoken to reconstruct token
check if reconstructed token is the same as current token
"if so, add subtoken count"
reset subtoken count and reconstructed token
break from loop if all tokens are accounted for
if tokens are unaccounted for
check if all tokens were matched to subtokens
subtokenize the sentence
transformer specific tokenization
set zero embeddings for empty sentences and exclude
determine into how many subtokens each token is split
remember tokenized sentences and their subtokenization
Models such as FNet do not have an attention_mask
set language IDs for XLM-style transformers
cls first pooling can be done without recreating sentence hidden states
encode inputs
make the tuple a tensor; makes working with it easier.
only use layers that will be outputted
remove padding tokens
set context if not set already
create expanded sentence and remember context offsets
move embeddings from context back to original sentence (if using context)
Expose base classses
Expose document embedding classes
Expose image embedding classes
Expose legacy embedding classes
Expose token embedding classes
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
GLOVE embeddings
TURIAN embeddings
KOMNINOS embeddings
pubmed embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
twitter embeddings
two-letter language code wiki embeddings
two-letter language code wiki embeddings
two-letter language code crawl embeddings
gensim version 4
gensim version 3
fix serialized models
"this is required to force the module on the cpu,"
"if a parent module is put to gpu, the _apply is called to each sub_module"
self.to(..) actually sets the device properly
this ignores the get_cached_vec method when loading older versions
it is needed for compatibility reasons
gensim version 4
gensim version 3
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
multilingual models
English models
Arabic
Bulgarian
Czech
Danish
German
Spanish
Basque
Persian
Finnish
French
Hebrew
Hindi
Croatian
Indonesian
Italian
Japanese
Malayalam
Dutch
Norwegian
Polish
Portuguese
Pubmed
Slovenian
Swedish
Tamil
Spanish clinical
CLEF HIPE Shared task
Amharic
load model if in pretrained model map
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
CLEF HIPE models are lowercased
embeddings are static if we don't do finetuning
embed a dummy sentence to determine embedding_length
set to eval mode
make compatible with serialized models (TODO: remove)
"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout"
make compatible with serialized models (TODO: remove)
gradients are enable if fine-tuning is enabled
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
offset mode that extracts at whitespace after last character
offset mode that extracts at last character
use the character language model embeddings as basis
length is twice the original character LM embedding length
these fields are for the embedding memory
whether to add only capitalized words to memory (faster runtime and lower memory consumption)
we re-compute embeddings dynamically at each epoch
set the memory method
memory is wiped each time we do a training run
"if we keep a pooling, it needs to be updated continuously"
update embedding
check token.text is empty or not
set aggregation operation
add embeddings after updating
this parameter is fixed
model architecture
model architecture
"""pl"","
download if necessary
load the model
"TODO: keep for backwards compatibility, but remove in future"
save the sentence piece model as binary file (not as path which may change)
write out the binary sentence piece model into the expected directory
"if the model was saved as binary and it is not found on disk, write to appropriate path"
"otherwise, use normal process and potentially trigger another download"
"once the modes if there, load it with sentence piece"
empty words get no embedding
all other words get embedded
"the default model for ELMo is the 'original' model, which is very large"
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
put on Cuda if available
embed a dummy sentence to determine embedding_length
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
GLOVE embeddings
read in test file if exists
read in dev file if exists
"find train, dev and test files if not specified"
Add tags for each annotated span
Tag all other token as Outer (O)
Remove leading and trailing whitespaces from annotated spans
Search start and end token index for current span
If end index is not found set to last token
Throw error if indices are not valid
Add IOB tags
get train data
read in test file if exists
read in dev file if exists
"find train, dev and test files if not specified"
special key for space after
special key for feature columns
special key for dependency head id
"store either Sentence objects in memory, or only file offsets"
"most data sets have the token text in the first column, if not, pass 'text' as column"
determine encoding of text file
identify which columns are spans and which are word-level
now load all sentences
skip first line if to selected
option 1: keep Sentence objects in memory
pointer to previous
parse next sentence
quit if last sentence reached
skip banned sentences
set previous and next sentence for context
append parsed sentence to list in memory
option 2: keep source data in memory
"read lines for next sentence, but don't parse"
quit if last sentence reached
append raw lines for each sentence
we make a distinction between word-level tags and span-level tags
read first sentence to determine which columns are span-labels
skip first line if to selected
"sentence_2 = self._convert_lines_to_sentence(self._read_next_sentence(file),"
word_level_tag_columns=column_name_map)
go through all annotations
the space after key is always word-levels
for column in self.word_level_tag_columns:
"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")"
"if sentence ends, break"
parse comments if possible
"otherwise, this line is a token. parse and add to sentence"
check if this sentence is a document boundary
add span labels
parse relations if they are set
head and tail span indices are 1-indexed and end index is inclusive
get fields from line
get head_id if exists (only in dependency parses)
initialize token
go through all columns
'feats' and 'misc' column should be split into different fields
special handling for whitespace after
add each other feature as label-value pair
get the task name (e.g. 'ner')
get the label value
add label
remap regular tag names
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
set sentence context using partials TODO: pointer to dataset is really inefficient
this dataset name
check if data there
column format
this dataset name
check if data there
column format
this dataset name
download data if necessary
download files if not present locally
we need to slightly modify the original files by adding some new lines after document separators
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
Remove CoNLL-U meta information in the last column
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
entity_mapping
this dataset name
download data if necessary
data validation
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
download files if not present locallys
column format
this dataset name
default dataset folder is the cache root
# download zip
merge the files in one as the zip is containing multiples files
column format
this dataset name
download data if necessary
"unzip the downloaded repo and merge the train, dev and test datasets"
column format
this dataset name
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
check if data there
create folder
download dataset
column format
this dataset name
download and parse data if necessary
create train test dev if not exist
column format
this dataset name
If the extracted corpus file is not yet present in dir
download zip if necessary
"extracted corpus is not present , so unpacking it."
column format
this dataset name
download zip
unpacking the zip
merge the files in one as the zip is containing multiples files
column format
this dataset name
"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)"
download files if not present locally
we need to modify the original files by adding new lines after after the end of each sentence
if only one language is given
column format
this dataset name
"use all languages if explicitly set to ""all"""
download data if necessary
initialize comlumncorpus and add it to list
column format
this dataset name
check if data there
code-switch uses the same training data than multi but provides a different test set.
"as the test set is not published, those two tasks are the same."
column format
this dataset name
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
this list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
get google drive id from list
download from google drive
unzip
"tar.extractall(language_folder,members=[tar.getmember(file_name)])"
transform data into required format
"the processed dataset has the additional ending ""_new"""
remove the unprocessed dataset
initialize comlumncorpus and add it to list
if no languages are given as argument all languages used in XTREME will be loaded
if only one language is given
column format
this dataset name
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
This list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
download from HU Server
unzip
transform data into required format
initialize comlumncorpus and add it to list
if only one language is given
column format
this dataset name
download data if necessary
initialize comlumncorpus and add it to list
download data if necessary
unpack and write out in CoNLL column-like format
column format
this dataset name
download data if necessary
data is not in IOB2 format. Thus we transform it to IOB2
column format
this dataset name
download data if necessary
column format
this dataset name
column format
this dataset name
rename according to train - test - dev - convention
column format
this dataset name
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
column format
this dataset name
download data if necessary
Add missing newline after header
Workaround for empty tokens
"Add ""real"" document marker"
Dataset split mapping
v2.0 only adds new language and splits for AJMC dataset
Special document marker for sample splits in AJMC dataset
this dataset name
default dataset folder is the cache root
download and parse data if necessary
iterate over all html files
"get rid of html syntax, we only need the text"
between all documents we write a separator symbol
skip empty strings
"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)"
"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention"
sentence splitting and tokenization
iterate through all annotations and add to corresponding tokens
find sentence to which annotation belongs
position within corresponding sentence
set annotation for tokens of entity mention
write to out-file in column format
"in case something goes wrong, delete the dataset and raise error"
this dataset name
download and parse data if necessary
from qwikidata.linked_data_interface import get_entity_dict_from_api
generate qid wikiname dictionaries
merge dictionaries
ignore first line
commented and empty lines
read all Q-IDs
ignore first line
request
this dataset name
we use the wikiids in the data instead of directly utilizing the wikipedia urls.
like this we can quickly check if the corresponding page exists
if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi
delete unprocessed file
collect all wikiids
create the dictionary
request
this dataset name
names of raw text documents
open output_file
iterate through all documents
split sentences and tokenize
iterate through all annotations and add to corresponding tokens
find sentence to which annotation belongs
position within corresponding sentence
set annotation for tokens of entity mention
write to out file
this dataset name
download and parse data if necessary
this dataset name
download and parse data if necessary
First parse the post titles
Keep track of how many and which entity mentions does a given post title have
Check if the current post title has an entity link and parse accordingly
Post titles with entity mentions (if any) are handled via this function
Then parse the comments
"Iterate over the comments.tsv file, until the end is reached"
"Keep track of the current comment thread and its corresponding key, on which the annotations are matched."
Each comment thread is handled as one 'document'.
Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.
This if-condition is needed to handle this problem.
"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure"
"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above"
"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle."
"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,"
and not just single letters into single rows.
If there are annotated entity mentions for given post title or a comment thread
"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence"
Write the token with a corresponding tag to file
"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed"
"If a comment thread or a post title has no entity link, all tokens are assigned the O tag"
Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized
"incorrectly, in order to keep the desired format (empty line as a sentence separator)."
"Thrown when the second check above happens, but the last token of a sentence is reached."
"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below."
"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS"
Check if further annotations belong to the current post title or comment thread as well
Stop when the end of an annotation file is reached
Check if further annotations belong to the current sentence as well
"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)"
Docstart
if there is more than one word in the chunk we write each in a separate line
print(chunks)
empty line after each sentence
convert the file to CoNLL
this dataset name
"check if data there, if not, download the data"
create folder
download data
transform data into column format if necessary
if no filenames are specified we use all the data
"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled"
also we remove 'raganato_ALL' from filenames in case its in the list
generate the test file
make column file and save to data_folder
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
this dataset name
download data if necessary
if True:
write CoNLL-U Plus header
"Some special cases (e.g., missing spaces before entity marker)"
necessary if text should be whitespace tokenizeable
Handle case where tail may occur before the head
this dataset name
write CoNLL-U Plus header
this dataset name
TODO: change data source to original CoNLL04 -- this dataset has span formatting errors
download data if necessary
write CoNLL-U Plus header
The span has ended.
We are entering a new span; reset indices
and active tag to new span.
We're inside a span.
Last token might have been a part of a valid span.
this dataset name
write CoNLL-U Plus header
"for source_file_path, target_filename in zip(source_file_paths, target_filenames):"
"with zip_file.open(source_file_path, mode=""r"") as source_file:"
target_file_path = Path(data_folder) / target_filename
"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:"
# write CoNLL-U Plus header
"target_file.write(""# global.columns = id form ner\n"")"
for example in json.load(source_file):
token_list = self._tacred_example_to_token_list(example)
target_file.write(token_list.serialize())
check if first tag row is already occupied
"if first tag row is occupied, use second tag row"
hardcoded mapping TODO: perhaps find nicer solution
"find train, dev and test files if not specified"
use test_file to create test split if available
use dev_file to create test split if available
"if data point contains black-listed label, do not use"
first check if valid sentence
"if so, add to indices"
"find train, dev and test files if not specified"
variables
different handling of in_memory data than streaming data
"most data sets have the token text in the first column, if not, pass 'text' as column"
test if format is OK
test if at least one label given
make sentence from text (and filter for length)
"if a pair column is defined, make a sentence pair object"
noinspection PyDefaultArgument
dataset name includes the split size
default dataset folder is the cache root
download data if necessary
download each of the 28 splits
create dataset directory if necessary
download senteval datasets if necessary und unzip
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
handle labels file
handle data file
Create flair compatible labels
"by defaut, map point score to POSITIVE / NEGATIVE values"
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file from CSV
create test.txt file from CSV
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create train dev and test files in fasttext format
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
convert to FastText format
download data if necessary
"if data is not downloaded yet, download it"
get the zip file
move original .tsv files to another folder
create train and dev splits in fasttext format
create eval_dataset file with no labels
this dataset name
default dataset folder is the cache root
download data if necessary
download datasets if necessary
create dataset directory if necessary
create correctly formated txt files
multiple labels are possible
this dataset name
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
download data if necessary
this dataset name
download data if necessary
create a separate directory for different tasks
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
check if dataset is supported
set file names
set file names
download and unzip in file structure if necessary
instantiate corpus
"find train, dev and test files if not specified"
"create DataPairDataset for train, test and dev file, if they are given"
stop if file does not exist
create a DataPair object from strings
"if in_memory is True we return a datapair, otherwise we create one from the lists of strings"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"reorder dev datasets to have same columns as in train set: 8, 9, and 11"
dev sets include 5 different annotations but we will only keep the gold label
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get test and dev sets
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
"if data not downloaded yet, download it"
get the zip file
"the downloaded files have json format, we transform them to tsv"
Function to transform JSON file to tsv for Recognizing Textual Entailment Data
remove json file
Uses dynamic programming approach to calculate maximum independent set in interval graph
with sum of all entity lengths as secondary key
calculate offset without current text
because we stick all passages of a document together
TODO For split entities we also annotate everything inbetween which might be a bad idea?
Try to fix incorrect annotations
print(
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
)
Ignore empty lines or relation annotations
FIX annotation of whitespaces (necessary for PDR)
One token may contain multiple entities -> deque all of them
column format
this dataset name
Create tokenization-dependent CONLL files. This is necessary to prevent
from caching issues (e.g. loading the same corpus with different sentence splitters)
column format
this dataset name
column format
this dataset name
Edge case: last token starts a new entity
Last document in file
column format
this dataset name
column format
this dataset name
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
Edge case: last token starts a new entity
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
column format
this dataset name
Read texts
Read annotations
column format
this dataset name
column format
this dataset name
column format
this dataset name
We need to apply a patch to correct the original training file
Articles title
Article abstract
Entity annotations
column format
this dataset name
Edge case: last token starts a new entity
Map all entities to chemicals
Map all entities to disease
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
column format
this dataset name
Incomplete article
Invalid XML syntax
column format
this dataset name
column format
this dataset name
if len(mid) != 3:
continue
Try to fix entity offsets
column format
this dataset name
There is still one illegal annotation in the file ..
column format
this dataset name
"Abstract first, title second to prevent issues with sentence splitting"
column format
this dataset name
column format
this dataset name
column format
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
this dataset name
column format
this dataset name
"Filter for specific entity types, by default no entities will be filtered"
Get original HUNER splits to retrieve a list of all document ids contained in V2
train and dev split of V2 will be train in V4
test split of V2 will be dev in V4
New documents in V4 will become test documents
column format
this dataset name
column format
this dataset name
default dataset folder is the cache root
cache Feidegger config file
cache Feidegger images
replace image URL with local cached file
append Sentence-Image data point
"in certain cases, multi-CPU data loading makes no sense and slows"
"everything down. For this reason, we detect if a dataset is in-memory:"
"if so, num_workers is set to 0 for faster processing"
cast to list if necessary
cast to list if necessary
"first, check if pymongo is installed"
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
Expose base classses
Expose all biomedical data sets used for the evaluation of BioBERT
-
-
-
-
Expose all biomedical data sets using the HUNER splits
Expose all biomedical data sets
Expose all document classification datasets
word sense disambiguation
Expose all entity linking datasets
Expose all relation extraction datasets
universal proposition banks
keyphrase detection datasets
other NER datasets
standard NER datasets
Expose all sequence labeling datasets
Expose all text-image datasets
Expose all text-text datasets
Expose all treebanks
"find train, dev and test files if not specified"
get train data
get test data
get dev data
option 1: read only sentence boundaries as offset positions
option 2: keep everything in memory
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
current token ID
handling for the awful UD multiword format
end of sentence
comments
ellipsis
if token is a multi-word
normal single-word tokens
"if we don't split multiwords, skip over component words"
add token
add morphological tags
derive whitespace logic for multiwords
print(token)
print(current_multiword_last_token)
print(current_multiword_first_token)
"if multi-word equals component tokens, there should be no whitespace"
go through all tokens in subword and set whitespace_after information
print(i)
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
this dataset name
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
"finally, print model card for information"
test corpus
create a TARS classifier
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
test corpus
create a TARS classifier
switch to a new task (TARS can do multiple tasks so you must define one)
initialize the text classifier trainer
start the training
"With end symbol, without start symbol, padding in front"
"Without end symbol, with start symbol, padding in back"
"Without end symbol, without start symbol, padding in front"
increment for last token in sentence if not followed by whitespace
clean up directory
clean up directory
initialize trainer
clean up results directory
initialize trainer
initialize trainer
initialize trainer
initialize trainer
initialize trainer
initialize trainer
train model for 2 epochs
load the checkpoint model and train until epoch 4
clean up results directory
initialize trainer
from flair.trainers.trainer_regression import RegressorTrainer
def test_trainer_evaluation(tasks_base_path):
"corpus, model, trainer = init(tasks_base_path)"
""
expected = model.evaluate(corpus.dev)
""
assert expected is not None
def test_trainer_results(tasks_base_path):
"corpus, model, trainer = init(tasks_base_path)"
"results = trainer.train(""regression_train/"", max_epochs=1)"
"assert results[""test_score""] > 0"
"assert len(results[""dev_loss_history""]) == 1"
"assert len(results[""dev_score_history""]) == 1"
"assert len(results[""train_loss_history""]) == 1"
example sentence
set 4 labels for 2 tokens ('love' is tagged twice)
check if there are three POS labels with correct text and values
check if there are is one SENTIMENT label with correct text and values
check if all tokens are correctly labeled
remove the pos label from the last word
there should be 2 POS labels left
now remove all pos tags
set 3 labels for 2 spans (HU is tagged twice)
check if there are three labels with correct text and values
check if there are two spans with correct text and values
"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
should be only one NER label left
and only one NER span
set 3 labels for 2 spans (HU is tagged twice with different tags)
check if there are three labels with correct text and values
check if there are two spans with correct text and values
"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
should be only one NER label left
and only one NER span
but there is also one orgtype span and label
and only one NER span
let's add the NER tag back
check if there are three labels with correct text and values
check if there are two spans with correct text and values
now remove all NER tags
set 3 labels for 2 spans (HU is tagged twice with different tags)
create two relation label
there should be two relation labels
there should be one syntactic labels
"there should be two relations, one with two and one with one label"
example sentence
add another topic label
example sentence
has sentiment value
has 4 part of speech tags
has 1 NER tag
should be in total 6 labels
example sentence
add two NER labels
get the four labels
check that only two of the respective data points are equal
make a sentence and some right context
TODO: is this desirable? Or should two sentences with same text still be considered different objects?
@pytest.mark.integration
initialize trainer
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
define search space
sequence tagger parameter
model trainer parameter
training parameter
find best parameter settings
clean up results directory
document embeddings parameter
training parameter
clean up results directory
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
get two corpora as one
"get training, test and dev data for full English UD corpus from web"
clean up data directory
"assert [token.get_tag(""head"").value for token in sent1.tokens] == ["
"""2"","
"""0"","
"""4"","
"""2"","
"""2"","
"""2"","
]
"Here, we use the default token annotation fields."
"We have manually checked, that these numbers are correct:"
"+1 offset, because of missing EOS marker at EOD"
load dataset
tagger without CRF
train
check if loaded model can predict
check if loaded model successfully fit the training data
load dataset
tagger without CRF
train
check if loaded model can predict
check if loaded model successfully fit the training data
load dataset
tagger without CRF
train
check if loaded model can predict
check if loaded model successfully fit the training data
load dataset
tagger without CRF
train
check if loaded model can predict
check if loaded model successfully fit the training data
check if model can predict
load model
chcek if model predicts correct label
check if loaded model successfully fit the training data
check if model can predict
load model
chcek if model predicts correct label
check if loaded model successfully fit the training data
check if model can predict
load model
chcek if model predicts correct label
check if loaded model successfully fit the training data
clean up file
train model for 2 epochs
load the checkpoint model and train until epoch 4
from allennlp.common.tqdm import Tqdm
mmap seems to be much more memory efficient
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
Extract all the contents of zip file in current directory
Extract all the contents of zip file in current directory
get cache path to put the file
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
first determine the distribution of classes in the dataset
weight for each sample
Create blocks
shuffle the blocks
concatenate the shuffled blocks
Create blocks
shuffle the blocks
concatenate the shuffled blocks
determine offsets for whitespace_after field
increment for last token in sentence if not followed by whitespace
determine offsets for whitespace_after field
conll 2000 column format
conll 03 NER column format
WNUT-17
-- WikiNER datasets
-- Universal Dependencies
Germanic
Romance
West-Slavic
South-Slavic
East-Slavic
Scandinavian
Asian
Language isolates
recent Universal Dependencies
other datasets
text classification format
text regression format
"first, try to fetch dataset online"
default dataset folder is the cache root
get string value if enum is passed
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
the CoNLL 03 task for German has an additional lemma column
the CoNLL 03 task for Dutch has no NP column
the CoNLL 03 task for Spanish only has two columns
the GERMEVAL task only has two columns: text and ner
WSD tasks may be put into this column format
"the UD corpora follow the CoNLL-U format, for which we have a special reader"
"for text classifiers, we use our own special format"
NER corpus for Basque
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
get train and test data
"read in test file if exists, otherwise sample 10% of train data as test dataset"
"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
convert tag scheme to iobes
automatically identify train / test / dev files
automatically identify train / test / dev files
"most data sets have the token text in the first column, if not, pass 'text' as column"
conll 2000 chunking task
Support both TREC-6 and TREC-50
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
Wikiner NER task
unpack and write out in CoNLL column-like format
CoNLL 02/03 NER
universal dependencies
--- UD Germanic
--- UD Romance
--- UD West-Slavic
--- UD Scandinavian
--- UD South-Slavic
--- UD Asian
this is the default init size of a lmdb database for embeddings
some non-used parameter to allow print
get db filename from embedding name
"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
SequenceTagger
TextClassifier
get db filename from embedding name
if embedding database already exists
"otherwise, push embedding to database"
if embedding database already exists
open the database in read mode
we need to set self.k
create and load the database in write mode
"no idea why, but we need to close and reopen the environment to avoid"
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
when opening new transaction !
init dictionaries
"in order to deal with unknown tokens, add <unk>"
set 'add_unk' if the dictionary was created with a version of Flair older than 0.9
set 'add_unk' depending on whether <unk> is a key
"if text is passed, instantiate sentence with tokens (words)"
log a warning if the dataset is empty
some sentences represent a document boundary (but most do not)
data with zero-width characters cannot be handled
set token idx if not set
non-set tags are OUT tags
anything that is not a BIOES tag is a SINGLE tag
anything that is not OUT is IN
single and begin tags start a new span
remember previous tag
"if label type is explicitly specified, get spans for this label type"
else determine all label types in sentence and get all spans
move sentence embeddings to device
move token embeddings to device
clear sentence embeddings
clear token embeddings
infer whitespace after field
add Sentence labels to output if they exist
add Token labels to output if they exist
add Sentence labels to output if they exist
add Token labels to output if they exist
No character at the corresponding code point: remove it
TODO: crude hack - replace with something better
set name
abort if no data is provided
sample test data from train if none is provided
sample dev data from train if none is provided
set train dev and test data
find out empty sentence indices
create subset of non-empty sentence indices
find out empty sentence indices
create subset of non-empty sentence indices
"if there are token labels of provided type, use these. Otherwise use sentence labels"
check for labels of words
if we are looking for sentence-level labels
check if sentence itself has labels
"if this is not a token-level prediction problem, add sentence-level labels to dictionary"
Make the tag dictionary
global variable: cache_root
global variable: device
global variable: embedding_storage_mode
# dummy return to fulfill trainer.train() needs
print(vec)
Attach optimizer
"convert `metrics` to float, in case it's a zero-dim Tensor"
if memory mode option 'none' delete everything
else delete only dynamic embeddings (otherwise autograd will keep everything in memory)
find out which ones are dynamic embeddings
find out which ones are dynamic embeddings
memory management - option 1: send everything to CPU (pin to memory if we train on GPU)
record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class)
"in Flair <0.9.1, optimizer and scheduler used to train model are not saved"
"write out a ""model card"" if one is set"
special handling for optimizer: remember optimizer class and state dictionary
save model
restore optimizer and scheduler to model card if set
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
see https://github.com/zalandoresearch/flair/issues/351
"read Dataset into data loader (if list of sentences passed, make Dataset first)"
loss calculation
variables for printing
variables for computing scores
remove any previously predicted labels
predict for batch
get the gold labels
add to all_predicted_values
make printout lines
"if the model is span-level, transfer to word-level annotations for printout"
"all labels default to ""O"""
set gold token-level
set predicted token-level
now print labels in CoNLL format
check if there is a label mismatch
print info
write all_predicted_values to out_file if set
make the evaluation dictionary
"finally, compute numbers"
"now, calculate evaluation numbers"
there is at least one gold label or one prediction (default)
issue error and default all evaluation numbers to 0.
line for log file
initialize the label dictionary
set up multi-label logic
loss weights and loss function
Initialize the weight tensor
filter empty sentences
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
remove previously predicted labels of this type
if anything could possibly be predicted
header for 'weights.txt'
"determine the column index of loss, f-score and accuracy for train, dev and test split"
then get all relevant values from the tsv
then get all relevant values from the tsv
plot i
save plots
save plots
plt.show()
save plot
auto-spawn on GPU if available
remove previous embeddings
clearing token embeddings to save memory
"read Dataset into data loader (if list of sentences passed, make Dataset first)"
#TODO: not saving lines yet
== similarity measures ==
helper class for ModelSimilarity
-- works with binary cross entropy loss --
"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}"
-- works with ranking/triplet loss --
normalize the embeddings
== similarity losses ==
"we want that logits for corresponding pairs are high, and for non-corresponding low"
TODO: this assumes eye matrix
"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa"
== similarity learner ==
"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both"
assumes that for each data pair there's at least one embedding per modality
pre-compute embeddings for all targets in evaluation dataset
compute the similarity
sort the similarity matrix across modality 1
"get the ranks, so +1 to start counting ranks from 1"
The conversion from old model's constructor interface
auto-spawn on GPU if available
pad strings with whitespaces to longest sentence
cut up the input into chunks of max charlength = chunk_size
push each chunk through the RNN language model
concatenate all chunks to make final output
initial hidden state
get predicted weights
divide by temperature
"to prevent overflow problem with small temperature values, substract largest value from all"
this makes a vector in which the largest value is 0
compute word weights with exponential function
try sampling multinomial distribution for next character
print(word_idx)
input ids
push list of character IDs through model
the target is always the next character
use cross entropy loss to compare output of forward pass with targets
exponentiate cross-entropy loss to calculate perplexity
serialize the language models and the constructor arguments (but nothing else)
special handling for deserializing language models
re-initialize language model with constructor arguments
copy over state dictionary to self
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
"in their ""self.train()"" method)"
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
Transform input data into TARS format
print(all_labels)
"if there are no labels, return a random sample as negatives"
print(sample)
"otherwise, go through all labels"
make sure the probabilities always sum up to 1
get and embed all labels by making a Sentence object that contains only the label text
get each label embedding and scale between 0 and 1
compute similarity matrix
"the higher the similarity, the greater the chance that a label is"
sampled as negative example
make label dictionary if no Dictionary object is passed
prepare dictionary of tags (without B- I- prefixes and without UNK)
check if candidate_label_set is empty
make list if only one candidate label is passed
create label dictionary
note current task
create a temporary task
make zero shot predictions
switch to the pre-existing task
prepare TARS dictionary
initialize a bare-bones sequence tagger
transformer separator
Store task specific labels since TARS can handle multiple tasks
make a tars sentence where all labels are O by default
overwrite O labels with tags
init new TARS classifier
set all task information
linear layers of internal classifier
return
with torch.no_grad():
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
go through each sentence in the batch
always remove tags first
get the span and its label
determine whether tokens in this span already have a label
only add if all tokens have no label
clearing token embeddings to save memory
prepare TARS dictionary
initialize a bare-bones sequence tagger
transformer separator
Store task specific labels since TARS can handle multiple tasks
init new TARS classifier
set all task information
linear layers of internal classifier
with torch.no_grad():
set context if not set already
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
go through each sentence in the batch
always remove tags first
add all labels that according to TARS match the text and are above threshold
do not add labels below confidence threshold
only use label with highest confidence if enforcing single-label predictions
get all label scores and do an argmax to get the best label
remove previously added labels and only add the best label
clearing token embeddings to save memory
if embed_separately == True the linear layer needs twice the length of the embeddings as input size
since we concatenate the embeddings of the two DataPoints in the DataPairs
representation for both sentences
set separator to concatenate two sentences
auto-spawn on GPU if available
linear layer
minimal return is scores and labels
set embeddings
set relation and entity label types
"whether to use gold entity pairs, and whether to filter entity pairs by type"
init dropouts
pooling operation to get embeddings for entites
"entity pairs could also be no relation at all, add default value for this case to dictionary"
decoder can be linear or nonlinear
super lame: make dictionary to find relation annotations for a given entity pair
get all entity spans
"go through cross product of entities, for each pair concat embeddings"
filter entity pairs according to their tags if set
get gold label for this relation (if one exists)
"if there is no gold label for this entity pair, set to 'O' (no relation)"
"if predicting, also remember sentences and label candidates"
if there's at least one entity pair in the sentence
embed sentences and get embeddings for each entity pair
get embeddings
stack and drop out (squeeze and unsqueeze)
send through decoder
"return either scores and gold labels (for loss calculation), or include label candidates for prediction"
if we concatenate the embeddings we need double input size in our linear layer
filter sentences with no candidates (no candidates means nothing can be linked anyway)
fields to return
"if the entire batch has no sentence with candidates, return empty"
"otherwise, embed sentence and send through prediction head"
embed all tokens
get the embeddings of the entity mentions
minimal return is scores and labels
set the dictionaries
"if we use a CRF, we must add special START and STOP tags to the dictionary"
Initialize the weight tensor
initialize the network architecture
dropouts
optional reprojection layer on top of word embeddings
bidirectional LSTM on top of embedding layer
Create initial hidden state and initialize it
TODO: Decide how to initialize the hidden state variables
self.hs_initializer(self.lstm_init_h)
self.hs_initializer(self.lstm_init_c)
final linear map to tag space
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
clearing token embeddings to save memory
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
"if initial hidden state is trainable, use this state"
word dropout only before LSTM - TODO: more experimentation needed
if self.use_word_dropout > 0.0:
sentence_tensor = self.word_dropout(sentence_tensor)
get the tags in this sentence
add tags as tensor
pad tags if using batch-CRF decoder
reduce raw values to avoid NaN during exp
broadcasting will do the job of reshaping and is more efficient than calling repeat
default value
core Flair models on Huggingface ModelHub
"Large NER models,"
Multilingual NER models
English POS models
Multilingual POS models
English SRL models
English chunking models
Language-specific NER models
English NER models
Multilingual NER models
English POS models
Multilingual POS models
English SRL models
English chunking models
Danish models
German models
French models
Dutch models
Malayalam models
Portuguese models
Keyphase models
Biomedical models
check if model name is a valid local file
"check if model key is remapped to HF key - if so, print out information"
get mapped name
output information
use mapped name instead
"if not, check if model key is remapped to direct download location. If so, download model"
special handling for the taggers by the @redewiegergabe project (TODO: move to model hub)
"for all other cases (not local file or special download location), use HF model hub"
"if not a local file, get from model hub"
use model name as subfolder
Lazy import
output information
"log.error(f"" - Error message: {e}"")"
"all_tag_prob=all_tag_prob,"
clear embeddings after predicting
load each model
check if the same embeddings were already loaded previously
"if the model uses StackedEmbedding, make a new stack with previous objects"
sort embeddings by key alphabetically
check previous embeddings and add if found
only re-use static embeddings
"if not found, use existing embedding"
initialize new stack
"of the model uses regular embedding, re-load if previous version found"
auto-spawn on GPU if available
embed sentences
make tensor for all embedded sentences in batch
send through decoder to get logits
minimal return is scores and labels
English sentiment models
Communicative Functions Model
embeddings
dictionaries
linear layer
all parameters will be pushed internally to the specified device
get all tokens in this mini-batch
minimal return is scores and labels
weights for loss function
iput size is two times wordembedding size since we use pair of words as input
"the output size is max_distance + 1, i.e. we allow 0,1,...,max_distance words between pairs"
regression
input size is two times word embedding size since we use pair of words as input
the output size is 1
auto-spawn on GPU if available
all input should be tensors
forward allows only a single sentcence!!
embed words of sentence
go through all pairs of words with a maximum number of max_distance in between
go through all pairs
2-dim matrix whose rows are the embeddings of word pairs of the sentence
So far only one sentence allowed
If list of sentences is handed the function works with the first sentence of the list
Assume data_points is a single sentence!!!
scores are the predictions for each word pair
"classification needs labels to be integers, regression needs labels to be float"
this is due to the different loss functions
only single sentences as input
gold labels
for output text file
for buckets
for average prediction
add some statistics to the output
use scikit-learn to evaluate
"we iterate over each sentence, instead of batches"
get single labels from scores
gold labels
for output text file
hot one vector of true value
hot one vector of predicted value
"speichert embeddings, falls embedding_storage!= 'None'"
"make ""classification report"""
get scores
"precision_score = round(metrics.precision_score(y_true, y_pred, average='macro', zero_division=0), 4)"
"recall_score = round(metrics.recall_score(y_true, y_pred, average='macro', zero_division=0), 4)"
line for log file
create a model card for this model with Flair and PyTorch version
also record Transformers version if library is loaded
remember all parameters used in train() call
add model card to model
cast string to Path
check for previously saved best models in the current training folder and delete them
"determine what splits (train, dev, test) to evaluate and log"
prepare loss logging file and set up header
"if optimizer class is passed, instantiate:"
load existing optimizer state dictionary if it exists
"minimize training loss if training with dev data, else maximize dev score"
"if scheduler is passed as a class, instantiate"
"if we load a checkpoint, we have already trained for epoch"
load existing scheduler state dictionary if it exists
update optimizer and scheduler in model card
"if training also uses dev/train data, include in training set"
initialize sampler if provided
init with default values if only class is provided
set dataset to sample from
At any point you can hit Ctrl + C to break out of training early.
update epoch in model card
get new learning rate
reload last best model if annealing with restarts is enabled
stop training if learning rate becomes too small
process mini-batches
zero the gradients on the model and optimizer
"if necessary, make batch_steps"
forward and backward for batch
forward pass
Backward
do the optimizer step
do the scheduler step if one-cycle or linear decay
get new learning rate
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
evaluate on train / dev / test split depending on training settings
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
calculate scores using dev data if available
append dev score to score history
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
determine if this is the best model or if we need to anneal
default mode: anneal against dev score
alternative: anneal against dev loss
alternative: anneal against train loss
determine bad epoch number
log bad epochs
output log file
make headers on first epoch
"if checkpoint is enabled, save model at each epoch"
Check whether to save best model
"if we do not use dev data for model selection, save final model"
test best model if test data is present
recover all arguments that were used to train this model
you can overwrite params with your own
surface nested arguments
resume training with these parameters
"if we are training over multiple datasets, do evaluation for each"
get and return the final test score of best model
cast string to Path
forward pass
update optimizer and scheduler
append current loss to list of losses for all iterations
compute averaged loss
"TextDataset returns a list. valid and test are only one file, so return the first element"
cast string to Path
error message if the validation dataset is too small
Shuffle training files randomly after serially iterating through corpus one
"iterate through training data, starting at self.split (for checkpointing)"
off by one for printing
go into train mode
reset variables
not really sure what this does
do the forward pass in the model
try to predict the targets
Backward
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
We detach the hidden state from how it was previously produced.
"If we didn't, the model would try backpropagating all the way to start of the dataset."
explicitly remove loss to clear up memory
##############################################################################
Save the model if the validation loss is the best we've seen so far.
##############################################################################
print info
##############################################################################
##############################################################################
final testing
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
"multilingual forward (English, German, French, Italian, Dutch, Polish)"
"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
Dutch forward
Dutch backward
Swedish forward
Swedish backward
French forward
French backward
Czech forward
Czech backward
Portuguese forward
Portuguese backward
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
1-camembert-base -> camembert-base
1-xlm-roberta-large -> xlm-roberta-large
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
The mask has 1 for real tokens and 0 for padding tokens. Only real
tokens are attended to.
Zero-pad up to the sequence length.
"first, find longest sentence in batch"
prepare id maps for BERT model
put encoded batch through BERT model to get all hidden states of all encoder layers
get aggregated embeddings for each BERT-subtoken in sentence
get the current sentence object
add concatenated embedding to sentence
use first subword embedding if pooling operation is 'first'
"otherwise, do a mean over all subwords in token"
"if only one sentence is passed, convert to list of sentence"
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
embed a dummy sentence to determine embedding_length
Avoid conflicts with flair's Token class
"<cls> token initially set to 1/D, so it attends to all image features equally"
add positional encodings
reshape the pixels into the sequence
layer norm after convolution and positional encodings
add <cls> token
"transformer requires input in the shape [h*w+1, b, d]"
the output is an embedding of <cls> token
temporary fix to disable tokenizer parallelism warning
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
do not print transformer warnings as these are confusing in this case
load tokenizer and transformer model
model name
"when initializing, embeddings are in eval mode by default"
embedding parameters
send mini-token through to check how many layers the model has
check whether CLS is at beginning or end
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
gradients are enabled if fine-tuning is enabled
"first, subtokenize each sentence and find out into how many subtokens each token was divided"
subtokenize sentences
tokenize and truncate to max subtokens (TODO: check better truncation strategies)
find longest sentence in batch
initialize batch tensors and mask
put encoded batch through transformer model to get all hidden states of all encoder layers
iterate over all subtokenized sentences
use scalar mix of embeddings if so selected
set the extracted embedding for the token
special handling for serializing transformer models
serialize the transformer models and the constructor arguments (but nothing else)
necessary for reverse compatibility with Flair <= 0.7
special handling for deserializing transformer models
load transformer model
constructor arguments
re-initialize transformer word embeddings with constructor arguments
for backward compatibility with previous models
"I have no idea why this is necessary, but otherwise it doesn't work"
reload tokenizer to get around serialization issues
optional fine-tuning on top of embedding layer
"if only one sentence is passed, convert to list of sentence"
"if only one sentence is passed, convert to list of sentence"
bidirectional RNN on top of embedding layer
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push through RNN
after-RNN dropout
extract embeddings from RNN
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
serialize the language models and the constructor arguments (but nothing else)
special handling for deserializing language models
re-initialize language model with constructor arguments
copy over state dictionary to self
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
"in their ""self.train()"" method)"
IMPORTANT: add embeddings as torch modules
iterate over sentences
"if its a forward LM, take last state"
"convert to plain strings, embedded in a list for the encode function"
CNN
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push CNN
after-CNN dropout
extract embeddings from CNN
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
"if only one sentence is passed, convert to list of sentence"
Expose base classses
Expose token embedding classes
Expose document embedding classes
Expose image embedding classes
Expose legacy embedding classes
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
GLOVE embeddings
TURIAN embeddings
KOMNINOS embeddings
pubmed embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
twitter embeddings
two-letter language code wiki embeddings
two-letter language code wiki embeddings
two-letter language code crawl embeddings
gensim version 4
gensim version 3
fix serialized models
"this is required to force the module on the cpu,"
"if a parent module is put to gpu, the _apply is called to each sub_module"
self.to(..) actually sets the device properly
this ignores the get_cached_vec method when loading older versions
it is needed for compatibility reasons
gensim version 4
gensim version 3
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
multilingual models
English models
Arabic
Bulgarian
Czech
Danish
German
Spanish
Basque
Persian
Finnish
French
Hebrew
Hindi
Croatian
Indonesian
Italian
Japanese
Malayalam
Dutch
Norwegian
Polish
Portuguese
Pubmed
Slovenian
Swedish
Tamil
Spanish clinical
CLEF HIPE Shared task
Amharic
load model if in pretrained model map
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
CLEF HIPE models are lowercased
embeddings are static if we don't do finetuning
embed a dummy sentence to determine embedding_length
set to eval mode
make compatible with serialized models (TODO: remove)
"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout"
make compatible with serialized models (TODO: remove)
gradients are enable if fine-tuning is enabled
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
offset mode that extracts at whitespace after last character
offset mode that extracts at last character
only clone if optimization mode is 'gpu'
use the character language model embeddings as basis
length is twice the original character LM embedding length
these fields are for the embedding memory
whether to add only capitalized words to memory (faster runtime and lower memory consumption)
we re-compute embeddings dynamically at each epoch
set the memory method
memory is wiped each time we do a training run
"if we keep a pooling, it needs to be updated continuously"
update embedding
check token.text is empty or not
set aggregation operation
add embeddings after updating
temporary fix to disable tokenizer parallelism warning
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
do not print transformer warnings as these are confusing in this case
load tokenizer and transformer model
"in the end, these models don't need this configuration"
model name
whether to detach gradients on overlong sentences
store whether to use context (and how much)
dropout contexts
"if using context, can we cross document boundaries?"
send self to flair-device
embedding parameters
send mini-token through to check how many layers the model has
calculate embedding length
return length
check if special tokens exist to circumvent error message
"most models have an intial BOS token, except for XLNet, T5 and GPT2"
"when initializing, embeddings are in eval mode by default"
remove special markup
"we require encoded subtokenized sentences, the mapping to original tokens and the number of"
parts that each sentence produces
"if we also use context, first expand sentence to include context"
set context if not set already
"in case of contextualization, we must remember non-expanded sentence"
create expanded sentence and remember context offsets
overwrite sentence with expanded sentence
subtokenize the sentence
transformer specific tokenization
set zero embeddings for empty sentences and exclude
determine into how many subtokens each token is split
remember tokenized sentences and their subtokenization
encode inputs
Models such as FNet do not have an attention_mask
determine which sentence was split into how many parts
set language IDs for XLM-style transformers
put encoded batch through transformer model to get all hidden states of all encoder layers
make the tuple a tensor; makes working with it easier.
gradients are enabled if fine-tuning is enabled
iterate over all subtokenized sentences
"remove stride_size//2 at end of sentence_hidden_state, and half at beginning of remainder,"
in order to get some context into the embeddings of these words.
also don't include the embedding of the extra [CLS] and [SEP] tokens.
"for each token, get embedding"
some tokens have no subtokens at all (if omitted by BERT tokenizer) so return zero vector
"get states from all selected layers, aggregate with pooling operation"
use layer mean of embeddings if so selected
set the extracted embedding for the token
move embeddings from context back to original sentence (if using context)
remember original sentence
get left context
get right context
empty contexts should not introduce whitespace tokens
make expanded sentence
iterate over subtokens and reconstruct tokens
remove special markup
TODO check if this is necessary is this method is called before prepare_for_model
check if reconstructed token is special begin token ([CLS] or similar)
some BERT tokenizers somehow omit words - in such cases skip to next token
append subtoken to reconstruct token
check if reconstructed token is the same as current token
"if so, add subtoken count"
reset subtoken count and reconstructed token
break from loop if all tokens are accounted for
if tokens are unaccounted for
check if all tokens were matched to subtokens
"""""""Returns the length of the embedding vector."""""""
special handling for serializing transformer models
serialize the transformer models and the constructor arguments (but nothing else)
necessary for reverse compatibility with Flair <= 0.7
special handling for deserializing transformer models
load transformer model
constructor arguments
re-initialize transformer word embeddings with constructor arguments
"I have no idea why this is necessary, but otherwise it doesn't work"
reload tokenizer to get around serialization issues
model architecture
model architecture
"""pl"","
download if necessary
load the model
"TODO: keep for backwards compatibility, but remove in future"
save the sentence piece model as binary file (not as path which may change)
write out the binary sentence piece model into the expected directory
"if the model was saved as binary and it is not found on disk, write to appropriate path"
"otherwise, use normal process and potentially trigger another download"
"once the modes if there, load it with sentence piece"
empty words get no embedding
all other words get embedded
"the default model for ELMo is the 'original' model, which is very large"
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
put on Cuda if available
embed a dummy sentence to determine embedding_length
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
GLOVE embeddings
get train data
read in test file if exists
read in dev file if exists
"find train, dev and test files if not specified"
special key for space after
"store either Sentence objects in memory, or only file offsets"
"most data sets have the token text in the first column, if not, pass 'text' as column"
determine encoding of text file
skip first line if to selected
option 1: read only sentence boundaries as offset positions
option 2: keep everything in memory
pointer to previous
"if sentence ends, break"
skip comments
"if sentence ends, convert and return"
check if this sentence is a document boundary
"otherwise, this line is a token. parse and add to sentence"
check if this sentence is a document boundary
"for example, transforming 'B-OBJ' to 'B-part-of-speech-object'"
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
set sentence context using partials
column format
this dataset name
default dataset folder is the cache root
check if data there
code-switch uses the same training data than multi but provides a different test set.
"as the test set is not published, those two tasks are the same."
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
download data if necessary
download files if not present locally
we need to slightly modify the original files by adding some new lines after document separators
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
Remove CoNLL-U meta information in the last column
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
entity_mapping
this dataset name
default dataset folder is the cache root
download data if necessary
data validation
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
download files if not present locallys
column format
this dataset name
default dataset folder is the cache root
# download zip
merge the files in one as the zip is containing multiples files
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"unzip the downloaded repo and merge the train, dev and test datasets"
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
check if data there
create folder
download dataset
column format
this dataset name
default dataset folder is the cache root
download and parse data if necessary
create train test dev if not exist
column format
this dataset name
default dataset folder is the cache root
If the extracted corpus file is not yet present in dir
download zip if necessary
"extracted corpus is not present , so unpacking it."
column format
this dataset name
default dataset folder is the cache root
download zip
unpacking the zip
merge the files in one as the zip is containing multiples files
column format
this dataset name
default dataset folder is the cache root
"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)"
download files if not present locally
we need to modify the original files by adding new lines after after the end of each sentence
if only one language is given
column format
this dataset name
default dataset folder is the cache root
"use all languages if explicitly set to ""all"""
download data if necessary
initialize comlumncorpus and add it to list
column format
this dataset name
default dataset folder is the cache root
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
this list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
get google drive id from list
download from google drive
unzip
"tar.extractall(language_folder,members=[tar.getmember(file_name)])"
transform data into required format
"the processed dataset has the additional ending ""_new"""
remove the unprocessed dataset
initialize comlumncorpus and add it to list
if no languages are given as argument all languages used in XTREME will be loaded
if only one language is given
column format
this dataset name
default dataset folder is the cache root
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
This list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
download from HU Server
unzip
transform data into required format
initialize comlumncorpus and add it to list
if only one language is given
column format
this dataset name
default dataset folder is the cache root
download data if necessary
initialize comlumncorpus and add it to list
download data if necessary
unpack and write out in CoNLL column-like format
column format
this dataset name
default dataset folder is the cache root
download data if necessary
data is not in IOB2 format. Thus we transform it to IOB2
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
rename according to train - test - dev - convention
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"TODO: Add a routine, that checks annotations for some widespread errors/inconsistencies??? (e.g. in AQUAINT corpus Iran-Iraq_War vs. Iran-Iraq_war)"
Create the annotation dictionary
this fct removes every second unknown label
this dataset name
default dataset folder is the cache root
download and parse data if necessary
iterate over all html files
"get rid of html syntax, we only need the text"
between all documents we write a separator symbol
skip empty strings
"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)"
"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention"
sentence splitting and tokenization
iterate through all annotations and add to corresponding tokens
find sentence to which annotation belongs
position within corresponding sentence
set annotation for tokens of entity mention
write to out-file in column format
"in case something goes wrong, delete the dataset and raise error"
this dataset name
default dataset folder is the cache root
download and parse data if necessary
from qwikidata.linked_data_interface import get_entity_dict_from_api
generate qid wikiname dictionaries
merge dictionaries
ignore first line
commented and empty lines
read all Q-IDs
ignore first line
request
this dataset name
default dataset folder is the cache root
we use the wikiids in the data instead of directly utilizing the wikipedia urls.
like this we can quickly check if the corresponding page exists
if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi
delete unprocessed file
collect all wikiids
create the dictionary
request
this dataset name
default dataset folder is the cache root
names of raw text documents
open output_file
iterate through all documents
split sentences and tokenize
iterate through all annotations and add to corresponding tokens
find sentence to which annotation belongs
position within corresponding sentence
set annotation for tokens of entity mention
write to out file
this dataset name
default dataset folder is the cache root
download and parse data if necessary
this dataset name
default dataset folder is the cache root
download and parse data if necessary
First parse the post titles
Keep track of how many and which entity mentions does a given post title have
Check if the current post title has an entity link and parse accordingly
Post titles with entity mentions (if any) are handled via this function
Then parse the comments
"Iterate over the comments.tsv file, until the end is reached"
"Keep track of the current comment thread and its corresponding key, on which the annotations are matched."
Each comment thread is handled as one 'document'.
Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.
This if-condition is needed to handle this problem.
"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure"
"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above"
"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle."
"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,"
and not just single letters into single rows.
If there are annotated entity mentions for given post title or a comment thread
"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence"
Write the token with a corresponding tag to file
"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed"
"If a comment thread or a post title has no entity link, all tokens are assigned the O tag"
Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized
"incorrectly, in order to keep the desired format (empty line as a sentence separator)."
"Thrown when the second check above happens, but the last token of a sentence is reached."
"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below."
"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS"
Check if further annotations belong to the current post title or comment thread as well
Stop when the end of an annotation file is reached
Check if further annotations belong to the current sentence as well
"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)"
Docstart
if there is more than one word in the chunk we write each in a separate line
print(chunks)
empty line after each sentence
convert the file to CoNLL
this dataset name
default dataset folder is the cache root
"check if data there, if not, download the data"
create folder
download data
transform data into column format if necessary
if no filenames are specified we use all the data
"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled"
also we remove 'raganato_ALL' from filenames in case its in the list
generate the test file
make column file and save to data_folder
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
default dataset folder is the cache root
"We check if the the UFSAC data has already been downloaded. If not, we download it."
Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
create folder
download data
"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
generate the test file
this dataset name
default dataset folder is the cache root
download data if necessary
if True:
write CoNLL-U Plus header
"Some special cases (e.g., missing spaces before entity marker)"
necessary if text should be whitespace tokenizeable
Handle case where tail may occur before the head
this dataset name
default dataset folder is the cache root
write CoNLL-U Plus header
this dataset name
default dataset folder is the cache root
TODO: change data source to original CoNLL04 -- this dataset has span formatting errors
download data if necessary
write CoNLL-U Plus header
The span has ended.
We are entering a new span; reset indices
and active tag to new span.
We're inside a span.
Last token might have been a part of a valid span.
this dataset name
default dataset folder is the cache root
write CoNLL-U Plus header
"for source_file_path, target_filename in zip(source_file_paths, target_filenames):"
"with zip_file.open(source_file_path, mode=""r"") as source_file:"
target_file_path = Path(data_folder) / target_filename
"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:"
# write CoNLL-U Plus header
"target_file.write(""# global.columns = id form ner\n"")"
for example in json.load(source_file):
token_list = self._tacred_example_to_token_list(example)
target_file.write(token_list.serialize())
check if first tag row is already occupied
"if first tag row is occupied, use second tag row"
hardcoded mapping TODO: perhaps find nicer solution
"find train, dev and test files if not specified"
use test_file to create test split if available
use dev_file to create test split if available
"if data point contains black-listed label, do not use"
first check if valid sentence
"if so, add to indices"
"find train, dev and test files if not specified"
variables
different handling of in_memory data than streaming data
"most data sets have the token text in the first column, if not, pass 'text' as column"
test if format is OK
test if at least one label given
make sentence from text (and filter for length)
"if a pair column is defined, make a sentence pair object"
noinspection PyDefaultArgument
dataset name includes the split size
default dataset folder is the cache root
download data if necessary
download each of the 28 splits
create dataset directory if necessary
download senteval datasets if necessary und unzip
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
"by defaut, map point score to POSITIVE / NEGATIVE values"
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file from CSV
create test.txt file from CSV
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create train dev and test files in fasttext format
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
convert to FastText format
if no base_path provided take cache root
download data if necessary
"if data is not downloaded yet, download it"
get the zip file
move original .tsv files to another folder
create train and dev splits in fasttext format
create eval_dataset file with no labels
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
download data if necessary
download datasets if necessary
create dataset directory if necessary
create correctly formated txt files
multiple labels are possible
this dataset name
default dataset folder is the cache root
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
default dataset folder is the cache root
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
create a separate directory for different tasks
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
check if dataset is supported
set file names
set file names
download and unzip in file structure if necessary
instantiate corpus
"find train, dev and test files if not specified"
"create DataPairDataset for train, test and dev file, if they are given"
stop if file does not exist
create a DataPair object from strings
"if in_memory is True we return a datapair, otherwise we create one from the lists of strings"
if no base_path provided take cache root
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
if no base_path provided take cache root
"if data is not downloaded yet, download it"
get the zip file
"reorder dev datasets to have same columns as in train set: 8, 9, and 11"
dev sets include 5 different annotations but we will only keep the gold label
"rename test file to eval_dataset, since it has no labels"
if no base_path provided take cache root
"if data is not downloaded yet, download it"
get test and dev sets
if no base_path provided take cache root
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
if no base_path provided take cache root
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
if no base_path provided take cache root
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
if no base_path provided take cache root
"if data not downloaded yet, download it"
get the zip file
"the downloaded files have json format, we transform them to tsv"
Function to transform JSON file to tsv for Recognizing Textual Entailment Data
remove json file
noinspection PyProtectedMember
noinspection PyProtectedMember
"find train, dev and test files if not specified"
get train data
get test data
get dev data
noinspection PyProtectedMember
"if no fields specified, check if the file is CoNLL plus formatted and get fields"
Validate fields and token_annotation_fields
noinspection PyProtectedMember
option 1: read only sentence boundaries as offset positions
option 2: keep everything in memory
pointer to previous
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
Build the sentence tokens and add the annotations.
"For fields that contain key-value annotations,"
we add the key as label type-name and the value as the label value.
head and tail span indices are 1-indexed and end index is inclusive
determine all NER label types in sentence and add all NER spans as sentence-level labels
Uses dynamic programming approach to calculate maximum independent set in interval graph
with sum of all entity lengths as secondary key
calculate offset without current text
because we stick all passages of a document together
TODO For split entities we also annotate everything inbetween which might be a bad idea?
Try to fix incorrect annotations
print(
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
)
Ignore empty lines or relation annotations
FIX annotation of whitespaces (necessary for PDR)
One token may contain multiple entities -> deque all of them
column format
this dataset name
default dataset folder is the cache root
Create tokenization-dependent CONLL files. This is necessary to prevent
from caching issues (e.g. loading the same corpus with different sentence splitters)
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
Edge case: last token starts a new entity
Last document in file
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
Edge case: last token starts a new entity
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
download file is huge => make default_dir visible so that derivative
corpora can all use the same download file
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
Read texts
Read annotations
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
We need to apply a patch to correct the original training file
Articles title
Article abstract
Entity annotations
column format
this dataset name
default dataset folder is the cache root
Edge case: last token starts a new entity
Map all entities to chemicals
Map all entities to disease
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
Incomplete article
Invalid XML syntax
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
if len(mid) != 3:
continue
Try to fix entity offsets
column format
this dataset name
default dataset folder is the cache root
There is still one illegal annotation in the file ..
column format
this dataset name
default dataset folder is the cache root
"Abstract first, title second to prevent issues with sentence splitting"
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
"Filter for specific entity types, by default no entities will be filtered"
Get original HUNER splits to retrieve a list of all document ids contained in V2
train and dev split of V2 will be train in V4
test split of V2 will be dev in V4
New documents in V4 will become test documents
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
cache Feidegger config file
cache Feidegger images
replace image URL with local cached file
append Sentence-Image data point
"in certain cases, multi-CPU data loading makes no sense and slows"
"everything down. For this reason, we detect if a dataset is in-memory:"
"if so, num_workers is set to 0 for faster processing"
cast to list if necessary
cast to list if necessary
"first, check if pymongo is installed"
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
Expose base classses
Expose all sequence labeling datasets
standard NER datasets
other NER datasets
keyphrase detection datasets
universal proposition banks
Expose all entity linking datasets
word sense disambiguation
Expose all document classification datasets
Expose all treebanks
Expose all text-text datasets
Expose all text-image datasets
Expose all biomedical data sets
Expose all biomedical data sets using the HUNER splits
-
-
-
-
Expose all biomedical data sets used for the evaluation of BioBERT
Expose all relation extraction datasets
"find train, dev and test files if not specified"
get train data
get test data
get dev data
option 1: read only sentence boundaries as offset positions
option 2: keep everything in memory
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
current token ID
handling for the awful UD multiword format
end of sentence
comments
ellipsis
if token is a multi-word
normal single-word tokens
"if we don't split multiwords, skip over component words"
add token
add morphological tags
derive whitespace logic for multiwords
print(token)
print(current_multiword_last_token)
print(current_multiword_first_token)
"if multi-word equals component tokens, there should be no whitespace"
go through all tokens in subword and set whitespace_after information
print(i)
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
"finally, print model card for information"
test corpus
create a TARS classifier
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
test corpus
create a TARS classifier
switch to a new task (TARS can do multiple tasks so you must define one)
initialize the text classifier trainer
start the training
"mini_batch_chunk_size=4,  # optionally set this if transformer is too much for your machine"
clean up file
bioes tags
bio tags
broken tags
all tags
all weird tags
tags with confidence
bioes tags
bioes tags
"city single-token, person and company multi-token"
increment for last token in sentence if not followed by whitespace
clean up directory
clean up directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
train model for 2 epochs
load the checkpoint model and train until epoch 4
clean up results directory
initialize trainer
clean up results directory
from flair.trainers.trainer_regression import RegressorTrainer
def test_trainer_evaluation(tasks_base_path):
"corpus, model, trainer = init(tasks_base_path)"
""
expected = model.evaluate(corpus.dev)
""
assert expected is not None
def test_trainer_results(tasks_base_path):
"corpus, model, trainer = init(tasks_base_path)"
"results = trainer.train(""regression_train/"", max_epochs=1)"
"assert results[""test_score""] > 0"
"assert len(results[""dev_loss_history""]) == 1"
"assert len(results[""dev_score_history""]) == 1"
"assert len(results[""train_loss_history""]) == 1"
@pytest.mark.integration
initialize trainer
"loaded_model.predict([sentence, sentence_empty])"
loaded_model.predict([sentence_empty])
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
clean up results directory
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
get two corpora as one
"get training, test and dev data for full English UD corpus from web"
clean up data directory
"Here, we use the default token annotation fields."
load dataset
tagger without CRF
train
check if loaded model can predict
check if loaded model successfully fit the training data
clean up results directory
load dataset
tagger without CRF
train
check if loaded model can predict
check if loaded model successfully fit the training data
clean up results directory
load dataset
tagger without CRF
train
check if loaded model can predict
check if loaded model successfully fit the training data
clean up results directory
load dataset
tagger without CRF
train
check if loaded model can predict
check if loaded model successfully fit the training data
clean up results directory
check if model can predict
load model
chcek if model predicts correct label
check if loaded model successfully fit the training data
clean up results directory
check if model can predict
load model
chcek if model predicts correct label
check if loaded model successfully fit the training data
clean up results directory
check if model can predict
load model
chcek if model predicts correct label
check if loaded model successfully fit the training data
clean up results directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
train model for 2 epochs
load the checkpoint model and train until epoch 4
clean up results directory
from allennlp.common.tqdm import Tqdm
mmap seems to be much more memory efficient
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
Extract all the contents of zip file in current directory
Extract all the contents of zip file in current directory
get cache path to put the file
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
first determine the distribution of classes in the dataset
weight for each sample
Create blocks
shuffle the blocks
concatenate the shuffled blocks
Create blocks
shuffle the blocks
concatenate the shuffled blocks
State initialization
Exponential moving average of gradient values
Exponential moving average of squared gradient values
Maintains max of all exp. moving avg. of sq. grad. values
Decay the first and second moment running average coefficient
Maintains the maximum of all 2nd moment running avg. till now
Use the max. for normalizing running avg. of gradient
determine offsets for whitespace_after field
increment for last token in sentence if not followed by whitespace
determine offsets for whitespace_after field
conll 2000 column format
conll 03 NER column format
WNUT-17
-- WikiNER datasets
-- Universal Dependencies
Germanic
Romance
West-Slavic
South-Slavic
East-Slavic
Scandinavian
Asian
Language isolates
recent Universal Dependencies
other datasets
text classification format
text regression format
"first, try to fetch dataset online"
default dataset folder is the cache root
get string value if enum is passed
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
the CoNLL 03 task for German has an additional lemma column
the CoNLL 03 task for Dutch has no NP column
the CoNLL 03 task for Spanish only has two columns
the GERMEVAL task only has two columns: text and ner
WSD tasks may be put into this column format
"the UD corpora follow the CoNLL-U format, for which we have a special reader"
"for text classifiers, we use our own special format"
NER corpus for Basque
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
get train and test data
"read in test file if exists, otherwise sample 10% of train data as test dataset"
"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
convert tag scheme to iobes
automatically identify train / test / dev files
automatically identify train / test / dev files
"most data sets have the token text in the first column, if not, pass 'text' as column"
conll 2000 chunking task
Support both TREC-6 and TREC-50
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
Wikiner NER task
unpack and write out in CoNLL column-like format
CoNLL 02/03 NER
universal dependencies
--- UD Germanic
--- UD Romance
--- UD West-Slavic
--- UD Scandinavian
--- UD South-Slavic
--- UD Asian
this is the default init size of a lmdb database for embeddings
some non-used parameter to allow print
get db filename from embedding name
"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
SequenceTagger
TextClassifier
get db filename from embedding name
if embedding database already exists
"otherwise, push embedding to database"
if embedding database already exists
open the database in read mode
we need to set self.k
create and load the database in write mode
"no idea why, but we need to close and reopen the environment to avoid"
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
when opening new transaction !
init dictionaries
"in order to deal with unknown tokens, add <unk>"
"if text is passed, instantiate sentence with tokens (words)"
log a warning if the dataset is empty
some sentences represent a document boundary (but most do not)
data with zero-width characters cannot be handled
set token idx if not set
non-set tags are OUT tags
anything that is not a BIOES tag is a SINGLE tag
anything that is not OUT is IN
single and begin tags start a new span
remember previous tag
"if label type is explicitly specified, get spans for this label type"
else determine all label types in sentence and get all spans
move sentence embeddings to device
move token embeddings to device
clear sentence embeddings
clear token embeddings
infer whitespace after field
add Sentence labels to output if they exist
add Token labels to output if they exist
add Sentence labels to output if they exist
add Token labels to output if they exist
No character at the corresponding code point: remove it
TODO: crude hack - replace with something better
set name
sample test data if none is provided
sample dev data if none is provided
set train dev and test data
find out empty sentence indices
create subset of non-empty sentence indices
find out empty sentence indices
create subset of non-empty sentence indices
"if there are token labels of provided type, use these. Otherwise use sentence labels"
if we are looking for sentence-level labels
check if sentence itself has labels
check for labels of words
"if this is not a token-level prediction problem, add sentence-level labels to dictionary"
Make the tag dictionary
global variable: cache_root
global variable: device
global variable: embedding_storage_mode
# dummy return to fulfill trainer.train() needs
print(vec)
Attach optimizer
"convert `metrics` to float, in case it's a zero-dim Tensor"
if memory mode option 'none' delete everything
else delete only dynamic embeddings (otherwise autograd will keep everything in memory)
find out which ones are dynamic embeddings
find out which ones are dynamic embeddings
memory management - option 1: send everything to CPU (pin to memory if we train on GPU)
record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class)
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
see https://github.com/zalandoresearch/flair/issues/351
"read Dataset into data loader (if list of sentences passed, make Dataset first)"
loss calculation
variables for printing
variables for computing scores
remove any previously predicted labels
predict for batch
get the gold labels
add to all_predicted_values
make printout lines
"if the model is span-level, transfer to word-level annotations for printout"
"all labels default to ""O"""
set gold token-level
set predicted token-level
now print labels in CoNLL format
check if there is a label mismatch
print info
write all_predicted_values to out_file if set
make the evaluation dictionary
"finally, compute numbers"
"now, calculate evaluation numbers"
there is at least one gold label or one prediction (default)
issue error and default all evaluation numbers to 0.
line for log file
initialize the label dictionary
self.label_dictionary.add_item('O')
set up multi-label logic
loss weights and loss function
Initialize the weight tensor
filter empty sentences
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
remove previously predicted labels of this type
if anything could possibly be predicted
header for 'weights.txt'
"determine the column index of loss, f-score and accuracy for train, dev and test split"
then get all relevant values from the tsv
then get all relevant values from the tsv
plot i
save plots
save plots
plt.show()
save plot
take the average over the last three scores of training
take average over the scores from the different training runs
remove previous embeddings
clearing token embeddings to save memory
"read Dataset into data loader (if list of sentences passed, make Dataset first)"
#TODO: not saving lines yet
== similarity measures ==
helper class for ModelSimilarity
-- works with binary cross entropy loss --
"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}"
-- works with ranking/triplet loss --
normalize the embeddings
== similarity losses ==
"we want that logits for corresponding pairs are high, and for non-corresponding low"
TODO: this assumes eye matrix
"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa"
== similarity learner ==
"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both"
assumes that for each data pair there's at least one embedding per modality
pre-compute embeddings for all targets in evaluation dataset
compute the similarity
sort the similarity matrix across modality 1
"get the ranks, so +1 to start counting ranks from 1"
The conversion from old model's constructor interface
auto-spawn on GPU if available
pad strings with whitespaces to longest sentence
cut up the input into chunks of max charlength = chunk_size
push each chunk through the RNN language model
concatenate all chunks to make final output
initial hidden state
get predicted weights
divide by temperature
"to prevent overflow problem with small temperature values, substract largest value from all"
this makes a vector in which the largest value is 0
compute word weights with exponential function
try sampling multinomial distribution for next character
print(word_idx)
input ids
push list of character IDs through model
the target is always the next character
use cross entropy loss to compare output of forward pass with targets
exponentiate cross-entropy loss to calculate perplexity
serialize the language models and the constructor arguments (but nothing else)
special handling for deserializing language models
re-initialize language model with constructor arguments
copy over state dictionary to self
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
"in their ""self.train()"" method)"
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
Transform input data into TARS format
print(all_labels)
"if there are no labels, return a random sample as negatives"
print(sample)
"otherwise, go through all labels"
make sure the probabilities always sum up to 1
get and embed all labels by making a Sentence object that contains only the label text
get each label embedding and scale between 0 and 1
compute similarity matrix
"the higher the similarity, the greater the chance that a label is"
sampled as negative example
make label dictionary if no Dictionary object is passed
prepare dictionary of tags (without B- I- prefixes)
check if candidate_label_set is empty
make list if only one candidate label is passed
"if list is passed, convert to set"
note current task
create a temporary task
make zero shot predictions
switch to the pre-existing task
prepare TARS dictionary
initialize a bare-bones sequence tagger
transformer separator
Store task specific labels since TARS can handle multiple tasks
make a tars sentence where all labels are O by default
overwrite O labels with tags
init new TARS classifier
set all task information
linear layers of internal classifier
return
with torch.no_grad():
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
go through each sentence in the batch
always remove tags first
get the span and its label
determine whether tokens in this span already have a label
only add if all tokens have no label
clearing token embeddings to save memory
prepare TARS dictionary
initialize a bare-bones sequence tagger
transformer separator
Store task specific labels since TARS can handle multiple tasks
init new TARS classifier
set all task information
linear layers of internal classifier
with torch.no_grad():
set context if not set already
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
go through each sentence in the batch
always remove tags first
clearing token embeddings to save memory
if embed_separately == True the linear layer needs twice the length of the embeddings as input size
since we concatenate the embeddings of the two DataPoints in the DataPairs
representation for both sentences
set separator to concatenate two sentences
auto-spawn on GPU if available
linear layer
minimal return is scores and labels
"entity pairs could also be no relation at all, add default value for this case to dictionary"
entity_pairs = []
super lame: make dictionary to find relation annotations for a given entity pair
get all entity spans
get embedding for each entity
"go through cross product of entities, for each pair concat embeddings"
get gold label for this relation (if one exists)
"if using gold spans only, skip all entity pairs that are not in gold data"
"if no gold label exists, and all spans are used, label defaults to 'O' (no relation)"
"if predicting, also remember sentences and label candidates"
"return either scores and gold labels (for loss calculation), or include label candidates for prediction"
if we concatenate the embeddings we need double input size in our linear layer
filter sentences with no candidates (no candidates means nothing can be linked anyway)
fields to return
"if the entire batch has no sentence with candidates, return empty"
"otherwise, embed sentence and send through prediction head"
embed all tokens
get the embeddings of the entity mentions
minimal return is scores and labels
set the dictionaries
"if we use a CRF, we must add special START and STOP tags to the dictionary"
Initialize the weight tensor
initialize the network architecture
dropouts
optional reprojection layer on top of word embeddings
bidirectional LSTM on top of embedding layer
Create initial hidden state and initialize it
TODO: Decide how to initialize the hidden state variables
self.hs_initializer(self.lstm_init_h)
self.hs_initializer(self.lstm_init_c)
final linear map to tag space
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
clearing token embeddings to save memory
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
"if initial hidden state is trainable, use this state"
word dropout only before LSTM - TODO: more experimentation needed
if self.use_word_dropout > 0.0:
sentence_tensor = self.word_dropout(sentence_tensor)
get the tags in this sentence
add tags as tensor
pad tags if using batch-CRF decoder
reduce raw values to avoid NaN during exp
broadcasting will do the job of reshaping and is more efficient than calling repeat
default value
core Flair models on Huggingface ModelHub
"Large NER models,"
Multilingual NER models
English POS models
Multilingual POS models
English SRL models
English chunking models
Language-specific NER models
English NER models
Multilingual NER models
English POS models
Multilingual POS models
English SRL models
English chunking models
Danish models
German models
French models
Dutch models
Malayalam models
Portuguese models
Keyphase models
Biomedical models
check if model name is a valid local file
"check if model key is remapped to HF key - if so, print out information"
get mapped name
output information
use mapped name instead
"if not, check if model key is remapped to direct download location. If so, download model"
special handling for the taggers by the @redewiegergabe project (TODO: move to model hub)
"for all other cases (not local file or special download location), use HF model hub"
"if not a local file, get from model hub"
use model name as subfolder
Lazy import
output information
"log.error(f"" - Error message: {e}"")"
"all_tag_prob=all_tag_prob,"
clear embeddings after predicting
load each model
check if the same embeddings were already loaded previously
"if the model uses StackedEmbedding, make a new stack with previous objects"
sort embeddings by key alphabetically
check previous embeddings and add if found
only re-use static embeddings
"if not found, use existing embedding"
initialize new stack
"of the model uses regular embedding, re-load if previous version found"
auto-spawn on GPU if available
embed sentences
make tensor for all embedded sentences in batch
send through decoder to get logits
minimal return is scores and labels
English sentiment models
Communicative Functions Model
embeddings
dictionaries
linear layer
all parameters will be pushed internally to the specified device
get all tokens in this mini-batch
minimal return is scores and labels
weights for loss function
iput size is two times wordembedding size since we use pair of words as input
"the output size is max_distance + 1, i.e. we allow 0,1,...,max_distance words between pairs"
regression
input size is two times word embedding size since we use pair of words as input
the output size is 1
auto-spawn on GPU if available
all input should be tensors
forward allows only a single sentcence!!
embed words of sentence
go through all pairs of words with a maximum number of max_distance in between
go through all pairs
2-dim matrix whose rows are the embeddings of word pairs of the sentence
So far only one sentence allowed
If list of sentences is handed the function works with the first sentence of the list
Assume data_points is a single sentence!!!
scores are the predictions for each word pair
"classification needs labels to be integers, regression needs labels to be float"
this is due to the different loss functions
only single sentences as input
gold labels
for output text file
for buckets
for average prediction
add some statistics to the output
use scikit-learn to evaluate
"we iterate over each sentence, instead of batches"
get single labels from scores
gold labels
for output text file
hot one vector of true value
hot one vector of predicted value
"speichert embeddings, falls embedding_storage!= 'None'"
"make ""classification report"""
get scores
"precision_score = round(metrics.precision_score(y_true, y_pred, average='macro', zero_division=0), 4)"
"recall_score = round(metrics.recall_score(y_true, y_pred, average='macro', zero_division=0), 4)"
line for log file
cast string to Path
check for previously saved best models in the current training folder and delete them
"determine what splits (train, dev, test) to evaluate and log"
prepare loss logging file and set up header
"minimize training loss if training with dev data, else maximize dev score"
"if we load a checkpoint, we have already trained for self.epoch"
"if training also uses dev/train data, include in training set"
initialize sampler if provided
init with default values if only class is provided
set dataset to sample from
At any point you can hit Ctrl + C to break out of training early.
get new learning rate
reload last best model if annealing with restarts is enabled
stop training if learning rate becomes too small
process mini-batches
zero the gradients on the model and optimizer
"if necessary, make batch_steps"
forward and backward for batch
forward pass
Backward
do the optimizer step
do the scheduler step if one-cycle
get new learning rate
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
evaluate on train / dev / test split depending on training settings
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
calculate scores using dev data if available
append dev score to score history
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
determine if this is the best model or if we need to anneal
default mode: anneal against dev score
alternative: anneal against dev loss
alternative: anneal against train loss
determine bad epoch number
log bad epochs
output log file
make headers on first epoch
"if checkpoint is enabled, save model at each epoch"
Check whether to save best model
"if we do not use dev data for model selection, save final model"
test best model if test data is present
"if we are training over multiple datasets, do evaluation for each"
get and return the final test score of best model
cast string to Path
forward pass
update optimizer and scheduler
"TextDataset returns a list. valid and test are only one file, so return the first element"
cast string to Path
error message if the validation dataset is too small
Shuffle training files randomly after serially iterating through corpus one
"iterate through training data, starting at self.split (for checkpointing)"
off by one for printing
go into train mode
reset variables
not really sure what this does
do the forward pass in the model
try to predict the targets
Backward
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
We detach the hidden state from how it was previously produced.
"If we didn't, the model would try backpropagating all the way to start of the dataset."
explicitly remove loss to clear up memory
##############################################################################
Save the model if the validation loss is the best we've seen so far.
##############################################################################
print info
##############################################################################
##############################################################################
final testing
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
"multilingual forward (English, German, French, Italian, Dutch, Polish)"
"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
Dutch forward
Dutch backward
Swedish forward
Swedish backward
French forward
French backward
Czech forward
Czech backward
Portuguese forward
Portuguese backward
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
1-camembert-base -> camembert-base
1-xlm-roberta-large -> xlm-roberta-large
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
The mask has 1 for real tokens and 0 for padding tokens. Only real
tokens are attended to.
Zero-pad up to the sequence length.
"first, find longest sentence in batch"
prepare id maps for BERT model
put encoded batch through BERT model to get all hidden states of all encoder layers
get aggregated embeddings for each BERT-subtoken in sentence
get the current sentence object
add concatenated embedding to sentence
use first subword embedding if pooling operation is 'first'
"otherwise, do a mean over all subwords in token"
"if only one sentence is passed, convert to list of sentence"
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
embed a dummy sentence to determine embedding_length
Avoid conflicts with flair's Token class
"<cls> token initially set to 1/D, so it attends to all image features equally"
add positional encodings
reshape the pixels into the sequence
layer norm after convolution and positional encodings
add <cls> token
"transformer requires input in the shape [h*w+1, b, d]"
the output is an embedding of <cls> token
temporary fix to disable tokenizer parallelism warning
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
do not print transformer warnings as these are confusing in this case
load tokenizer and transformer model
model name
"when initializing, embeddings are in eval mode by default"
embedding parameters
send mini-token through to check how many layers the model has
check whether CLS is at beginning or end
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
gradients are enabled if fine-tuning is enabled
"first, subtokenize each sentence and find out into how many subtokens each token was divided"
subtokenize sentences
tokenize and truncate to max subtokens (TODO: check better truncation strategies)
find longest sentence in batch
initialize batch tensors and mask
put encoded batch through transformer model to get all hidden states of all encoder layers
iterate over all subtokenized sentences
use scalar mix of embeddings if so selected
set the extracted embedding for the token
special handling for serializing transformer models
serialize the transformer models and the constructor arguments (but nothing else)
necessary for reverse compatibility with Flair <= 0.7
special handling for deserializing transformer models
load transformer model
constructor arguments
re-initialize transformer word embeddings with constructor arguments
for backward compatibility with previous models
"I have no idea why this is necessary, but otherwise it doesn't work"
reload tokenizer to get around serialization issues
optional fine-tuning on top of embedding layer
"if only one sentence is passed, convert to list of sentence"
"if only one sentence is passed, convert to list of sentence"
bidirectional RNN on top of embedding layer
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push through RNN
after-RNN dropout
extract embeddings from RNN
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
serialize the language models and the constructor arguments (but nothing else)
special handling for deserializing language models
re-initialize language model with constructor arguments
copy over state dictionary to self
"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
"in their ""self.train()"" method)"
IMPORTANT: add embeddings as torch modules
iterate over sentences
"if its a forward LM, take last state"
"convert to plain strings, embedded in a list for the encode function"
CNN
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push CNN
after-CNN dropout
extract embeddings from CNN
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
"if only one sentence is passed, convert to list of sentence"
Expose base classses
Expose token embedding classes
Expose document embedding classes
Expose image embedding classes
Expose legacy embedding classes
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
GLOVE embeddings
TURIAN embeddings
KOMNINOS embeddings
pubmed embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
twitter embeddings
two-letter language code wiki embeddings
two-letter language code wiki embeddings
two-letter language code crawl embeddings
fix serialized models
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
multilingual models
English models
Arabic
Bulgarian
Czech
Danish
German
Spanish
Basque
Persian
Finnish
French
Hebrew
Hindi
Croatian
Indonesian
Italian
Japanese
Malayalam
Dutch
Norwegian
Polish
Portuguese
Pubmed
Slovenian
Swedish
Tamil
Spanish clinical
CLEF HIPE Shared task
load model if in pretrained model map
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
CLEF HIPE models are lowercased
embeddings are static if we don't do finetuning
embed a dummy sentence to determine embedding_length
set to eval mode
make compatible with serialized models (TODO: remove)
"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout"
make compatible with serialized models (TODO: remove)
gradients are enable if fine-tuning is enabled
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
offset mode that extracts at whitespace after last character
offset mode that extracts at last character
only clone if optimization mode is 'gpu'
use the character language model embeddings as basis
length is twice the original character LM embedding length
these fields are for the embedding memory
whether to add only capitalized words to memory (faster runtime and lower memory consumption)
we re-compute embeddings dynamically at each epoch
set the memory method
memory is wiped each time we do a training run
"if we keep a pooling, it needs to be updated continuously"
update embedding
check token.text is empty or not
set aggregation operation
add embeddings after updating
temporary fix to disable tokenizer parallelism warning
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
do not print transformer warnings as these are confusing in this case
load tokenizer and transformer model
"in the end, these models don't need this configuration"
model name
whether to detach gradients on overlong sentences
store whether to use context (and how much)
dropout contexts
"if using context, can we cross document boundaries?"
send self to flair-device
embedding parameters
send mini-token through to check how many layers the model has
calculate embedding length
return length
check if special tokens exist to circumvent error message
"most models have an intial BOS token, except for XLNet, T5 and GPT2"
"when initializing, embeddings are in eval mode by default"
remove special markup
"we require encoded subtokenized sentences, the mapping to original tokens and the number of"
parts that each sentence produces
"if we also use context, first expand sentence to include context"
set context if not set already
"in case of contextualization, we must remember non-expanded sentence"
create expanded sentence and remember context offsets
overwrite sentence with expanded sentence
subtokenize the sentence
transformer specific tokenization
set zero embeddings for empty sentences and return
determine into how many subtokens each token is split
encode inputs
overlong sentences are handled as multiple splits
find longest sentence in batch
initialize batch tensors and mask
put encoded batch through transformer model to get all hidden states of all encoder layers
make the tuple a tensor; makes working with it easier.
gradients are enabled if fine-tuning is enabled
iterate over all subtokenized sentences
"remove stride_size//2 at end of sentence_hidden_state, and half at beginning of remainder,"
in order to get some context into the embeddings of these words.
also don't include the embedding of the extra [CLS] and [SEP] tokens.
"for each token, get embedding"
some tokens have no subtokens at all (if omitted by BERT tokenizer) so return zero vector
"get states from all selected layers, aggregate with pooling operation"
use layer mean of embeddings if so selected
set the extracted embedding for the token
move embeddings from context back to original sentence (if using context)
remember original sentence
get left context
get right context
empty contexts should not introduce whitespace tokens
make expanded sentence
iterate over subtokens and reconstruct tokens
remove special markup
TODO check if this is necessary is this method is called before prepare_for_model
check if reconstructed token is special begin token ([CLS] or similar)
some BERT tokenizers somehow omit words - in such cases skip to next token
append subtoken to reconstruct token
check if reconstructed token is the same as current token
"if so, add subtoken count"
reset subtoken count and reconstructed token
break from loop if all tokens are accounted for
if tokens are unaccounted for
check if all tokens were matched to subtokens
"""""""Returns the length of the embedding vector."""""""
special handling for serializing transformer models
serialize the transformer models and the constructor arguments (but nothing else)
necessary for reverse compatibility with Flair <= 0.7
special handling for deserializing transformer models
load transformer model
constructor arguments
re-initialize transformer word embeddings with constructor arguments
"I have no idea why this is necessary, but otherwise it doesn't work"
reload tokenizer to get around serialization issues
max_tokens = 500
model architecture
model architecture
download if necessary
load the model
"TODO: keep for backwards compatibility, but remove in future"
save the sentence piece model as binary file (not as path which may change)
write out the binary sentence piece model into the expected directory
"if the model was saved as binary and it is not found on disk, write to appropriate path"
"otherwise, use normal process and potentially trigger another download"
"once the modes if there, load it with sentence piece"
empty words get no embedding
all other words get embedded
"the default model for ELMo is the 'original' model, which is very large"
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
put on Cuda if available
embed a dummy sentence to determine embedding_length
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
GLOVE embeddings
"find train, dev and test files if not specified"
get train data
read in test file if exists
read in dev file if exists
special key for space after
"store either Sentence objects in memory, or only file offsets"
"most data sets have the token text in the first column, if not, pass 'text' as column"
determine encoding of text file
skip first line if to selected
option 1: read only sentence boundaries as offset positions
option 2: keep everything in memory
pointer to previous
"if sentence ends, break"
skip comments
"if sentence ends, convert and return"
check if this sentence is a document boundary
"otherwise, this line is a token. parse and add to sentence"
check if this sentence is a document boundary
"for example, transforming 'B-OBJ' to 'B-part-of-speech-object'"
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
set sentence context using partials
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
download data if necessary
download files if not present locally
we need to slightly modify the original files by adding some new lines after document separators
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
Remove CoNLL-U meta information in the last column
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
entity_mapping
this dataset name
default dataset folder is the cache root
download data if necessary
data validation
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
download files if not present locallys
column format
this dataset name
default dataset folder is the cache root
# download zip
merge the files in one as the zip is containing multiples files
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"unzip the downloaded repo and merge the train, dev and test datasets"
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
check if data there
create folder
download dataset
column format
this dataset name
default dataset folder is the cache root
download and parse data if necessary
create train test dev if not exist
column format
this dataset name
default dataset folder is the cache root
If the extracted corpus file is not yet present in dir
download zip if necessary
"extracted corpus is not present , so unpacking it."
column format
this dataset name
default dataset folder is the cache root
download zip
unpacking the zip
merge the files in one as the zip is containing multiples files
column format
this dataset name
default dataset folder is the cache root
"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)"
download files if not present locally
we need to modify the original files by adding new lines after after the end of each sentence
if only one language is given
column format
this dataset name
default dataset folder is the cache root
"use all languages if explicitly set to ""all"""
download data if necessary
initialize comlumncorpus and add it to list
column format
this dataset name
default dataset folder is the cache root
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
this list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
get google drive id from list
download from google drive
unzip
"tar.extractall(language_folder,members=[tar.getmember(file_name)])"
transform data into required format
"the processed dataset has the additional ending ""_new"""
remove the unprocessed dataset
initialize comlumncorpus and add it to list
if no languages are given as argument all languages used in XTREME will be loaded
if only one language is given
column format
this dataset name
default dataset folder is the cache root
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
This list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
download from HU Server
unzip
transform data into required format
initialize comlumncorpus and add it to list
if only one language is given
column format
this dataset name
default dataset folder is the cache root
download data if necessary
initialize comlumncorpus and add it to list
download data if necessary
unpack and write out in CoNLL column-like format
column format
this dataset name
default dataset folder is the cache root
download data if necessary
data is not in IOB2 format. Thus we transform it to IOB2
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
rename according to train - test - dev - convention
column format
this dataset name
default dataset folder is the cache root
column format
""
since only the WordNet 3.0 version for senses is consistently available for all provided datasets we will
only consider this version
""
also we ignore the id annotation used in datasets that were originally created for evaluation tasks
""
if the other annotations should be needed simply add the columns in correct order according
to the chosen datasets here and respectively change the values of the blacklist array and
the range value of the else case in the token for loop in the from_ufsac_to_conll function
this dataset name
default dataset folder is the cache root
check if data there
determine correct CoNLL files
tokens to ignore (edit here for variation)
counter to keep track how many tags have been found in line
variable to count of how many words a chunk consists
indicates if surface form is chunk or not
array to save tags temporarily for handling chunks
cut token to get chunk
save single words of chunk
handle first word of chunk
edit here for variation
check if converted file exists
convert the file to CoNLL
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"TODO: Add a routine, that checks annotations for some widespread errors/inconsistencies??? (e.g. in AQUAINT corpus Iran-Iraq_War vs. Iran-Iraq_war)"
Create the annotation dictionary
this fct removes every second unknown label
this dataset name
default dataset folder is the cache root
download and parse data if necessary
iterate over all html files
"get rid of html syntax, we only need the text"
between all documents we write a separator symbol
skip empty strings
"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)"
"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention"
sentence splitting and tokenization
iterate through all annotations and add to corresponding tokens
find sentence to which annotation belongs
position within corresponding sentence
set annotation for tokens of entity mention
write to out-file in column format
"in case something goes wrong, delete the dataset and raise error"
this dataset name
default dataset folder is the cache root
download and parse data if necessary
from qwikidata.linked_data_interface import get_entity_dict_from_api
generate qid wikiname dictionaries
merge dictionaries
ignore first line
commented and empty lines
read all Q-IDs
ignore first line
request
this dataset name
default dataset folder is the cache root
we use the wikiids in the data instead of directly utilizing the wikipedia urls.
like this we can quickly check if the corresponding page exists
if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi
delete unprocessed file
collect all wikiids
create the dictionary
request
this dataset name
default dataset folder is the cache root
names of raw text documents
open output_file
iterate through all documents
split sentences and tokenize
iterate through all annotations and add to corresponding tokens
find sentence to which annotation belongs
position within corresponding sentence
set annotation for tokens of entity mention
write to out file
this dataset name
default dataset folder is the cache root
download and parse data if necessary
this dataset name
default dataset folder is the cache root
download and parse data if necessary
First parse the post titles
Keep track of how many and which entity mentions does a given post title have
Check if the current post title has an entity link and parse accordingly
Post titles with entity mentions (if any) are handled via this function
Then parse the comments
"Iterate over the comments.tsv file, until the end is reached"
"Keep track of the current comment thread and its corresponding key, on which the annotations are matched."
Each comment thread is handled as one 'document'.
Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.
This if-condition is needed to handle this problem.
"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure"
"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above"
"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle."
"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,"
and not just single letters into single rows.
If there are annotated entity mentions for given post title or a comment thread
"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence"
Write the token with a corresponding tag to file
"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed"
"If a comment thread or a post title has no entity link, all tokens are assigned the O tag"
Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized
"incorrectly, in order to keep the desired format (empty line as a sentence separator)."
"Thrown when the second check above happens, but the last token of a sentence is reached."
"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below."
"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS"
Check if further annotations belong to the current post title or comment thread as well
Stop when the end of an annotation file is reached
Check if further annotations belong to the current sentence as well
"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)"
this dataset name
default dataset folder is the cache root
download data if necessary
if True:
write CoNLL-U Plus header
"Some special cases (e.g., missing spaces before entity marker)"
necessary if text should be whitespace tokenizeable
Handle case where tail may occur before the head
this dataset name
default dataset folder is the cache root
write CoNLL-U Plus header
this dataset name
default dataset folder is the cache root
TODO: change data source to original CoNLL04 -- this dataset has span formatting errors
download data if necessary
write CoNLL-U Plus header
The span has ended.
We are entering a new span; reset indices
and active tag to new span.
We're inside a span.
Last token might have been a part of a valid span.
this dataset name
default dataset folder is the cache root
write CoNLL-U Plus header
"for source_file_path, target_filename in zip(source_file_paths, target_filenames):"
"with zip_file.open(source_file_path, mode=""r"") as source_file:"
target_file_path = Path(data_folder) / target_filename
"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:"
# write CoNLL-U Plus header
"target_file.write(""# global.columns = id form ner\n"")"
for example in json.load(source_file):
token_list = self._tacred_example_to_token_list(example)
target_file.write(token_list.serialize())
hardcoded mapping TODO: perhaps find nicer solution
"find train, dev and test files if not specified"
use test_file to create test split if available
use dev_file to create test split if available
"if data point contains black-listed label, do not use"
first check if valid sentence
"if so, add to indices"
"find train, dev and test files if not specified"
variables
different handling of in_memory data than streaming data
"most data sets have the token text in the first column, if not, pass 'text' as column"
test if format is OK
test if at least one label given
make sentence from text (and filter for length)
"if a pair column is defined, make a sentence pair object"
noinspection PyDefaultArgument
dataset name includes the split size
default dataset folder is the cache root
download data if necessary
download each of the 28 splits
create dataset directory if necessary
download senteval datasets if necessary und unzip
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
"by defaut, map point score to POSITIVE / NEGATIVE values"
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file from CSV
create test.txt file from CSV
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create train dev and test files in fasttext format
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
convert to FastText format
if no base_path provided take cache root
download data if necessary
"if data is not downloaded yet, download it"
get the zip file
move original .tsv files to another folder
create train and dev splits in fasttext format
create eval_dataset file with no labels
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
download data if necessary
download datasets if necessary
create dataset directory if necessary
create correctly formated txt files
multiple labels are possible
this dataset name
default dataset folder is the cache root
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
default dataset folder is the cache root
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
create a separate directory for different tasks
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
check if dataset is supported
set file names
set file names
download and unzip in file structure if necessary
instantiate corpus
"find train, dev and test files if not specified"
"create DataPairDataset for train, test and dev file, if they are given"
stop if file does not exist
create a DataPair object from strings
"if in_memory is True we return a datapair, otherwise we create one from the lists of strings"
if no base_path provided take cache root
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
if no base_path provided take cache root
"if data is not downloaded yet, download it"
get the zip file
"reorder dev datasets to have same columns as in train set: 8, 9, and 11"
dev sets include 5 different annotations but we will only keep the gold label
"rename test file to eval_dataset, since it has no labels"
if no base_path provided take cache root
"if data is not downloaded yet, download it"
get test and dev sets
if no base_path provided take cache root
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
if no base_path provided take cache root
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
if no base_path provided take cache root
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
if no base_path provided take cache root
"if data not downloaded yet, download it"
get the zip file
"the downloaded files have json format, we transform them to tsv"
Function to transform JSON file to tsv for Recognizing Textual Entailment Data
remove json file
"find train, dev and test files if not specified"
get train data
get test data
get dev data
"if no fields specified, check if the file is CoNLL plus formatted and get fields"
option 1: read only sentence boundaries as offset positions
option 2: keep everything in memory
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
current token ID
relations: List[Relation] = []
head and tail span indices are 1-indexed and end index is inclusive
determine all NER label types in sentence and add all NER spans as sentence-level labels
Uses dynamic programming approach to calculate maximum independent set in interval graph
with sum of all entity lengths as secondary key
calculate offset without current text
because we stick all passages of a document together
TODO For split entities we also annotate everything inbetween which might be a bad idea?
Try to fix incorrect annotations
print(
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
)
Ignore empty lines or relation annotations
FIX annotation of whitespaces (necessary for PDR)
One token may contain multiple entities -> deque all of them
column format
this dataset name
default dataset folder is the cache root
Create tokenization-dependent CONLL files. This is necessary to prevent
from caching issues (e.g. loading the same corpus with different sentence splitters)
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
Edge case: last token starts a new entity
Last document in file
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
Edge case: last token starts a new entity
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
download file is huge => make default_dir visible so that derivative
corpora can all use the same download file
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
Read texts
Read annotations
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
We need to apply a patch to correct the original training file
Articles title
Article abstract
Entity annotations
column format
this dataset name
default dataset folder is the cache root
Edge case: last token starts a new entity
Map all entities to chemicals
Map all entities to disease
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
Incomplete article
Invalid XML syntax
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
if len(mid) != 3:
continue
Try to fix entity offsets
column format
this dataset name
default dataset folder is the cache root
There is still one illegal annotation in the file ..
column format
this dataset name
default dataset folder is the cache root
"Abstract first, title second to prevent issues with sentence splitting"
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
"Filter for specific entity types, by default no entities will be filtered"
Get original HUNER splits to retrieve a list of all document ids contained in V2
train and dev split of V2 will be train in V4
test split of V2 will be dev in V4
New documents in V4 will become test documents
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
cache Feidegger config file
cache Feidegger images
replace image URL with local cached file
append Sentence-Image data point
"in certain cases, multi-CPU data loading makes no sense and slows"
"everything down. For this reason, we detect if a dataset is in-memory:"
"if so, num_workers is set to 0 for faster processing"
cast to list if necessary
cast to list if necessary
"first, check if pymongo is installed"
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
Expose base classses
Expose all sequence labeling datasets
standard NER datasets
other NER datasets
keyphrase detection datasets
word sense disambiugation
universal proposition banks
Expose all entity linking datasets
Expose all document classification datasets
Expose all treebanks
Expose all text-text datasets
Expose all text-image datasets
Expose all biomedical data sets
Expose all biomedical data sets using the HUNER splits
-
-
-
-
Expose all biomedical data sets used for the evaluation of BioBERT
Expose all relation extraction datasets
"find train, dev and test files if not specified"
get train data
get test data
get dev data
option 1: read only sentence boundaries as offset positions
option 2: keep everything in memory
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
current token ID
handling for the awful UD multiword format
end of sentence
comments
ellipsis
if token is a multi-word
normal single-word tokens
"if we don't split multiwords, skip over component words"
add token
add morphological tags
derive whitespace logic for multiwords
print(token)
print(current_multiword_last_token)
print(current_multiword_first_token)
"if multi-word equals component tokens, there should be no whitespace"
go through all tokens in subword and set whitespace_after information
print(i)
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
test corpus
create a TARS classifier
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
clean up file
bioes tags
bio tags
broken tags
all tags
all weird tags
tags with confidence
bioes tags
bioes tags
"city single-token, person and company multi-token"
increment for last token in sentence if not followed by whitespace
clean up directory
clean up directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
clean up results directory
initialize trainer
clean up results directory
from flair.trainers.trainer_regression import RegressorTrainer
def test_trainer_evaluation(tasks_base_path):
"corpus, model, trainer = init(tasks_base_path)"
""
expected = model.evaluate(corpus.dev)
""
assert expected is not None
def test_trainer_results(tasks_base_path):
"corpus, model, trainer = init(tasks_base_path)"
"results = trainer.train(""regression_train/"", max_epochs=1)"
"assert results[""test_score""] > 0"
"assert len(results[""dev_loss_history""]) == 1"
"assert len(results[""dev_score_history""]) == 1"
"assert len(results[""train_loss_history""]) == 1"
@pytest.mark.integration
initialize trainer
"loaded_model.predict([sentence, sentence_empty])"
loaded_model.predict([sentence_empty])
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
clean up results directory
define search space
sequence tagger parameter
model trainer parameter
training parameter
find best parameter settings
clean up results directory
document embeddings parameter
training parameter
clean up results directory
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
get two corpora as one
"get training, test and dev data for full English UD corpus from web"
clean up data directory
load dataset
tagger without CRF
train
check if loaded model can predict
check if loaded model successfully fit the training data
clean up results directory
load dataset
tagger without CRF
train
check if loaded model can predict
check if loaded model successfully fit the training data
clean up results directory
load dataset
tagger without CRF
train
check if loaded model can predict
check if loaded model successfully fit the training data
clean up results directory
load dataset
tagger without CRF
train
check if loaded model can predict
check if loaded model successfully fit the training data
clean up results directory
check if model can predict
load model
chcek if model predicts correct label
check if loaded model successfully fit the training data
clean up results directory
check if model can predict
load model
chcek if model predicts correct label
check if loaded model successfully fit the training data
clean up results directory
check if model can predict
load model
chcek if model predicts correct label
check if loaded model successfully fit the training data
clean up results directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
def test_labels_to_indices(tasks_base_path):
"corpus = flair.datasets.ClassificationCorpus(tasks_base_path / ""ag_news"", label_type=""topic"")"
label_dict = corpus.make_label_dictionary()
"model = TextClassifier(document_embeddings,"
"label_dictionary=label_dict,"
"label_type=""topic"","
multi_label=False)
""
result = model._labels_to_indices(corpus.train)
""
for i in range(len(corpus.train)):
expected = label_dict.get_idx_for_item(corpus.train[i].labels[0].value)
actual = result[i].item()
""
assert expected == actual
""
""
def test_labels_to_one_hot(tasks_base_path):
"corpus = flair.datasets.ClassificationCorpus(tasks_base_path / ""ag_news"", label_type=""topic"")"
label_dict = corpus.make_label_dictionary()
"model = TextClassifier(document_embeddings,"
"label_dictionary=label_dict,"
"label_type=""topic"","
multi_label=False)
""
result = model._labels_to_one_hot(corpus.train)
""
for i in range(len(corpus.train)):
expected = label_dict.get_idx_for_item(corpus.train[i].labels[0].value)
actual = result[i]
""
for idx in range(len(label_dict)):
if idx == expected:
assert actual[idx] == 1
else:
assert actual[idx] == 0
1. get the corpus
2. what tag do we want to predict?
3. make the tag dictionary from the corpus
initialize embeddings
comment in this line to use character embeddings
"CharacterEmbeddings(),"
comment in these lines to use contextual string embeddings
""
"FlairEmbeddings('news-forward'),"
""
"FlairEmbeddings('news-backward'),"
initialize sequence tagger
initialize trainer
from allennlp.common.tqdm import Tqdm
mmap seems to be much more memory efficient
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
Extract all the contents of zip file in current directory
Extract all the contents of zip file in current directory
get cache path to put the file
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
first determine the distribution of classes in the dataset
weight for each sample
Create blocks
shuffle the blocks
concatenate the shuffled blocks
Create blocks
shuffle the blocks
concatenate the shuffled blocks
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
see https://github.com/zalandoresearch/flair/issues/351
State initialization
Exponential moving average of gradient values
Exponential moving average of squared gradient values
Maintains max of all exp. moving avg. of sq. grad. values
Decay the first and second moment running average coefficient
Maintains the maximum of all 2nd moment running avg. till now
Use the max. for normalizing running avg. of gradient
determine offsets for whitespace_after field
increment for last token in sentence if not followed by whitespace
determine offsets for whitespace_after field
conll 2000 column format
conll 03 NER column format
WNUT-17
-- WikiNER datasets
-- Universal Dependencies
Germanic
Romance
West-Slavic
South-Slavic
East-Slavic
Scandinavian
Asian
Language isolates
recent Universal Dependencies
other datasets
text classification format
text regression format
"first, try to fetch dataset online"
default dataset folder is the cache root
get string value if enum is passed
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
the CoNLL 03 task for German has an additional lemma column
the CoNLL 03 task for Dutch has no NP column
the CoNLL 03 task for Spanish only has two columns
the GERMEVAL task only has two columns: text and ner
WSD tasks may be put into this column format
"the UD corpora follow the CoNLL-U format, for which we have a special reader"
"for text classifiers, we use our own special format"
NER corpus for Basque
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
get train and test data
"read in test file if exists, otherwise sample 10% of train data as test dataset"
"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
convert tag scheme to iobes
automatically identify train / test / dev files
automatically identify train / test / dev files
"most data sets have the token text in the first column, if not, pass 'text' as column"
conll 2000 chunking task
Support both TREC-6 and TREC-50
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
Wikiner NER task
unpack and write out in CoNLL column-like format
CoNLL 02/03 NER
universal dependencies
--- UD Germanic
--- UD Romance
--- UD West-Slavic
--- UD Scandinavian
--- UD South-Slavic
--- UD Asian
this is the default init size of a lmdb database for embeddings
some non-used parameter to allow print
get db filename from embedding name
"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
SequenceTagger
TextClassifier
get db filename from embedding name
if embedding database already exists
"otherwise, push embedding to database"
if embedding database already exists
open the database in read mode
we need to set self.k
create and load the database in write mode
"no idea why, but we need to close and reopen the environment to avoid"
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
when opening new transaction !
init dictionaries
"in order to deal with unknown tokens, add <unk>"
"We don't want to create a SpaceTokenizer object each time this function is called,"
so delegate the call directly to the static run_tokenize method
"We don't want to create a SegtokTokenizer object each time this function is called,"
so delegate the call directly to the static run_tokenize method
"if text is passed, instantiate sentence with tokens (words)"
log a warning if the dataset is empty
some sentences represent a document boundary (but most do not)
data with zero-width characters cannot be handled
set token idx if not set
non-set tags are OUT tags
anything that is not a BIOES tag is a SINGLE tag
anything that is not OUT is IN
single and begin tags start a new span
remember previous tag
"if label type is explicitly specified, get spans for this label type"
else determine all label types in sentence and get all spans
move sentence embeddings to device
move token embeddings to device
clear sentence embeddings
clear token embeddings
infer whitespace after field
add Sentence labels to output if they exist
add Token labels to output if they exist
add Sentence labels to output if they exist
add Token labels to output if they exist
No character at the corresponding code point: remove it
set name
sample test data if none is provided
sample dev data if none is provided
set train dev and test data
find out empty sentence indices
create subset of non-empty sentence indices
find out empty sentence indices
create subset of non-empty sentence indices
check if sentence itself has labels
check for labels of words
Make the tag dictionary
global variable: cache_root
global variable: device
global variable: embedding_storage_mode
# dummy return to fulfill trainer.train() needs
print(vec)
Attach optimizer
"convert `metrics` to float, in case it's a zero-dim Tensor"
if memory mode option 'none' delete everything
else delete only dynamic embeddings (otherwise autograd will keep everything in memory)
find out which ones are dynamic embeddings
find out which ones are dynamic embeddings
memory management - option 1: send everything to CPU (pin to memory if we train on GPU)
record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class)
header for 'weights.txt'
"determine the column index of loss, f-score and accuracy for train, dev and test split"
then get all relevant values from the tsv
then get all relevant values from the tsv
plot i
save plots
save plots
plt.show()
save plot
take the average over the last three scores of training
take average over the scores from the different training runs
remove previous embeddings
clearing token embeddings to save memory
"read Dataset into data loader (if list of sentences passed, make Dataset first)"
#TODO: not saving lines yet
== similarity measures ==
helper class for ModelSimilarity
-- works with binary cross entropy loss --
"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}"
-- works with ranking/triplet loss --
normalize the embeddings
== similarity losses ==
"we want that logits for corresponding pairs are high, and for non-corresponding low"
TODO: this assumes eye matrix
"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa"
== similarity learner ==
"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both"
assumes that for each data pair there's at least one embedding per modality
pre-compute embeddings for all targets in evaluation dataset
compute the similarity
sort the similarity matrix across modality 1
"get the ranks, so +1 to start counting ranks from 1"
The conversion from old model's constructor interface
auto-spawn on GPU if available
pad strings with whitespaces to longest sentence
cut up the input into chunks of max charlength = chunk_size
push each chunk through the RNN language model
concatenate all chunks to make final output
initial hidden state
get predicted weights
divide by temperature
"to prevent overflow problem with small temperature values, substract largest value from all"
this makes a vector in which the largest value is 0
compute word weights with exponential function
try sampling multinomial distribution for next character
print(word_idx)
input ids
push list of character IDs through model
the target is always the next character
use cross entropy loss to compare output of forward pass with targets
exponentiate cross-entropy loss to calculate perplexity
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
set the dictionaries
"if we use a CRF, we must add special START and STOP tags to the dictionary"
Initialize the weight tensor
initialize the network architecture
dropouts
optional reprojection layer on top of word embeddings
bidirectional LSTM on top of embedding layer
Create initial hidden state and initialize it
TODO: Decide how to initialize the hidden state variables
self.hs_initializer(self.lstm_init_h)
self.hs_initializer(self.lstm_init_c)
final linear map to tag space
set context if not set already
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
clearing token embeddings to save memory
predict for batch
make list of gold tags
make list of predicted tags
"check for true positives, false positives and false negatives"
also write to file in BIO format to use old conlleval script
check if in gold spans
check if in predicted spans
"read Dataset into data loader (if list of sentences passed, make Dataset first)"
"if span F1 needs to be used, use separate eval method"
"else, use scikit-learn to evaluate"
predict for batch
add gold tag
add predicted tag
for file output
use sklearn
"make ""classification report"""
report over all in case there are no labels
get scores
line for log file
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
"if initial hidden state is trainable, use this state"
word dropout only before LSTM - TODO: more experimentation needed
if self.use_word_dropout > 0.0:
sentence_tensor = self.word_dropout(sentence_tensor)
get the tags in this sentence
add tags as tensor
pad tags if using batch-CRF decoder
reduce raw values to avoid NaN during exp
broadcasting will do the job of reshaping and is more efficient than calling repeat
default value
core Flair models on Huggingface ModelHub
"Large NER models,"
Multilingual NER models
English POS models
Multilingual POS models
English SRL models
English chunking models
Language-specific NER models
English NER models
Multilingual NER models
English POS models
Multilingual POS models
English SRL models
English chunking models
Danish models
German models
French models
Dutch models
Malayalam models
Portuguese models
Keyphase models
Biomedical models
check if model name is a valid local file
"check if model key is remapped to HF key - if so, print out information"
get mapped name
output information
use mapped name instead
"if not, check if model key is remapped to direct download location. If so, download model"
special handling for the taggers by the @redewiegergabe project (TODO: move to model hub)
"for all other cases (not local file or special download location), use HF model hub"
"if not a local file, get from model hub"
use model name as subfolder
Lazy import
output information
"log.error(f"" - Error message: {e}"")"
clear embeddings after predicting
load each model
check if the same embeddings were already loaded previously
"if the model uses StackedEmbedding, make a new stack with previous objects"
sort embeddings by key alphabetically
check previous embeddings and add if found
only re-use static embeddings
"if not found, use existing embedding"
initialize new stack
"of the model uses regular embedding, re-load if previous version found"
Initialize the weight tensor
auto-spawn on GPU if available
filter empty sentences
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
clearing token embeddings to save memory
"read Dataset into data loader (if list of sentences passed, make Dataset first)"
use scikit-learn to evaluate
remove previously predicted labels
get the gold labels
predict for batch
get the predicted labels
remove predicted labels
"make ""classification report"""
get scores
line for log file
English sentiment models
Communicative Functions Model
Initialize TextClassifier
if bi_mode == True the linear layer needs twice the length of the embeddings as input size
since we concatenate the embeddings of the two DataPoints in the DataPairs
TODO: Transformers use special separator symbols in the beginning and between elements
of datapair. Here should be a case dinstintion between the different transformers.
linear layer
Drop unnecessary attributes from Parent class
prepare binary label dictionary
Store task specific labels since TARS can handle multiple tasks
make label dictionary if no Dictionary object is passed
get and embed all labels by making a Sentence object that contains only the label text
get each label embedding and scale between 0 and 1
compute similarity matrix
"the higher the similarity, the greater the chance that a label is"
sampled as negative example
make sure the probabilities always sum up to 1
init new TARS classifier
set all task information
linear layers of internal classifier
Transform input data into TARS format
"M: num_classes in task, N: num_samples"
reshape scores MN x 2 -> N x M x 2
import torch
a = torch.arange(30)
"b = torch.reshape(-1, 3, 2)"
"c = b[:,:,1]"
target shape N x M
Transform label_scores
Transform label_scores into current task's desired format
"TARS does not do a softmax, so confidence of the best predicted class might be very low."
Therefore enforce a min confidence of 0.5 for a match.
make list if only one candidate label is passed
"if list is passed, convert to set"
check if candidate_label_set is empty
note current task
create a temporary task
make zero shot predictions
switch to the pre-existing task
remember current task
predict with each task model
switch to the pre-existing task
embeddings
dictionaries
linear layer
F-beta score
all parameters will be pushed internally to the specified device
"read Dataset into data loader (if list of sentences passed, make Dataset first)"
"if span F1 needs to be used, use separate eval method"
"else, use scikit-learn to evaluate"
predict for batch
add gold tag
add predicted tag
for file output
use sklearn
"make ""classification report"""
report over all in case there are no labels
get scores
line for log file
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
clearing token embeddings to save memory
get the tags in this sentence
add tags as tensor
predict for batch
make list of gold tags
make list of predicted tags
"check for true positives, false positives and false negatives"
also write to file in BIO format to use old conlleval script
check if in gold spans
check if in predicted spans
weights for loss function
iput size is two times wordembedding size since we use pair of words as input
"the output size is max_distance + 1, i.e. we allow 0,1,...,max_distance words between pairs"
regression
input size is two times word embedding size since we use pair of words as input
the output size is 1
auto-spawn on GPU if available
all input should be tensors
forward allows only a single sentcence!!
embed words of sentence
go through all pairs of words with a maximum number of max_distance in between
go through all pairs
2-dim matrix whose rows are the embeddings of word pairs of the sentence
So far only one sentence allowed
If list of sentences is handed the function works with the first sentence of the list
Assume data_points is a single sentence!!!
scores are the predictions for each word pair
"classification needs labels to be integers, regression needs labels to be float"
this is due to the different loss functions
only single sentences as input
gold labels
for output text file
for buckets
for average prediction
add some statistics to the output
use scikit-learn to evaluate
"we iterate over each sentence, instead of batches"
get single labels from scores
gold labels
for output text file
hot one vector of true value
hot one vector of predicted value
"speichert embeddings, falls embedding_storage!= 'None'"
"make ""classification report"""
get scores
"precision_score = round(metrics.precision_score(y_true, y_pred, average='macro', zero_division=0), 4)"
"recall_score = round(metrics.recall_score(y_true, y_pred, average='macro', zero_division=0), 4)"
line for log file
cast string to Path
"determine what splits (train, dev, test) to evaluate and log"
prepare loss logging file and set up header
"minimize training loss if training with dev data, else maximize dev score"
"if training also uses dev/train data, include in training set"
initialize sampler if provided
init with default values if only class is provided
set dataset to sample from
At any point you can hit Ctrl + C to break out of training early.
get new learning rate
reload last best model if annealing with restarts is enabled
stop training if learning rate becomes too small
process mini-batches
zero the gradients on the model and optimizer
"if necessary, make batch_steps"
forward and backward for batch
forward pass
Backward
do the optimizer step
do the scheduler step if one-cycle
get new learning rate
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
"anneal against train loss if training with dev, otherwise anneal against dev score"
evaluate on train / dev / test split depending on training settings
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
calculate scores using dev data if available
append dev score to score history
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
determine learning rate annealing through scheduler. Use auxiliary metric for AnnealOnPlateau
determine bad epoch number
log bad epochs
output log file
make headers on first epoch
"if checkpoint is enabled, save model at each epoch"
"if we use dev data, remember best model based on dev evaluation score"
"if we do not use dev data for model selection, save final model"
test best model if test data is present
"if we are training over multiple datasets, do evaluation for each"
get and return the final test score of best model
cast string to Path
forward pass
update optimizer and scheduler
Add chars to the dictionary
charsplit file content
charsplit file content
Add words to the dictionary
Tokenize file content
"TextDataset returns a list. valid and test are only one file, so return the first element"
cast string to Path
error message if the validation dataset is too small
Shuffle training files randomly after serially iterating through corpus one
"iterate through training data, starting at self.split (for checkpointing)"
off by one for printing
go into train mode
reset variables
not really sure what this does
do the forward pass in the model
try to predict the targets
Backward
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
We detach the hidden state from how it was previously produced.
"If we didn't, the model would try backpropagating all the way to start of the dataset."
explicitly remove loss to clear up memory
##############################################################################
Save the model if the validation loss is the best we've seen so far.
##############################################################################
print info
##############################################################################
##############################################################################
final testing
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
"multilingual forward (English, German, French, Italian, Dutch, Polish)"
"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
Dutch forward
Dutch backward
Swedish forward
Swedish backward
French forward
French backward
Czech forward
Czech backward
Portuguese forward
Portuguese backward
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
1-camembert-base -> camembert-base
1-xlm-roberta-large -> xlm-roberta-large
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
The mask has 1 for real tokens and 0 for padding tokens. Only real
tokens are attended to.
Zero-pad up to the sequence length.
"first, find longest sentence in batch"
prepare id maps for BERT model
put encoded batch through BERT model to get all hidden states of all encoder layers
get aggregated embeddings for each BERT-subtoken in sentence
get the current sentence object
add concatenated embedding to sentence
use first subword embedding if pooling operation is 'first'
"otherwise, do a mean over all subwords in token"
"if only one sentence is passed, convert to list of sentence"
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
embed a dummy sentence to determine embedding_length
Avoid conflicts with flair's Token class
"<cls> token initially set to 1/D, so it attends to all image features equally"
add positional encodings
reshape the pixels into the sequence
layer norm after convolution and positional encodings
add <cls> token
"transformer requires input in the shape [h*w+1, b, d]"
the output is an embedding of <cls> token
temporary fix to disable tokenizer parallelism warning
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
load tokenizer and transformer model
model name
"when initializing, embeddings are in eval mode by default"
embedding parameters
send mini-token through to check how many layers the model has
check whether CLS is at beginning or end
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
using list comprehension
gradients are enabled if fine-tuning is enabled
"first, subtokenize each sentence and find out into how many subtokens each token was divided"
subtokenize sentences
tokenize and truncate to max subtokens (TODO: check better truncation strategies)
find longest sentence in batch
initialize batch tensors and mask
put encoded batch through transformer model to get all hidden states of all encoder layers
iterate over all subtokenized sentences
use scalar mix of embeddings if so selected
set the extracted embedding for the token
special handling for serializing transformer models
serialize the transformer models and the constructor arguments (but nothing else)
necessary for reverse compatibility with Flair <= 0.7
special handling for deserializing transformer models
load transformer model
constructor arguments
re-initialize transformer word embeddings with constructor arguments
"I have no idea why this is necessary, but otherwise it doesn't work"
reload tokenizer to get around serialization issues
optional fine-tuning on top of embedding layer
"if only one sentence is passed, convert to list of sentence"
"if only one sentence is passed, convert to list of sentence"
bidirectional RNN on top of embedding layer
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push through RNN
after-RNN dropout
extract embeddings from RNN
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
IMPORTANT: add embeddings as torch modules
iterate over sentences
"if its a forward LM, take last state"
"convert to plain strings, embedded in a list for the encode function"
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
"if only one sentence is passed, convert to list of sentence"
Expose base classses
Expose token embedding classes
Expose document embedding classes
Expose image embedding classes
Expose legacy embedding classes
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
GLOVE embeddings
TURIAN embeddings
KOMNINOS embeddings
pubmed embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
twitter embeddings
two-letter language code wiki embeddings
two-letter language code wiki embeddings
two-letter language code crawl embeddings
fix serialized models
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
multilingual models
English models
Arabic
Bulgarian
Czech
Danish
German
Spanish
Basque
Persian
Finnish
French
Hebrew
Hindi
Croatian
Indonesian
Italian
Japanese
Malayalam
Dutch
Norwegian
Polish
Portuguese
Pubmed
Slovenian
Swedish
Tamil
CLEF HIPE Shared task
load model if in pretrained model map
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
embeddings are static if we don't do finetuning
embed a dummy sentence to determine embedding_length
set to eval mode
make compatible with serialized models (TODO: remove)
make compatible with serialized models (TODO: remove)
gradients are enable if fine-tuning is enabled
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
offset mode that extracts at whitespace after last character
offset mode that extracts at last character
only clone if optimization mode is 'gpu'
use the character language model embeddings as basis
length is twice the original character LM embedding length
these fields are for the embedding memory
whether to add only capitalized words to memory (faster runtime and lower memory consumption)
we re-compute embeddings dynamically at each epoch
set the memory method
memory is wiped each time we do a training run
"if we keep a pooling, it needs to be updated continuously"
update embedding
check token.text is empty or not
set aggregation operation
add embeddings after updating
temporary fix to disable tokenizer parallelism warning
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
load tokenizer and transformer model
model name
whether to detach gradients on overlong sentences
store whether to use context (and how much)
dropout contexts
"if using context, can we cross document boundaries?"
"when initializing, embeddings are in eval mode by default"
embedding parameters
send mini-token through to check how many layers the model has
calculate embedding length
return length
check if special tokens exist to circumvent error message
"most models have an intial BOS token, except for XLNet, T5 and GPT2"
remove special markup
embed each sentence separately
"TODO: keep for backwards compatibility, but remove in future"
"some pretrained models do not have this property, applying default settings now."
can be set manually after loading the model.
"if we also use context, first expand sentence to include context"
"in case of contextualization, we must remember non-expanded sentence"
create expanded sentence and remember context offsets
overwrite sentence with expanded sentence
subtokenize the sentence
method 1: subtokenize sentence
"subtokenized_sentence = self.tokenizer.encode(tokenized_string, add_special_tokens=True)"
method 2:
transformer specific tokenization
set zero embeddings for empty sentences and return
determine into how many subtokens each token is split
"if sentence is too long, will be split into multiple parts"
check if transformer version 3 is used - in this case use old handling
get sentence as list of subtoken ids
"else if current transformer is used, use default handling"
overlong sentences are handled as multiple splits
embed each sentence split
initialize batch tensors and mask
propagate gradients if fine-tuning and only during training
increase memory effectiveness by skipping all but last sentence split
put encoded batch through transformer model to get all hidden states of all encoder layers
get hidden states as single tensor
put splits back together into one tensor using overlapping strides
"for each token, get embedding"
some tokens have no subtokens at all (if omitted by BERT tokenizer) so return zero vector
"get states from all selected layers, aggregate with pooling operation"
use layer mean of embeddings if so selected
set the extracted embedding for the token
move embeddings from context back to original sentence (if using context)
remember original sentence
get left context
get right context
make expanded sentence
iterate over subtokens and reconstruct tokens
remove special markup
TODO check if this is necessary is this method is called before prepare_for_model
check if reconstructed token is special begin token ([CLS] or similar)
some BERT tokenizers somehow omit words - in such cases skip to next token
append subtoken to reconstruct token
check if reconstructed token is the same as current token
"if so, add subtoken count"
reset subtoken count and reconstructed token
break from loop if all tokens are accounted for
if tokens are unaccounted for
check if all tokens were matched to subtokens
"""""""Returns the length of the embedding vector."""""""
special handling for serializing transformer models
serialize the transformer models and the constructor arguments (but nothing else)
necessary for reverse compatibility with Flair <= 0.7
special handling for deserializing transformer models
load transformer model
constructor arguments
re-initialize transformer word embeddings with constructor arguments
"I have no idea why this is necessary, but otherwise it doesn't work"
reload tokenizer to get around serialization issues
max_tokens = 500
model architecture
model architecture
download if necessary
load the model
"TODO: keep for backwards compatibility, but remove in future"
save the sentence piece model as binary file (not as path which may change)
write out the binary sentence piece model into the expected directory
"if the model was saved as binary and it is not found on disk, write to appropriate path"
"otherwise, use normal process and potentially trigger another download"
"once the modes if there, load it with sentence piece"
empty words get no embedding
all other words get embedded
"the default model for ELMo is the 'original' model, which is very large"
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
put on Cuda if available
embed a dummy sentence to determine embedding_length
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
GLOVE embeddings
"find train, dev and test files if not specified"
get train data
read in test file if exists
read in dev file if exists
special key for space after
"store either Sentence objects in memory, or only file offsets"
"most data sets have the token text in the first column, if not, pass 'text' as column"
determine encoding of text file
skip first line if to selected
option 1: read only sentence boundaries as offset positions
option 2: keep everything in memory
pointer to previous
"if sentence ends, break"
skip comments
"if sentence ends, convert and return"
check if this sentence is a document boundary
"otherwise, this line is a token. parse and add to sentence"
check if this sentence is a document boundary
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
set sentence context using partials
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
"tag_to_bioes=tag_to_bioes,"
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
download data if necessary
download files if not present locally
we need to slightly modify the original files by adding some new lines after document separators
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"cached_path(f""{STACKOVERFLOW_NER_path}train_merged_labels.txt"", Path(""datasets"") / dataset_name) # TODO: what is this?"
column format
this dataset name
default dataset folder is the cache root
If the extracted corpus file is not yet present in dir
download zip if necessary
"extracted corpus is not present , so unpacking it."
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
Remove CoNLL-U meta information in the last column
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
rename according to train - test - dev - convention
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
data is not in IOB2 format. Thus we transform it to IOB2
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
tokens to ignore (edit here for variation)
counter to keep track how many tags have been found in line
variable to count of how many words a chunk consists
indicates if surface form is chunk or not
array to save tags temporarily for handling chunks
cut token to get chunk
save single words of chunk
handle first word of chunk
edit here for variation
check if converted file exists
convert the file to CoNLL
column format
""
since only the WordNet 3.0 version for senses is consistently available for all provided datasets we will
only consider this version
""
also we ignore the id annotation used in datasets that were originally created for evaluation tasks
""
if the other annotations should be needed simply add the columns in correct order according
to the chosen datasets here and respectively change the values of the blacklist array and
the range value of the else case in the token for loop in the from_ufsac_to_conll function
this dataset name
default dataset folder is the cache root
check if data there
determine correct CoNLL files
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
this list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
get google drive id from list
download from google drive
unzip
"tar.extractall(language_folder,members=[tar.getmember(file_name)])"
transform data into required format
"the processed dataset has the additional ending ""_new"""
remove the unprocessed dataset
initialize comlumncorpus and add it to list
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"unzip the downloaded repo and merge the train, dev and test datasets"
download data if necessary
unpack and write out in CoNLL column-like format
if no languages are given as argument all languages used in XTREME will be loaded
if only one language is given
column format
this dataset name
default dataset folder is the cache root
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
This list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
download from HU Server
unzip
transform data into required format
initialize comlumncorpus and add it to list
"find train, dev and test files if not specified"
use test_file to create test split if available
use dev_file to create test split if available
"if data point contains black-listed label, do not use"
first check if valid sentence
"if so, add to indices"
"find train, dev and test files if not specified"
variables
different handling of in_memory data than streaming data
"most data sets have the token text in the first column, if not, pass 'text' as column"
test if format is OK
test if at least one label given
noinspection PyDefaultArgument
dataset name includes the split size
default dataset folder is the cache root
download data if necessary
download each of the 28 splits
create dataset directory if necessary
download senteval datasets if necessary und unzip
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
"by defaut, map point score to POSITIVE / NEGATIVE values"
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file from CSV
create test.txt file from CSV
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
convert to FastText format
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
download data if necessary
download datasets if necessary
create dataset directory if necessary
create correctly formated txt files
multiple labels are possible
this dataset name
default dataset folder is the cache root
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
default dataset folder is the cache root
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
default dataset folder is the cache root
download data if necessary
create a separate directory for different tasks
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
check if dataset is supported
set file names
download and unzip in file structure if necessary
instantiate corpus
"find train, dev and test files if not specified"
"create DataPairDataset for train, test and dev file, if they are given"
stop if file does not exist
create a DataPair object from strings
"if in_memory is True we return a datapair, otherwise we create one from the lists of strings"
if no base_path provided take cache root
"if data is not downloaded yet, download it"
get the zip file
"rename test file to eval_dataset, since it has no labels"
if no base_path provided take cache root
"if data not downloaded yet, download it"
get the zip file
"the downloaded files have json format, we transform them to tsv"
Function to transform JSON file to tsv for Recognizing Textual Entailment Data
remove json file
Uses dynamic programming approach to calculate maximum independent set in interval graph
with sum of all entity lengths as secondary key
calculate offset without current text
because we stick all passages of a document together
TODO For split entities we also annotate everything inbetween which might be a bad idea?
Try to fix incorrect annotations
print(
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
)
Ignore empty lines or relation annotations
FIX annotation of whitespaces (necessary for PDR)
One token may contain multiple entities -> deque all of them
column format
this dataset name
default dataset folder is the cache root
Create tokenization-dependent CONLL files. This is necessary to prevent
from caching issues (e.g. loading the same corpus with different sentence splitters)
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
Edge case: last token starts a new entity
Last document in file
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
Edge case: last token starts a new entity
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
download file is huge => make default_dir visible so that derivative
corpora can all use the same download file
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
Read texts
Read annotations
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
We need to apply a patch to correct the original training file
Articles title
Article abstract
Entity annotations
column format
this dataset name
default dataset folder is the cache root
Edge case: last token starts a new entity
Map all entities to chemicals
Map all entities to disease
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
Incomplete article
Invalid XML syntax
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
if len(mid) != 3:
continue
Try to fix entity offsets
column format
this dataset name
default dataset folder is the cache root
There is still one illegal annotation in the file ..
column format
this dataset name
default dataset folder is the cache root
"Abstract first, title second to prevent issues with sentence splitting"
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
"Filter for specific entity types, by default no entities will be filtered"
Get original HUNER splits to retrieve a list of all document ids contained in V2
train and dev split of V2 will be train in V4
test split of V2 will be dev in V4
New documents in V4 will become test documents
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
cache Feidegger config file
cache Feidegger images
replace image URL with local cached file
append Sentence-Image data point
"in certain cases, multi-CPU data loading makes no sense and slows"
"everything down. For this reason, we detect if a dataset is in-memory:"
"if so, num_workers is set to 0 for faster processing"
cast to list if necessary
cast to list if necessary
"first, check if pymongo is installed"
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
Expose base classses
Expose all sequence labeling datasets
Expose all document classification datasets
Expose all treebanks
Expose all text-text datasets
Expose all text-image datasets
Expose all biomedical data sets
Expose all biomedical data sets using the HUNER splits
-
-
-
-
Expose all biomedical data sets used for the evaluation of BioBERT
"find train, dev and test files if not specified"
get train data
get test data
get dev data
option 1: read only sentence boundaries as offset positions
option 2: keep everything in memory
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
current token ID
handling for the awful UD multiword format
end of sentence
comments
ellipsis
if token is a multi-word
normal single-word tokens
"if we don't split multiwords, skip over component words"
add token
add morphological tags
derive whitespace logic for multiwords
print(token)
print(current_multiword_last_token)
print(current_multiword_first_token)
"if multi-word equals component tokens, there should be no whitespace"
go through all tokens in subword and set whitespace_after information
print(i)
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
test corpus
create a TARS classifier
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
clean up file
bioes tags
bio tags
broken tags
all tags
all weird tags
tags with confidence
bioes tags
bioes tags
increment for last token in sentence if not followed by whitespace
clean up directory
clean up directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
clean up results directory
initialize trainer
clean up results directory
def test_multiclass_metrics():
""
"metric = Metric(""Test"")"
"available_labels = [""A"", ""B"", ""C""]"
""
"predictions = [""A"", ""B""]"
"true_values = [""A""]"
TextClassifier._evaluate_sentence_for_text_classification(
"metric, available_labels, predictions, true_values"
)
""
"predictions = [""C"", ""B""]"
"true_values = [""A"", ""B""]"
TextClassifier._evaluate_sentence_for_text_classification(
"metric, available_labels, predictions, true_values"
)
""
print(metric)
from flair.trainers.trainer_regression import RegressorTrainer
def test_trainer_results(tasks_base_path):
"corpus, model, trainer = init(tasks_base_path)"
"results = trainer.train(""regression_train/"", max_epochs=1)"
"assert results[""test_score""] > 0"
"assert len(results[""dev_loss_history""]) == 1"
"assert len(results[""dev_score_history""]) == 1"
"assert len(results[""train_loss_history""]) == 1"
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
clean up results directory
define search space
sequence tagger parameter
model trainer parameter
training parameter
find best parameter settings
clean up results directory
document embeddings parameter
training parameter
clean up results directory
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
get two corpora as one
"get training, test and dev data for full English UD corpus from web"
clean up data directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
1. get the corpus
2. what tag do we want to predict?
3. make the tag dictionary from the corpus
initialize embeddings
comment in this line to use character embeddings
"CharacterEmbeddings(),"
comment in these lines to use contextual string embeddings
""
"FlairEmbeddings('news-forward'),"
""
"FlairEmbeddings('news-backward'),"
initialize sequence tagger
initialize trainer
from allennlp.common.tqdm import Tqdm
mmap seems to be much more memory efficient
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
Extract all the contents of zip file in current directory
Extract all the contents of zip file in current directory
get cache path to put the file
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
first determine the distribution of classes in the dataset
weight for each sample
Create blocks
shuffle the blocks
concatenate the shuffled blocks
Create blocks
shuffle the blocks
concatenate the shuffled blocks
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
see https://github.com/zalandoresearch/flair/issues/351
State initialization
Exponential moving average of gradient values
Exponential moving average of squared gradient values
Maintains max of all exp. moving avg. of sq. grad. values
Decay the first and second moment running average coefficient
Maintains the maximum of all 2nd moment running avg. till now
Use the max. for normalizing running avg. of gradient
determine offsets for whitespace_after field
increment for last token in sentence if not followed by whitespace
determine offsets for whitespace_after field
conll 2000 column format
conll 03 NER column format
WNUT-17
-- WikiNER datasets
-- Universal Dependencies
Germanic
Romance
West-Slavic
South-Slavic
East-Slavic
Scandinavian
Asian
Language isolates
recent Universal Dependencies
other datasets
text classification format
text regression format
"first, try to fetch dataset online"
default dataset folder is the cache root
get string value if enum is passed
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
the CoNLL 03 task for German has an additional lemma column
the CoNLL 03 task for Dutch has no NP column
the CoNLL 03 task for Spanish only has two columns
the GERMEVAL task only has two columns: text and ner
WSD tasks may be put into this column format
"the UD corpora follow the CoNLL-U format, for which we have a special reader"
"for text classifiers, we use our own special format"
NER corpus for Basque
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
get train and test data
"read in test file if exists, otherwise sample 10% of train data as test dataset"
"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
convert tag scheme to iobes
automatically identify train / test / dev files
automatically identify train / test / dev files
"most data sets have the token text in the first column, if not, pass 'text' as column"
conll 2000 chunking task
Support both TREC-6 and TREC-50
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
Wikiner NER task
unpack and write out in CoNLL column-like format
CoNLL 02/03 NER
universal dependencies
--- UD Germanic
--- UD Romance
--- UD West-Slavic
--- UD Scandinavian
--- UD South-Slavic
--- UD Asian
this is the default init size of a lmdb database for embeddings
some non-used parameter to allow print
get db filename from embedding name
"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
SequenceTagger
TextClassifier
get db filename from embedding name
if embedding database already exists
"otherwise, push embedding to database"
if embedding database already exists
open the database in read mode
we need to set self.k
create and load the database in write mode
"no idea why, but we need to close and reopen the environment to avoid"
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
when opening new transaction !
init dictionaries
"in order to deal with unknown tokens, add <unk>"
"We don't want to create a SpaceTokenizer object each time this function is called,"
so delegate the call directly to the static run_tokenize method
"We don't want to create a SegtokTokenizer object each time this function is called,"
so delegate the call directly to the static run_tokenize method
"if text is passed, instantiate sentence with tokens (words)"
log a warning if the dataset is empty
data with zero-width characters cannot be handled
set token idx if not set
non-set tags are OUT tags
anything that is not a BIOES tag is a SINGLE tag
anything that is not OUT is IN
single and begin tags start a new span
remember previous tag
"if label type is explicitly specified, get spans for this label type"
else determine all label types in sentence and get all spans
move sentence embeddings to device
move token embeddings to device
clear sentence embeddings
clear token embeddings
infer whitespace after field
add Sentence labels to output if they exist
add Token labels to output if they exist
add Sentence labels to output if they exist
add Token labels to output if they exist
No character at the corresponding code point: remove it
set name
sample test data if none is provided
sample dev data if none is provided
set train dev and test data
find out empty sentence indices
create subset of non-empty sentence indices
find out empty sentence indices
create subset of non-empty sentence indices
check if sentence itself has labels
check for labels of words
Make the tag dictionary
global variable: cache_root
global variable: device
global variable: embedding_storage_mode
# dummy return to fulfill trainer.train() needs
print(vec)
Attach optimizer
"convert `metrics` to float, in case it's a zero-dim Tensor"
if memory mode option 'none' delete everything
else delete only dynamic embeddings (otherwise autograd will keep everything in memory)
find out which ones are dynamic embeddings
find out which ones are dynamic embeddings
memory management - option 1: send everything to CPU (pin to memory if we train on GPU)
record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class)
header for 'weights.txt'
"determine the column index of loss, f-score and accuracy for train, dev and test split"
then get all relevant values from the tsv
then get all relevant values from the tsv
plot i
save plots
save plots
plt.show()
save plot
take the average over the last three scores of training
take average over the scores from the different training runs
remove previous embeddings
clearing token embeddings to save memory
"read Dataset into data loader (if list of sentences passed, make Dataset first)"
#TODO: not saving lines yet
== similarity measures ==
helper class for ModelSimilarity
-- works with binary cross entropy loss --
"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}"
-- works with ranking/triplet loss --
normalize the embeddings
== similarity losses ==
"we want that logits for corresponding pairs are high, and for non-corresponding low"
TODO: this assumes eye matrix
"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa"
== similarity learner ==
"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both"
assumes that for each data pair there's at least one embedding per modality
pre-compute embeddings for all targets in evaluation dataset
compute the similarity
sort the similarity matrix across modality 1
"get the ranks, so +1 to start counting ranks from 1"
The conversion from old model's constructor interface
auto-spawn on GPU if available
pad strings with whitespaces to longest sentence
cut up the input into chunks of max charlength = chunk_size
push each chunk through the RNN language model
concatenate all chunks to make final output
initial hidden state
get predicted weights
divide by temperature
"to prevent overflow problem with small temperature values, substract largest value from all"
this makes a vector in which the largest value is 0
compute word weights with exponential function
try sampling multinomial distribution for next character
print(word_idx)
input ids
push list of character IDs through model
the target is always the next character
use cross entropy loss to compare output of forward pass with targets
exponentiate cross-entropy loss to calculate perplexity
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
set the dictionaries
"if we use a CRF, we must add special START and STOP tags to the dictionary"
Initialize the weight tensor
initialize the network architecture
dropouts
optional reprojection layer on top of word embeddings
bidirectional LSTM on top of embedding layer
Create initial hidden state and initialize it
TODO: Decide how to initialize the hidden state variables
self.hs_initializer(self.lstm_init_h)
self.hs_initializer(self.lstm_init_c)
final linear map to tag space
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
clearing token embeddings to save memory
predict for batch
make list of gold tags
make list of predicted tags
"check for true positives, false positives and false negatives"
also write to file in BIO format to use old conlleval script
check if in gold spans
check if in predicted spans
"read Dataset into data loader (if list of sentences passed, make Dataset first)"
"if span F1 needs to be used, use separate eval method"
"else, use scikit-learn to evaluate"
predict for batch
add gold tag
add predicted tag
for file output
use sklearn
"make ""classification report"""
report over all in case there are no labels
get scores
line for log file
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
"if initial hidden state is trainable, use this state"
word dropout only before LSTM - TODO: more experimentation needed
if self.use_word_dropout > 0.0:
sentence_tensor = self.word_dropout(sentence_tensor)
get the tags in this sentence
add tags as tensor
pad tags if using batch-CRF decoder
reduce raw values to avoid NaN during exp
broadcasting will do the job of reshaping and is more efficient than calling repeat
default value
English NER models
Multilingual NER models
English POS models
Multilingual POS models
English SRL models
English chunking models
Danish models
German models
French models
Dutch models
Malayalam models
Portuguese models
Keyphase models
Biomedical models
the historical German taggers by the @redewiegergabe project
Fallback to Hugging Face model hub
e.g. stefan-it/flair-ner-conll03 is a valid namespace
and  stefan-it/flair-ner-conll03@main supports specifying a commit/branch name
Lazy import
clear embeddings after predicting
load each model
check if the same embeddings were already loaded previously
"if the model uses StackedEmbedding, make a new stack with previous objects"
sort embeddings by key alphabetically
check previous embeddings and add if found
only re-use static embeddings
"if not found, use existing embedding"
initialize new stack
"of the model uses regular embedding, re-load if previous version found"
Initialize the weight tensor
auto-spawn on GPU if available
filter empty sentences
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
clearing token embeddings to save memory
"read Dataset into data loader (if list of sentences passed, make Dataset first)"
use scikit-learn to evaluate
remove previously predicted labels
get the gold labels
predict for batch
get the predicted labels
remove predicted labels
"make ""classification report"""
get scores
line for log file
English sentiment models
Communicative Functions Model
Drop unnecessary attributes from Parent class
prepare binary label dictionary
Store task specific labels since TARS can handle multiple tasks
make label dictionary if no Dictionary object is passed
get and embed all labels by making a Sentence object that contains only the label text
get each label embedding and scale between 0 and 1
compute similarity matrix
"the higher the similarity, the greater the chance that a label is"
sampled as negative example
make sure the probabilities always sum up to 1
Transform input data into TARS format
"M: num_classes in task, N: num_samples"
reshape scores MN x 2 -> N x M x 2
import torch
a = torch.arange(30)
"b = torch.reshape(-1, 3, 2)"
"c = b[:,:,1]"
target shape N x M
Transform label_scores
Transform label_scores into current task's desired format
"TARS does not do a softmax, so confidence of the best predicted class might be very low."
Therefore enforce a min confidence of 0.5 for a match.
make list if only one candidate label is passed
"if list is passed, convert to set"
check if candidate_label_set is empty
note current task
create a temporary task
make zero shot predictions
switch to the pre-existing task
remember current task
predict with each task model
switch to the pre-existing task
embeddings
dictionaries
linear layer
F-beta score
all parameters will be pushed internally to the specified device
"read Dataset into data loader (if list of sentences passed, make Dataset first)"
"if span F1 needs to be used, use separate eval method"
"else, use scikit-learn to evaluate"
predict for batch
add gold tag
add predicted tag
for file output
use sklearn
"make ""classification report"""
report over all in case there are no labels
get scores
line for log file
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
clearing token embeddings to save memory
get the tags in this sentence
add tags as tensor
predict for batch
make list of gold tags
make list of predicted tags
"check for true positives, false positives and false negatives"
also write to file in BIO format to use old conlleval script
check if in gold spans
check if in predicted spans
weights for loss function
iput size is two times wordembedding size since we use pair of words as input
"the output size is max_distance + 1, i.e. we allow 0,1,...,max_distance words between pairs"
regression
input size is two times word embedding size since we use pair of words as input
the output size is 1
auto-spawn on GPU if available
all input should be tensors
forward allows only a single sentcence!!
embed words of sentence
go through all pairs of words with a maximum number of max_distance in between
go through all pairs
2-dim matrix whose rows are the embeddings of word pairs of the sentence
So far only one sentence allowed
If list of sentences is handed the function works with the first sentence of the list
Assume data_points is a single sentence!!!
scores are the predictions for each word pair
"classification needs labels to be integers, regression needs labels to be float"
this is due to the different loss functions
only single sentences as input
gold labels
for output text file
for buckets
for average prediction
add some statistics to the output
use scikit-learn to evaluate
"we iterate over each sentence, instead of batches"
get single labels from scores
gold labels
for output text file
hot one vector of true value
hot one vector of predicted value
"speichert embeddings, falls embedding_storage!= 'None'"
"make ""classification report"""
get scores
"precision_score = round(metrics.precision_score(y_true, y_pred, average='macro', zero_division=0), 4)"
"recall_score = round(metrics.recall_score(y_true, y_pred, average='macro', zero_division=0), 4)"
line for log file
cast string to Path
"determine what splits (train, dev, test) to evaluate and log"
prepare loss logging file and set up header
"minimize training loss if training with dev data, else maximize dev score"
"if training also uses dev data, include in training set"
initialize sampler if provided
init with default values if only class is provided
set dataset to sample from
At any point you can hit Ctrl + C to break out of training early.
get new learning rate
reload last best model if annealing with restarts is enabled
stop training if learning rate becomes too small
process mini-batches
zero the gradients on the model and optimizer
"if necessary, make batch_steps"
forward and backward for batch
forward pass
Backward
do the optimizer step
do the scheduler step if one-cycle
get new learning rate
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
"anneal against train loss if training with dev, otherwise anneal against dev score"
evaluate on train / dev / test split depending on training settings
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
calculate scores using dev data if available
append dev score to score history
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
determine learning rate annealing through scheduler. Use auxiliary metric for AnnealOnPlateau
determine bad epoch number
log bad epochs
output log file
make headers on first epoch
"if checkpoint is enabled, save model at each epoch"
"if we use dev data, remember best model based on dev evaluation score"
"if we do not use dev data for model selection, save final model"
test best model if test data is present
"if we are training over multiple datasets, do evaluation for each"
get and return the final test score of best model
cast string to Path
forward pass
update optimizer and scheduler
Add chars to the dictionary
charsplit file content
charsplit file content
Add words to the dictionary
Tokenize file content
"TextDataset returns a list. valid and test are only one file, so return the first element"
cast string to Path
error message if the validation dataset is too small
Shuffle training files randomly after serially iterating through corpus one
"iterate through training data, starting at self.split (for checkpointing)"
off by one for printing
go into train mode
reset variables
not really sure what this does
do the forward pass in the model
try to predict the targets
Backward
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
We detach the hidden state from how it was previously produced.
"If we didn't, the model would try backpropagating all the way to start of the dataset."
explicitly remove loss to clear up memory
##############################################################################
Save the model if the validation loss is the best we've seen so far.
##############################################################################
print info
##############################################################################
##############################################################################
final testing
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
"multilingual forward (English, German, French, Italian, Dutch, Polish)"
"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
Dutch forward
Dutch backward
Swedish forward
Swedish backward
French forward
French backward
Czech forward
Czech backward
Portuguese forward
Portuguese backward
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
1-camembert-base -> camembert-base
1-xlm-roberta-large -> xlm-roberta-large
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
The mask has 1 for real tokens and 0 for padding tokens. Only real
tokens are attended to.
Zero-pad up to the sequence length.
"first, find longest sentence in batch"
prepare id maps for BERT model
put encoded batch through BERT model to get all hidden states of all encoder layers
get aggregated embeddings for each BERT-subtoken in sentence
get the current sentence object
add concatenated embedding to sentence
use first subword embedding if pooling operation is 'first'
"otherwise, do a mean over all subwords in token"
"if only one sentence is passed, convert to list of sentence"
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
embed a dummy sentence to determine embedding_length
Avoid conflicts with flair's Token class
"<cls> token initially set to 1/D, so it attends to all image features equally"
add positional encodings
reshape the pixels into the sequence
layer norm after convolution and positional encodings
add <cls> token
"transformer requires input in the shape [h*w+1, b, d]"
the output is an embedding of <cls> token
temporary fix to disable tokenizer parallelism warning
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
load tokenizer and transformer model
model name
"when initializing, embeddings are in eval mode by default"
embedding parameters
send mini-token through to check how many layers the model has
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
using list comprehension
gradients are enabled if fine-tuning is enabled
"first, subtokenize each sentence and find out into how many subtokens each token was divided"
subtokenize sentences
tokenize and truncate to max subtokens (TODO: check better truncation strategies)
find longest sentence in batch
initialize batch tensors and mask
put encoded batch through transformer model to get all hidden states of all encoder layers
iterate over all subtokenized sentences
use scalar mix of embeddings if so selected
set the extracted embedding for the token
reload tokenizer to get around serialization issues
optional fine-tuning on top of embedding layer
"if only one sentence is passed, convert to list of sentence"
bidirectional RNN on top of embedding layer
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push through RNN
after-RNN dropout
extract embeddings from RNN
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
IMPORTANT: add embeddings as torch modules
iterate over sentences
"if its a forward LM, take last state"
"convert to plain strings, embedded in a list for the encode function"
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
"if only one sentence is passed, convert to list of sentence"
Expose base classses
Expose token embedding classes
Expose document embedding classes
Expose image embedding classes
Expose legacy embedding classes
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
GLOVE embeddings
TURIAN embeddings
KOMNINOS embeddings
pubmed embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
twitter embeddings
two-letter language code wiki embeddings
two-letter language code wiki embeddings
two-letter language code crawl embeddings
fix serialized models
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
multilingual models
English models
Arabic
Bulgarian
Czech
Danish
German
Spanish
Basque
Persian
Finnish
French
Hebrew
Hindi
Croatian
Indonesian
Italian
Japanese
Malayalam
Dutch
Norwegian
Polish
Portuguese
Pubmed
Slovenian
Swedish
Tamil
CLEF HIPE Shared task
load model if in pretrained model map
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
embeddings are static if we don't do finetuning
embed a dummy sentence to determine embedding_length
set to eval mode
make compatible with serialized models (TODO: remove)
make compatible with serialized models (TODO: remove)
gradients are enable if fine-tuning is enabled
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
offset mode that extracts at whitespace after last character
offset mode that extracts at last character
only clone if optimization mode is 'gpu'
use the character language model embeddings as basis
length is twice the original character LM embedding length
these fields are for the embedding memory
whether to add only capitalized words to memory (faster runtime and lower memory consumption)
we re-compute embeddings dynamically at each epoch
set the memory method
memory is wiped each time we do a training run
"if we keep a pooling, it needs to be updated continuously"
update embedding
check token.text is empty or not
set aggregation operation
add embeddings after updating
temporary fix to disable tokenizer parallelism warning
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
load tokenizer and transformer model
model name
"when initializing, embeddings are in eval mode by default"
embedding parameters
send mini-token through to check how many layers the model has
"self.mix = ScalarMix(mixture_size=len(self.layer_indexes), trainable=False)"
check if special tokens exist to circumvent error message
"most models have an intial BOS token, except for XLNet, T5 and GPT2"
split into micro batches of size self.batch_size before pushing through transformer
embed each micro-batch
remove special markup
"first, subtokenize each sentence and find out into how many subtokens each token was divided"
"TODO: keep for backwards compatibility, but remove in future"
"some pretrained models do not have this property, applying default settings now."
can be set manually after loading the model.
method 1: subtokenize sentence
"subtokenized_sentence = self.tokenizer.encode(tokenized_string, add_special_tokens=True)"
method 2:
transformer specific tokenization
empty sentences get zero embeddings
only embed non-empty sentences and if there is at least one
find longest sentence in batch
initialize batch tensors and mask
put encoded batch through transformer model to get all hidden states of all encoder layers
make the tuple a tensor; makes working with it easier.
gradients are enabled if fine-tuning is enabled
iterate over all subtokenized sentences
"remove stride_size//2 at end of sentence_hidden_state, and half at beginning of remainder,"
in order to get some context into the embeddings of these words.
also don't include the embedding of the extra [CLS] and [SEP] tokens.
"for each token, get embedding"
some tokens have no subtokens at all (if omitted by BERT tokenizer) so return zero vector
"get states from all selected layers, aggregate with pooling operation"
use scalar mix of embeddings if so selected
sm_embeddings = self.mix(subtoken_embeddings)
set the extracted embedding for the token
iterate over subtokens and reconstruct tokens
remove special markup
TODO check if this is necessary is this method is called before prepare_for_model
check if reconstructed token is special begin token ([CLS] or similar)
some BERT tokenizers somehow omit words - in such cases skip to next token
append subtoken to reconstruct token
check if reconstructed token is the same as current token
"if so, add subtoken count"
reset subtoken count and reconstructed token
break from loop if all tokens are accounted for
if tokens are unaccounted for
check if all tokens were matched to subtokens
"if fine-tuning is not enabled (i.e. a ""feature-based approach"" used), this"
module should never be in training mode
reload tokenizer to get around serialization issues
max_tokens = 500
model architecture
model architecture
download if necessary
load the model
"TODO: keep for backwards compatibility, but remove in future"
save the sentence piece model as binary file (not as path which may change)
write out the binary sentence piece model into the expected directory
"if the model was saved as binary and it is not found on disk, write to appropriate path"
"otherwise, use normal process and potentially trigger another download"
"once the modes if there, load it with sentence piece"
empty words get no embedding
all other words get embedded
"the default model for ELMo is the 'original' model, which is very large"
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
put on Cuda if available
embed a dummy sentence to determine embedding_length
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
GLOVE embeddings
"find train, dev and test files if not specified"
get train data
read in test file if exists
read in dev file if exists
special key for space after
"store either Sentence objects in memory, or only file offsets"
"most data sets have the token text in the first column, if not, pass 'text' as column"
determine encoding of text file
skip first line if to selected
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
"tag_to_bioes=tag_to_bioes,"
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
Remove CoNLL-U meta information in the last column
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
rename according to train - test - dev - convention
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
dataset name
data folder: default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
data is not in IOB2 format. Thus we transform it to IOB2
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
this list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
get google drive id from list
download from google drive
unzip
"tar.extractall(language_folder,members=[tar.getmember(file_name)])"
transform data into required format
"the processed dataset has the additional ending ""_new"""
remove the unprocessed dataset
initialize comlumncorpus and add it to list
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"unzip the downloaded repo and merge the train, dev and test datasets"
download data if necessary
unpack and write out in CoNLL column-like format
if no languages are given as argument all languages used in XTREME will be loaded
if only one language is given
column format
this dataset name
default dataset folder is the cache root
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
This list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
download from HU Server
unzip
transform data into required format
initialize comlumncorpus and add it to list
"find train, dev and test files if not specified"
use test_file to create test split if available
use dev_file to create test split if available
"if data point contains black-listed label, do not use"
first check if valid sentence
"if so, add to indices"
"find train, dev and test files if not specified"
variables
different handling of in_memory data than streaming data
"most data sets have the token text in the first column, if not, pass 'text' as column"
test if format is OK
test if at least one label given
noinspection PyDefaultArgument
dataset name includes the split size
default dataset folder is the cache root
download data if necessary
download each of the 28 splits
create dataset directory if necessary
download senteval datasets if necessary und unzip
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
"by defaut, map point score to POSITIVE / NEGATIVE values"
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file from CSV
create test.txt file from CSV
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
convert to FastText format
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
download data if necessary
download datasets if necessary
create dataset directory if necessary
create correctly formated txt files
multiple labels are possible
this dataset name
default dataset folder is the cache root
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
default dataset folder is the cache root
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
check if dataset is supported
set file names
download and unzip in file structure if necessary
instantiate corpus
Uses dynamic programming approach to calculate maximum independent set in interval graph
with sum of all entity lengths as secondary key
calculate offset without current text
because we stick all passages of a document together
TODO For split entities we also annotate everything inbetween which might be a bad idea?
Try to fix incorrect annotations
print(
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
)
Ignore empty lines or relation annotations
FIX annotation of whitespaces (necessary for PDR)
One token may contain multiple entities -> deque all of them
column format
this dataset name
default dataset folder is the cache root
Create tokenization-dependent CONLL files. This is necessary to prevent
from caching issues (e.g. loading the same corpus with different sentence splitters)
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
Edge case: last token starts a new entity
Last document in file
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
Edge case: last token starts a new entity
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
download file is huge => make default_dir visible so that derivative
corpora can all use the same download file
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
Read texts
Read annotations
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
We need to apply a patch to correct the original training file
Articles title
Article abstract
Entity annotations
column format
this dataset name
default dataset folder is the cache root
Edge case: last token starts a new entity
Map all entities to chemicals
Map all entities to disease
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
Incomplete article
Invalid XML syntax
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
if len(mid) != 3:
continue
Try to fix entity offsets
column format
this dataset name
default dataset folder is the cache root
There is still one illegal annotation in the file ..
column format
this dataset name
default dataset folder is the cache root
"Abstract first, title second to prevent issues with sentence splitting"
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
"Filter for specific entity types, by default no entities will be filtered"
Get original HUNER splits to retrieve a list of all document ids contained in V2
train and dev split of V2 will be train in V4
test split of V2 will be dev in V4
New documents in V4 will become test documents
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
cache Feidegger config file
cache Feidegger images
replace image URL with local cached file
append Sentence-Image data point
"in certain cases, multi-CPU data loading makes no sense and slows"
"everything down. For this reason, we detect if a dataset is in-memory:"
"if so, num_workers is set to 0 for faster processing"
cast to list if necessary
cast to list if necessary
"first, check if pymongo is installed"
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
Expose base classses
Expose all sequence labeling datasets
Expose all document classification datasets
Expose all treebanks
Expose all text-text datasets
Expose all text-image datasets
Expose all biomedical data sets
Expose all biomedical data sets using the HUNER splits
-
-
-
-
Expose all biomedical data sets used for the evaluation of BioBERT
"find train, dev and test files if not specified"
get train data
get test data
get dev data
option 1: read only sentence boundaries as offset positions
option 2: keep everything in memory
"if in memory, retrieve parsed sentence"
else skip to position in file where sentence begins
current token ID
handling for the awful UD multiword format
end of sentence
comments
ellipsis
if token is a multi-word
normal single-word tokens
"if we don't split multiwords, skip over component words"
add token
add morphological tags
derive whitespace logic for multiwords
print(token)
print(current_multiword_last_token)
print(current_multiword_first_token)
"if multi-word equals component tokens, there should be no whitespace"
go through all tokens in subword and set whitespace_after information
print(i)
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
test corpus
create a TARS classifier
check if right number of classes
switch to task with only one label
check if right number of classes
switch to task with three labels provided as list
check if right number of classes
switch to task with four labels provided as set
check if right number of classes
switch to task with two labels provided as Dictionary
check if right number of classes
clean up file
bioes tags
bio tags
broken tags
all tags
all weird tags
tags with confidence
bioes tags
bioes tags
increment for last token in sentence if not followed by whitespace
clean up directory
clean up directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
clean up results directory
initialize trainer
clean up results directory
def test_multiclass_metrics():
""
"metric = Metric(""Test"")"
"available_labels = [""A"", ""B"", ""C""]"
""
"predictions = [""A"", ""B""]"
"true_values = [""A""]"
TextClassifier._evaluate_sentence_for_text_classification(
"metric, available_labels, predictions, true_values"
)
""
"predictions = [""C"", ""B""]"
"true_values = [""A"", ""B""]"
TextClassifier._evaluate_sentence_for_text_classification(
"metric, available_labels, predictions, true_values"
)
""
print(metric)
from flair.trainers.trainer_regression import RegressorTrainer
def test_trainer_results(tasks_base_path):
"corpus, model, trainer = init(tasks_base_path)"
"results = trainer.train(""regression_train/"", max_epochs=1)"
"assert results[""test_score""] > 0"
"assert len(results[""dev_loss_history""]) == 1"
"assert len(results[""dev_score_history""]) == 1"
"assert len(results[""train_loss_history""]) == 1"
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
clean up results directory
define search space
sequence tagger parameter
model trainer parameter
training parameter
find best parameter settings
clean up results directory
document embeddings parameter
training parameter
clean up results directory
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
get two corpora as one
"get training, test and dev data for full English UD corpus from web"
clean up data directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
1. get the corpus
2. what tag do we want to predict?
3. make the tag dictionary from the corpus
initialize embeddings
comment in this line to use character embeddings
"CharacterEmbeddings(),"
comment in these lines to use contextual string embeddings
""
"FlairEmbeddings('news-forward'),"
""
"FlairEmbeddings('news-backward'),"
initialize sequence tagger
initialize trainer
from allennlp.common.tqdm import Tqdm
mmap seems to be much more memory efficient
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
Extract all the contents of zip file in current directory
Extract all the contents of zip file in current directory
get cache path to put the file
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
first determine the distribution of classes in the dataset
weight for each sample
Create blocks
shuffle the blocks
concatenate the shuffled blocks
Create blocks
shuffle the blocks
concatenate the shuffled blocks
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
see https://github.com/zalandoresearch/flair/issues/351
State initialization
Exponential moving average of gradient values
Exponential moving average of squared gradient values
Maintains max of all exp. moving avg. of sq. grad. values
Decay the first and second moment running average coefficient
Maintains the maximum of all 2nd moment running avg. till now
Use the max. for normalizing running avg. of gradient
determine offsets for whitespace_after field
increment for last token in sentence if not followed by whitespace
determine offsets for whitespace_after field
conll 2000 column format
conll 03 NER column format
WNUT-17
-- WikiNER datasets
-- Universal Dependencies
Germanic
Romance
West-Slavic
South-Slavic
East-Slavic
Scandinavian
Asian
Language isolates
recent Universal Dependencies
other datasets
text classification format
text regression format
"first, try to fetch dataset online"
default dataset folder is the cache root
get string value if enum is passed
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
the CoNLL 03 task for German has an additional lemma column
the CoNLL 03 task for Dutch has no NP column
the CoNLL 03 task for Spanish only has two columns
the GERMEVAL task only has two columns: text and ner
WSD tasks may be put into this column format
"the UD corpora follow the CoNLL-U format, for which we have a special reader"
"for text classifiers, we use our own special format"
NER corpus for Basque
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
get train and test data
"read in test file if exists, otherwise sample 10% of train data as test dataset"
"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
convert tag scheme to iobes
automatically identify train / test / dev files
automatically identify train / test / dev files
"most data sets have the token text in the first column, if not, pass 'text' as column"
conll 2000 chunking task
Support both TREC-6 and TREC-50
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
Wikiner NER task
unpack and write out in CoNLL column-like format
CoNLL 02/03 NER
universal dependencies
--- UD Germanic
--- UD Romance
--- UD West-Slavic
--- UD Scandinavian
--- UD South-Slavic
--- UD Asian
this is the default init size of a lmdb database for embeddings
some non-used parameter to allow print
get db filename from embedding name
"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
SequenceTagger
TextClassifier
get db filename from embedding name
if embedding database already exists
"otherwise, push embedding to database"
if embedding database already exists
open the database in read mode
we need to set self.k
create and load the database in write mode
"no idea why, but we need to close and reopen the environment to avoid"
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
when opening new transaction !
init dictionaries
"in order to deal with unknown tokens, add <unk>"
"We don't want to create a SpaceTokenizer object each time this function is called,"
so delegate the call directly to the static run_tokenize method
"We don't want to create a SegtokTokenizer object each time this function is called,"
so delegate the call directly to the static run_tokenize method
"if text is passed, instantiate sentence with tokens (words)"
log a warning if the dataset is empty
data with zero-width characters cannot be handled
set token idx if not set
non-set tags are OUT tags
anything that is not a BIOES tag is a SINGLE tag
anything that is not OUT is IN
single and begin tags start a new span
remember previous tag
"if label type is explicitly specified, get spans for this label type"
else determine all label types in sentence and get all spans
move sentence embeddings to device
move token embeddings to device
clear sentence embeddings
clear token embeddings
infer whitespace after field
add Sentence labels to output if they exist
add Token labels to output if they exist
add Sentence labels to output if they exist
add Token labels to output if they exist
No character at the corresponding code point: remove it
set name
sample test data if none is provided
sample dev data if none is provided
set train dev and test data
find out empty sentence indices
create subset of non-empty sentence indices
find out empty sentence indices
create subset of non-empty sentence indices
check if sentence itself has labels
check for labels of words
Make the tag dictionary
global variable: cache_root
global variable: device
global variable: embedding_storage_mode
# dummy return to fulfill trainer.train() needs
print(vec)
Attach optimizer
"convert `metrics` to float, in case it's a zero-dim Tensor"
if memory mode option 'none' delete everything
else delete only dynamic embeddings (otherwise autograd will keep everything in memory)
find out which ones are dynamic embeddings
find out which ones are dynamic embeddings
memory management - option 1: send everything to CPU (pin to memory if we train on GPU)
record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class)
header for 'weights.txt'
"determine the column index of loss, f-score and accuracy for train, dev and test split"
then get all relevant values from the tsv
then get all relevant values from the tsv
plot i
save plots
save plots
plt.show()
save plot
take the average over the last three scores of training
take average over the scores from the different training runs
remove previous embeddings
clearing token embeddings to save memory
"read Dataset into data loader (if list of sentences passed, make Dataset first)"
#TODO: not saving lines yet
== similarity measures ==
helper class for ModelSimilarity
-- works with binary cross entropy loss --
"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}"
-- works with ranking/triplet loss --
normalize the embeddings
== similarity losses ==
"we want that logits for corresponding pairs are high, and for non-corresponding low"
TODO: this assumes eye matrix
"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa"
== similarity learner ==
"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both"
assumes that for each data pair there's at least one embedding per modality
pre-compute embeddings for all targets in evaluation dataset
compute the similarity
sort the similarity matrix across modality 1
"get the ranks, so +1 to start counting ranks from 1"
The conversion from old model's constructor interface
auto-spawn on GPU if available
pad strings with whitespaces to longest sentence
cut up the input into chunks of max charlength = chunk_size
push each chunk through the RNN language model
concatenate all chunks to make final output
initial hidden state
get predicted weights
divide by temperature
"to prevent overflow problem with small temperature values, substract largest value from all"
this makes a vector in which the largest value is 0
compute word weights with exponential function
try sampling multinomial distribution for next character
print(word_idx)
input ids
push list of character IDs through model
the target is always the next character
use cross entropy loss to compare output of forward pass with targets
exponentiate cross-entropy loss to calculate perplexity
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
set the dictionaries
"if we use a CRF, we must add special START and STOP tags to the dictionary"
Initialize the weight tensor
initialize the network architecture
dropouts
optional reprojection layer on top of word embeddings
bidirectional LSTM on top of embedding layer
Create initial hidden state and initialize it
TODO: Decide how to initialize the hidden state variables
self.hs_initializer(self.lstm_init_h)
self.hs_initializer(self.lstm_init_c)
final linear map to tag space
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
clearing token embeddings to save memory
predict for batch
make list of gold tags
make list of predicted tags
"check for true positives, false positives and false negatives"
also write to file in BIO format to use old conlleval script
check if in gold spans
check if in predicted spans
"read Dataset into data loader (if list of sentences passed, make Dataset first)"
"if span F1 needs to be used, use separate eval method"
"else, use scikit-learn to evaluate"
predict for batch
add gold tag
add predicted tag
for file output
use sklearn
"make ""classification report"""
get scores
line for log file
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
"if initial hidden state is trainable, use this state"
word dropout only before LSTM - TODO: more experimentation needed
if self.use_word_dropout > 0.0:
sentence_tensor = self.word_dropout(sentence_tensor)
get the tags in this sentence
add tags as tensor
pad tags if using batch-CRF decoder
reduce raw values to avoid NaN during exp
broadcasting will do the job of reshaping and is more efficient than calling repeat
default value
English NER models
Multilingual NER models
English POS models
Multilingual POS models
English SRL models
English chunking models
Danish models
German models
French models
Dutch models
Malayalam models
Portuguese models
Keyphase models
Biomedical models
the historical German taggers by the @redewiegergabe project
clear embeddings after predicting
load each model
check if the same embeddings were already loaded previously
"if the model uses StackedEmbedding, make a new stack with previous objects"
sort embeddings by key alphabetically
check previous embeddings and add if found
only re-use static embeddings
"if not found, use existing embedding"
initialize new stack
"of the model uses regular embedding, re-load if previous version found"
Initialize the weight tensor
auto-spawn on GPU if available
filter empty sentences
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
clearing token embeddings to save memory
"read Dataset into data loader (if list of sentences passed, make Dataset first)"
use scikit-learn to evaluate
remove previously predicted labels
get the gold labels
predict for batch
get the predicted labels
remove predicted labels
"make ""classification report"""
get scores
line for log file
English sentiment models
Communicative Functions Model
cast string to Path
"determine what splits (train, dev, test) to evaluate and log"
prepare loss logging file and set up header
"minimize training loss if training with dev data, else maximize dev score"
"if training also uses dev data, include in training set"
initialize sampler if provided
init with default values if only class is provided
set dataset to sample from
At any point you can hit Ctrl + C to break out of training early.
get new learning rate
reload last best model if annealing with restarts is enabled
stop training if learning rate becomes too small
process mini-batches
zero the gradients on the model and optimizer
"if necessary, make batch_steps"
forward and backward for batch
forward pass
Backward
do the optimizer step
do the scheduler step if one-cycle
get new learning rate
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
"anneal against train loss if training with dev, otherwise anneal against dev score"
evaluate on train / dev / test split depending on training settings
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
calculate scores using dev data if available
append dev score to score history
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
determine learning rate annealing through scheduler. Use auxiliary metric for AnnealOnPlateau
determine bad epoch number
log bad epochs
output log file
make headers on first epoch
"if checkpoint is enabled, save model at each epoch"
"if we use dev data, remember best model based on dev evaluation score"
"if we do not use dev data for model selection, save final model"
test best model if test data is present
"if we are training over multiple datasets, do evaluation for each"
get and return the final test score of best model
cast string to Path
forward pass
update optimizer and scheduler
Add chars to the dictionary
charsplit file content
charsplit file content
Add words to the dictionary
Tokenize file content
"TextDataset returns a list. valid and test are only one file, so return the first element"
cast string to Path
error message if the validation dataset is too small
Shuffle training files randomly after serially iterating through corpus one
"iterate through training data, starting at self.split (for checkpointing)"
off by one for printing
go into train mode
reset variables
not really sure what this does
do the forward pass in the model
try to predict the targets
Backward
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
We detach the hidden state from how it was previously produced.
"If we didn't, the model would try backpropagating all the way to start of the dataset."
explicitly remove loss to clear up memory
##############################################################################
Save the model if the validation loss is the best we've seen so far.
##############################################################################
print info
##############################################################################
##############################################################################
final testing
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
"multilingual forward (English, German, French, Italian, Dutch, Polish)"
"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
Dutch forward
Dutch backward
Swedish forward
Swedish backward
French forward
French backward
Czech forward
Czech backward
Portuguese forward
Portuguese backward
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
1-camembert-base -> camembert-base
1-xlm-roberta-large -> xlm-roberta-large
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
The mask has 1 for real tokens and 0 for padding tokens. Only real
tokens are attended to.
Zero-pad up to the sequence length.
"first, find longest sentence in batch"
prepare id maps for BERT model
put encoded batch through BERT model to get all hidden states of all encoder layers
get aggregated embeddings for each BERT-subtoken in sentence
get the current sentence object
add concatenated embedding to sentence
use first subword embedding if pooling operation is 'first'
"otherwise, do a mean over all subwords in token"
"if only one sentence is passed, convert to list of sentence"
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
embed a dummy sentence to determine embedding_length
Avoid conflicts with flair's Token class
"<cls> token initially set to 1/D, so it attends to all image features equally"
add positional encodings
reshape the pixels into the sequence
layer norm after convolution and positional encodings
add <cls> token
"transformer requires input in the shape [h*w+1, b, d]"
the output is an embedding of <cls> token
temporary fix to disable tokenizer parallelism warning
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
load tokenizer and transformer model
model name
"when initializing, embeddings are in eval mode by default"
embedding parameters
send mini-token through to check how many layers the model has
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
using list comprehension
gradients are enabled if fine-tuning is enabled
"first, subtokenize each sentence and find out into how many subtokens each token was divided"
subtokenize sentences
tokenize and truncate to max subtokens (TODO: check better truncation strategies)
find longest sentence in batch
initialize batch tensors and mask
put encoded batch through transformer model to get all hidden states of all encoder layers
iterate over all subtokenized sentences
use scalar mix of embeddings if so selected
set the extracted embedding for the token
reload tokenizer to get around serialization issues
optional fine-tuning on top of embedding layer
"if only one sentence is passed, convert to list of sentence"
bidirectional RNN on top of embedding layer
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push through RNN
after-RNN dropout
extract embeddings from RNN
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
IMPORTANT: add embeddings as torch modules
iterate over sentences
"if its a forward LM, take last state"
"convert to plain strings, embedded in a list for the encode function"
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
"if only one sentence is passed, convert to list of sentence"
Expose base classses
Expose token embedding classes
Expose document embedding classes
Expose image embedding classes
Expose legacy embedding classes
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
GLOVE embeddings
TURIAN embeddings
KOMNINOS embeddings
pubmed embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
twitter embeddings
two-letter language code wiki embeddings
two-letter language code wiki embeddings
two-letter language code crawl embeddings
fix serialized models
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
multilingual models
English models
Arabic
Bulgarian
Czech
Danish
German
Spanish
Basque
Persian
Finnish
French
Hebrew
Hindi
Croatian
Indonesian
Italian
Japanese
Malayalam
Dutch
Norwegian
Polish
Portuguese
Pubmed
Slovenian
Swedish
Tamil
CLEF HIPE Shared task
load model if in pretrained model map
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
embeddings are static if we don't do finetuning
embed a dummy sentence to determine embedding_length
set to eval mode
make compatible with serialized models (TODO: remove)
make compatible with serialized models (TODO: remove)
gradients are enable if fine-tuning is enabled
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
offset mode that extracts at whitespace after last character
offset mode that extracts at last character
only clone if optimization mode is 'gpu'
use the character language model embeddings as basis
length is twice the original character LM embedding length
these fields are for the embedding memory
whether to add only capitalized words to memory (faster runtime and lower memory consumption)
we re-compute embeddings dynamically at each epoch
set the memory method
memory is wiped each time we do a training run
"if we keep a pooling, it needs to be updated continuously"
update embedding
check token.text is empty or not
set aggregation operation
add embeddings after updating
temporary fix to disable tokenizer parallelism warning
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
load tokenizer and transformer model
model name
"when initializing, embeddings are in eval mode by default"
embedding parameters
send mini-token through to check how many layers the model has
"self.mix = ScalarMix(mixture_size=len(self.layer_indexes), trainable=False)"
check if special tokens exist to circumvent error message
"most models have an intial BOS token, except for XLNet, T5 and GPT2"
split into micro batches of size self.batch_size before pushing through transformer
embed each micro-batch
remove special markup
"first, subtokenize each sentence and find out into how many subtokens each token was divided"
"TODO: keep for backwards compatibility, but remove in future"
"some pretrained models do not have this property, applying default settings now."
can be set manually after loading the model.
method 1: subtokenize sentence
"subtokenized_sentence = self.tokenizer.encode(tokenized_string, add_special_tokens=True)"
method 2:
transformer specific tokenization
empty sentences get zero embeddings
only embed non-empty sentences and if there is at least one
find longest sentence in batch
initialize batch tensors and mask
put encoded batch through transformer model to get all hidden states of all encoder layers
make the tuple a tensor; makes working with it easier.
gradients are enabled if fine-tuning is enabled
iterate over all subtokenized sentences
"remove stride_size//2 at end of sentence_hidden_state, and half at beginning of remainder,"
in order to get some context into the embeddings of these words.
also don't include the embedding of the extra [CLS] and [SEP] tokens.
"for each token, get embedding"
some tokens have no subtokens at all (if omitted by BERT tokenizer) so return zero vector
"get states from all selected layers, aggregate with pooling operation"
use scalar mix of embeddings if so selected
sm_embeddings = self.mix(subtoken_embeddings)
set the extracted embedding for the token
iterate over subtokens and reconstruct tokens
remove special markup
TODO check if this is necessary is this method is called before prepare_for_model
check if reconstructed token is special begin token ([CLS] or similar)
some BERT tokenizers somehow omit words - in such cases skip to next token
append subtoken to reconstruct token
check if reconstructed token is the same as current token
"if so, add subtoken count"
reset subtoken count and reconstructed token
break from loop if all tokens are accounted for
if tokens are unaccounted for
check if all tokens were matched to subtokens
"if fine-tuning is not enabled (i.e. a ""feature-based approach"" used), this"
module should never be in training mode
reload tokenizer to get around serialization issues
max_tokens = 500
model architecture
model architecture
download if necessary
load the model
"TODO: keep for backwards compatibility, but remove in future"
save the sentence piece model as binary file (not as path which may change)
write out the binary sentence piece model into the expected directory
"if the model was saved as binary and it is not found on disk, write to appropriate path"
"otherwise, use normal process and potentially trigger another download"
"once the modes if there, load it with sentence piece"
empty words get no embedding
all other words get embedded
"the default model for ELMo is the 'original' model, which is very large"
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
put on Cuda if available
embed a dummy sentence to determine embedding_length
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
GLOVE embeddings
"find train, dev and test files if not specified"
get train data
read in test file if exists
read in dev file if exists
special key for space after
"store either Sentence objects in memory, or only file offsets"
"most data sets have the token text in the first column, if not, pass 'text' as column"
determine encoding of text file
skip first line if to selected
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
if no languages are given as argument all languages used in XTREME will be loaded
if only one language is given
column format
this dataset name
default dataset folder is the cache root
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
This list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
download from HU Server
unzip
transform data into required format
initialize comlumncorpus and add it to list
column format
this dataset name
default dataset folder is the cache root
"For each language in languages, the file is downloaded if not existent"
Then a comlumncorpus of that data is created and saved in a list
this list is handed to the multicorpus
list that contains the columncopora
download data if necessary
"if language not downloaded yet, download it"
create folder
get google drive id from list
download from google drive
unzip
"tar.extractall(language_folder,members=[tar.getmember(file_name)])"
transform data into required format
"the processed dataset has the additional ending ""_new"""
remove the unprocessed dataset
initialize comlumncorpus and add it to list
column format
this dataset name
default dataset folder is the cache root
download data if necessary
Remove CoNLL-U meta information in the last column
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
rename according to train - test - dev - convention
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
data is not in IOB2 format. Thus we transform it to IOB2
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
download data if necessary
unpack and write out in CoNLL column-like format
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"find train, dev and test files if not specified"
use test_file to create test split if available
use dev_file to create test split if available
"if data point contains black-listed label, do not use"
first check if valid sentence
"if so, add to indices"
"find train, dev and test files if not specified"
variables
different handling of in_memory data than streaming data
"most data sets have the token text in the first column, if not, pass 'text' as column"
test if format is OK
test if at least one label given
noinspection PyDefaultArgument
dataset name includes the split size
default dataset folder is the cache root
download data if necessary
download each of the 28 splits
create dataset directory if necessary
download senteval datasets if necessary und unzip
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
"by defaut, map point score to POSITIVE / NEGATIVE values"
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file from CSV
create test.txt file from CSV
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
convert to FastText format
this dataset name
default dataset folder is the cache root
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
default dataset folder is the cache root
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
check if dataset is supported
set file names
download and unzip in file structure if necessary
instantiate corpus
Uses dynamic programming approach to calculate maximum independent set in interval graph
with sum of all entity lengths as secondary key
calculate offset without current text
because we stick all passages of a document together
TODO For split entities we also annotate everything inbetween which might be a bad idea?
Try to fix incorrect annotations
print(
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
)
Ignore empty lines or relation annotations
FIX annotation of whitespaces (necessary for PDR)
One token may contain multiple entities -> deque all of them
column format
this dataset name
default dataset folder is the cache root
Create tokenization-dependent CONLL files. This is necessary to prevent
from caching issues (e.g. loading the same corpus with different sentence splitters)
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
Edge case: last token starts a new entity
Last document in file
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
Edge case: last token starts a new entity
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
download file is huge => make default_dir visible so that derivative
corpora can all use the same download file
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
Read texts
Read annotations
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
We need to apply a patch to correct the original training file
Articles title
Article abstract
Entity annotations
column format
this dataset name
default dataset folder is the cache root
Edge case: last token starts a new entity
Map all entities to chemicals
Map all entities to disease
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
Incomplete article
Invalid XML syntax
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
if len(mid) != 3:
continue
Try to fix entity offsets
column format
this dataset name
default dataset folder is the cache root
There is still one illegal annotation in the file ..
column format
this dataset name
default dataset folder is the cache root
"Abstract first, title second to prevent issues with sentence splitting"
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
"Filter for specific entity types, by default no entities will be filtered"
Get original HUNER splits to retrieve a list of all document ids contained in V2
train and dev split of V2 will be train in V4
test split of V2 will be dev in V4
New documents in V4 will become test documents
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
cache Feidegger config file
cache Feidegger images
replace image URL with local cached file
append Sentence-Image data point
"in certain cases, multi-CPU data loading makes no sense and slows"
"everything down. For this reason, we detect if a dataset is in-memory:"
"if so, num_workers is set to 0 for faster processing"
cast to list if necessary
cast to list if necessary
"first, check if pymongo is installed"
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
Expose base classses
Expose all sequence labeling datasets
Expose all document classification datasets
Expose all treebanks
Expose all text-text datasets
Expose all text-image datasets
Expose all biomedical data sets
Expose all biomedical data sets using the HUNER splits
-
-
-
-
Expose all biomedical data sets used for the evaluation of BioBERT
"find train, dev and test files if not specified"
get train data
get test data
get dev data
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
clean up file
bioes tags
bio tags
broken tags
all tags
all weird tags
tags with confidence
bioes tags
bioes tags
increment for last token in sentence if not followed by whitespace
clean up directory
clean up directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
clean up results directory
initialize trainer
clean up results directory
def test_multiclass_metrics():
""
"metric = Metric(""Test"")"
"available_labels = [""A"", ""B"", ""C""]"
""
"predictions = [""A"", ""B""]"
"true_values = [""A""]"
TextClassifier._evaluate_sentence_for_text_classification(
"metric, available_labels, predictions, true_values"
)
""
"predictions = [""C"", ""B""]"
"true_values = [""A"", ""B""]"
TextClassifier._evaluate_sentence_for_text_classification(
"metric, available_labels, predictions, true_values"
)
""
print(metric)
from flair.trainers.trainer_regression import RegressorTrainer
def test_trainer_results(tasks_base_path):
"corpus, model, trainer = init(tasks_base_path)"
"results = trainer.train(""regression_train/"", max_epochs=1)"
"assert results[""test_score""] > 0"
"assert len(results[""dev_loss_history""]) == 1"
"assert len(results[""dev_score_history""]) == 1"
"assert len(results[""train_loss_history""]) == 1"
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
clean up results directory
define search space
sequence tagger parameter
model trainer parameter
training parameter
find best parameter settings
clean up results directory
document embeddings parameter
training parameter
clean up results directory
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
get two corpora as one
"get training, test and dev data for full English UD corpus from web"
clean up data directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
1. get the corpus
2. what tag do we want to predict?
3. make the tag dictionary from the corpus
initialize embeddings
comment in this line to use character embeddings
"CharacterEmbeddings(),"
comment in these lines to use contextual string embeddings
""
"FlairEmbeddings('news-forward'),"
""
"FlairEmbeddings('news-backward'),"
initialize sequence tagger
initialize trainer
from allennlp.common.tqdm import Tqdm
mmap seems to be much more memory efficient
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
Extract all the contents of zip file in current directory
Extract all the contents of zip file in current directory
get cache path to put the file
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
first determine the distribution of classes in the dataset
weight for each sample
Create blocks
shuffle the blocks
concatenate the shuffled blocks
Create blocks
shuffle the blocks
concatenate the shuffled blocks
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
see https://github.com/zalandoresearch/flair/issues/351
State initialization
Exponential moving average of gradient values
Exponential moving average of squared gradient values
Maintains max of all exp. moving avg. of sq. grad. values
Decay the first and second moment running average coefficient
Maintains the maximum of all 2nd moment running avg. till now
Use the max. for normalizing running avg. of gradient
determine offsets for whitespace_after field
increment for last token in sentence if not followed by whitespace
determine offsets for whitespace_after field
conll 2000 column format
conll 03 NER column format
WNUT-17
-- WikiNER datasets
-- Universal Dependencies
Germanic
Romance
West-Slavic
South-Slavic
East-Slavic
Scandinavian
Asian
Language isolates
recent Universal Dependencies
other datasets
text classification format
text regression format
"first, try to fetch dataset online"
default dataset folder is the cache root
get string value if enum is passed
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
the CoNLL 03 task for German has an additional lemma column
the CoNLL 03 task for Dutch has no NP column
the CoNLL 03 task for Spanish only has two columns
the GERMEVAL task only has two columns: text and ner
WSD tasks may be put into this column format
"the UD corpora follow the CoNLL-U format, for which we have a special reader"
"for text classifiers, we use our own special format"
NER corpus for Basque
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
get train and test data
"read in test file if exists, otherwise sample 10% of train data as test dataset"
"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
convert tag scheme to iobes
automatically identify train / test / dev files
automatically identify train / test / dev files
"most data sets have the token text in the first column, if not, pass 'text' as column"
conll 2000 chunking task
Support both TREC-6 and TREC-50
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
Wikiner NER task
unpack and write out in CoNLL column-like format
CoNLL 02/03 NER
universal dependencies
--- UD Germanic
--- UD Romance
--- UD West-Slavic
--- UD Scandinavian
--- UD South-Slavic
--- UD Asian
this is the default init size of a lmdb database for embeddings
some non-used parameter to allow print
get db filename from embedding name
"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
SequenceTagger
TextClassifier
get db filename from embedding name
if embedding database already exists
"otherwise, push embedding to database"
if embedding database already exists
open the database in read mode
we need to set self.k
create and load the database in write mode
"no idea why, but we need to close and reopen the environment to avoid"
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
when opening new transaction !
init dictionaries
"in order to deal with unknown tokens, add <unk>"
"We don't want to create a SpaceTokenizer object each time this function is called,"
so delegate the call directly to the static run_tokenize method
"We don't want to create a SegtokTokenizer object each time this function is called,"
so delegate the call directly to the static run_tokenize method
"if text is passed, instantiate sentence with tokens (words)"
log a warning if the dataset is empty
data with zero-width characters cannot be handled
set token idx if not set
non-set tags are OUT tags
anything that is not a BIOES tag is a SINGLE tag
anything that is not OUT is IN
single and begin tags start a new span
remember previous tag
"if label type is explicitly specified, get spans for this label type"
else determine all label types in sentence and get all spans
move sentence embeddings to device
move token embeddings to device
clear sentence embeddings
clear token embeddings
infer whitespace after field
add Sentence labels to output if they exist
add Token labels to output if they exist
add Sentence labels to output if they exist
add Token labels to output if they exist
No character at the corresponding code point: remove it
set name
sample test data if none is provided
sample dev data if none is provided
set train dev and test data
find out empty sentence indices
create subset of non-empty sentence indices
find out empty sentence indices
create subset of non-empty sentence indices
check if sentence itself has labels
check for labels of words
Make the tag dictionary
global variable: cache_root
global variable: device
global variable: embedding_storage_mode
# dummy return to fulfill trainer.train() needs
print(vec)
Attach optimizer
"convert `metrics` to float, in case it's a zero-dim Tensor"
if memory mode option 'none' delete everything
else delete only dynamic embeddings (otherwise autograd will keep everything in memory)
find out which ones are dynamic embeddings
find out which ones are dynamic embeddings
memory management - option 1: send everything to CPU (pin to memory if we train on GPU)
record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class)
header for 'weights.txt'
"determine the column index of loss, f-score and accuracy for train, dev and test split"
then get all relevant values from the tsv
then get all relevant values from the tsv
plot i
save plots
save plots
plt.show()
save plot
take the average over the last three scores of training
take average over the scores from the different training runs
remove previous embeddings
clearing token embeddings to save memory
"read Dataset into data loader (if list of sentences passed, make Dataset first)"
#TODO: not saving lines yet
== similarity measures ==
helper class for ModelSimilarity
-- works with binary cross entropy loss --
"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}"
-- works with ranking/triplet loss --
normalize the embeddings
== similarity losses ==
"we want that logits for corresponding pairs are high, and for non-corresponding low"
TODO: this assumes eye matrix
"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa"
== similarity learner ==
"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both"
assumes that for each data pair there's at least one embedding per modality
pre-compute embeddings for all targets in evaluation dataset
compute the similarity
sort the similarity matrix across modality 1
"get the ranks, so +1 to start counting ranks from 1"
The conversion from old model's constructor interface
auto-spawn on GPU if available
pad strings with whitespaces to longest sentence
cut up the input into chunks of max charlength = chunk_size
push each chunk through the RNN language model
concatenate all chunks to make final output
initial hidden state
get predicted weights
divide by temperature
"to prevent overflow problem with small temperature values, substract largest value from all"
this makes a vector in which the largest value is 0
compute word weights with exponential function
try sampling multinomial distribution for next character
print(word_idx)
input ids
push list of character IDs through model
the target is always the next character
use cross entropy loss to compare output of forward pass with targets
exponentiate cross-entropy loss to calculate perplexity
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
set the dictionaries
"if we use a CRF, we must add special START and STOP tags to the dictionary"
Initialize the weight tensor
initialize the network architecture
dropouts
optional reprojection layer on top of word embeddings
bidirectional LSTM on top of embedding layer
Create initial hidden state and initialize it
TODO: Decide how to initialize the hidden state variables
self.hs_initializer(self.lstm_init_h)
self.hs_initializer(self.lstm_init_c)
final linear map to tag space
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
clearing token embeddings to save memory
predict for batch
make list of gold tags
make list of predicted tags
"check for true positives, false positives and false negatives"
also write to file in BIO format to use old conlleval script
check if in gold spans
check if in predicted spans
"read Dataset into data loader (if list of sentences passed, make Dataset first)"
"if span F1 needs to be used, use separate eval method"
"else, use scikit-learn to evaluate"
predict for batch
add gold tag
add predicted tag
for file output
use sklearn
"make ""classification report"""
get scores
line for log file
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
"if initial hidden state is trainable, use this state"
word dropout only before LSTM - TODO: more experimentation needed
if self.use_word_dropout > 0.0:
sentence_tensor = self.word_dropout(sentence_tensor)
get the tags in this sentence
add tags as tensor
pad tags if using batch-CRF decoder
reduce raw values to avoid NaN during exp
broadcasting will do the job of reshaping and is more efficient than calling repeat
default value
the historical German taggers by the @redewiegergabe project
clear embeddings after predicting
load each model
check if the same embeddings were already loaded previously
"if the model uses StackedEmbedding, make a new stack with previous objects"
sort embeddings by key alphabetically
check previous embeddings and add if found
only re-use static embeddings
"if not found, use existing embedding"
initialize new stack
"of the model uses regular embedding, re-load if previous version found"
Initialize the weight tensor
auto-spawn on GPU if available
filter empty sentences
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
clearing token embeddings to save memory
"read Dataset into data loader (if list of sentences passed, make Dataset first)"
use scikit-learn to evaluate
remove previously predicted labels
get the gold labels
predict for batch
get the predicted labels
remove predicted labels
"make ""classification report"""
get scores
line for log file
English sentiment models
Communicative Functions Model
cast string to Path
"determine what splits (train, dev, test) to evaluate and log"
prepare loss logging file and set up header
"minimize training loss if training with dev data, else maximize dev score"
"if training also uses dev data, include in training set"
initialize sampler if provided
init with default values if only class is provided
set dataset to sample from
At any point you can hit Ctrl + C to break out of training early.
get new learning rate
reload last best model if annealing with restarts is enabled
stop training if learning rate becomes too small
process mini-batches
zero the gradients on the model and optimizer
"if necessary, make batch_steps"
forward and backward for batch
forward pass
Backward
do the optimizer step
do the scheduler step if one-cycle
get new learning rate
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
"anneal against train loss if training with dev, otherwise anneal against dev score"
evaluate on train / dev / test split depending on training settings
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
calculate scores using dev data if available
append dev score to score history
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
determine learning rate annealing through scheduler. Use auxiliary metric for AnnealOnPlateau
determine bad epoch number
log bad epochs
output log file
make headers on first epoch
"if checkpoint is enabled, save model at each epoch"
"if we use dev data, remember best model based on dev evaluation score"
"if we do not use dev data for model selection, save final model"
test best model if test data is present
"if we are training over multiple datasets, do evaluation for each"
get and return the final test score of best model
cast string to Path
forward pass
update optimizer and scheduler
Add chars to the dictionary
charsplit file content
charsplit file content
Add words to the dictionary
Tokenize file content
"TextDataset returns a list. valid and test are only one file, so return the first element"
cast string to Path
error message if the validation dataset is too small
Shuffle training files randomly after serially iterating through corpus one
"iterate through training data, starting at self.split (for checkpointing)"
off by one for printing
go into train mode
reset variables
not really sure what this does
do the forward pass in the model
try to predict the targets
Backward
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
We detach the hidden state from how it was previously produced.
"If we didn't, the model would try backpropagating all the way to start of the dataset."
explicitly remove loss to clear up memory
##############################################################################
Save the model if the validation loss is the best we've seen so far.
##############################################################################
print info
##############################################################################
##############################################################################
final testing
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
"multilingual forward (English, German, French, Italian, Dutch, Polish)"
"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
Dutch forward
Dutch backward
Swedish forward
Swedish backward
French forward
French backward
Czech forward
Czech backward
Portuguese forward
Portuguese backward
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
1-camembert-base -> camembert-base
1-xlm-roberta-large -> xlm-roberta-large
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
The mask has 1 for real tokens and 0 for padding tokens. Only real
tokens are attended to.
Zero-pad up to the sequence length.
"first, find longest sentence in batch"
prepare id maps for BERT model
put encoded batch through BERT model to get all hidden states of all encoder layers
get aggregated embeddings for each BERT-subtoken in sentence
get the current sentence object
add concatenated embedding to sentence
use first subword embedding if pooling operation is 'first'
"otherwise, do a mean over all subwords in token"
"if only one sentence is passed, convert to list of sentence"
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
embed a dummy sentence to determine embedding_length
Avoid conflicts with flair's Token class
"<cls> token initially set to 1/D, so it attends to all image features equally"
add positional encodings
reshape the pixels into the sequence
layer norm after convolution and positional encodings
add <cls> token
"transformer requires input in the shape [h*w+1, b, d]"
the output is an embedding of <cls> token
temporary fix to disable tokenizer parallelism warning
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
load tokenizer and transformer model
model name
"when initializing, embeddings are in eval mode by default"
embedding parameters
send mini-token through to check how many layers the model has
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
using list comprehension
gradients are enabled if fine-tuning is enabled
"first, subtokenize each sentence and find out into how many subtokens each token was divided"
subtokenize sentences
tokenize and truncate to max subtokens (TODO: check better truncation strategies)
find longest sentence in batch
initialize batch tensors and mask
put encoded batch through transformer model to get all hidden states of all encoder layers
iterate over all subtokenized sentences
use scalar mix of embeddings if so selected
set the extracted embedding for the token
reload tokenizer to get around serialization issues
optional fine-tuning on top of embedding layer
"if only one sentence is passed, convert to list of sentence"
bidirectional RNN on top of embedding layer
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push through RNN
after-RNN dropout
extract embeddings from RNN
models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
"check if this is the case and if so, set it"
IMPORTANT: add embeddings as torch modules
iterate over sentences
"if its a forward LM, take last state"
"convert to plain strings, embedded in a list for the encode function"
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
"if only one sentence is passed, convert to list of sentence"
Expose base classses
Expose token embedding classes
Expose document embedding classes
Expose image embedding classes
Expose legacy embedding classes
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
GLOVE embeddings
TURIAN embeddings
KOMNINOS embeddings
pubmed embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
twitter embeddings
two-letter language code wiki embeddings
two-letter language code wiki embeddings
two-letter language code crawl embeddings
fix serialized models
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
multilingual models
English models
Arabic
Bulgarian
Czech
Danish
German
Spanish
Basque
Persian
Finnish
French
Hebrew
Hindi
Croatian
Indonesian
Italian
Japanese
Malayalam
Dutch
Norwegian
Polish
Portuguese
Pubmed
Slovenian
Swedish
Tamil
CLEF HIPE Shared task
load model if in pretrained model map
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
embeddings are static if we don't do finetuning
embed a dummy sentence to determine embedding_length
set to eval mode
make compatible with serialized models (TODO: remove)
make compatible with serialized models (TODO: remove)
gradients are enable if fine-tuning is enabled
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
offset mode that extracts at whitespace after last character
offset mode that extracts at last character
only clone if optimization mode is 'gpu'
use the character language model embeddings as basis
length is twice the original character LM embedding length
these fields are for the embedding memory
whether to add only capitalized words to memory (faster runtime and lower memory consumption)
we re-compute embeddings dynamically at each epoch
set the memory method
memory is wiped each time we do a training run
"if we keep a pooling, it needs to be updated continuously"
update embedding
check token.text is empty or not
set aggregation operation
add embeddings after updating
temporary fix to disable tokenizer parallelism warning
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
load tokenizer and transformer model
model name
"when initializing, embeddings are in eval mode by default"
embedding parameters
send mini-token through to check how many layers the model has
"self.mix = ScalarMix(mixture_size=len(self.layer_indexes), trainable=False)"
check if special tokens exist to circumvent error message
"most models have an intial BOS token, except for XLNet, T5 and GPT2"
split into micro batches of size self.batch_size before pushing through transformer
embed each micro-batch
remove special markup
"first, subtokenize each sentence and find out into how many subtokens each token was divided"
"TODO: keep for backwards compatibility, but remove in future"
"some pretrained models do not have this property, applying default settings now."
can be set manually after loading the model.
method 1: subtokenize sentence
"subtokenized_sentence = self.tokenizer.encode(tokenized_string, add_special_tokens=True)"
method 2:
transformer specific tokenization
empty sentences get zero embeddings
only embed non-empty sentences and if there is at least one
find longest sentence in batch
initialize batch tensors and mask
put encoded batch through transformer model to get all hidden states of all encoder layers
make the tuple a tensor; makes working with it easier.
gradients are enabled if fine-tuning is enabled
iterate over all subtokenized sentences
"remove stride_size//2 at end of sentence_hidden_state, and half at beginning of remainder,"
in order to get some context into the embeddings of these words.
also don't include the embedding of the extra [CLS] and [SEP] tokens.
"for each token, get embedding"
some tokens have no subtokens at all (if omitted by BERT tokenizer) so return zero vector
"get states from all selected layers, aggregate with pooling operation"
use scalar mix of embeddings if so selected
sm_embeddings = self.mix(subtoken_embeddings)
set the extracted embedding for the token
iterate over subtokens and reconstruct tokens
remove special markup
TODO check if this is necessary is this method is called before prepare_for_model
check if reconstructed token is special begin token ([CLS] or similar)
some BERT tokenizers somehow omit words - in such cases skip to next token
append subtoken to reconstruct token
check if reconstructed token is the same as current token
"if so, add subtoken count"
reset subtoken count and reconstructed token
break from loop if all tokens are accounted for
if tokens are unaccounted for
check if all tokens were matched to subtokens
"if fine-tuning is not enabled (i.e. a ""feature-based approach"" used), this"
module should never be in training mode
reload tokenizer to get around serialization issues
max_tokens = 500
model architecture
model architecture
download if necessary
load the model
"TODO: keep for backwards compatibility, but remove in future"
save the sentence piece model as binary file (not as path which may change)
write out the binary sentence piece model into the expected directory
"if the model was saved as binary and it is not found on disk, write to appropriate path"
"otherwise, use normal process and potentially trigger another download"
"once the modes if there, load it with sentence piece"
empty words get no embedding
all other words get embedded
"the default model for ELMo is the 'original' model, which is very large"
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
put on Cuda if available
embed a dummy sentence to determine embedding_length
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
GLOVE embeddings
"find train, dev and test files if not specified"
get train data
read in test file if exists
read in dev file if exists
special key for space after
"store either Sentence objects in memory, or only file offsets"
"most data sets have the token text in the first column, if not, pass 'text' as column"
determine encoding of text file
skip first line if to selected
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
Remove CoNLL-U meta information in the last column
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
rename according to train - test - dev - convention
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
data is not in IOB2 format. Thus we transform it to IOB2
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
download data if necessary
unpack and write out in CoNLL column-like format
"find train, dev and test files if not specified"
use test_file to create test split if available
use dev_file to create test split if available
"if data point contains black-listed label, do not use"
first check if valid sentence
"if so, add to indices"
"find train, dev and test files if not specified"
variables
different handling of in_memory data than streaming data
"most data sets have the token text in the first column, if not, pass 'text' as column"
test if format is OK
test if at least one label given
noinspection PyDefaultArgument
dataset name includes the split size
default dataset folder is the cache root
download data if necessary
download each of the 28 splits
create dataset directory if necessary
download senteval datasets if necessary und unzip
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
"by defaut, map point score to POSITIVE / NEGATIVE values"
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file from CSV
create test.txt file from CSV
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
convert to FastText format
this dataset name
default dataset folder is the cache root
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
default dataset folder is the cache root
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
check if dataset is supported
set file names
download and unzip in file structure if necessary
instantiate corpus
Uses dynamic programming approach to calculate maximum independent set in interval graph
with sum of all entity lengths as secondary key
calculate offset without current text
because we stick all passages of a document together
TODO For split entities we also annotate everything inbetween which might be a bad idea?
Try to fix incorrect annotations
print(
"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
)
Ignore empty lines or relation annotations
FIX annotation of whitespaces (necessary for PDR)
One token may contain multiple entities -> deque all of them
column format
this dataset name
default dataset folder is the cache root
Create tokenization-dependent CONLL files. This is necessary to prevent
from caching issues (e.g. loading the same corpus with different sentence splitters)
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
Edge case: last token starts a new entity
Last document in file
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
In the huner split files there is no information whether a given id originates
from the train or test file of the original corpus - so we have to adapt corpus
splitting here
Edge case: last token starts a new entity
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
download file is huge => make default_dir visible so that derivative
corpora can all use the same download file
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
Read texts
Read annotations
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
We need to apply a patch to correct the original training file
Articles title
Article abstract
Entity annotations
column format
this dataset name
default dataset folder is the cache root
Edge case: last token starts a new entity
Map all entities to chemicals
Map all entities to disease
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
Incomplete article
Invalid XML syntax
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
if len(mid) != 3:
continue
Try to fix entity offsets
column format
this dataset name
default dataset folder is the cache root
There is still one illegal annotation in the file ..
column format
this dataset name
default dataset folder is the cache root
"Abstract first, title second to prevent issues with sentence splitting"
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
"Filter for specific entity types, by default no entities will be filtered"
Get original HUNER splits to retrieve a list of all document ids contained in V2
train and dev split of V2 will be train in V4
test split of V2 will be dev in V4
New documents in V4 will become test documents
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
cache Feidegger config file
cache Feidegger images
replace image URL with local cached file
append Sentence-Image data point
"in certain cases, multi-CPU data loading makes no sense and slows"
"everything down. For this reason, we detect if a dataset is in-memory:"
"if so, num_workers is set to 0 for faster processing"
cast to list if necessary
cast to list if necessary
"first, check if pymongo is installed"
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
Expose base classses
Expose all sequence labeling datasets
Expose all document classification datasets
Expose all treebanks
Expose all text-text datasets
Expose all text-image datasets
Expose all biomedical data sets
Expose all biomedical data sets using the HUNER splits
-
-
-
-
Expose all biomedical data sets used for the evaluation of BioBERT
"find train, dev and test files if not specified"
get train data
get test data
get dev data
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
clean up file
bioes tags
bio tags
broken tags
all tags
all weird tags
tags with confidence
bioes tags
bioes tags
increment for last token in sentence if not followed by whitespace
clean up directory
clean up directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
clean up results directory
initialize trainer
clean up results directory
def test_multiclass_metrics():
""
"metric = Metric(""Test"")"
"available_labels = [""A"", ""B"", ""C""]"
""
"predictions = [""A"", ""B""]"
"true_values = [""A""]"
TextClassifier._evaluate_sentence_for_text_classification(
"metric, available_labels, predictions, true_values"
)
""
"predictions = [""C"", ""B""]"
"true_values = [""A"", ""B""]"
TextClassifier._evaluate_sentence_for_text_classification(
"metric, available_labels, predictions, true_values"
)
""
print(metric)
from flair.trainers.trainer_regression import RegressorTrainer
def test_trainer_results(tasks_base_path):
"corpus, model, trainer = init(tasks_base_path)"
"results = trainer.train(""regression_train/"", max_epochs=1)"
"assert results[""test_score""] > 0"
"assert len(results[""dev_loss_history""]) == 1"
"assert len(results[""dev_score_history""]) == 1"
"assert len(results[""train_loss_history""]) == 1"
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
clean up results directory
define search space
sequence tagger parameter
model trainer parameter
training parameter
find best parameter settings
clean up results directory
document embeddings parameter
training parameter
clean up results directory
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
get two corpora as one
"get training, test and dev data for full English UD corpus from web"
clean up data directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
1. get the corpus
2. what tag do we want to predict?
3. make the tag dictionary from the corpus
initialize embeddings
comment in this line to use character embeddings
"CharacterEmbeddings(),"
comment in these lines to use contextual string embeddings
""
"FlairEmbeddings('news-forward'),"
""
"FlairEmbeddings('news-backward'),"
initialize sequence tagger
initialize trainer
from allennlp.common.tqdm import Tqdm
mmap seems to be much more memory efficient
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
Extract all the contents of zip file in current directory
get cache path to put the file
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
first determine the distribution of classes in the dataset
weight for each sample
Create blocks
shuffle the blocks
concatenate the shuffled blocks
Create blocks
shuffle the blocks
concatenate the shuffled blocks
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
see https://github.com/zalandoresearch/flair/issues/351
State initialization
Exponential moving average of gradient values
Exponential moving average of squared gradient values
Maintains max of all exp. moving avg. of sq. grad. values
Decay the first and second moment running average coefficient
Maintains the maximum of all 2nd moment running avg. till now
Use the max. for normalizing running avg. of gradient
determine offsets for whitespace_after field
increment for last token in sentence if not followed by whitespace
determine offsets for whitespace_after field
conll 2000 column format
conll 03 NER column format
WNUT-17
-- WikiNER datasets
-- Universal Dependencies
Germanic
Romance
West-Slavic
South-Slavic
East-Slavic
Scandinavian
Asian
Language isolates
recent Universal Dependencies
other datasets
text classification format
text regression format
"first, try to fetch dataset online"
default dataset folder is the cache root
get string value if enum is passed
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
the CoNLL 03 task for German has an additional lemma column
the CoNLL 03 task for Dutch has no NP column
the CoNLL 03 task for Spanish only has two columns
the GERMEVAL task only has two columns: text and ner
WSD tasks may be put into this column format
"the UD corpora follow the CoNLL-U format, for which we have a special reader"
"for text classifiers, we use our own special format"
NER corpus for Basque
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
get train and test data
"read in test file if exists, otherwise sample 10% of train data as test dataset"
"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
convert tag scheme to iobes
automatically identify train / test / dev files
automatically identify train / test / dev files
"most data sets have the token text in the first column, if not, pass 'text' as column"
conll 2000 chunking task
Support both TREC-6 and TREC-50
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
Wikiner NER task
unpack and write out in CoNLL column-like format
CoNLL 02/03 NER
universal dependencies
--- UD Germanic
--- UD Romance
--- UD West-Slavic
--- UD Scandinavian
--- UD South-Slavic
--- UD Asian
this is the default init size of a lmdb database for embeddings
some non-used parameter to allow print
get db filename from embedding name
"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
SequenceTagger
TextClassifier
get db filename from embedding name
if embedding database already exists
"otherwise, push embedding to database"
if embedding database already exists
open the database in read mode
we need to set self.k
create and load the database in write mode
"no idea why, but we need to close and reopen the environment to avoid"
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
when opening new transaction !
init dictionaries
"in order to deal with unknown tokens, add <unk>"
"We don't want to create a SpaceTokenizer object each time this function is called,"
so delegate the call directly to the static run_tokenize method
"We don't want to create a SegtokTokenizer object each time this function is called,"
so delegate the call directly to the static run_tokenize method
"if text is passed, instantiate sentence with tokens (words)"
log a warning if the dataset is empty
data with zero-width characters cannot be handled
set token idx if not set
non-set tags are OUT tags
anything that is not a BIOES tag is a SINGLE tag
anything that is not OUT is IN
single and begin tags start a new span
remember previous tag
move sentence embeddings to device
move token embeddings to device
clear sentence embeddings
clear token embeddings
infer whitespace after field
add Sentence labels to output if they exist
add Token labels to output if they exist
add Sentence labels to output if they exist
add Token labels to output if they exist
No character at the corresponding code point: remove it
set name
sample test data if none is provided
sample dev data if none is provided
set train dev and test data
find out empty sentence indices
create subset of non-empty sentence indices
check if sentence itself has labels
check for labels of words
Make the tag dictionary
global variable: cache_root
global variable: device
global variable: embedding_storage_mode
# dummy return to fulfill trainer.train() needs
Attach optimizer
"convert `metrics` to float, in case it's a zero-dim Tensor"
if memory mode option 'none' delete everything
else delete only dynamic embeddings (otherwise autograd will keep everything in memory)
find out which ones are dynamic embeddings
find out which ones are dynamic embeddings
memory management - option 1: send everything to CPU (pin to memory if we train on GPU)
record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class)
header for 'weights.txt'
"determine the column index of loss, f-score and accuracy for train, dev and test split"
then get all relevant values from the tsv
then get all relevant values from the tsv
plot i
save plots
save plots
plt.show()
save plot
take the average over the last three scores of training
take average over the scores from the different training runs
remove previous embeddings
clearing token embeddings to save memory
"read Dataset into data loader (if list of sentences passed, make Dataset first)"
#TODO: not saving lines yet
== similarity measures ==
helper class for ModelSimilarity
-- works with binary cross entropy loss --
"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}"
-- works with ranking/triplet loss --
normalize the embeddings
== similarity losses ==
"we want that logits for corresponding pairs are high, and for non-corresponding low"
TODO: this assumes eye matrix
"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa"
== similarity learner ==
"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both"
assumes that for each data pair there's at least one embedding per modality
pre-compute embeddings for all targets in evaluation dataset
compute the similarity
sort the similarity matrix across modality 1
"get the ranks, so +1 to start counting ranks from 1"
The conversion from old model's constructor interface
auto-spawn on GPU if available
pad strings with whitespaces to longest sentence
cut up the input into chunks of max charlength = chunk_size
push each chunk through the RNN language model
concatenate all chunks to make final output
initial hidden state
get predicted weights
divide by temperature
"to prevent overflow problem with small temperature values, substract largest value from all"
this makes a vector in which the largest value is 0
compute word weights with exponential function
try sampling multinomial distribution for next character
print(word_idx)
input ids
push list of character IDs through model
the target is always the next character
use cross entropy loss to compare output of forward pass with targets
exponentiate cross-entropy loss to calculate perplexity
fixed RNN change format for torch 1.4.0
set the dictionaries
"if we use a CRF, we must add special START and STOP tags to the dictionary"
Initialize the weight tensor
initialize the network architecture
dropouts
optional reprojection layer on top of word embeddings
bidirectional LSTM on top of embedding layer
Create initial hidden state and initialize it
TODO: Decide how to initialize the hidden state variables
self.hs_initializer(self.lstm_init_h)
self.hs_initializer(self.lstm_init_c)
final linear map to tag space
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
clearing token embeddings to save memory
predict for batch
make list of gold tags
make list of predicted tags
"check for true positives, false positives and false negatives"
also write to file in BIO format to use old conlleval script
check if in gold spans
check if in predicted spans
"read Dataset into data loader (if list of sentences passed, make Dataset first)"
"if span F1 needs to be used, use separate eval method"
"else, use scikit-learn to evaluate"
predict for batch
add gold tag
add predicted tag
for file output
use sklearn
"make ""classification report"""
get scores
line for log file
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
"if initial hidden state is trainable, use this state"
word dropout only before LSTM - TODO: more experimentation needed
if self.use_word_dropout > 0.0:
sentence_tensor = self.word_dropout(sentence_tensor)
get the tags in this sentence
add tags as tensor
pad tags if using batch-CRF decoder
reduce raw values to avoid NaN during exp
broadcasting will do the job of reshaping and is more efficient than calling repeat
default value
the historical German taggers by the @redewiegergabe project
Initialize the weight tensor
auto-spawn on GPU if available
filter empty sentences
reverse sort all sequences by their length
progress bar for verbosity
stop if all sentences are empty
clearing token embeddings to save memory
"read Dataset into data loader (if list of sentences passed, make Dataset first)"
use scikit-learn to evaluate
predict for batch
remove predicted labels
"make ""classification report"""
get scores
line for log file
English sentiment models
Communicative Functions Model
cast string to Path
"determine what splits (train, dev, test) to evaluate and log"
prepare loss logging file and set up header
"minimize training loss if training with dev data, else maximize dev score"
"if training also uses dev data, include in training set"
initialize sampler if provided
init with default values if only class is provided
set dataset to sample from
At any point you can hit Ctrl + C to break out of training early.
get new learning rate
reload last best model if annealing with restarts is enabled
stop training if learning rate becomes too small
process mini-batches
zero the gradients on the model and optimizer
"if necessary, make batch_steps"
forward and backward for batch
forward pass
Backward
do the optimizer step
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
"anneal against train loss if training with dev, otherwise anneal against dev score"
evaluate on train / dev / test split depending on training settings
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
calculate scores using dev data if available
append dev score to score history
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
determine learning rate annealing through scheduler. Use auxiliary metric for AnnealOnPlateau
determine bad epoch number
log bad epochs
output log file
make headers on first epoch
"if checkpoint is enabled, save model at each epoch"
"if we use dev data, remember best model based on dev evaluation score"
"if we do not use dev data for model selection, save final model"
test best model if test data is present
"if we are training over multiple datasets, do evaluation for each"
get and return the final test score of best model
cast string to Path
forward pass
update optimizer and scheduler
Add chars to the dictionary
charsplit file content
charsplit file content
Add words to the dictionary
Tokenize file content
"TextDataset returns a list. valid and test are only one file, so return the first element"
cast string to Path
error message if the validation dataset is too small
Shuffle training files randomly after serially iterating through corpus one
"iterate through training data, starting at self.split (for checkpointing)"
off by one for printing
go into train mode
reset variables
not really sure what this does
do the forward pass in the model
try to predict the targets
Backward
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
We detach the hidden state from how it was previously produced.
"If we didn't, the model would try backpropagating all the way to start of the dataset."
explicitly remove loss to clear up memory
##############################################################################
Save the model if the validation loss is the best we've seen so far.
##############################################################################
print info
##############################################################################
##############################################################################
final testing
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
"multilingual forward (English, German, French, Italian, Dutch, Polish)"
"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
Dutch forward
Dutch backward
Swedish forward
Swedish backward
French forward
French backward
Czech forward
Czech backward
Portuguese forward
Portuguese backward
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
1-camembert-base -> camembert-base
1-xlm-roberta-large -> xlm-roberta-large
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
The mask has 1 for real tokens and 0 for padding tokens. Only real
tokens are attended to.
Zero-pad up to the sequence length.
"first, find longest sentence in batch"
prepare id maps for BERT model
put encoded batch through BERT model to get all hidden states of all encoder layers
get aggregated embeddings for each BERT-subtoken in sentence
get the current sentence object
add concatenated embedding to sentence
use first subword embedding if pooling operation is 'first'
"otherwise, do a mean over all subwords in token"
"if only one sentence is passed, convert to list of sentence"
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
embed a dummy sentence to determine embedding_length
Avoid conflicts with flair's Token class
"<cls> token initially set to 1/D, so it attends to all image features equally"
add positional encodings
reshape the pixels into the sequence
layer norm after convolution and positional encodings
add <cls> token
"transformer requires input in the shape [h*w+1, b, d]"
the output is an embedding of <cls> token
temporary fix to disable tokenizer parallelism warning
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
load tokenizer and transformer model
model name
"when initializing, embeddings are in eval mode by default"
embedding parameters
send mini-token through to check how many layers the model has
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
using list comprehension
gradients are enabled if fine-tuning is enabled
"first, subtokenize each sentence and find out into how many subtokens each token was divided"
subtokenize sentences
tokenize and truncate to 512 subtokens (TODO: check better truncation strategies)
find longest sentence in batch
initialize batch tensors and mask
put encoded batch through transformer model to get all hidden states of all encoder layers
iterate over all subtokenized sentences
use scalar mix of embeddings if so selected
set the extracted embedding for the token
reload tokenizer to get around serialization issues
optional fine-tuning on top of embedding layer
"if only one sentence is passed, convert to list of sentence"
bidirectional RNN on top of embedding layer
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push through RNN
after-RNN dropout
extract embeddings from RNN
fixed RNN change format for torch 1.4.0
IMPORTANT: add embeddings as torch modules
iterate over sentences
"if its a forward LM, take last state"
"convert to plain strings, embedded in a list for the encode function"
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
"if only one sentence is passed, convert to list of sentence"
Expose base classses
Expose token embedding classes
Expose document embedding classes
Expose image embedding classes
Expose legacy embedding classes
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
GLOVE embeddings
TURIAN embeddings
KOMNINOS embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
twitter embeddings
two-letter language code wiki embeddings
two-letter language code wiki embeddings
two-letter language code crawl embeddings
fix serialized models
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
multilingual models
English models
Arabic
Bulgarian
Czech
Danish
German
Spanish
Basque
Persian
Finnish
French
Hebrew
Hindi
Croatian
Indonesian
Italian
Japanese
Malayalam
Dutch
Norwegian
Polish
Portuguese
Pubmed
Slovenian
Swedish
Tamil
CLEF HIPE Shared task
load model if in pretrained model map
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
embeddings are static if we don't do finetuning
embed a dummy sentence to determine embedding_length
set to eval mode
make compatible with serialized models (TODO: remove)
make compatible with serialized models (TODO: remove)
gradients are enable if fine-tuning is enabled
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
offset mode that extracts at whitespace after last character
offset mode that extracts at last character
only clone if optimization mode is 'gpu'
use the character language model embeddings as basis
length is twice the original character LM embedding length
these fields are for the embedding memory
whether to add only capitalized words to memory (faster runtime and lower memory consumption)
we re-compute embeddings dynamically at each epoch
set the memory method
memory is wiped each time we do a training run
"if we keep a pooling, it needs to be updated continuously"
update embedding
check token.text is empty or not
set aggregation operation
add embeddings after updating
temporary fix to disable tokenizer parallelism warning
(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
load tokenizer and transformer model
model name
"when initializing, embeddings are in eval mode by default"
embedding parameters
send mini-token through to check how many layers the model has
"self.mix = ScalarMix(mixture_size=len(self.layer_indexes), trainable=False)"
check if special tokens exist to circumvent error message
"most models have an intial BOS token, except for XLNet, T5 and GPT2"
split into micro batches of size self.batch_size before pushing through transformer
embed each micro-batch
remove special markup
"first, subtokenize each sentence and find out into how many subtokens each token was divided"
"TODO: keep for backwards compatibility, but remove in future"
"some pretrained models do not have this property, applying default settings now."
can be set manually after loading the model.
method 1: subtokenize sentence
"subtokenized_sentence = self.tokenizer.encode(tokenized_string, add_special_tokens=True)"
method 2:
transformer specific tokenization
empty sentences get zero embeddings
only embed non-empty sentences and if there is at least one
find longest sentence in batch
initialize batch tensors and mask
put encoded batch through transformer model to get all hidden states of all encoder layers
make the tuple a tensor; makes working with it easier.
gradients are enabled if fine-tuning is enabled
iterate over all subtokenized sentences
"remove stride_size//2 at end of sentence_hidden_state, and half at beginning of remainder,"
in order to get some context into the embeddings of these words.
also don't include the embedding of the extra [CLS] and [SEP] tokens.
"for each token, get embedding"
some tokens have no subtokens at all (if omitted by BERT tokenizer) so return zero vector
"get states from all selected layers, aggregate with pooling operation"
use scalar mix of embeddings if so selected
sm_embeddings = self.mix(subtoken_embeddings)
set the extracted embedding for the token
iterate over subtokens and reconstruct tokens
remove special markup
TODO check if this is necessary is this method is called before prepare_for_model
check if reconstructed token is special begin token ([CLS] or similar)
some BERT tokenizers somehow omit words - in such cases skip to next token
append subtoken to reconstruct token
check if reconstructed token is the same as current token
"if so, add subtoken count"
reset subtoken count and reconstructed token
break from loop if all tokens are accounted for
if tokens are unaccounted for
check if all tokens were matched to subtokens
"if fine-tuning is not enabled (i.e. a ""feature-based approach"" used), this"
module should never be in training mode
reload tokenizer to get around serialization issues
max_tokens = 500
model architecture
model architecture
download if necessary
load the model
"TODO: keep for backwards compatibility, but remove in future"
save the sentence piece model as binary file (not as path which may change)
write out the binary sentence piece model into the expected directory
"if the model was saved as binary and it is not found on disk, write to appropriate path"
"otherwise, use normal process and potentially trigger another download"
"once the modes if there, load it with sentence piece"
empty words get no embedding
all other words get embedded
"the default model for ELMo is the 'original' model, which is very large"
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
put on Cuda if available
embed a dummy sentence to determine embedding_length
ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
GLOVE embeddings
"find train, dev and test files if not specified"
get train data
read in test file if exists
read in dev file if exists
special key for space after
"store either Sentence objects in memory, or only file offsets"
"most data sets have the token text in the first column, if not, pass 'text' as column"
determine encoding of text file
skip first line if to selected
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
Remove CoNLL-U meta information in the last column
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
rename according to train - test - dev - convention
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
data is not in IOB2 format. Thus we transform it to IOB2
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
download data if necessary
unpack and write out in CoNLL column-like format
"find train, dev and test files if not specified"
use test_file to create test split if available
use dev_file to create test split if available
"if data point contains black-listed label, do not use"
first check if valid sentence
"if so, add to indices"
"find train, dev and test files if not specified"
variables
different handling of in_memory data than streaming data
"most data sets have the token text in the first column, if not, pass 'text' as column"
test if format is OK
test if at least one label given
noinspection PyDefaultArgument
dataset name includes the split size
default dataset folder is the cache root
download data if necessary
download each of the 28 splits
create dataset directory if necessary
download senteval datasets if necessary und unzip
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
"by defaut, map point score to POSITIVE / NEGATIVE values"
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file from CSV
create test.txt file from CSV
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
convert to FastText format
this dataset name
default dataset folder is the cache root
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
default dataset folder is the cache root
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
check if dataset is supported
set file names
download and unzip in file structure if necessary
instantiate corpus
cache Feidegger config file
cache Feidegger images
replace image URL with local cached file
append Sentence-Image data point
"in certain cases, multi-CPU data loading makes no sense and slows"
"everything down. For this reason, we detect if a dataset is in-memory:"
"if so, num_workers is set to 0 for faster processing"
cast to list if necessary
cast to list if necessary
"first, check if pymongo is installed"
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
Expose base classses
Expose all sequence labeling datasets
Expose all document classification datasets
Expose all treebanks
Expose all text-text datasets
Expose all text-image datasets
"find train, dev and test files if not specified"
get train data
get test data
get dev data
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
skip because it is optional https://github.com/flairNLP/flair/pull/1296
clean up file
bioes tags
bio tags
broken tags
all tags
all weird tags
tags with confidence
bioes tags
bioes tags
clean up directory
clean up directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
clean up results directory
initialize trainer
clean up results directory
def test_multiclass_metrics():
""
"metric = Metric(""Test"")"
"available_labels = [""A"", ""B"", ""C""]"
""
"predictions = [""A"", ""B""]"
"true_values = [""A""]"
TextClassifier._evaluate_sentence_for_text_classification(
"metric, available_labels, predictions, true_values"
)
""
"predictions = [""C"", ""B""]"
"true_values = [""A"", ""B""]"
TextClassifier._evaluate_sentence_for_text_classification(
"metric, available_labels, predictions, true_values"
)
""
print(metric)
from flair.trainers.trainer_regression import RegressorTrainer
def test_trainer_results(tasks_base_path):
"corpus, model, trainer = init(tasks_base_path)"
"results = trainer.train(""regression_train/"", max_epochs=1)"
"assert results[""test_score""] > 0"
"assert len(results[""dev_loss_history""]) == 1"
"assert len(results[""dev_score_history""]) == 1"
"assert len(results[""train_loss_history""]) == 1"
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
clean up results directory
define search space
sequence tagger parameter
model trainer parameter
training parameter
find best parameter settings
clean up results directory
document embeddings parameter
training parameter
clean up results directory
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
get two corpora as one
"get training, test and dev data for full English UD corpus from web"
clean up data directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
1. get the corpus
2. what tag do we want to predict?
3. make the tag dictionary from the corpus
initialize embeddings
comment in this line to use character embeddings
"CharacterEmbeddings(),"
comment in these lines to use contextual string embeddings
""
"FlairEmbeddings('news-forward'),"
""
"FlairEmbeddings('news-backward'),"
initialize sequence tagger
initialize trainer
from allennlp.common.tqdm import Tqdm
mmap seems to be much more memory efficient
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
Extract all the contents of zip file in current directory
get cache path to put the file
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
first determine the distribution of classes in the dataset
weight for each sample
Create blocks
shuffle the blocks
concatenate the shuffled blocks
Create blocks
shuffle the blocks
concatenate the shuffled blocks
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
see https://github.com/zalandoresearch/flair/issues/351
State initialization
Exponential moving average of gradient values
Exponential moving average of squared gradient values
Maintains max of all exp. moving avg. of sq. grad. values
Decay the first and second moment running average coefficient
Maintains the maximum of all 2nd moment running avg. till now
Use the max. for normalizing running avg. of gradient
conll 2000 column format
conll 03 NER column format
WNUT-17
-- WikiNER datasets
-- Universal Dependencies
Germanic
Romance
West-Slavic
South-Slavic
East-Slavic
Scandinavian
Asian
Language isolates
recent Universal Dependencies
other datasets
text classification format
text regression format
"first, try to fetch dataset online"
default dataset folder is the cache root
get string value if enum is passed
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
the CoNLL 03 task for German has an additional lemma column
the CoNLL 03 task for Dutch has no NP column
the CoNLL 03 task for Spanish only has two columns
the GERMEVAL task only has two columns: text and ner
WSD tasks may be put into this column format
"the UD corpora follow the CoNLL-U format, for which we have a special reader"
"for text classifiers, we use our own special format"
NER corpus for Basque
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
get train and test data
"read in test file if exists, otherwise sample 10% of train data as test dataset"
"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
convert tag scheme to iobes
automatically identify train / test / dev files
automatically identify train / test / dev files
"most data sets have the token text in the first column, if not, pass 'text' as column"
conll 2000 chunking task
Support both TREC-6 and TREC-50
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
Wikiner NER task
unpack and write out in CoNLL column-like format
CoNLL 02/03 NER
universal dependencies
--- UD Germanic
--- UD Romance
--- UD West-Slavic
--- UD Scandinavian
--- UD South-Slavic
--- UD Asian
this is the default init size of a lmdb database for embeddings
some non-used parameter to allow print
get db filename from embedding name
"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
SequenceTagger
TextClassifier
get db filename from embedding name
if embedding database already exists
"otherwise, push embedding to database"
if embedding database already exists
open the database in read mode
we need to set self.k
create and load the database in write mode
"no idea why, but we need to close and reopen the environment to avoid"
mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
when opening new transaction !
init dictionaries
"in order to deal with unknown tokens, add <unk>"
"if text is passed, instantiate sentence with tokens (words)"
log a warning if the dataset is empty
data with zero-width characters cannot be handled
set token idx if not set
non-set tags are OUT tags
anything that is not a BIOES tag is a SINGLE tag
anything that is not OUT is IN
single and begin tags start a new span
remember previous tag
move sentence embeddings to device
move token embeddings to device
clear sentence embeddings
clear token embeddings
infer whitespace after field
add Sentence labels to output if they exist
add Token labels to output if they exist
add Sentence labels to output if they exist
add Token labels to output if they exist
No character at the corresponding code point: remove it
set name
sample test data if none is provided
sample dev data if none is provided
set train dev and test data
find out empty sentence indices
create subset of non-empty sentence indices
check if sentence itself has labels
check for labels of words
Make the tag dictionary
increment for last token in sentence if not followed by whitespace
determine offsets for whitespace_after field
determine offsets for whitespace_after field
global variable: cache_root
global variable: device
global variable: embedding_storage_mode
# dummy return to fulfill trainer.train() needs
Attach optimizer
"convert `metrics` to float, in case it's a zero-dim Tensor"
if memory mode option 'none' delete everything
else delete only dynamic embeddings (otherwise autograd will keep everything in memory)
find out which ones are dynamic embeddings
find out which ones are dynamic embeddings
memory management - option 1: send everything to CPU (pin to memory if we train on GPU)
record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class)
header for 'weights.txt'
"determine the column index of loss, f-score and accuracy for train, dev and test split"
then get all relevant values from the tsv
then get all relevant values from the tsv
print(rows)
"figsize = (16, 16)"
plot i
save plots
save plots
plt.show()
save plot
take the average over the last three scores of training
take average over the scores from the different training runs
remove previous embeddings
clearing token embeddings to save memory
#TODO: not saving lines yet
== similarity measures ==
helper class for ModelSimilarity
-- works with binary cross entropy loss --
"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}"
-- works with ranking/triplet loss --
normalize the embeddings
== similarity losses ==
"we want that logits for corresponding pairs are high, and for non-corresponding low"
TODO: this assumes eye matrix
"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa"
== similarity learner ==
"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both"
assumes that for each data pair there's at least one embedding per modality
pre-compute embeddings for all targets in evaluation dataset
compute the similarity
sort the similarity matrix across modality 1
"get the ranks, so +1 to start counting ranks from 1"
The conversion from old model's constructor interface
auto-spawn on GPU if available
pad strings with whitespaces to longest sentence
cut up the input into chunks of max charlength = chunk_size
push each chunk through the RNN language model
concatenate all chunks to make final output
initial hidden state
get predicted weights
divide by temperature
"to prevent overflow problem with small temperature values, substract largest value from all"
this makes a vector in which the largest value is 0
compute word weights with exponential function
try sampling multinomial distribution for next character
print(word_idx)
input ids
push list of character IDs through model
the target is always the next character
use cross entropy loss to compare output of forward pass with targets
exponentiate cross-entropy loss to calculate perplexity
fixed RNN change format for torch 1.4.0
set the dictionaries
"if we use a CRF, we must add special START and STOP tags to the dictionary"
Initialize the weight tensor
initialize the network architecture
dropouts
"if no dimensionality for reprojection layer is set, reproject to equal dimension"
bidirectional LSTM on top of embedding layer
Create initial hidden state and initialize it
TODO: Decide how to initialize the hidden state variables
self.hs_initializer(self.lstm_init_h)
self.hs_initializer(self.lstm_init_c)
final linear map to tag space
reverse sort all sequences by their length
remove previous embeddings
progress bar for verbosity
stop if all sentences are empty
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
clearing token embeddings to save memory
append both to file for evaluation
make list of gold tags
make list of predicted tags
"check for true positives, false positives and false negatives"
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
"if initial hidden state is trainable, use this state"
word dropout only before LSTM - TODO: more experimentation needed
if self.use_word_dropout > 0.0:
sentence_tensor = self.word_dropout(sentence_tensor)
get the tags in this sentence
add tags as tensor
pad tags if using batch-CRF decoder
reduce raw values to avoid NaN during exp
broadcasting will do the job of reshaping and is more efficient than calling repeat
default value
the historical German taggers by the @redewiegergabe project
Initialize the weight tensor
auto-spawn on GPU if available
filter empty sentences
reverse sort all sequences by their length
remove previous embeddings
progress bar for verbosity
stop if all sentences are empty
clearing token embeddings to save memory
English sentiment models
cast string to Path
"determine what splits (train, dev, test) to evaluate and log"
prepare loss logging file and set up header
"minimize training loss if training with dev data, else maximize dev score"
"if training also uses dev data, include in training set"
initialize sampler if provided
init with default values if only class is provided
set dataset to sample from
At any point you can hit Ctrl + C to break out of training early.
get new learning rate
reload last best model if annealing with restarts is enabled
stop training if learning rate becomes too small
process mini-batches
zero the gradients on the model and optimizer
"if necessary, make batch_steps"
forward and backward for batch
forward pass
Backward
do the optimizer step
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
"anneal against train loss if training with dev, otherwise anneal against dev score"
evaluate on train / dev / test split depending on training settings
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
calculate scores using dev data if available
append dev score to score history
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
determine learning rate annealing through scheduler. Use auxiliary metric for AnnealOnPlateau
determine bad epoch number
log bad epochs
output log file
make headers on first epoch
"if checkpoint is enabled, save model at each epoch"
"if we use dev data, remember best model based on dev evaluation score"
"if we do not use dev data for model selection, save final model"
test best model if test data is present
"if we are training over multiple datasets, do evaluation for each"
get and return the final test score of best model
cast string to Path
forward pass
update optimizer and scheduler
Add chars to the dictionary
charsplit file content
charsplit file content
Add words to the dictionary
Tokenize file content
"TextDataset returns a list. valid and test are only one file, so return the first element"
cast string to Path
error message if the validation dataset is too small
Shuffle training files randomly after serially iterating through corpus one
"iterate through training data, starting at self.split (for checkpointing)"
off by one for printing
go into train mode
reset variables
not really sure what this does
do the forward pass in the model
try to predict the targets
Backward
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
We detach the hidden state from how it was previously produced.
"If we didn't, the model would try backpropagating all the way to start of the dataset."
explicitly remove loss to clear up memory
##############################################################################
Save the model if the validation loss is the best we've seen so far.
##############################################################################
print info
##############################################################################
##############################################################################
final testing
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
"multilingual forward (English, German, French, Italian, Dutch, Polish)"
"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
Dutch forward
Dutch backward
Swedish forward
Swedish backward
French forward
French backward
Czech forward
Czech backward
Portuguese forward
Portuguese backward
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
1-camembert-base -> camembert-base
1-xlm-roberta-large -> xlm-roberta-large
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
The mask has 1 for real tokens and 0 for padding tokens. Only real
tokens are attended to.
Zero-pad up to the sequence length.
"first, find longest sentence in batch"
prepare id maps for BERT model
put encoded batch through BERT model to get all hidden states of all encoder layers
get aggregated embeddings for each BERT-subtoken in sentence
get the current sentence object
add concatenated embedding to sentence
use first subword embedding if pooling operation is 'first'
"otherwise, do a mean over all subwords in token"
"if only one sentence is passed, convert to list of sentence"
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
embed a dummy sentence to determine embedding_length
Avoid conflicts with flair's Token class
"<cls> token initially set to 1/D, so it attends to all image features equally"
add positional encodings
reshape the pixels into the sequence
layer norm after convolution and positional encodings
add <cls> token
"transformer requires input in the shape [h*w+1, b, d]"
the output is an embedding of <cls> token
load tokenizer and transformer model
model name
"when initializing, embeddings are in eval mode by default"
embedding parameters
send mini-token through to check how many layers the model has
"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
using list comprehension
gradients are enabled if fine-tuning is enabled
"first, subtokenize each sentence and find out into how many subtokens each token was divided"
subtokenize sentences
tokenize and truncate to 512 subtokens (TODO: check better truncation strategies)
find longest sentence in batch
initialize batch tensors and mask
put encoded batch through transformer model to get all hidden states of all encoder layers
iterate over all subtokenized sentences
use scalar mix of embeddings if so selected
set the extracted embedding for the token
reload tokenizer to get around serialization issues
optional fine-tuning on top of embedding layer
"if only one sentence is passed, convert to list of sentence"
bidirectional RNN on top of embedding layer
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push through RNN
after-RNN dropout
extract embeddings from RNN
fixed RNN change format for torch 1.4.0
IMPORTANT: add embeddings as torch modules
iterate over sentences
"if its a forward LM, take last state"
"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
"if only one sentence is passed, convert to list of sentence"
Expose base classses
Expose token embedding classes
Expose document embedding classes
Expose image embedding classes
Expose legacy embedding classes
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
GLOVE embeddings
TURIAN embeddings
KOMNINOS embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
twitter embeddings
two-letter language code wiki embeddings
two-letter language code wiki embeddings
two-letter language code crawl embeddings
fix serialized models
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
multilingual models
English models
Arabic
Bulgarian
Czech
Danish
German
Spanish
Basque
Persian
Finnish
French
Hebrew
Hindi
Croatian
Indonesian
Italian
Japanese
Malayalam
Dutch
Norwegian
Polish
Portuguese
Pubmed
Slovenian
Swedish
Tamil
CLEF HIPE Shared task
load model if in pretrained model map
Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
embeddings are static if we don't do finetuning
embed a dummy sentence to determine embedding_length
set to eval mode
make compatible with serialized models (TODO: remove)
make compatible with serialized models (TODO: remove)
gradients are enable if fine-tuning is enabled
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
offset mode that extracts at whitespace after last character
offset mode that extracts at last character
only clone if optimization mode is 'gpu'
use the character language model embeddings as basis
length is twice the original character LM embedding length
these fields are for the embedding memory
whether to add only capitalized words to memory (faster runtime and lower memory consumption)
we re-compute embeddings dynamically at each epoch
set the memory method
memory is wiped each time we do a training run
"if we keep a pooling, it needs to be updated continuously"
update embedding
check token.text is empty or not
add embeddings after updating
load tokenizer and transformer model
model name
"when initializing, embeddings are in eval mode by default"
embedding parameters
send mini-token through to check how many layers the model has
"self.mix = ScalarMix(mixture_size=len(self.layer_indexes), trainable=False)"
check if special tokens exist to circumvent error message
"most models have an intial BOS token, except for XLNet, T5 and GPT2"
split into micro batches of size self.batch_size before pushing through transformer
embed each micro-batch
remove special markup
"first, subtokenize each sentence and find out into how many subtokens each token was divided"
method 1: subtokenize sentence
"subtokenized_sentence = self.tokenizer.encode(tokenized_string, add_special_tokens=True)"
method 2:
print(subtokens)
iterate over subtokens and reconstruct tokens
remove special markup
append subtoken to reconstruct token
check if reconstructed token is special begin token ([CLS] or similar)
check if reconstructed token is the same as current token
"if so, add subtoken count"
reset subtoken count and reconstructed token
break from loop if all tokens are accounted for
check if all tokens were matched to subtokens
find longest sentence in batch
initialize batch tensors and mask
put encoded batch through transformer model to get all hidden states of all encoder layers
gradients are enabled if fine-tuning is enabled
iterate over all subtokenized sentences
"for each token, get embedding"
"get states from all selected layers, aggregate with pooling operation"
use scalar mix of embeddings if so selected
sm_embeddings = self.mix(subtoken_embeddings)
set the extracted embedding for the token
"if fine-tuning is not enabled (i.e. a ""feature-based approach"" used), this"
module should never be in training mode
reload tokenizer to get around serialization issues
max_tokens = 500
model architecture
model architecture
download if necessary
load the model
"TODO: keep for backwards compatibility, but remove in future"
save the sentence piece model as binary file (not as path which may change)
write out the binary sentence piece model into the expected directory
"if the model was saved as binary and it is not found on disk, write to appropriate path"
"otherwise, use normal process and potentially trigger another download"
"once the modes if there, load it with sentence piece"
empty words get no embedding
all other words get embedded
"the default model for ELMo is the 'original' model, which is very large"
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
put on Cuda if available
embed a dummy sentence to determine embedding_length
GLOVE embeddings
"find train, dev and test files if not specified"
get train data
read in test file if exists
read in dev file if exists
special key for space after
"store either Sentence objects in memory, or only file offsets"
"most data sets have the token text in the first column, if not, pass 'text' as column"
determine encoding of text file
skip first line if to selected
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
Remove CoNLL-U meta information in the last column
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
download data if necessary
unpack and write out in CoNLL column-like format
"find train, dev and test files if not specified"
use test_file to create test split if available
use dev_file to create test split if available
"if data point contains black-listed label, do not use"
first check if valid sentence
"if so, add to indices"
"find train, dev and test files if not specified"
variables
different handling of in_memory data than streaming data
"most data sets have the token text in the first column, if not, pass 'text' as column"
test if format is OK
test if at least one label given
noinspection PyDefaultArgument
dataset name includes the split size
default dataset folder is the cache root
download data if necessary
download each of the 28 splits
create dataset directory if necessary
download senteval datasets if necessary und unzip
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
"by defaut, map point score to POSITIVE / NEGATIVE values"
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file from CSV
create test.txt file from CSV
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create dataset directory if necessary
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
create train.txt file by iterating over pos and neg file
this dataset name
default dataset folder is the cache root
download data if necessary
download senteval datasets if necessary und unzip
convert to FastText format
this dataset name
default dataset folder is the cache root
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
default dataset folder is the cache root
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
check if dataset is supported
set file names
download and unzip in file structure if necessary
instantiate corpus
cache Feidegger config file
cache Feidegger images
replace image URL with local cached file
append Sentence-Image data point
"in certain cases, multi-CPU data loading makes no sense and slows"
"everything down. For this reason, we detect if a dataset is in-memory:"
"if so, num_workers is set to 0 for faster processing"
cast to list if necessary
cast to list if necessary
"first, check if pymongo is installed"
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
Expose base classses
Expose all sequence labeling datasets
Expose all document classification datasets
Expose all treebanks
Expose all text-text datasets
Expose all text-image datasets
"find train, dev and test files if not specified"
get train data
get test data
get dev data
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
skip because it is optional https://github.com/flairNLP/flair/pull/1296
def test_create_sentence_using_japanese_tokenizer():
"sentence: Sentence = Sentence("""", use_tokenizer=build_japanese_tokenizer())"
""
assert 5 == len(sentence.tokens)
"assert """" == sentence.tokens[0].text"
"assert """" == sentence.tokens[1].text"
"assert """" == sentence.tokens[2].text"
"assert """" == sentence.tokens[3].text"
"assert """" == sentence.tokens[4].text"
clean up file
bioes tags
bio tags
broken tags
all tags
all weird tags
tags with confidence
bioes tags
bioes tags
clean up directory
clean up directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
clean up results directory
initialize trainer
clean up results directory
def test_multiclass_metrics():
""
"metric = Metric(""Test"")"
"available_labels = [""A"", ""B"", ""C""]"
""
"predictions = [""A"", ""B""]"
"true_values = [""A""]"
TextClassifier._evaluate_sentence_for_text_classification(
"metric, available_labels, predictions, true_values"
)
""
"predictions = [""C"", ""B""]"
"true_values = [""A"", ""B""]"
TextClassifier._evaluate_sentence_for_text_classification(
"metric, available_labels, predictions, true_values"
)
""
print(metric)
from flair.trainers.trainer_regression import RegressorTrainer
def test_trainer_results(tasks_base_path):
"corpus, model, trainer = init(tasks_base_path)"
"results = trainer.train(""regression_train/"", max_epochs=1)"
"assert results[""test_score""] > 0"
"assert len(results[""dev_loss_history""]) == 1"
"assert len(results[""dev_score_history""]) == 1"
"assert len(results[""train_loss_history""]) == 1"
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
clean up results directory
define search space
sequence tagger parameter
model trainer parameter
training parameter
find best parameter settings
clean up results directory
document embeddings parameter
training parameter
clean up results directory
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
get two corpora as one
"get training, test and dev data for full English UD corpus from web"
clean up data directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
1. get the corpus
2. what tag do we want to predict?
3. make the tag dictionary from the corpus
initialize embeddings
comment in this line to use character embeddings
"CharacterEmbeddings(),"
comment in these lines to use contextual string embeddings
""
"FlairEmbeddings('news-forward'),"
""
"FlairEmbeddings('news-backward'),"
initialize sequence tagger
initialize trainer
"if only one sentence is passed, convert to list of sentence"
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
GLOVE embeddings
TURIAN embeddings
KOMNINOS embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
twitter embeddings
two-letter language code wiki embeddings
two-letter language code wiki embeddings
two-letter language code crawl embeddings
fix serialized models
max_tokens = 500
model architecture
model architecture
download if necessary
load the model
empty words get no embedding
all other words get embedded
"the default model for ELMo is the 'original' model, which is very large"
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
put on Cuda if available
embed a dummy sentence to determine embedding_length
embed a dummy sentence to determine embedding_length
Avoid conflicts with flair's Token class
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
1-camembert-base -> camembert-base
1-xlm-roberta-large -> xlm-roberta-large
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
multilingual models
English models
Arabic
Bulgarian
Czech
Danish
German
Spanish
Basque
Persian
Finnish
French
Hebrew
Hindi
Croatian
Indonesian
Italian
Japanese
Dutch
Norwegian
Polish
Portuguese
Pubmed
Slovenian
Swedish
Tamil
load model if in pretrained model map
embeddings are static if we don't do finetuning
embed a dummy sentence to determine embedding_length
set to eval mode
make compatible with serialized models (TODO: remove)
gradients are enable if fine-tuning is enabled
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
only clone if optimization mode is 'gpu'
use the character language model embeddings as basis
length is twice the original character LM embedding length
these fields are for the embedding memory
whether to add only capitalized words to memory (faster runtime and lower memory consumption)
we re-compute embeddings dynamically at each epoch
set the memory method
memory is wiped each time we do a training run
"if we keep a pooling, it needs to be updated continuously"
update embedding
check token.text is empty or not
add embeddings after updating
The mask has 1 for real tokens and 0 for padding tokens. Only real
tokens are attended to.
Zero-pad up to the sequence length.
"first, find longest sentence in batch"
prepare id maps for BERT model
put encoded batch through BERT model to get all hidden states of all encoder layers
get aggregated embeddings for each BERT-subtoken in sentence
get the current sentence object
add concatenated embedding to sentence
use first subword embedding if pooling operation is 'first'
"otherwise, do a mean over all subwords in token"
"multilingual forward (English, German, French, Italian, Dutch, Polish)"
"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
Dutch forward
Dutch backward
Swedish forward
Swedish backward
French forward
French backward
Czech forward
Czech backward
Portuguese forward
Portuguese backward
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
"if only one sentence is passed, convert to list of sentence"
optional fine-tuning on top of embedding layer
"if only one sentence is passed, convert to list of sentence"
bidirectional RNN on top of embedding layer
dropouts
TODO: remove in future versions
embed words in the sentence
before-RNN dropout
reproject if set
push through RNN
after-RNN dropout
extract embeddings from RNN
fixed RNN change format for torch 1.4.0
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
IMPORTANT: add embeddings as torch modules
iterate over sentences
"if its a forward LM, take last state"
GLOVE embeddings
"<cls> token initially set to 1/D, so it attends to all image features equally"
add positional encodings
reshape the pixels into the sequence
layer norm after convolution and positional encodings
add <cls> token
"transformer requires input in the shape [h*w+1, b, d]"
the output is an embedding of <cls> token
"TODO: keep for backwards compatibility, but remove in future"
save the sentence piece model as binary file (not as path which may change)
write out the binary sentence piece model into the expected directory
"if the model was saved as binary and it is not found on disk, write to appropriate path"
"otherwise, use normal process and potentially trigger another download"
"once the modes if there, load it with sentence piece"
from allennlp.common.tqdm import Tqdm
mmap seems to be much more memory efficient
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
Extract all the contents of zip file in current directory
get cache path to put the file
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
first determine the distribution of classes in the dataset
weight for each sample
Create blocks
shuffle the blocks
concatenate the shuffled blocks
Create blocks
shuffle the blocks
concatenate the shuffled blocks
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
see https://github.com/zalandoresearch/flair/issues/351
State initialization
Exponential moving average of gradient values
Exponential moving average of squared gradient values
Maintains max of all exp. moving avg. of sq. grad. values
Decay the first and second moment running average coefficient
Maintains the maximum of all 2nd moment running avg. till now
Use the max. for normalizing running avg. of gradient
conll 2000 column format
conll 03 NER column format
WNUT-17
-- WikiNER datasets
-- Universal Dependencies
Germanic
Romance
West-Slavic
South-Slavic
East-Slavic
Scandinavian
Asian
Language isolates
recent Universal Dependencies
other datasets
text classification format
text regression format
"first, try to fetch dataset online"
default dataset folder is the cache root
get string value if enum is passed
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
the CoNLL 03 task for German has an additional lemma column
the CoNLL 03 task for Dutch has no NP column
the CoNLL 03 task for Spanish only has two columns
the GERMEVAL task only has two columns: text and ner
WSD tasks may be put into this column format
"the UD corpora follow the CoNLL-U format, for which we have a special reader"
"for text classifiers, we use our own special format"
NER corpus for Basque
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
get train and test data
"read in test file if exists, otherwise sample 10% of train data as test dataset"
"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
convert tag scheme to iobes
automatically identify train / test / dev files
automatically identify train / test / dev files
"most data sets have the token text in the first column, if not, pass 'text' as column"
conll 2000 chunking task
Support both TREC-6 and TREC-50
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
Wikiner NER task
unpack and write out in CoNLL column-like format
CoNLL 02/03 NER
universal dependencies
--- UD Germanic
--- UD Romance
--- UD West-Slavic
--- UD Scandinavian
--- UD South-Slavic
--- UD Asian
some non-used parameter to allow print
get db filename from embedding name
if embedding database already exists
"otherwise, push embedding to database"
init dictionaries
"in order to deal with unknown tokens, add <unk>"
increment for last token in sentence if not followed by whitespace
determine offsets for whitespace_after field
determine offsets for whitespace_after field
"if text is passed, instantiate sentence with tokens (words)"
log a warning if the dataset is empty
set token idx if not set
non-set tags are OUT tags
anything that is not a BIOES tag is a SINGLE tag
anything that is not OUT is IN
single and begin tags start a new span
remember previous tag
move sentence embeddings to device
move token embeddings to device
clear sentence embeddings
clear token embeddings
infer whitespace after field
No character at the corresponding code point: remove it
find out empty sentence indices
create subset of non-empty sentence indices
Make the tag dictionary
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
get train data
"read in test file if exists, otherwise sample 10% of train data as test dataset"
"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
automatically identify train / test / dev files
get train data
get test data
get dev data
automatically identify train / test / dev files
use test_file to create test split if available
"otherwise, sample test data from train data"
use dev_file to create test split if available
"otherwise, sample dev data from dev data"
cache Feidegger config file
cache Feidegger images
replace image URL with local cached file
automatically identify train / test / dev files
check if dataset is supported
set file names
download and unzip in file structure if necessary
instantiate corpus
cast to list if necessary
cast to list if necessary
"store either Sentence objects in memory, or only file offsets"
"most data sets have the token text in the first column, if not, pass 'text' as column"
determine encoding of text file
variables
different handling of in_memory data than streaming data
"most data sets have the token text in the first column, if not, pass 'text' as column"
test if format is OK
test if at least one label given
"first, check if pymongo is installed"
append Sentence-Image data point
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
check if data there
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
default dataset folder is the cache root
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
download data if necessary
unpack and write out in CoNLL column-like format
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"in certain cases, multi-CPU data loading makes no sense and slows"
"everything down. For this reason, we detect if a dataset is in-memory:"
"if so, num_workers is set to 0 for faster processing"
global variable: cache_root
global variable: device
global variable: embedding_storage_mode
# dummy return to fulfill trainer.train() needs
if memory mode option 'none' delete everything
else delete only dynamic embeddings (otherwise autograd will keep everything in memory)
find out which ones are dynamic embeddings
find out which ones are dynamic embeddings
memory management - option 1: send everything to CPU (pin to memory if we train on GPU)
record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class)
header for 'weights.txt'
"determine the column index of loss, f-score and accuracy for train, dev and test split"
then get all relevant values from the tsv
then get all relevant values from the tsv
print(rows)
"figsize = (16, 16)"
plot i
save plots
save plots
plt.show()
save plot
take the average over the last three scores of training
take average over the scores from the different training runs
remove previous embeddings
clearing token embeddings to save memory
#TODO: not saving lines yet
== similarity measures ==
helper class for ModelSimilarity
-- works with binary cross entropy loss --
"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}"
-- works with ranking/triplet loss --
normalize the embeddings
== similarity losses ==
"we want that logits for corresponding pairs are high, and for non-corresponding low"
TODO: this assumes eye matrix
"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa"
== similarity learner ==
"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both"
assumes that for each data pair there's at least one embedding per modality
pre-compute embeddings for all targets in evaluation dataset
compute the similarity
sort the similarity matrix across modality 1
"get the ranks, so +1 to start counting ranks from 1"
The conversion from old model's constructor interface
auto-spawn on GPU if available
pad strings with whitespaces to longest sentence
cut up the input into chunks of max charlength = chunk_size
push each chunk through the RNN language model
concatenate all chunks to make final output
initial hidden state
get predicted weights
divide by temperature
"to prevent overflow problem with small temperature values, substract largest value from all"
this makes a vector in which the largest value is 0
compute word weights with exponential function
try sampling multinomial distribution for next character
print(word_idx)
input ids
push list of character IDs through model
the target is always the next character
use cross entropy loss to compare output of forward pass with targets
exponentiate cross-entropy loss to calculate perplexity
fixed RNN change format for torch 1.4.0
set the dictionaries
Initialize the weight tensor
initialize the network architecture
dropouts
bidirectional LSTM on top of embedding layer
Create initial hidden state and initialize it
TODO: Decide how to initialize the hidden state variables
self.hs_initializer(self.lstm_init_h)
self.hs_initializer(self.lstm_init_c)
final linear map to tag space
reverse sort all sequences by their length
remove previous embeddings
progress bar for verbosity
stop if all sentences are empty
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
clearing token embeddings to save memory
append both to file for evaluation
make list of gold tags
make list of predicted tags
"check for true positives, false positives and false negatives"
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
"if initial hidden state is trainable, use this state"
word dropout only before LSTM - TODO: more experimentation needed
if self.use_word_dropout > 0.0:
sentence_tensor = self.word_dropout(sentence_tensor)
get the tags in this sentence
add tags as tensor
pad tags if using batch-CRF decoder
reduce raw values to avoid NaN during exp
broadcasting will do the job of reshaping and is more efficient than calling repeat
default value
Initialize the weight tensor
auto-spawn on GPU if available
reverse sort all sequences by their length
remove previous embeddings
progress bar for verbosity
stop if all sentences are empty
clearing token embeddings to save memory
cast string to Path
"determine what splits (train, dev, test) to evaluate and log"
prepare loss logging file and set up header
"minimize training loss if training with dev data, else maximize dev score"
"if training also uses dev data, include in training set"
initialize sampler if provided
init with default values if only class is provided
set dataset to sample from
At any point you can hit Ctrl + C to break out of training early.
get new learning rate
reload last best model if annealing with restarts is enabled
stop training if learning rate becomes too small
process mini-batches
zero the gradients on the model and optimizer
"if necessary, make batch_steps"
forward and backward for batch
forward pass
Backward
do the optimizer step
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
"anneal against train loss if training with dev, otherwise anneal against dev score"
evaluate on train / dev / test split depending on training settings
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
calculate scores using dev data if available
append dev score to score history
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
determine learning rate annealing through scheduler
determine bad epoch number
log bad epochs
output log file
make headers on first epoch
"if checkpoint is enabled, save model at each epoch"
"if we use dev data, remember best model based on dev evaluation score"
"if we do not use dev data for model selection, save final model"
test best model if test data is present
"if we are training over multiple datasets, do evaluation for each"
get and return the final test score of best model
cast string to Path
forward pass
update optimizer and scheduler
Add chars to the dictionary
charsplit file content
charsplit file content
Add words to the dictionary
Tokenize file content
"TextDataset returns a list. valid and test are only one file, so return the first element"
cast string to Path
error message if the validation dataset is too small
Shuffle training files randomly after serially iterating through corpus one
"iterate through training data, starting at self.split (for checkpointing)"
off by one for printing
go into train mode
reset variables
not really sure what this does
do the forward pass in the model
try to predict the targets
Backward
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
We detach the hidden state from how it was previously produced.
"If we didn't, the model would try backpropagating all the way to start of the dataset."
explicitly remove loss to clear up memory
##############################################################################
Save the model if the validation loss is the best we've seen so far.
##############################################################################
print info
##############################################################################
##############################################################################
final testing
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
skip because it is optional https://github.com/flairNLP/flair/pull/1296
def test_create_sentence_using_japanese_tokenizer():
"sentence: Sentence = Sentence("""", use_tokenizer=build_japanese_tokenizer())"
""
assert 5 == len(sentence.tokens)
"assert """" == sentence.tokens[0].text"
"assert """" == sentence.tokens[1].text"
"assert """" == sentence.tokens[2].text"
"assert """" == sentence.tokens[3].text"
"assert """" == sentence.tokens[4].text"
clean up file
bioes tags
bio tags
broken tags
all tags
all weird tags
tags with confidence
bioes tags
bioes tags
clean up directory
clean up directory
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
get two corpora as one
"get training, test and dev data for full English UD corpus from web"
clean up data directory
def test_multiclass_metrics():
""
"metric = Metric(""Test"")"
"available_labels = [""A"", ""B"", ""C""]"
""
"predictions = [""A"", ""B""]"
"true_values = [""A""]"
TextClassifier._evaluate_sentence_for_text_classification(
"metric, available_labels, predictions, true_values"
)
""
"predictions = [""C"", ""B""]"
"true_values = [""A"", ""B""]"
TextClassifier._evaluate_sentence_for_text_classification(
"metric, available_labels, predictions, true_values"
)
""
print(metric)
from flair.trainers.trainer_regression import RegressorTrainer
def test_trainer_results(tasks_base_path):
"corpus, model, trainer = init(tasks_base_path)"
"results = trainer.train(""regression_train/"", max_epochs=1)"
"assert results[""test_score""] > 0"
"assert len(results[""dev_loss_history""]) == 1"
"assert len(results[""dev_score_history""]) == 1"
"assert len(results[""train_loss_history""]) == 1"
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
initialize trainer
clean up results directory
document_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(
"[flair_embeddings], 128, 1, False"
)
clean up results directory
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
clean up results directory
get default dictionary
get the example corpus and process at character level in forward direction
define search space
sequence tagger parameter
model trainer parameter
training parameter
find best parameter settings
clean up results directory
document embeddings parameter
training parameter
clean up results directory
0           1      2       3        4         5       6      7      8       9      10     11     12     13    14      15
""
"'<s>',      'Ber', 'lin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'pupp', 'ete', 'er', 'to', 'see', '.',  '</s>'"
\     /       |        |         |       |      |      |         \      |      /     |      |      |
Berlin      and    Munich     have      a     lot     of           puppeteer        to    see     .
""
0          1        2         3       4      5       6               7             8     9      10
First subword embedding
Last subword embedding
First token is splitted into two subwords.
"As we use ""last"" as pooling operation, we consider the last subword as ""first token"" here"
First and last subword embedding
Mean of all subword embeddings
Check embedding dimension when using multiple layers
Check embedding dimension when using multiple layers and scalar mix
0             1           2            3          4         5         6        7       8       9        10        11         12
""
"'berlin</w>', 'and</w>', 'munich</w>', 'have</w>', 'a</w>', 'lot</w>', 'of</w>', 'pupp', 'ete', 'er</w>', 'to</w>', 'see</w>', '.</w>'"
|             |           |            |          |         |         |         \      |      /          |         |          |
Berlin         and        Munich        have        a        lot        of           puppeteer             to       see         .
""
0             1           2            3          4         5         6                7                  8        9          10
First subword embedding
Last subword embedding
First and last subword embedding
Mean of all subword embeddings
Check embedding dimension when using multiple layers
Check embedding dimension when using multiple layers and scalar mix
0           1      2       3        4         5       6      7      8       9      10     11     12     13    14          15
""
"'<|endoftext|>', 'Ber', 'lin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'pupp', 'ete', 'er', 'to', 'see', '.', '<|endoftext|>'"
\     /       |        |         |       |      |      |         \      |      /     |      |      |
Berlin      and    Munich     have      a     lot     of           puppeteer        to    see     .
""
0          1        2         3       4      5       6               7             8     9      10
First subword embedding
Last subword embedding
First token is splitted into two subwords.
"As we use ""last"" as pooling operation, we consider the last subword as ""first token"" here"
First and last subword embedding
Mean of all subword embeddings
Check embedding dimension when using multiple layers
Check embedding dimension when using multiple layers and scalar mix
0        1         2         3         4      5      6      7        8        9      10      11   12   13     14
""
"'<s>', 'Berlin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'puppet', 'eer', 'to', 'see', '', '.', '</s>'"
|          |         |         |      |      |      |         \      /       |       |     \    /
Berlin      and     Munich     have     a     lot     of       puppeteer       to     see       .
""
0          1         2         3      4      5       6           7           8       9        10
First subword embedding
Last subword embedding
First and last subword embedding
Mean of all subword embeddings
Check embedding dimension when using multiple layers
Check embedding dimension when using multiple layers and scalar mix
0       1        2        3     4     5      6        7        8      9     10     11
""
"'Berlin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'puppeteer', 'to', 'see', '.', '<eos>'"
|       |        |        |     |     |      |        |        |      |     |
Berlin    and    Munich    have    a    lot    of    puppeteer    to    see    .
""
0       1        2        3     4     5      6        7        8      9     10
Check embedding dimension when using multiple layers
Check embedding dimension when using multiple layers and scalar mix
0      1             2           3            4          5         6         7         8       9      10        11       12         13        14
""
"<s>  'berlin</w>', 'and</w>', 'munich</w>', 'have</w>', 'a</w>', 'lot</w>', 'of</w>', 'pupp', 'ete', 'er</w>', 'to</w>', 'see</w>', '.</w>', '</s>"
|             |           |            |          |         |         |         \      |      /          |         |          |
Berlin         and        Munich        have        a        lot        of           puppeteer             to       see         .
""
0             1           2            3          4         5          6               7                  8        9          10
First subword embedding
Last subword embedding
First and last subword embedding
Mean of all subword embeddings
Check embedding dimension when using multiple layers
Check embedding dimension when using multiple layers and scalar mix
0       1          2         3         4      5       6       7    8      9
""
"'<s>',   'J',      ""'"",     'aime',   'le', 'ca', 'member', 't', '!', '</s>'"
\          |         /         |      \       |        /   |
J'aime                le         camembert        !
""
0                   1              2            3
First subword embedding
Last subword embedding
First and last subword embedding
Mean of all subword embeddings
Check embedding dimension when using multiple layers
Check embedding dimension when using multiple layers and scalar mix
"get training, test and dev data"
1. get the corpus
2. what tag do we want to predict?
3. make the tag dictionary from the corpus
initialize embeddings
comment in this line to use character embeddings
"CharacterEmbeddings(),"
comment in these lines to use contextual string embeddings
""
"FlairEmbeddings('news-forward'),"
""
"FlairEmbeddings('news-backward'),"
initialize sequence tagger
initialize trainer
"if only one sentence is passed, convert to list of sentence"
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
GLOVE embeddings
TURIAN embeddings
KOMNINOS embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
twitter embeddings
two-letter language code wiki embeddings
two-letter language code wiki embeddings
two-letter language code crawl embeddings
fix serialized models
max_tokens = 500
model architecture
download if necessary
load the model
empty words get no embedding
all other words get embedded
"the default model for ELMo is the 'original' model, which is very large"
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
put on Cuda if available
embed a dummy sentence to determine embedding_length
embed a dummy sentence to determine embedding_length
Avoid conflicts with flair's Token class
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
multilingual models
English models
Arabic
Bulgarian
Czech
Danish
German
Spanish
Basque
Persian
Finnish
French
Hebrew
Hindi
Croatian
Indonesian
Italian
Japanese
Dutch
Norwegian
Polish
Portuguese
Pubmed
Slovenian
Swedish
Tamil
load model if in pretrained model map
embeddings are static if we don't do finetuning
embed a dummy sentence to determine embedding_length
set to eval mode
make compatible with serialized models (TODO: remove)
gradients are enable if fine-tuning is enabled
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
only clone if optimization mode is 'gpu'
use the character language model embeddings as basis
length is twice the original character LM embedding length
these fields are for the embedding memory
whether to add only capitalized words to memory (faster runtime and lower memory consumption)
we re-compute embeddings dynamically at each epoch
set the memory method
memory is wiped each time we do a training run
"if we keep a pooling, it needs to be updated continuously"
update embedding
check token.text is empty or not
add embeddings after updating
The mask has 1 for real tokens and 0 for padding tokens. Only real
tokens are attended to.
Zero-pad up to the sequence length.
"first, find longest sentence in batch"
prepare id maps for BERT model
put encoded batch through BERT model to get all hidden states of all encoder layers
get aggregated embeddings for each BERT-subtoken in sentence
get the current sentence object
add concatenated embedding to sentence
use first subword embedding if pooling operation is 'first'
"otherwise, do a mean over all subwords in token"
"multilingual forward (English, German, French, Italian, Dutch, Polish)"
"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
Dutch forward
Dutch backward
Swedish forward
Swedish backward
French forward
French backward
Czech forward
Czech backward
Portuguese forward
Portuguese backward
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
"if only one sentence is passed, convert to list of sentence"
optional fine-tuning on top of embedding layer
"if only one sentence is passed, convert to list of sentence"
bidirectional RNN on top of embedding layer
dropouts
TODO: remove in future versions
embed words in the sentence
initialize zero-padded word embeddings tensor
sentence_tensor = torch.zeros(
[
"len(sentences),"
"longest_token_sequence_in_batch,"
"self.embeddings.embedding_length,"
"],"
"dtype=torch.float,"
"device=flair.device,"
)
""
"for s_id, sentence in enumerate(sentences):"
# fill values with word embeddings
all_embs = list()
""
"for index_token, token in enumerate(sentence):"
embs = token.get_each_embedding()
if not all_embs:
all_embs = [list() for _ in range(len(embs))]
"for index_emb, emb in enumerate(embs):"
all_embs[index_emb].append(emb)
""
concat_word_emb = [torch.stack(embs) for embs in all_embs]
"concat_sentence_emb = torch.cat(concat_word_emb, dim=1)"
sentence_tensor[s_id][: len(sentence)] = concat_sentence_emb
before-RNN dropout
reproject if set
push through RNN
after-RNN dropout
extract embeddings from RNN
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
IMPORTANT: add embeddings as torch modules
iterate over sentences
"if its a forward LM, take last state"
GLOVE embeddings
"<cls> token initially set to 1/D, so it attends to all image features equally"
add positional encodings
reshape the pixels into the sequence
layer norm after convolution and positional encodings
add <cls> token
"transformer requires input in the shape [h*w+1, b, d]"
the output is an embedding of <cls> token
"TODO: keep for backwards compatibility, but remove in future"
save the sentence piece model as binary file (not as path which may change)
write out the binary sentence piece model into the expected directory
"if the model was saved as binary and it is not found on disk, write to appropriate path"
"otherwise, use normal process and potentially trigger another download"
"once the modes if there, load it with sentence piece"
from allennlp.common.tqdm import Tqdm
mmap seems to be much more memory efficient
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
Extract all the contents of zip file in current directory
get cache path to put the file
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
first determine the distribution of classes in the dataset
weight for each sample
Create blocks
shuffle the blocks
concatenate the shuffled blocks
Create blocks
shuffle the blocks
concatenate the shuffled blocks
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
see https://github.com/zalandoresearch/flair/issues/351
State initialization
Exponential moving average of gradient values
Exponential moving average of squared gradient values
Maintains max of all exp. moving avg. of sq. grad. values
Decay the first and second moment running average coefficient
Maintains the maximum of all 2nd moment running avg. till now
Use the max. for normalizing running avg. of gradient
conll 2000 column format
conll 03 NER column format
WNUT-17
-- WikiNER datasets
-- Universal Dependencies
Germanic
Romance
West-Slavic
South-Slavic
East-Slavic
Scandinavian
Asian
Language isolates
recent Universal Dependencies
other datasets
text classification format
text regression format
"first, try to fetch dataset online"
default dataset folder is the cache root
get string value if enum is passed
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
the CoNLL 03 task for German has an additional lemma column
the CoNLL 03 task for Dutch has no NP column
the CoNLL 03 task for Spanish only has two columns
the GERMEVAL task only has two columns: text and ner
WSD tasks may be put into this column format
"the UD corpora follow the CoNLL-U format, for which we have a special reader"
"for text classifiers, we use our own special format"
NER corpus for Basque
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
get train and test data
"read in test file if exists, otherwise sample 10% of train data as test dataset"
"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
convert tag scheme to iobes
automatically identify train / test / dev files
automatically identify train / test / dev files
"most data sets have the token text in the first column, if not, pass 'text' as column"
conll 2000 chunking task
Support both TREC-6 and TREC-50
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
Wikiner NER task
unpack and write out in CoNLL column-like format
CoNLL 02/03 NER
universal dependencies
--- UD Germanic
--- UD Romance
--- UD West-Slavic
--- UD Scandinavian
--- UD South-Slavic
--- UD Asian
init dictionaries
"in order to deal with unknown tokens, add <unk>"
increment for last token in sentence if not followed by whitespace
determine offsets for whitespace_after field
determine offsets for whitespace_after field
"if text is passed, instantiate sentence with tokens (words)"
log a warning if the dataset is empty
set token idx if not set
non-set tags are OUT tags
anything that is not a BIOES tag is a SINGLE tag
anything that is not OUT is IN
single and begin tags start a new span
remember previous tag
move sentence embeddings to device
move token embeddings to device
clear sentence embeddings
clear token embeddings
infer whitespace after field
No character at the corresponding code point: remove it
find out empty sentence indices
create subset of non-empty sentence indices
Make the tag dictionary
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
get train data
"read in test file if exists, otherwise sample 10% of train data as test dataset"
"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
automatically identify train / test / dev files
get train data
get test data
get dev data
automatically identify train / test / dev files
use test_file to create test split if available
"otherwise, sample test data from train data"
use dev_file to create test split if available
"otherwise, sample dev data from dev data"
cache Feidegger config file
cache Feidegger images
replace image URL with local cached file
automatically identify train / test / dev files
check if dataset is supported
set file names
download and unzip in file structure if necessary
instantiate corpus
cast to list if necessary
cast to list if necessary
"store either Sentence objects in memory, or only file offsets"
"most data sets have the token text in the first column, if not, pass 'text' as column"
determine encoding of text file
variables
different handling of in_memory data than streaming data
"most data sets have the token text in the first column, if not, pass 'text' as column"
test if format is OK
test if at least one label given
append Sentence-Image data point
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
check if data there
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
default dataset folder is the cache root
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
download data if necessary
unpack and write out in CoNLL column-like format
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"in certain cases, multi-CPU data loading makes no sense and slows"
"everything down. For this reason, we detect if a dataset is in-memory:"
"if so, num_workers is set to 0 for faster processing"
global variable: cache_root
global variable: device
global variable: embedding_storage_mode
# dummy return to fulfill trainer.train() needs
if memory mode option 'none' delete everything
else delete only dynamic embeddings (otherwise autograd will keep everything in memory)
find out which ones are dynamic embeddings
find out which ones are dynamic embeddings
memory management - option 1: send everything to CPU (pin to memory if we train on GPU)
record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class)
to enable %matplotlib inline if running in ipynb
change from Agg to TkAgg for interative mode
change from Agg to TkAgg for interative mode
header for 'weights.txt'
"determine the column index of loss, f-score and accuracy for train, dev and test split"
then get all relevant values from the tsv
then get all relevant values from the tsv
print(rows)
"figsize = (16, 16)"
plot i
save plots
save plots
plt.show()
save plot
take the average over the last three scores of training
take average over the scores from the different training runs
remove previous embeddings
clearing token embeddings to save memory
#TODO: not saving lines yet
== similarity measures ==
helper class for ModelSimilarity
-- works with binary cross entropy loss --
"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}"
-- works with ranking/triplet loss --
normalize the embeddings
== similarity losses ==
"we want that logits for corresponding pairs are high, and for non-corresponding low"
TODO: this assumes eye matrix
"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa"
== similarity learner ==
"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both"
assumes that for each data pair there's at least one embedding per modality
pre-compute embeddings for all targets in evaluation dataset
compute the similarity
sort the similarity matrix across modality 1
"get the ranks, so +1 to start counting ranks from 1"
The conversion from old model's constructor interface
auto-spawn on GPU if available
pad strings with whitespaces to longest sentence
cut up the input into chunks of max charlength = chunk_size
push each chunk through the RNN language model
concatenate all chunks to make final output
initial hidden state
get predicted weights
divide by temperature
"to prevent overflow problem with small temperature values, substract largest value from all"
this makes a vector in which the largest value is 0
compute word weights with exponential function
try sampling multinomial distribution for next character
print(word_idx)
input ids
push list of character IDs through model
the target is always the next character
use cross entropy loss to compare output of forward pass with targets
exponentiate cross-entropy loss to calculate perplexity
set the dictionaries
initialize the network architecture
dropouts
bidirectional LSTM on top of embedding layer
Create initial hidden state and initialize it
TODO: Decide how to initialize the hidden state variables
self.hs_initializer(self.lstm_init_h)
self.hs_initializer(self.lstm_init_c)
final linear map to tag space
reverse sort all sequences by their length
remove previous embeddings
progress bar for verbosity
stop if all sentences are empty
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
clearing token embeddings to save memory
append both to file for evaluation
make list of gold tags
make list of predicted tags
"check for true positives, false positives and false negatives"
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
"if initial hidden state is trainable, use this state"
word dropout only before LSTM - TODO: more experimentation needed
if self.use_word_dropout > 0.0:
sentence_tensor = self.word_dropout(sentence_tensor)
get the tags in this sentence
add tags as tensor
pad tags if using batch-CRF decoder
reduce raw values to avoid NaN during exp
broadcasting will do the job of reshaping and is more efficient than calling repeat
default value
auto-spawn on GPU if available
reverse sort all sequences by their length
remove previous embeddings
progress bar for verbosity
stop if all sentences are empty
clearing token embeddings to save memory
cast string to Path
"determine what splits (train, dev, test) to evaluate and log"
prepare loss logging file and set up header
"minimize training loss if training with dev data, else maximize dev score"
"if training also uses dev data, include in training set"
initialize sampler if provided
init with default values if only class is provided
set dataset to sample from
At any point you can hit Ctrl + C to break out of training early.
get new learning rate
reload last best model if annealing with restarts is enabled
stop training if learning rate becomes too small
process mini-batches
zero the gradients on the model and optimizer
"if necessary, make batch_steps"
forward and backward for batch
forward pass
Backward
do the optimizer step
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
"anneal against train loss if training with dev, otherwise anneal against dev score"
evaluate on train / dev / test split depending on training settings
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
calculate scores using dev data if available
append dev score to score history
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
determine learning rate annealing through scheduler
determine bad epoch number
log bad epochs
output log file
make headers on first epoch
"if checkpoint is enabled, save model at each epoch"
"if we use dev data, remember best model based on dev evaluation score"
"if we do not use dev data for model selection, save final model"
test best model if test data is present
"if we are training over multiple datasets, do evaluation for each"
get and return the final test score of best model
cast string to Path
forward pass
update optimizer and scheduler
Add chars to the dictionary
charsplit file content
charsplit file content
Add words to the dictionary
Tokenize file content
"TextDataset returns a list. valid and test are only one file, so return the first element"
cast string to Path
Shuffle training files randomly after serially iterating through corpus one
"iterate through training data, starting at self.split (for checkpointing)"
off by one for printing
go into train mode
reset variables
not really sure what this does
do the forward pass in the model
try to predict the targets
Backward
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
We detach the hidden state from how it was previously produced.
"If we didn't, the model would try backpropagating all the way to start of the dataset."
explicitly remove loss to clear up memory
##############################################################################
Save the model if the validation loss is the best we've seen so far.
##############################################################################
print info
##############################################################################
##############################################################################
final testing
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
clean up file
bioes tags
bio tags
broken tags
all tags
all weird tags
tags with confidence
bioes tags
bioes tags
clean up directory
clean up directory
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
get two corpora as one
"get training, test and dev data for full English UD corpus from web"
clean up data directory
def test_multiclass_metrics():
""
"metric = Metric(""Test"")"
"available_labels = [""A"", ""B"", ""C""]"
""
"predictions = [""A"", ""B""]"
"true_values = [""A""]"
TextClassifier._evaluate_sentence_for_text_classification(
"metric, available_labels, predictions, true_values"
)
""
"predictions = [""C"", ""B""]"
"true_values = [""A"", ""B""]"
TextClassifier._evaluate_sentence_for_text_classification(
"metric, available_labels, predictions, true_values"
)
""
print(metric)
from flair.trainers.trainer_regression import RegressorTrainer
def test_trainer_results(tasks_base_path):
"corpus, model, trainer = init(tasks_base_path)"
"results = trainer.train(""regression_train/"", max_epochs=1)"
"assert results[""test_score""] > 0"
"assert len(results[""dev_loss_history""]) == 1"
"assert len(results[""dev_score_history""]) == 1"
"assert len(results[""train_loss_history""]) == 1"
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
initialize trainer
clean up results directory
clean up results directory
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
clean up results directory
get default dictionary
get the example corpus and process at character level in forward direction
define search space
sequence tagger parameter
model trainer parameter
training parameter
find best parameter settings
clean up results directory
document embeddings parameter
training parameter
clean up results directory
0           1      2       3        4         5       6      7      8       9      10     11     12     13    14      15
""
"'<s>',      'Ber', 'lin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'pupp', 'ete', 'er', 'to', 'see', '.',  '</s>'"
\     /       |        |         |       |      |      |         \      |      /     |      |      |
Berlin      and    Munich     have      a     lot     of           puppeteer        to    see     .
""
0          1        2         3       4      5       6               7             8     9      10
First subword embedding
Last subword embedding
First token is splitted into two subwords.
"As we use ""last"" as pooling operation, we consider the last subword as ""first token"" here"
First and last subword embedding
Mean of all subword embeddings
Check embedding dimension when using multiple layers
Check embedding dimension when using multiple layers and scalar mix
0             1           2            3          4         5         6        7       8       9        10        11         12
""
"'berlin</w>', 'and</w>', 'munich</w>', 'have</w>', 'a</w>', 'lot</w>', 'of</w>', 'pupp', 'ete', 'er</w>', 'to</w>', 'see</w>', '.</w>'"
|             |           |            |          |         |         |         \      |      /          |         |          |
Berlin         and        Munich        have        a        lot        of           puppeteer             to       see         .
""
0             1           2            3          4         5         6                7                  8        9          10
First subword embedding
Last subword embedding
First and last subword embedding
Mean of all subword embeddings
Check embedding dimension when using multiple layers
Check embedding dimension when using multiple layers and scalar mix
0           1      2       3        4         5       6      7      8       9      10     11     12     13    14          15
""
"'<|endoftext|>', 'Ber', 'lin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'pupp', 'ete', 'er', 'to', 'see', '.', '<|endoftext|>'"
\     /       |        |         |       |      |      |         \      |      /     |      |      |
Berlin      and    Munich     have      a     lot     of           puppeteer        to    see     .
""
0          1        2         3       4      5       6               7             8     9      10
First subword embedding
Last subword embedding
First token is splitted into two subwords.
"As we use ""last"" as pooling operation, we consider the last subword as ""first token"" here"
First and last subword embedding
Mean of all subword embeddings
Check embedding dimension when using multiple layers
Check embedding dimension when using multiple layers and scalar mix
0        1         2         3         4      5      6      7        8        9      10      11   12   13     14
""
"'<s>', 'Berlin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'puppet', 'eer', 'to', 'see', '', '.', '</s>'"
|          |         |         |      |      |      |         \      /       |       |     \    /
Berlin      and     Munich     have     a     lot     of       puppeteer       to     see       .
""
0          1         2         3      4      5       6           7           8       9        10
First subword embedding
Last subword embedding
First and last subword embedding
Mean of all subword embeddings
Check embedding dimension when using multiple layers
Check embedding dimension when using multiple layers and scalar mix
0       1        2        3     4     5      6        7        8      9     10     11
""
"'Berlin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'puppeteer', 'to', 'see', '.', '<eos>'"
|       |        |        |     |     |      |        |        |      |     |
Berlin    and    Munich    have    a    lot    of    puppeteer    to    see    .
""
0       1        2        3     4     5      6        7        8      9     10
Check embedding dimension when using multiple layers
Check embedding dimension when using multiple layers and scalar mix
0      1             2           3            4          5         6         7         8       9      10        11       12         13        14
""
"<s>  'berlin</w>', 'and</w>', 'munich</w>', 'have</w>', 'a</w>', 'lot</w>', 'of</w>', 'pupp', 'ete', 'er</w>', 'to</w>', 'see</w>', '.</w>', '</s>"
|             |           |            |          |         |         |         \      |      /          |         |          |
Berlin         and        Munich        have        a        lot        of           puppeteer             to       see         .
""
0             1           2            3          4         5          6               7                  8        9          10
First subword embedding
Last subword embedding
First and last subword embedding
Mean of all subword embeddings
Check embedding dimension when using multiple layers
Check embedding dimension when using multiple layers and scalar mix
"get training, test and dev data"
1. get the corpus
2. what tag do we want to predict?
3. make the tag dictionary from the corpus
initialize embeddings
comment in this line to use character embeddings
"CharacterEmbeddings(),"
comment in these lines to use contextual string embeddings
""
"FlairEmbeddings('news-forward'),"
""
"FlairEmbeddings('news-backward'),"
initialize sequence tagger
initialize trainer
"if only one sentence is passed, convert to list of sentence"
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
GLOVE embeddings
TURIAN embeddings
KOMNINOS embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
twitter embeddings
two-letter language code wiki embeddings
two-letter language code wiki embeddings
two-letter language code crawl embeddings
fix serialized models
max_tokens = 500
model architecture
save the sentence piece model as binary file (not as path which may change)
write out the binary sentence piece model into the expected directory
"if the model was saved as binary and it is not found on disk, write to appropriate path"
"otherwise, use normal process and potentially trigger another download"
"once the modes if there, load it with sentence piece"
download if necessary
load the model
empty words get no embedding
all other words get embedded
"the default model for ELMo is the 'original' model, which is very large"
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
put on Cuda if available
embed a dummy sentence to determine embedding_length
embed a dummy sentence to determine embedding_length
Avoid conflicts with flair's Token class
Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
multilingual models
English models
Arabic
Bulgarian
Czech
Danish
German
Spanish
Basque
Persian
Finnish
French
Hebrew
Hindi
Croatian
Indonesian
Italian
Japanese
Dutch
Norwegian
Polish
Portuguese
Pubmed
Slovenian
Swedish
Tamil
load model if in pretrained model map
embeddings are static if we don't do finetuning
embed a dummy sentence to determine embedding_length
set to eval mode
make compatible with serialized models (TODO: remove)
gradients are enable if fine-tuning is enabled
"if this is not possible, use LM to generate embedding. First, get text sentences"
pad strings with whitespaces to longest sentence
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
use the character language model embeddings as basis
length is twice the original character LM embedding length
these fields are for the embedding memory
whether to add only capitalized words to memory (faster runtime and lower memory consumption)
we re-compute embeddings dynamically at each epoch
set the memory method
memory is wiped each time we do a training run
"if we keep a pooling, it needs to be updated continuously"
update embedding
add embeddings after updating
The mask has 1 for real tokens and 0 for padding tokens. Only real
tokens are attended to.
Zero-pad up to the sequence length.
"first, find longest sentence in batch"
prepare id maps for BERT model
put encoded batch through BERT model to get all hidden states of all encoder layers
get aggregated embeddings for each BERT-subtoken in sentence
get the current sentence object
add concatenated embedding to sentence
use first subword embedding if pooling operation is 'first'
"otherwise, do a mean over all subwords in token"
"multilingual forward (English, German, French, Italian, Dutch, Polish)"
"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
Dutch forward
Dutch backward
Swedish forward
Swedish backward
French forward
French backward
Czech forward
Czech backward
Portuguese forward
Portuguese backward
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
pad strings with whitespaces to longest sentence
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
"if only one sentence is passed, convert to list of sentence"
optional fine-tuning on top of embedding layer
"if only one sentence is passed, convert to list of sentence"
bidirectional RNN on top of embedding layer
dropouts
"the permutation that sorts the sentences by length, descending"
the inverse permutation that restores the input order; it's an index tensor therefore LongTensor
sort sentences by number of tokens
all_sentence_tensors = []
initialize zero-padded word embeddings tensor
fill values with word embeddings
TODO: this can only be removed once the implementations of word_dropout and locked_dropout have a batch_first mode
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM RNN
--------------------------------------------------------------------
restore original order of sentences in the batch
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
IMPORTANT: add embeddings as torch modules
iterate over sentences
"if its a forward LM, take last state"
GLOVE embeddings
from allennlp.common.tqdm import Tqdm
mmap seems to be much more memory efficient
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
unpack and write out in CoNLL column-like format
Extract all the contents of zip file in current directory
get cache path to put the file
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
first determine the distribution of classes in the dataset
weight for each sample
Create blocks
shuffle the blocks
concatenate the shuffled blocks
Create blocks
shuffle the blocks
concatenate the shuffled blocks
additional fields for model checkpointing
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
see https://github.com/zalandoresearch/flair/issues/351
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
see https://github.com/zalandoresearch/flair/issues/351
State initialization
Exponential moving average of gradient values
Exponential moving average of squared gradient values
Maintains max of all exp. moving avg. of sq. grad. values
Decay the first and second moment running average coefficient
Maintains the maximum of all 2nd moment running avg. till now
Use the max. for normalizing running avg. of gradient
conll 2000 column format
conll 03 NER column format
WNUT-17
-- WikiNER datasets
-- Universal Dependencies
Germanic
Romance
West-Slavic
South-Slavic
East-Slavic
Scandinavian
Asian
Language isolates
recent Universal Dependencies
other datasets
text classification format
text regression format
"first, try to fetch dataset online"
default dataset folder is the cache root
get string value if enum is passed
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
the CoNLL 03 task for German has an additional lemma column
the CoNLL 03 task for Dutch has no NP column
the CoNLL 03 task for Spanish only has two columns
the GERMEVAL task only has two columns: text and ner
WSD tasks may be put into this column format
"the UD corpora follow the CoNLL-U format, for which we have a special reader"
"for text classifiers, we use our own special format"
NER corpus for Basque
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
get train and test data
"read in test file if exists, otherwise sample 10% of train data as test dataset"
"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
convert tag scheme to iobes
automatically identify train / test / dev files
automatically identify train / test / dev files
"most data sets have the token text in the first column, if not, pass 'text' as column"
conll 2000 chunking task
Support both TREC-6 and TREC-50
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
Wikiner NER task
unpack and write out in CoNLL column-like format
CoNLL 02/03 NER
universal dependencies
--- UD Germanic
--- UD Romance
--- UD West-Slavic
--- UD Scandinavian
--- UD South-Slavic
--- UD Asian
init dictionaries
"in order to deal with unknown tokens, add <unk>"
"if text is passed, instantiate sentence with tokens (words)"
tokenize the text first if option selected
use segtok for tokenization
determine offsets for whitespace_after field
otherwise assumes whitespace tokenized text
add each word in tokenized string as Token object to Sentence
increment for last token in sentence if not followed by whtespace
log a warning if the dataset is empty
set token idx if not set
non-set tags are OUT tags
anything that is not a BIOES tag is a SINGLE tag
anything that is not OUT is IN
single and begin tags start a new span
remember previous tag
move sentence embeddings to device
move token embeddings to device
clear sentence embeddings
clear token embeddings
infer whitespace after field
find out empty sentence indices
create subset of non-empty sentence indices
Make the tag dictionary
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
get train data
"read in test file if exists, otherwise sample 10% of train data as test dataset"
"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
automatically identify train / test / dev files
get train data
get test data
get dev data
automatically identify train / test / dev files
automatically identify train / test / dev files
cast to list if necessary
"store either Sentence objects in memory, or only file offsets"
"most data sets have the token text in the first column, if not, pass 'text' as column"
determine encoding of text file
variables
different handling of in_memory data than streaming data
"most data sets have the token text in the first column, if not, pass 'text' as column"
test if format is OK
test if at least one label given
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
check if data there
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
default dataset folder is the cache root
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
download data if necessary
unpack and write out in CoNLL column-like format
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
"in certain cases, multi-CPU data loading makes no sense and slows"
"everything down. For this reason, we detect if a dataset is in-memory:"
"if so, num_workers is set to 0 for faster processing"
global variable: cache_root
global variable: device
# dummy return to fulfill trainer.train() needs
if memory mode option 'none' delete everything
else delete only dynamic embeddings (otherwise autograd will keep everything in memory)
find out which ones are dynamic embeddings
find out which ones are dynamic embeddings
memory management - option 1: send everything to CPU
to enable %matplotlib inline if running in ipynb
change from Agg to TkAgg for interative mode
change from Agg to TkAgg for interative mode
header for 'weights.txt'
"determine the column index of loss, f-score and accuracy for train, dev and test split"
then get all relevant values from the tsv
then get all relevant values from the tsv
print(rows)
"figsize = (16, 16)"
plot i
save plots
save plots
plt.show()
save plot
take the average over the last three scores of training
take average over the scores from the different training runs
remove previous embeddings
clearing token embeddings to save memory
#TODO: not saving lines yet
auto-spawn on GPU if available
cut up the input into chunks of max charlength = chunk_size
push each chunk through the RNN language model
concatenate all chunks to make final output
initial hidden state
get predicted weights
divide by temperature
"to prevent overflow problem with small temperature values, substract largest value from all"
this makes a vector in which the largest value is 0
compute word weights with exponential function
try sampling multinomial distribution for next character
print(word_idx)
input ids
push list of character IDs through model
the target is always the next character
use cross entropy loss to compare output of forward pass with targets
exponentiate cross-entropy loss to calculate perplexity
set the dictionaries
initialize the network architecture
dropouts
bidirectional LSTM on top of embedding layer
Create initial hidden state and initialize it
TODO: Decide how to initialize the hidden state variables
self.hs_initializer(self.lstm_init_h)
self.hs_initializer(self.lstm_init_c)
final linear map to tag space
append both to file for evaluation
make list of gold tags
make list of predicted tags
"check for true positives, false positives and false negatives"
remove previous embeddings
reverse sort all sequences by their length
make mini-batches
progress bar for verbosity
"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
clearing token embeddings to save memory
initialize zero-padded word embeddings tensor
fill values with word embeddings
TODO: this can only be removed once the implementations of word_dropout and locked_dropout have a batch_first mode
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
"if initial hidden state is trainable, use this state"
word dropout only before LSTM - TODO: more experimentation needed
if self.use_word_dropout > 0.0:
sentence_tensor = self.word_dropout(sentence_tensor)
transpose to batch_first mode
get the tags in this sentence
add tags as tensor
pad tags if using batch-CRF decoder
return all scores if so selected
auto-spawn on GPU if available
remove previous embeddings
clearing token embeddings to save memory
cast string to Path
"determine what splits (train, dev, test) to evaluate and log"
prepare loss logging file and set up header
"minimize training loss if training with dev data, else maximize dev score"
"if training also uses dev data, include in training set"
At any point you can hit Ctrl + C to break out of training early.
get new learning rate
reload last best model if annealing with restarts is enabled
stop training if learning rate becomes too small
process mini-batches
Backward
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
"anneal against train loss if training with dev, otherwise anneal against dev score"
evaluate on train / dev / test split depending on training settings
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
calculate scores using dev data if available
append dev score to score history
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
determine learning rate annealing through scheduler
determine bad epoch number
log bad epochs
output log file
make headers on first epoch
"if checkpoint is enable, save model at each epoch"
"if we use dev data, remember best model based on dev evaluation score"
"if we do not use dev data for model selection, save final model"
test best model if test data is present
"if we are training over multiple datasets, do evaluation for each"
get and return the final test score of best model
cast string to Path
Add chars to the dictionary
charsplit file content
charsplit file content
Add words to the dictionary
Tokenize file content
"TextDataset returns a list. valid and test are only one file, so return the first element"
cast string to Path
Shuffle training files randomly after serially iterating through corpus one
"iterate through training data, starting at self.split (for checkpointing)"
off by one for printing
go into train mode
reset variables
not really sure what this does
do the forward pass in the model
try to predict the targets
Backward
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
We detach the hidden state from how it was previously produced.
"If we didn't, the model would try backpropagating all the way to start of the dataset."
explicitly remove loss to clear up memory
##############################################################################
Save the model if the validation loss is the best we've seen so far.
##############################################################################
print info
##############################################################################
##############################################################################
final testing
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
clean up file
bioes tags
bio tags
broken tags
all tags
all weird tags
tags with confidence
bioes tags
bioes tags
clean up directory
clean up directory
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
get two corpora as one
"get training, test and dev data for full English UD corpus from web"
clean up data directory
def test_multiclass_metrics():
""
"metric = Metric(""Test"")"
"available_labels = [""A"", ""B"", ""C""]"
""
"predictions = [""A"", ""B""]"
"true_values = [""A""]"
TextClassifier._evaluate_sentence_for_text_classification(
"metric, available_labels, predictions, true_values"
)
""
"predictions = [""C"", ""B""]"
"true_values = [""A"", ""B""]"
TextClassifier._evaluate_sentence_for_text_classification(
"metric, available_labels, predictions, true_values"
)
""
print(metric)
from flair.trainers.trainer_regression import RegressorTrainer
def test_trainer_results(tasks_base_path):
"corpus, model, trainer = init(tasks_base_path)"
"results = trainer.train(""regression_train/"", max_epochs=1)"
"assert results[""test_score""] > 0"
"assert len(results[""dev_loss_history""]) == 1"
"assert len(results[""dev_score_history""]) == 1"
"assert len(results[""train_loss_history""]) == 1"
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
initialize trainer
clean up results directory
clean up results directory
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
clean up results directory
get default dictionary
get the example corpus and process at character level in forward direction
define search space
sequence tagger parameter
model trainer parameter
training parameter
find best parameter settings
clean up results directory
document embeddings parameter
training parameter
clean up results directory
0           1      2       3        4         5       6      7      8       9      10     11     12     13    14      15
""
"'<s>',      'Ber', 'lin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'pupp', 'ete', 'er', 'to', 'see', '.',  '</s>'"
\     /       |        |         |       |      |      |         \      |      /     |      |      |
Berlin      and    Munich     have      a     lot     of           puppeteer        to    see     .
""
0          1        2         3       4      5       6               7             8     9      10
First subword embedding
Last subword embedding
First token is splitted into two subwords.
"As we use ""last"" as pooling operation, we consider the last subword as ""first token"" here"
First and last subword embedding
Mean of all subword embeddings
Check embedding dimension when using multiple layers
Check embedding dimension when using multiple layers and scalar mix
0             1           2            3          4         5         6        7       8       9        10        11         12
""
"'berlin</w>', 'and</w>', 'munich</w>', 'have</w>', 'a</w>', 'lot</w>', 'of</w>', 'pupp', 'ete', 'er</w>', 'to</w>', 'see</w>', '.</w>'"
|             |           |            |          |         |         |         \      |      /          |         |          |
Berlin         and        Munich        have        a        lot        of           puppeteer             to       see         .
""
0             1           2            3          4         5         6                7                  8        9          10
First subword embedding
Last subword embedding
First and last subword embedding
Mean of all subword embeddings
Check embedding dimension when using multiple layers
Check embedding dimension when using multiple layers and scalar mix
0           1      2       3        4         5       6      7      8       9      10     11     12     13    14          15
""
"'<|endoftext|>', 'Ber', 'lin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'pupp', 'ete', 'er', 'to', 'see', '.', '<|endoftext|>'"
\     /       |        |         |       |      |      |         \      |      /     |      |      |
Berlin      and    Munich     have      a     lot     of           puppeteer        to    see     .
""
0          1        2         3       4      5       6               7             8     9      10
First subword embedding
Last subword embedding
First token is splitted into two subwords.
"As we use ""last"" as pooling operation, we consider the last subword as ""first token"" here"
First and last subword embedding
Mean of all subword embeddings
Check embedding dimension when using multiple layers
Check embedding dimension when using multiple layers and scalar mix
0        1         2         3         4      5      6      7        8        9      10      11   12   13     14
""
"'<s>', 'Berlin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'puppet', 'eer', 'to', 'see', '', '.', '</s>'"
|          |         |         |      |      |      |         \      /       |       |     \    /
Berlin      and     Munich     have     a     lot     of       puppeteer       to     see       .
""
0          1         2         3      4      5       6           7           8       9        10
First subword embedding
Last subword embedding
First and last subword embedding
Mean of all subword embeddings
Check embedding dimension when using multiple layers
Check embedding dimension when using multiple layers and scalar mix
0       1        2        3     4     5      6        7        8      9     10     11
""
"'Berlin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'puppeteer', 'to', 'see', '.', '<eos>'"
|       |        |        |     |     |      |        |        |      |     |
Berlin    and    Munich    have    a    lot    of    puppeteer    to    see    .
""
0       1        2        3     4     5      6        7        8      9     10
Check embedding dimension when using multiple layers
Check embedding dimension when using multiple layers and scalar mix
0      1             2           3            4          5         6         7         8       9      10        11       12         13        14
""
"<s>  'berlin</w>', 'and</w>', 'munich</w>', 'have</w>', 'a</w>', 'lot</w>', 'of</w>', 'pupp', 'ete', 'er</w>', 'to</w>', 'see</w>', '.</w>', '</s>"
|             |           |            |          |         |         |         \      |      /          |         |          |
Berlin         and        Munich        have        a        lot        of           puppeteer             to       see         .
""
0             1           2            3          4         5          6               7                  8        9          10
First subword embedding
Last subword embedding
First and last subword embedding
Mean of all subword embeddings
Check embedding dimension when using multiple layers
Check embedding dimension when using multiple layers and scalar mix
"get training, test and dev data"
1. get the corpus
2. what tag do we want to predict?
3. make the tag dictionary from the corpus
initialize embeddings
comment in this line to use character embeddings
"CharacterEmbeddings(),"
comment in these lines to use contextual string embeddings
""
"FlairEmbeddings('news-forward'),"
""
"FlairEmbeddings('news-backward'),"
initialize sequence tagger
initialize trainer
"if only one sentence is passed, convert to list of sentence"
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
GLOVE embeddings
TURIAN embeddings
KOMNINOS embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
twitter embeddings
two-letter language code wiki embeddings
two-letter language code wiki embeddings
two-letter language code crawl embeddings
max_tokens = 500
model architecture
save the sentence piece model as binary file (not as path which may change)
write out the binary sentence piece model into the expected directory
"if the model was saved as binary and it is not found on disk, write to appropriate path"
"otherwise, use normal process and potentially trigger another download"
"once the modes if there, load it with sentence piece"
empty words get no embedding
all other words get embedded
"the default model for ELMo is the 'original' model, which is very large"
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
put on Cuda if available
embed a dummy sentence to determine embedding_length
embed a dummy sentence to determine embedding_length
Avoid conflicts with flair's Token class
Use embedding of first subword
Use embedding of first and last subword
"Otherwise, use mean over all subwords in token"
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
multilingual models
English models
Arabic
Bulgarian
Czech
Danish
German
Spanish
Basque
Farsi
Finnish
French
Hebrew
Hindi
Croatian
Indonesian
Italian
Japanese
Dutch
Norwegian
Polish
Portuguese
Pubmed
Slovenian
Swedish
load model if in pretrained model map
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
make compatible with serialized models
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
pad strings with whitespaces to longest sentence
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
use the character language model embeddings as basis
length is twice the original character LM embedding length
these fields are for the embedding memory
whether to add only capitalized words to memory (faster runtime and lower memory consumption)
we re-compute embeddings dynamically at each epoch
set the memory method
memory is wiped each time we do a training run
"if we keep a pooling, it needs to be updated continuously"
update embedding
add embeddings after updating
The mask has 1 for real tokens and 0 for padding tokens. Only real
tokens are attended to.
Zero-pad up to the sequence length.
"first, find longest sentence in batch"
prepare id maps for BERT model
put encoded batch through BERT model to get all hidden states of all encoder layers
get aggregated embeddings for each BERT-subtoken in sentence
get the current sentence object
add concatenated embedding to sentence
use first subword embedding if pooling operation is 'first'
"otherwise, do a mean over all subwords in token"
"multilingual forward (English, German, French, Italian, Dutch, Polish)"
"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
Dutch forward
Dutch backward
Swedish forward
Swedish backward
French forward
French backward
Czech forward
Czech backward
Portuguese forward
Portuguese backward
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
pad strings with whitespaces to longest sentence
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
"if only one sentence is passed, convert to list of sentence"
optional fine-tuning on top of embedding layer
"if only one sentence is passed, convert to list of sentence"
bidirectional RNN on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM RNN
--------------------------------------------------------------------
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
iterate over sentences
"if its a forward LM, take last state"
GLOVE embeddings
from allennlp.common.tqdm import Tqdm
mmap seems to be much more memory efficient
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
unpack and write out in CoNLL column-like format
Extract all the contents of zip file in current directory
get cache path to put the file
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
additional fields for model checkpointing
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
see https://github.com/zalandoresearch/flair/issues/351
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
see https://github.com/zalandoresearch/flair/issues/351
State initialization
Exponential moving average of gradient values
Exponential moving average of squared gradient values
Maintains max of all exp. moving avg. of sq. grad. values
Decay the first and second moment running average coefficient
Maintains the maximum of all 2nd moment running avg. till now
Use the max. for normalizing running avg. of gradient
conll 2000 column format
conll 03 NER column format
WNUT-17
-- WikiNER datasets
-- Universal Dependencies
Germanic
Romance
West-Slavic
South-Slavic
East-Slavic
Scandinavian
Asian
Language isolates
recent Universal Dependencies
other datasets
text classification format
text regression format
"first, try to fetch dataset online"
default dataset folder is the cache root
get string value if enum is passed
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
the CoNLL 03 task for German has an additional lemma column
the CoNLL 03 task for Dutch has no NP column
the CoNLL 03 task for Spanish only has two columns
the GERMEVAL task only has two columns: text and ner
WSD tasks may be put into this column format
"the UD corpora follow the CoNLL-U format, for which we have a special reader"
"for text classifiers, we use our own special format"
NER corpus for Basque
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
get train and test data
"read in test file if exists, otherwise sample 10% of train data as test dataset"
"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
convert tag scheme to iobes
automatically identify train / test / dev files
automatically identify train / test / dev files
"most data sets have the token text in the first column, if not, pass 'text' as column"
conll 2000 chunking task
Support both TREC-6 and TREC-50
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
Wikiner NER task
unpack and write out in CoNLL column-like format
CoNLL 02/03 NER
universal dependencies
--- UD Germanic
--- UD Romance
--- UD West-Slavic
--- UD Scandinavian
--- UD South-Slavic
--- UD Asian
init dictionaries
"in order to deal with unknown tokens, add <unk>"
"if text is passed, instantiate sentence with tokens (words)"
tokenize the text first if option selected
use segtok for tokenization
determine offsets for whitespace_after field
otherwise assumes whitespace tokenized text
add each word in tokenized string as Token object to Sentence
increment for last token in sentence if not followed by whtespace
log a warning if the dataset is empty
set token idx if not set
non-set tags are OUT tags
anything that is not a BIOES tag is a SINGLE tag
anything that is not OUT is IN
single and begin tags start a new span
remember previous tag
infer whitespace after field
Make the tag dictionary
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
get train data
"read in test file if exists, otherwise sample 10% of train data as test dataset"
"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
automatically identify train / test / dev files
get train data
get test data
get dev data
automatically identify train / test / dev files
"store either Sentence objects in memory, or only file offsets"
"most data sets have the token text in the first column, if not, pass 'text' as column"
determine encoding of text file
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
check if data there
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
check if data there
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
default dataset folder is the cache root
download data if necessary
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
this dataset name
default dataset folder is the cache root
download data if necessary
download data if necessary
unpack and write out in CoNLL column-like format
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
column format
this dataset name
default dataset folder is the cache root
download data if necessary
global variable: cache_root
global variable: device
# dummy return to fulfill trainer.train() needs
header for 'weights.txt'
"determine the column index of loss, f-score and accuracy for train, dev and test split"
then get all relevant values from the tsv
then get all relevant values from the tsv
plot i
save plots
save plots
save plot
take the average over the last three scores of training
take average over the scores from the different training runs
#TODO: not saving lines yet
auto-spawn on GPU if available
cut up the input into chunks of max charlength = chunk_size
push each chunk through the RNN language model
concatenate all chunks to make final output
initial hidden state
get predicted weights
divide by temperature
"to prevent overflow problem with small temperature values, substract largest value from all"
this makes a vector in which the largest value is 0
compute word weights with exponential function
try sampling multinomial distribution for next character
print(word_idx)
input ids
push list of character IDs through model
the target is always the next character
use cross entropy loss to compare output of forward pass with targets
exponentiate cross-entropy loss to calculate perplexity
set the dictionaries
initialize the network architecture
dropouts
bidirectional LSTM on top of embedding layer
final linear map to tag space
append both to file for evaluation
make list of gold tags
make list of predicted tags
"check for true positives, false positives and false negatives"
remove previous embeddings
revere sort all sequences by their length
make mini-batches
progress bar for verbosity
clearing token embeddings to save memory
initialize zero-padded word embeddings tensor
fill values with word embeddings
get the tags in this sentence
add tags as tensor
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
word dropout only before LSTM - TODO: more experimentation needed
if self.use_word_dropout > 0.0:
sentence_tensor = self.word_dropout(sentence_tensor)
get the tags in this sentence
add tags as tensor
pad tags if using batch-CRF decoder
auto-spawn on GPU if available
cast string to Path
"determine what splits (train, dev, test) to evaluate and log"
"minimize training loss if training with dev data, else maximize dev score"
"if training also uses dev data, include in training set"
At any point you can hit Ctrl + C to break out of training early.
reload last best model if annealing with restarts is enabled
stop training if learning rate becomes too small
"anneal against train loss if training with dev, otherwise anneal against dev score"
calculate scores using dev data if available
append dev score to score history
"if checkpoint is enable, save model at each epoch"
"if we use dev data, remember best model based on dev evaluation score"
"if we do not use dev data for model selection, save final model"
test best model if test data is present
"if we are training over multiple datasets, do evaluation for each"
get and return the final test score of best model
cast string to Path
Add chars to the dictionary
charsplit file content
charsplit file content
Add words to the dictionary
Tokenize file content
"TextDataset returns a list. valid and test are only one file, so return the first element"
cast string to Path
Shuffle training files randomly after serially iterating through corpus one
"iterate through training data, starting at self.split (for checkpointing)"
off by one for printing
go into train mode
reset variables
not really sure what this does
do the forward pass in the model
try to predict the targets
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
We detach the hidden state from how it was previously produced.
"If we didn't, the model would try backpropagating all the way to start of the dataset."
explicitly remove loss to clear up memory
##############################################################################
Save the model if the validation loss is the best we've seen so far.
##############################################################################
print info
##############################################################################
##############################################################################
final testing
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
clean up file
bioes tags
bio tags
broken tags
all tags
all weird tags
tags with confidence
bioes tags
bioes tags
clean up directory
clean up directory
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
get two corpora as one
"get training, test and dev data for full English UD corpus from web"
clean up data directory
def test_multiclass_metrics():
""
"metric = Metric(""Test"")"
"available_labels = [""A"", ""B"", ""C""]"
""
"predictions = [""A"", ""B""]"
"true_values = [""A""]"
TextClassifier._evaluate_sentence_for_text_classification(
"metric, available_labels, predictions, true_values"
)
""
"predictions = [""C"", ""B""]"
"true_values = [""A"", ""B""]"
TextClassifier._evaluate_sentence_for_text_classification(
"metric, available_labels, predictions, true_values"
)
""
print(metric)
from flair.trainers.trainer_regression import RegressorTrainer
def test_trainer_results(tasks_base_path):
"corpus, model, trainer = init(tasks_base_path)"
"results = trainer.train(""regression_train/"", max_epochs=1)"
"assert results[""test_score""] > 0"
"assert len(results[""dev_loss_history""]) == 1"
"assert len(results[""dev_score_history""]) == 1"
"assert len(results[""train_loss_history""]) == 1"
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
make a temporary cache directory that we remove afterwards
initialize trainer
remove the cache directory
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
initialize trainer
clean up results directory
clean up results directory
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
clean up results directory
get default dictionary
get the example corpus and process at character level in forward direction
define search space
sequence tagger parameter
model trainer parameter
training parameter
find best parameter settings
clean up results directory
document embeddings parameter
training parameter
clean up results directory
"get training, test and dev data"
1. get the corpus
2. what tag do we want to predict?
3. make the tag dictionary from the corpus
initialize embeddings
comment in this line to use character embeddings
"CharacterEmbeddings(),"
comment in these lines to use contextual string embeddings
""
"CharLMEmbeddings('news-forward'),"
""
"CharLMEmbeddings('news-backward'),"
initialize sequence tagger
initialize trainer
"if only one sentence is passed, convert to list of sentence"
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
GLOVE embeddings
TURIAN embeddings
KOMNIOS embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
twitter embeddings
two-letter language code wiki embeddings
two-letter language code wiki embeddings
two-letter language code crawl embeddings
save the sentence piece model as binary file (not as path which may change)
write out the binary sentence piece model into the expected directory
"if the model was saved as binary and it is not found on disk, write to appropriate path"
"otherwise, use normal process and potentially trigger another download"
"once the modes if there, load it with sentence piece"
empty words get no embedding
all other words get embedded
"the default model for ELMo is the 'original' model, which is very large"
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
put on Cuda if available
embed a dummy sentence to determine embedding_length
embed a dummy sentence to determine embedding_length
Avoid conflicts with flair's Token class
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
"multilingual forward (English, German, French, Italian, Dutch, Polish)"
"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
"multilingual forward fast (English, German, French, Italian, Dutch, Polish)"
"multilingual backward fast (English, German, French, Italian, Dutch, Polish)"
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
Dutch forward
Dutch backward
Swedish forward
Swedish backward
French forward
French backward
Czech forward
Czech backward
Portuguese forward
Portuguese backward
Basque forward
Basque backward
Spanish forward fast
Spanish backward fast
Spanish forward
Spanish backward
Pubmed forward
Pubmed backward
Japanese forward
Japanese backward
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
make compatible with serialized models
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
pad strings with whitespaces to longest sentence
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
use the character language model embeddings as basis
length is twice the original character LM embedding length
these fields are for the embedding memory
whether to add only capitalized words to memory (faster runtime and lower memory consumption)
we re-compute embeddings dynamically at each epoch
set the memory method
memory is wiped each time we do a training run
"if we keep a pooling, it needs to be updated continuously"
update embedding
add embeddings after updating
The mask has 1 for real tokens and 0 for padding tokens. Only real
tokens are attended to.
Zero-pad up to the sequence length.
"first, find longest sentence in batch"
prepare id maps for BERT model
put encoded batch through BERT model to get all hidden states of all encoder layers
get aggregated embeddings for each BERT-subtoken in sentence
get the current sentence object
add concatenated embedding to sentence
use first subword embedding if pooling operation is 'first'
"otherwise, do a mean over all subwords in token"
"multilingual forward (English, German, French, Italian, Dutch, Polish)"
"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
Dutch forward
Dutch backward
Swedish forward
Swedish backward
French forward
French backward
Czech forward
Czech backward
Portuguese forward
Portuguese backward
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
pad strings with whitespaces to longest sentence
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
"if only one sentence is passed, convert to list of sentence"
"if only one sentence is passed, convert to list of sentence"
bidirectional RNN on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM RNN
--------------------------------------------------------------------
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
iterate over sentences
"if its a forward LM, take last state"
from allennlp.common.tqdm import Tqdm
mmap seems to be much more memory efficient
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
State initialization
Exponential moving average of gradient values
Exponential moving average of squared gradient values
Maintains max of all exp. moving avg. of sq. grad. values
Decay the first and second moment running average coefficient
Maintains the maximum of all 2nd moment running avg. till now
Use the max. for normalizing running avg. of gradient
conll 2000 column format
conll 03 NER column format
WNUT-17
-- WikiNER datasets
-- Universal Dependencies
Germanic
Romance
West-Slavic
South-Slavic
East-Slavic
Scandinavian
Asian
Language isolates
other datasets
text classification format
"first, try to fetch dataset online"
default dataset folder is the cache root
get string value if enum is passed
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
the CoNLL 03 task for German has an additional lemma column
the CoNLL 03 task for Dutch has no NP column
the CoNLL 03 task for Spanish only has two columns
the GERMEVAL task only has two columns: text and ner
WSD tasks may be put into this column format
"the UD corpora follow the CoNLL-U format, for which we have a special reader"
"for text classifiers, we use our own special format"
NER corpus for Basque
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
get train and test data
"read in test file if exists, otherwise sample 10% of train data as test dataset"
"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
convert tag scheme to iobes
automatically identify train / test / dev files
automatically identify train / test / dev files
"most data sets have the token text in the first column, if not, pass 'text' as column"
conll 2000 chunking task
Support both TREC-6 and TREC-50
Create flair compatible labels
TREC-6 : NUM:dist -> __label__NUM
TREC-50: NUM:dist -> __label__NUM:dist
Wikiner NER task
unpack and write out in CoNLL column-like format
CoNLL 02/03 NER
universal dependencies
--- UD Germanic
--- UD Romance
--- UD West-Slavic
--- UD Scandinavian
--- UD South-Slavic
--- UD Asian
init dictionaries
"in order to deal with unknown tokens, add <unk>"
"if text is passed, instantiate sentence with tokens (words)"
tokenize the text first if option selected
use segtok for tokenization
determine offsets for whitespace_after field
otherwise assumes whitespace tokenized text
add each word in tokenized string as Token object to Sentence
increment for last token in sentence if not followed by whtespace
set token idx if not set
non-set tags are OUT tags
anything that is not a BIOES tag is a SINGLE tag
anything that is not OUT is IN
single and begin tags start a new span
remember previous tag
infer whitespace after field
Make the tag dictionary
Make the tag dictionary
header for 'weights.txt'
"determine the column index of loss, f-score and accuracy for train, dev and test split"
then get all relevant values from the tsv
then get all relevant values from the tsv
plot i
save plots
plot 1
plot 2
plot 3
save plots
save plot
take the average over the last three scores of training
take average over the scores from the different training runs
auto-spawn on GPU if available
cut up the input into chunks of max charlength = chunk_size
push each chunk through the RNN language model
concatenate all chunks to make final output
initial hidden state
get predicted weights
divide by temperature
"to prevent overflow problem with small temperature values, substract largest value from all"
this makes a vector in which the largest value is 0
compute word weights with exponential function
try sampling multinomial distribution for next character
print(word_idx)
input ids
push list of character IDs through model
the target is always the next character
use cross entropy loss to compare output of forward pass with targets
exponentiate cross-entropy loss to calculate perplexity
set the dictionaries
initialize the network architecture
dropouts
bidirectional LSTM on top of embedding layer
final linear map to tag space
ATTENTION: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive
serialization of torch objects
https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
see https://github.com/zalandoresearch/flair/issues/351
remove previous embeddings
revere sort all sequences by their length
make mini-batches
progress bar for verbosity
clearing token embeddings to save memory
"if sorting is enabled, sort sentences by number of tokens"
initialize zero-padded word embeddings tensor
fill values with word embeddings
get the tags in this sentence
add tags as tensor
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
word dropout only before LSTM - TODO: more experimentation needed
if self.use_word_dropout > 0.0:
sentence_tensor = self.word_dropout(sentence_tensor)
pad tags if using batch-CRF decoder
auto-spawn on GPU if available
ATTENTION: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive
serialization of torch objects
https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings
load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
see https://github.com/zalandoresearch/flair/issues/351
cast string to Path
annealing scheduler
"if training also uses dev data, include in training set"
At any point you can hit Ctrl + C to break out of training early.
reload last best model if annealing with restarts is enabled
stop training if learning rate becomes too small
calculate scores using dev data if available
append dev score to score history
"anneal against train loss if training with dev, otherwise anneal against dev score"
"if checkpoint is enable, save model at each epoch"
"if we use dev data, remember best model based on dev evaluation score"
"if we do not use dev data for model selection, save final model"
test best model if test data is present
"if we are training over multiple datasets, do evaluation for each"
get and return the final test score of best model
append both to file for evaluation
make list of gold tags
make list of predicted tags
"check for true positives, false positives and false negatives"
cast string to Path
Add chars to the dictionary
charsplit file content
charsplit file content
Add words to the dictionary
Tokenize file content
"TextDataset returns a list. valid and test are only one file, so return the first element"
cast string to Path
Shuffle training files randomly after serially iterating through corpus one
"iterate through training data, starting at self.split (for checkpointing)"
off by one for printing
go into train mode
reset variables
not really sure what this does
do the forward pass in the model
try to predict the targets
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
We detach the hidden state from how it was previously produced.
"If we didn't, the model would try backpropagating all the way to start of the dataset."
explicitly remove loss to clear up memory
##############################################################################
Save the model if the validation loss is the best we've seen so far.
##############################################################################
print info
##############################################################################
##############################################################################
final testing
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
clean up file
bioes tags
bio tags
broken tags
all tags
all weird tags
tags with confidence
bioes tags
bioes tags
clean up directory
clean up directory
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
get two corpora as one
"get training, test and dev data for full English UD corpus from web"
clean up data directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
make a temporary cache directory that we remove afterwards
initialize trainer
remove the cache directory
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
clean up results directory
"corpus = NLPTaskDataFetcher.load_corpus('multi_class', base_path=tasks_base_path)"
clean up results directory
clean up results directory
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
initialize trainer
clean up results directory
clean up results directory
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
clean up results directory
get default dictionary
get the example corpus and process at character level in forward direction
define search space
sequence tagger parameter
model trainer parameter
training parameter
find best parameter settings
clean up results directory
document embeddings parameter
training parameter
clean up results directory
1. get the corpus
2. what tag do we want to predict?
3. make the tag dictionary from the corpus
initialize embeddings
comment in this line to use character embeddings
"CharacterEmbeddings(),"
comment in these lines to use contextual string embeddings
""
"CharLMEmbeddings('news-forward'),"
""
"CharLMEmbeddings('news-backward'),"
initialize sequence tagger
initialize trainer
"if only one sentence is passed, convert to list of sentence"
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
GLOVE embeddings
KOMNIOS embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
twitter embeddings
two-letter language code wiki embeddings
two-letter language code wiki embeddings
two-letter language code crawl embeddings
"the default model for ELMo is the 'original' model, which is very large"
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
put on Cuda if available
embed a dummy sentence to determine embedding_length
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
"multilingual forward (English, German, French, Italian, Dutch, Polish)"
"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
"multilingual forward fast (English, German, French, Italian, Dutch, Polish)"
"multilingual backward fast (English, German, French, Italian, Dutch, Polish)"
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
Dutch forward
Dutch backward
Swedish forward
Swedish backward
French forward
French backward
Czech forward
Czech backward
Portuguese forward
Portuguese backward
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
pad strings with whitespaces to longest sentence
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
use the character language model embeddings as basis
length is twice the original character LM embedding length
these fields are for the embedding memory
whether to add only capitalized words to memory (faster runtime and lower memory consumption)
we re-compute embeddings dynamically at each epoch
set the memory method
memory is wiped each time we do a training run
"if we keep a pooling, it needs to be updated continuously"
update embedding
add embeddings after updating
"the default model for ELMo is the 'original' model, which is very large"
"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
put on Cuda if available
embed a dummy sentence to determine embedding_length
The mask has 1 for real tokens and 0 for padding tokens. Only real
tokens are attended to.
Zero-pad up to the sequence length.
"first, find longest sentence in batch"
prepare id maps for BERT model
put encoded batch through BERT model to get all hidden states of all encoder layers
get aggregated embeddings for each BERT-subtoken in sentence
get the current sentence object
add concatenated embedding to sentence
use first subword embedding if pooling operation is 'first'
"otherwise, do a mean over all subwords in token"
"multilingual forward (English, German, French, Italian, Dutch, Polish)"
"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
Dutch forward
Dutch backward
Swedish forward
Swedish backward
French forward
French backward
Czech forward
Czech backward
Portuguese forward
Portuguese backward
initialize cache if use_cache set
embed a dummy sentence to determine embedding_length
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
"if cache is used, try setting embeddings from cache first"
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
pad strings with whitespaces to longest sentence
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
"if only one sentence is passed, convert to list of sentence"
"if only one sentence is passed, convert to list of sentence"
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
iterate over sentences
"if its a forward LM, take last state"
from allennlp.common.tqdm import Tqdm
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
State initialization
Exponential moving average of gradient values
Exponential moving average of squared gradient values
Maintains max of all exp. moving avg. of sq. grad. values
Decay the first and second moment running average coefficient
Maintains the maximum of all 2nd moment running avg. till now
Use the max. for normalizing running avg. of gradient
conll 2000 column format
conll 03 NER column format
WNUT-17
-- WikiNER datasets
-- Universal Dependencies
Germanic
Romance
West-Slavic
South-Slavic
East-Slavic
Scandinavian
Asian
other datasets
text classification format
"first, try to fetch dataset online"
default dataset folder is the cache root
get string value if enum is passed
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
the CoNLL 03 task for German has an additional lemma column
the CoNLL 03 task for Dutch has no NP column
the CoNLL 03 task for Spanish only has two columns
the GERMEVAL task only has two columns: text and ner
WSD tasks may be put into this column format
"the UD corpora follow the CoNLL-U format, for which we have a special reader"
"for text classifiers, we use our own special format"
automatically identify train / test / dev files
"if no test file is found, take any file with 'test' in name"
get train and test data
"read in test file if exists, otherwise sample 10% of train data as test dataset"
"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
convert tag scheme to iobes
automatically identify train / test / dev files
automatically identify train / test / dev files
"most data sets have the token text in the first column, if not, pass 'text' as column"
conll 2000 chunking task
Wikiner NER task
unpack and write out in CoNLL column-like format
CoNLL 02/03 NER
universal dependencies
--- UD Germanic
--- UD Romance
--- UD West-Slavic
--- UD Scandinavian
--- UD South-Slavic
--- UD Asian
init dictionaries
"in order to deal with unknown tokens, add <unk>"
"if text is passed, instantiate sentence with tokens (words)"
tokenize the text first if option selected
use segtok for tokenization
determine offsets for whitespace_after field
otherwise assumes whitespace tokenized text
add each word in tokenized string as Token object to Sentence
increment for last token in sentence if not followed by whtespace
set token idx if not set
non-set tags are OUT tags
anything that is not a BIOES tag is a SINGLE tag
anything that is not OUT is IN
single and begin tags start a new span
remember previous tag
infer whitespace after field
Make the tag dictionary
Make the tag dictionary
header for 'weights.txt'
"determine the column index of loss, f-score and accuracy for train, dev and test split"
then get all relevant values from the tsv
then get all relevant values from the tsv
plot i
save plots
plot 1
plot 2
plot 3
save plots
save plot
take the average over the last three scores of training
take average over the scores from the different training runs
auto-spawn on GPU if available
set the dictionaries
initialize the network architecture
dropouts
bidirectional LSTM on top of embedding layer
final linear map to tag space
ATTENTION: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive
serialization of torch objects
https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings
remove previous embeddings
make mini-batches
"first, sort sentences by number of tokens"
initialize zero-padded word embeddings tensor
fill values with word embeddings
get the tags in this sentence
add tags as tensor
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
word dropout only before LSTM - TODO: more experimentation needed
if self.use_word_dropout > 0.0:
sentence_tensor = self.word_dropout(sentence_tensor)
pad tags if using batch-CRF decoder
auto-spawn on GPU if available
ATTENTION: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive
serialization of torch objects
https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings
cast string to Path
annealing scheduler
"if training also uses dev data, include in training set"
At any point you can hit Ctrl + C to break out of training early.
reload last best model if annealing with restarts is enabled
stop training if learning rate becomes too small
calculate scores using dev data if available
append dev score to score history
"anneal against train loss if training with dev, otherwise anneal against dev score"
"if checkpoint is enable, save model at each epoch"
"if we use dev data, remember best model based on dev evaluation score"
"if we do not use dev data for model selection, save final model"
test best model on test data
"if we are training over multiple datasets, do evaluation for each"
get and return the final test score of best model
append both to file for evaluation
make list of gold tags
make list of predicted tags
"check for true positives, false positives and false negatives"
cast string to Path
""
Add chars to the dictionary
charsplit file content
charsplit file content
Add words to the dictionary
Tokenize file content
cast string to Path
"an epoch has a number, so calculate total max splits bby multiplying max_epochs with number_of_splits"
"after pass over all splits, increment epoch count"
go into train mode
reset variables
not really sure what this does
do batches
"Starting each batch, we detach the hidden state from how it was previously produced."
"If we didn't, the model would try backpropagating all the way to start of the dataset."
do the forward pass in the model
try to predict the targets
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
##############################################################################
TEST
##############################################################################
Save the model if the validation loss is the best we've seen so far.
##############################################################################
print info
##############################################################################
##############################################################################
final testing
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
clean up file
bioes tags
bio tags
broken tags
all tags
all weird tags
tags with confidence
bioes tags
bioes tags
clean up directory
clean up directory
clean up directory
clean up directory
clean up directory
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
get two corpora as one
"get training, test and dev data for full English UD corpus from web"
clean up data directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
make a temporary cache directory that we remove afterwards
initialize trainer
remove the cache directory
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
initialize trainer
clean up results directory
clean up results directory
clean up results directory
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
initialize trainer
clean up results directory
clean up results directory
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
clean up results directory
get default dictionary
get the example corpus and process at character level in forward direction
define search space
sequence tagger parameter
model trainer parameter
training parameter
find best parameter settings
clean up results directory
document embeddings parameter
training parameter
clean up results directory
1. get the corpus
2. what tag do we want to predict?
3. make the tag dictionary from the corpus
initialize embeddings
comment in this line to use character embeddings
"CharacterEmbeddings(),"
comment in these lines to use contextual string embeddings
""
"CharLMEmbeddings('news-forward'),"
""
"CharLMEmbeddings('news-backward'),"
initialize sequence tagger
initialize trainer
"if only one sentence is passed, convert to list of sentence"
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
GLOVE embeddings
KOMNIOS embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
other language fasttext embeddings
add label if in training mode
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
Slovenian forward
Slovenian backward
Bulgarian forward
Bulgarian backward
caching variables
set to eval mode
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
this whole block is for compatibility with older serialized models  TODO: remove in version 0.4
"if cache is used, try setting embeddings from cache first"
lazy initialization of cache
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
pad strings with whitespaces to longest sentence
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
"if only one sentence is passed, convert to list of sentence"
"if only one sentence is passed, convert to list of sentence"
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
iterate over sentences
"if its a forward LM, take last state"
from allennlp.common.tqdm import Tqdm
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
conll column format
conll-u format
text classification format
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
the CoNLL 03 task for German has an additional lemma column
the GERMEVAL task only has two columns: text and ner
WSD tasks may be put into this column format
"the UD corpora follow the CoNLL-U format, for which we have a special reader"
"get train, test and dev data"
"get train, test and dev data"
"get train, test and dev data"
"get train, test and dev data"
"for text classifiers, we use our own special format"
"for text classifiers, we use our own special format"
"TODO: move all paths to use pathlib.Path, for now convert to str for compatibility"
get train and test data
sample dev data from train
convert tag scheme to iobes
"most data sets have the token text in the first column, if not, pass 'text' as column"
init dictionaries
"in order to deal with unknown tokens, add <unk>"
"if text is passed, instantiate sentence with tokens (words)"
tokenize the text first if option selected
use segtok for tokenization
determine offsets for whitespace_after field
otherwise assumes whitespace tokenized text
add each word in tokenized string as Token object to Sentence
set token idx if not set
non-set tags are OUT tags
anything that is not a BIOES tag is a SINGLE tag
anything that is not OUT is IN
single and begin tags start a new span
remember previous tag
infer whitespace after field
Make the tag dictionary
print(tag)
header for 'loss.tsv'
header for 'weights.txt'
plot i
save plots
plot 1
plot 2
plot 3
save plots
auto-spawn on GPU if available
initial hidden state
set the dictionaries
initialize the network architecture
dropouts
bidirectional LSTM on top of embedding layer
final linear map to tag space
suppress torch warnings:
https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings
"first, sort sentences by number of tokens"
initialize zero-padded word embeddings tensor
fill values with word embeddings
get the tags in this sentence
add tags as tensor
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
word dropout only before LSTM - TODO: more experimentation needed
if self.use_word_dropout > 0.0:
sentence_tensor = self.word_dropout(sentence_tensor)
pad tags if using batch-CRF decoder
remove previous embeddings
make mini-batches
get the predicted tag
auto-spawn on GPU if available
ATTENTION: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive
serialization of torch objects
https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings
annealing scheduler
"if training also uses dev data, include in training set"
At any point you can hit Ctrl + C to break out of training early.
reload last best model if annealing with restarts is enabled
stop training if learning rate becomes too small
"Step 4. Compute the loss, gradients, and update the parameters by calling optimizer.step()"
switch to eval mode
"if checkpointing is enable, save model at each epoch"
"anneal against train loss if training with dev, otherwise anneal against dev score"
logging info
"if we use dev data, remember best model based on dev evaluation score"
"if we do not use dev data for model selection, save final model"
get the predicted tag
add predicted tags
append both to file for evaluation
make list of gold tags
make list of predicted tags
"check for true positives, false positives and false negatives"
""
Add chars to the dictionary
charsplit file content
charsplit file content
Add words to the dictionary
Tokenize file content
"an epoch has a number, so calculate total max splits bby multiplying max_epochs with number_of_splits"
"after pass over all splits, increment epoch count"
go into train mode
reset variables
not really sure what this does
do batches
"Starting each batch, we detach the hidden state from how it was previously produced."
"If we didn't, the model would try backpropagating all the way to start of the dataset."
do the forward pass in the model
try to predict the targets
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
##############################################################################
TEST
##############################################################################
Save the model if the validation loss is the best we've seen so far.
##############################################################################
print info
##############################################################################
##############################################################################
final testing
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
"if training also uses dev data, include in training set"
At any point you can hit Ctrl + C to break out of training early.
reload last best model if annealing with restarts is enabled
stop training if learning rate becomes too small
"if checkpoint is enable, save model at each epoch"
"anneal against train loss if training with dev, otherwise anneal against dev score"
"if we use dev data, remember best model based on dev evaluation score"
clean up file
bioes tags
bio tags
broken tags
all tags
all weird tags
tags with confidence
bioes tags
bioes tags
clean up directory
clean up directory
clean up directory
clean up directory
clean up directory
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
initialize trainer
clean up results directory
initialize trainer
clean up results directory
make a temporary cache directory that we remove afterwards
initialize trainer
remove the cache directory
clean up results directory
initialize trainer
clean up results directory
clean up results directory
clean up results directory
clean up results directory
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
1. get the corpus
2. what tag do we want to predict?
3. make the tag dictionary from the corpus
initialize embeddings
comment in this line to use character embeddings
"CharacterEmbeddings(),"
comment in these lines to use contextual string embeddings
""
"CharLMEmbeddings('news-forward'),"
""
"CharLMEmbeddings('news-backward'),"
initialize sequence tagger
initialize trainer
"if only one sentence is passed, convert to list of sentence"
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
GLOVE embeddings
twitter embeddings
KOMNIOS embeddings
NUMBERBATCH embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
GERMAN FASTTEXT embeddings
NUMBERBATCH embeddings
SWEDISCH FASTTEXT embeddings
add label if in training mode
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
caching variables
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
this whole block is for compatibility with older serialized models  TODO: remove in version 0.4
"if cache is used, try setting embeddings from cache first"
lazy initialization of cache
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
pad strings with whitespaces to longest sentence
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
"if only one sentence is passed, convert to list of sentence"
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
iterate over sentences
"if its a forward LM, take last state"
from allennlp.common.tqdm import Tqdm
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
conll column format
conll-u format
text classification format
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
the CoNLL 03 task for German has an additional lemma column
the GERMEVAL task only has two columns: text and ner
WSD tasks may be put into this column format
"the UD corpora follow the CoNLL-U format, for which we have a special reader"
"get train, test and dev data"
"get train, test and dev data"
"get train, test and dev data"
"get train, test and dev data"
"for text classifiers, we use our own special format"
"for text classifiers, we use our own special format"
get train and test data
sample dev data from train
convert tag scheme to iobes
"most data sets have the token text in the first column, if not, pass 'text' as column"
init dictionaries
"in order to deal with unknown tokens, add <unk>"
"if text is passed, instantiate sentence with tokens (words)"
tokenize the text first if option selected
use segtok for tokenization
determine offsets for whitespace_after field
otherwise assumes whitespace tokenized text
add each word in tokenized string as Token object to Sentence
set token idx if not set
non-set tags are OUT tags
anything that is not a BIOES tag is a SINGLE tag
anything that is not OUT is IN
single and begin tags start a new span
remember previous tag
infer whitespace after field
Make the tag dictionary
print(tag)
header for 'loss.tsv'
header for 'weights.txt'
plot i
save plots
plot 1
plot 2
plot 3
save plots
auto-spawn on GPU if available
set the dictionaries
initialize the network architecture
dropouts
bidirectional LSTM on top of embedding layer
final linear map to tag space
"first, sort sentences by number of tokens"
initialize zero-padded word embeddings tensor
fill values with word embeddings
get the tags in this sentence
add tags as tensor
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
pad tags if using batch-CRF decoder
remove previous embeddings
make mini-batches
get the predicted tag
auto-spawn on GPU if available
ATTENTION: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive
serialization of torch objects
annealing scheduler
"if training also uses dev data, include in training set"
At any point you can hit Ctrl + C to break out of training early.
reload last best model if annealing with restarts is enabled
stop training if learning rate becomes too small
"Step 4. Compute the loss, gradients, and update the parameters by calling optimizer.step()"
switch to eval mode
"if checkpointing is enable, save model at each epoch"
"anneal against train loss if training with dev, otherwise anneal against dev score"
logging info
"if we use dev data, remember best model based on dev evaluation score"
"if we do not use dev data for model selection, save final model"
get the predicted tag
add predicted tags
append both to file for evaluation
make list of gold tags
make list of predicted tags
"check for true positives, false positives and false negatives"
""
Add chars to the dictionary
charsplit file content
charsplit file content
Add words to the dictionary
Tokenize file content
"an epoch has a number, so calculate total max splits bby multiplying max_epochs with number_of_splits"
"after pass over all splits, increment epoch count"
go into train mode
reset variables
not really sure what this does
do batches
"Starting each batch, we detach the hidden state from how it was previously produced."
"If we didn't, the model would try backpropagating all the way to start of the dataset."
do the forward pass in the model
try to predict the targets
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
##############################################################################
TEST
##############################################################################
Save the model if the validation loss is the best we've seen so far.
##############################################################################
print info
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
"if training also uses dev data, include in training set"
At any point you can hit Ctrl + C to break out of training early.
reload last best model if annealing with restarts is enabled
stop training if learning rate becomes too small
"if checkpoint is enable, save model at each epoch"
"anneal against train loss if training with dev, otherwise anneal against dev score"
"if we use dev data, remember best model based on dev evaluation score"
clean up file
with pytest.raises(ValueError):
label.name = ''
bioes tags
bio tags
broken tags
all tags
all weird tags
tags with confidence
bioes tags
bioes tags
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
clean up directory
clean up directory
clean up directory
clean up directory
clean up directory
clean up results directory
clean up results directory
test tagging
test re-tagging
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
initialize trainer
clean up results directory
initialize trainer
clean up results directory
make a temporary cache directory that we remove afterwards
initialize trainer
remove the cache directory
clean up results directory
initialize trainer
clean up results directory
clean up results directory
clean up results directory
clean up results directory
initialize trainer
clean up results directory
1. get the corpus
2. what tag do we want to predict?
3. make the tag dictionary from the corpus
initialize embeddings
comment in this line to use character embeddings
"CharacterEmbeddings(),"
comment in these lines to use contextual string embeddings
""
"CharLMEmbeddings('news-forward'),"
""
"CharLMEmbeddings('news-backward'),"
initialize sequence tagger
initialize trainer
"if only one sentence is passed, convert to list of sentence"
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
GLOVE embeddings
twitter embeddings
KOMNIOS embeddings
NUMBERBATCH embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
GERMAN FASTTEXT embeddings
NUMBERBATCH embeddings
SWEDISCH FASTTEXT embeddings
add label if in training mode
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-german-forward
mix-german-backward
common crawl Polish forward
common crawl Polish backward
caching variables
Copy the object's state from self.__dict__ which contains
all our instance attributes. Always use the dict.copy()
method to avoid modifying the original state.
Remove the unpicklable entries.
"by default, use_cache is false (for older pre-trained models TODO: remove in version 0.4)"
"if cache is used, try setting embeddings from cache first"
lazy initialization of cache
try populating embeddings from cache
"if this is not possible, use LM to generate embedding. First, get text sentences"
pad strings with whitespaces to longest sentence
get hidden states from language model
take first or last hidden states from language model as word representation
if self.tokenized_lm or token.whitespace_after:
"if only one sentence is passed, convert to list of sentence"
bidirectional LSTM on top of embedding layer
dropouts
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
iterate over sentences
"if its a forward LM, take last state"
from allennlp.common.tqdm import Tqdm
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
conll column format
conll-u format
text classification format
"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
the CoNLL 03 task for German has an additional lemma column
the GERMEVAL task only has two columns: text and ner
WSD tasks may be put into this column format
"the UD corpora follow the CoNLL-U format, for which we have a special reader"
"get train, test and dev data"
"get train, test and dev data"
"get train, test and dev data"
"get train, test and dev data"
"for text classifiers, we use our own special format"
"for text classifiers, we use our own special format"
get train and test data
sample dev data from train
convert tag scheme to iobes
"most data sets have the token text in the first column, if not, pass 'text' as column"
init dictionaries
"in order to deal with unknown tokens, add <unk>"
"if text is passed, instantiate sentence with tokens (words)"
tokenize the text first if option selected
use segtok for tokenization
determine offsets for whitespace_after field
otherwise assumes whitespace tokenized text
add each word in tokenized string as Token object to Sentence
set token idx if not set
non-set tags are OUT tags
anything that is not a BIOES tag is a SINGLE tag
anything that is not OUT is IN
single and begin tags start a new span
remember previous tag
infer whitespace after field
Make the tag dictionary
print(tag)
header for 'loss.tsv'
header for 'weights.txt'
plot i
save plots
plot 1
plot 2
plot 3
save plots
auto-spawn on GPU if available
set the dictionaries
initialize the network architecture
dropouts
bidirectional LSTM on top of embedding layer
final linear map to tag space
"first, sort sentences by number of tokens"
initialize zero-padded word embeddings tensor
fill values with word embeddings
get the tags in this sentence
add tags as tensor
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
use word dropout if set
pad tags if using batch-CRF decoder
remove previous embeddings
make mini-batches
get the predicted tag
auto-spawn on GPU if available
ATTENTION: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive
serialization of torch objects
annealing scheduler
"if training also uses dev data, include in training set"
At any point you can hit Ctrl + C to break out of training early.
reload last best model if annealing with restarts is enabled
stop training if learning rate becomes too small
"Step 4. Compute the loss, gradients, and update the parameters by calling optimizer.step()"
switch to eval mode
"if checkpointing is enable, save model at each epoch"
"anneal against train loss if training with dev, otherwise anneal against dev score"
logging info
"if we use dev data, remember best model based on dev evaluation score"
"if we do not use dev data for model selection, save final model"
get the predicted tag
add predicted tags
append both to file for evaluation
make list of gold tags
make list of predicted tags
"check for true positives, false positives and false negatives"
""
Add chars to the dictionary
charsplit file content
charsplit file content
Add words to the dictionary
Tokenize file content
"an epoch has a number, so calculate total max splits bby multiplying max_epochs with number_of_splits"
"after pass over all splits, increment epoch count"
go into train mode
reset variables
not really sure what this does
do batches
"Starting each batch, we detach the hidden state from how it was previously produced."
"If we didn't, the model would try backpropagating all the way to start of the dataset."
do the forward pass in the model
try to predict the targets
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
##############################################################################
TEST
##############################################################################
Save the model if the validation loss is the best we've seen so far.
##############################################################################
print info
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
"if training also uses dev data, include in training set"
At any point you can hit Ctrl + C to break out of training early.
record overall best dev scores and best loss
"anneal against train loss if training with dev, otherwise anneal against dev score"
clean up file
with pytest.raises(ValueError):
label.name = ''
bioes tags
bio tags
broken tags
all tags
all weird tags
tags with confidence
bioes tags
bioes tags
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
clean up directory
clean up directory
clean up directory
clean up directory
clean up directory
clean up results directory
clean up results directory
test tagging
test re-tagging
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
initialize trainer
clean up results directory
1. get the corpus
2. what tag do we want to predict?
3. make the tag dictionary from the corpus
initialize embeddings
comment in this line to use character embeddings
"CharacterEmbeddings(),"
comment in these lines to use contextual string embeddings
""
"CharLMEmbeddings('news-forward'),"
""
"CharLMEmbeddings('news-backward'),"
initialize sequence tagger
initialize trainer
"if only one sentence is passed, convert to list of sentence"
IMPORTANT: add embeddings as torch modules
"if only one sentence is passed, convert to list of sentence"
GLOVE embeddings
twitter embeddings
KOMNIOS embeddings
NUMBERBATCH embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
GERMAN FASTTEXT embeddings
NUMBERBATCH embeddings
SWEDISCH FASTTEXT embeddings
use list of common characters if none provided
translate words in sentence into ints using dictionary
"sort words by length, for batching and masking"
chars for rnn processing
news-english-forward
news-english-backward
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-english-forward
mix-english-backward
find longest sentence by characters
get states from LM
"if only one sentence is passed, convert to list of sentence"
bidirectional LSTM on top of embedding layer
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
--------------------------------------------------------------------
EXTRACT EMBEDDINGS FROM LSTM
--------------------------------------------------------------------
iterate over sentences
"if its a forward LM, take last state"
from allennlp.common.tqdm import Tqdm
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
print(line)
print(line)
init dictionaries
"in order to deal with unknown tokens, add <unk>"
"optionally, directly instantiate with sentence tokens"
"tokenize the text first if option selected, otherwise assumes whitespace tokenized text"
add each word in tokenized string as Token object to Sentence
set token idx if not set
"def to_tag_string(self, tag_type: str = 'tag') -> str:"
""
list = []
for token in self.tokens:
list.append(token.text)
if token.get_tag(tag_type) == '' or token.get_tag(tag_type) == 'O': continue
list.append('<' + token.get_tag(tag_type) + '>')
return ' '.join(list)
""
def to_ner_string(self) -> str:
list = []
for token in self.tokens:
if token.get_tag('ner') == 'O' or token.get_tag('ner') == '':
list.append(token.text)
else:
list.append(token.text)
list.append('<' + token.get_tag('ner') + '>')
return ' '.join(list)
Make the tag dictionary
auto-spawn on GPU if available
vec 2D: 1 * tagset_size
set the dictionaries
initialize the network architecture
self.dropout = nn.Dropout(0.5)
bidirectional LSTM on top of embedding layer
final linear map to tag space
"trans is also a score tensor, not a probability: THIS THING NEEDS TO GO!!!!"
ACHTUNG: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive
serialization of torch objects
"first, sort sentences by number of tokens"
get the tags in this sentence
get the tag
get the word embeddings
pad shorter sentences out
padded tensor for entire batch
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
sentence_tensor = self.nonlinearity(sentence_tensor)
"tags is ground_truth, a list of ints, length is len(sentence)"
"feats is a 2D tensor, len(sentence) * tagset_size"
analogous to forward
"calculate the score of the ground_truth, in CRF"
calculate in log domain
feats is len(sentence) * tagset_size
initialize alpha with a Tensor with values all equal to -10000.
Z(x)
viterbi to get tag_seq
get the predicted tag
remove previous embeddings
make mini-batches
get the predicted tag
viterbi to get tag_seq
overall_score += score
auto-spawn on GPU if available
ATTENTION: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive
serialization of torch objects
"if training also uses dev data, include in training set"
At any point you can hit Ctrl + C to break out of training early.
"Step 4. Compute the loss, gradients, and update the parameters by calling optimizer.step()"
switch to eval mode
switch back to train mode
"anneal against train loss if training with dev, otherwise anneal against dev score"
save if model is current best and we use dev data for model selection
"if we do not use dev data for model selection, save final model"
Step 3. Run our forward pass.
Step 5. Compute predictions
get the predicted tag
get the gold tag
append both to file for evaluation
positives
true positives
false positive
negatives
true negative
false negative
get the eval script
parse the result file
""
print(chars)
Add chars to the dictionary
charsplit file content
charsplit file content
Add words to the dictionary
Tokenize file content
go into train mode
reset variables
not really sure what this does
do batches
"Starting each batch, we detach the hidden state from how it was previously produced."
"If we didn't, the model would try backpropagating all the way to start of the dataset."
do the forward pass in the model
try to predict the targets
`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
##############################################################################
TEST
##############################################################################
Save the model if the validation loss is the best we've seen so far.
##############################################################################
print info
##############################################################################
Turn on evaluation mode which disables dropout.
Work out how cleanly we can divide the dataset into bsz parts.
Trim off any extra elements that wouldn't cleanly fit (remainders).
Evenly divide the data across the bsz batches.
"if training also uses dev data, include in training set"
At any point you can hit Ctrl + C to break out of training early.
record overall best dev scores and best loss
IMPORTANT: Switch to eval mode
IMPORTANT: Switch back to train mode
"anneal against train loss if training with dev, otherwise anneal against dev score"
clean up file
get default dictionary
init forward LM with 128 hidden states and 1 layer
get the example corpus and process at character level in forward direction
train the language model
use the character LM as embeddings to embed the example sentence 'I love Berlin'
clean up results directory
clean up results directory
clean up results directory
test tagging
test re-tagging
"get training, test and dev data"
"get training, test and dev data"
"get training, test and dev data"
initialize trainer
clean up results directory
1. get the corpus
2. what tag do we want to predict?
3. make the tag dictionary from the corpus
initialize embeddings
comment in this line to use character embeddings
comment in these lines to use contextual string embeddings
initialize sequence tagger
initialize trainer
"if only one sentence is passed, convert to list of sentence"
IMPORTANT: add embeddings as torch modules
GLOVE embeddings
KOMNIOS embeddings
NUMBERBATCH embeddings
FT-CRAWL embeddings
FT-CRAWL embeddings
GERMAN FASTTEXT embeddings
NUMBERBATCH embeddings
SWEDISCH FASTTEXT embeddings
get list of common characters if none provided
load dictionary
print(self.char_dictionary.item2idx)
translate words in sentence into ints using dictionary
print(token)
"sort words by length, for batching and masking"
chars for rnn processing
news-english-forward
news-english-backward
mix-english-forward
mix-english-backward
mix-english-forward
mix-english-backward
find longest sentence by characters
print(sentences_padded)
get states from LM
if not torch.cuda.is_available():
embedding = embedding.cpu()
lines: List[str] = []
lines.append(vec)
"if only one sentence is passed, convert to list of sentence"
mean_embedding /= len(paragraph.tokens)
self.embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=word_embeddings)
self.__embedding_length: int = hidden_states
bidirectional LSTM on top of embedding layer
"first, sort sentences by number of tokens"
go through each sentence in batch
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
iterate over sentences
"if its a forward LM, take last state"
from allennlp.common.tqdm import Tqdm
Remove quotes from etag
"If there is an etag, it's everything after the first period"
"Otherwise, use None"
"URL, so get it from the cache (downloading if necessary)"
"File, and it exists."
"File, but it doesn't exist."
Something unknown
TODO(joelgrus): do we want to do checksums or anything like that?
get cache path to put the file
make HEAD request to check ETag
add ETag to filename if it exists
"etag = response.headers.get(""ETag"")"
"Download to temporary file, then copy to cache dir once finished."
Otherwise you get corrupt cache entries if the download gets interrupted.
GET file object
These defaults are the same as the argument defaults in tqdm.
init dictionaries
"in order to deal with unknown tokens, add <unk>"
"optionally, directly instantiate with sentence tokens"
"tokenize the text first if option selected, otherwise assumes whitespace tokenized text"
add each word in tokenized string as Token object to Sentence
set token idx if not set
"def to_tag_string(self, tag_type: str = 'tag') -> str:"
""
list = []
for token in self.tokens:
list.append(token.text)
if token.get_tag(tag_type) == '' or token.get_tag(tag_type) == 'O': continue
list.append('<' + token.get_tag(tag_type) + '>')
return ' '.join(list)
""
def to_ner_string(self) -> str:
list = []
for token in self.tokens:
if token.get_tag('ner') == 'O' or token.get_tag('ner') == '':
list.append(token.text)
else:
list.append(token.text)
list.append('<' + token.get_tag('ner') + '>')
return ' '.join(list)
Make the tag dictionary
print(line)
print(line)
""
print(chars)
Add chars to the dictionary
charsplit file content
charsplit file content
Add words to the dictionary
Tokenize file content
"if training also uses dev data, include in training set"
At any point you can hit Ctrl + C to break out of training early.
record overall best dev scores and best loss
best_dev_score = 0
best_loss: float = 10000
this variable is used for annealing schemes
"Step 4. Compute the loss, gradients, and update the parameters by calling optimizer.step()"
IMPORTANT: Switch to eval mode
IMPORTANT: Switch back to train mode
checkpoint model
is this the best model so far?
"if dev data is used for model selection, use dev F1 score to determine best model"
"if dev data is used for training, use training loss to determine best model"
save model
anneal after 3 epochs of no improvement if anneal mode
print info
Step 3. Run our forward pass.
Step 5. Compute predictions
print(token)
get the predicted tag
get the gold tag
append both to file for evaluation
parse the result file
vec 2D: 1 * tagset_size
set the dictionaries
initialize the network architecture
self.dropout = nn.Dropout(0.5)
bidirectional LSTM on top of embedding layer
final linear map to tag space
"trans is also a score tensor, not a probability: THIS THING NEEDS TO GO!!!!"
ACHTUNG: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive
serialization of torch objects
"first, sort sentences by number of tokens"
print(sent)
print(sent.tokens[0].get_embedding()[0:7])
go through each sentence in batch
get the tags in this sentence
get the tag
PADDING: pad shorter sentences out
ADD TO SENTENCE LIST: add the representation
--------------------------------------------------------------------
GET REPRESENTATION FOR ENTIRE BATCH
--------------------------------------------------------------------
--------------------------------------------------------------------
FF PART
--------------------------------------------------------------------
print(tags)
"tags is ground_truth, a list of ints, length is len(sentence)"
"feats is a 2D tensor, len(sentence) * tagset_size"
analogous to forward
"sentence, tags is a list of ints"
"features is a 2D tensor, len(sentence) * self.tagset_size"
for sentence in sentences:
print(sentence)
"calculate the score of the ground_truth, in CRF"
calculate in log domain
feats is len(sentence) * tagset_size
initialize alpha with a Tensor with values all equal to -10000.
Z(x)
viterbi to get tag_seq
get the predicted tag
